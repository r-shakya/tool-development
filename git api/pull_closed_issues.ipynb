{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]> 1\n",
      "<Response [200]> 2\n",
      "<Response [200]> 3\n",
      "<Response [200]> 4\n",
      "<Response [200]> 5\n",
      "<Response [200]> 6\n",
      "<Response [200]> 7\n",
      "<Response [200]> 8\n",
      "<Response [200]> 9\n",
      "<Response [200]> 10\n",
      "<Response [200]> 11\n",
      "<Response [200]> 12\n",
      "<Response [200]> 13\n",
      "<Response [200]> 14\n",
      "<Response [200]> 15\n",
      "<Response [200]> 16\n",
      "<Response [200]> 17\n",
      "<Response [200]> 18\n",
      "<Response [200]> 19\n",
      "<Response [200]> 20\n",
      "<Response [200]> 21\n",
      "<Response [200]> 22\n",
      "<Response [200]> 23\n",
      "<Response [200]> 24\n",
      "<Response [200]> 25\n",
      "<Response [200]> 26\n",
      "<Response [200]> 27\n",
      "<Response [200]> 28\n",
      "<Response [200]> 29\n",
      "<Response [200]> 30\n",
      "<Response [200]> 31\n",
      "<Response [200]> 32\n",
      "<Response [200]> 33\n",
      "<Response [200]> 34\n",
      "<Response [200]> 35\n",
      "<Response [200]> 36\n",
      "<Response [200]> 37\n",
      "<Response [200]> 38\n",
      "<Response [200]> 39\n",
      "<Response [200]> 40\n",
      "<Response [200]> 41\n",
      "<Response [200]> 42\n",
      "<Response [200]> 43\n",
      "<Response [200]> 44\n",
      "<Response [200]> 45\n",
      "<Response [200]> 46\n",
      "<Response [200]> 47\n",
      "<Response [200]> 48\n",
      "<Response [200]> 49\n",
      "<Response [200]> 50\n"
     ]
    }
   ],
   "source": [
    "issues_list=[]\n",
    "username=\"scikit-learn\"\n",
    "repo=\"scikit-learn\"\n",
    "url=f\"https://api.github.com/repos/{username}/{repo}/issues\"\n",
    "pageno=1\n",
    "for res in range(50):\n",
    "    params = {\n",
    "        \"state\": \"closed\",\n",
    "        \"per_page\": \"100\",\n",
    "        \"page\":pageno,\n",
    "    }\n",
    "    headers = {\n",
    "    }\n",
    "    response = requests.get(url,headers=headers, params=params)\n",
    "    issues_list=issues_list+response.json()\n",
    "    print(response,pageno)\n",
    "    pageno=pageno+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]> 51\n",
      "<Response [200]> 52\n",
      "<Response [200]> 53\n",
      "<Response [200]> 54\n",
      "<Response [200]> 55\n",
      "<Response [200]> 56\n",
      "<Response [200]> 57\n",
      "<Response [200]> 58\n",
      "<Response [200]> 59\n",
      "<Response [200]> 60\n"
     ]
    }
   ],
   "source": [
    "for res in range(10):\n",
    "    params = {\n",
    "        \"state\": \"closed\",\n",
    "        \"per_page\": \"100\",\n",
    "        \"page\":pageno,\n",
    "    }\n",
    "    headers = {\n",
    "    }\n",
    "    response = requests.get(url,headers=headers, params=params)\n",
    "    issues_list=issues_list+response.json()\n",
    "    print(response,pageno)\n",
    "    pageno=pageno+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n",
      "6000\n"
     ]
    }
   ],
   "source": [
    "print(len(issues_list))\n",
    "\n",
    "import json\n",
    "\n",
    "with open('scikit-learn_closed_6000.json', 'w') as json_file:\n",
    "    json.dump(issues_list, json_file)\n",
    "\n",
    "f = open('scikit-learn_closed_6000.json','r')\n",
    "\n",
    "data = json.load(f) \n",
    "\n",
    "print(len(data))\n",
    "\n",
    "f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]> 51\n",
      "<Response [200]> 52\n",
      "<Response [200]> 53\n",
      "<Response [200]> 54\n",
      "<Response [200]> 55\n",
      "<Response [200]> 56\n",
      "<Response [200]> 57\n",
      "<Response [200]> 58\n",
      "<Response [200]> 59\n",
      "<Response [200]> 60\n"
     ]
    }
   ],
   "source": [
    "for res in range(10):\n",
    "    params = {\n",
    "        \"state\": \"closed\",\n",
    "        \"per_page\": \"100\",\n",
    "        \"page\":pageno,\n",
    "    }\n",
    "    headers = {\n",
    "    }\n",
    "    response = requests.get(url,headers=headers, params=params)\n",
    "    issues_list=issues_list+response.json()\n",
    "    print(response,pageno)\n",
    "    pageno=pageno+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n",
      "6000\n"
     ]
    }
   ],
   "source": [
    "print(len(issues_list))\n",
    "\n",
    "with open('tensorflow_closed_6000.json', 'w') as json_file:\n",
    "    json.dump(issues_list, json_file)\n",
    "\n",
    "f = open('tensorflow_closed_6000.json','r')\n",
    "\n",
    "data = json.load(f) \n",
    "\n",
    "print(len(data))\n",
    "\n",
    "f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'url': 'https://api.github.com/repos/tensorflow/tensorflow/issues/47498', 'repository_url': 'https://api.github.com/repos/tensorflow/tensorflow', 'labels_url': 'https://api.github.com/repos/tensorflow/tensorflow/issues/47498/labels{/name}', 'comments_url': 'https://api.github.com/repos/tensorflow/tensorflow/issues/47498/comments', 'events_url': 'https://api.github.com/repos/tensorflow/tensorflow/issues/47498/events', 'html_url': 'https://github.com/tensorflow/tensorflow/issues/47498', 'id': 819743638, 'node_id': 'MDU6SXNzdWU4MTk3NDM2Mzg=', 'number': 47498, 'title': '-1073741795', 'user': {'login': 'Junaidkhan1809', 'id': 72540582, 'node_id': 'MDQ6VXNlcjcyNTQwNTgy', 'avatar_url': 'https://avatars.githubusercontent.com/u/72540582?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/Junaidkhan1809', 'html_url': 'https://github.com/Junaidkhan1809', 'followers_url': 'https://api.github.com/users/Junaidkhan1809/followers', 'following_url': 'https://api.github.com/users/Junaidkhan1809/following{/other_user}', 'gists_url': 'https://api.github.com/users/Junaidkhan1809/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/Junaidkhan1809/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/Junaidkhan1809/subscriptions', 'organizations_url': 'https://api.github.com/users/Junaidkhan1809/orgs', 'repos_url': 'https://api.github.com/users/Junaidkhan1809/repos', 'events_url': 'https://api.github.com/users/Junaidkhan1809/events{/privacy}', 'received_events_url': 'https://api.github.com/users/Junaidkhan1809/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 386191887, 'node_id': 'MDU6TGFiZWwzODYxOTE4ODc=', 'url': 'https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response', 'name': 'stat:awaiting response', 'color': 'f4b400', 'default': False, 'description': 'Status  - Awaiting response from author'}, {'id': 473173351, 'node_id': 'MDU6TGFiZWw0NzMxNzMzNTE=', 'url': 'https://api.github.com/repos/tensorflow/tensorflow/labels/type:build/install', 'name': 'type:build/install', 'color': '159b2e', 'default': False, 'description': 'Build and install issues'}], 'state': 'closed', 'locked': False, 'assignee': {'login': 'Saduf2019', 'id': 59822926, 'node_id': 'MDQ6VXNlcjU5ODIyOTI2', 'avatar_url': 'https://avatars.githubusercontent.com/u/59822926?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/Saduf2019', 'html_url': 'https://github.com/Saduf2019', 'followers_url': 'https://api.github.com/users/Saduf2019/followers', 'following_url': 'https://api.github.com/users/Saduf2019/following{/other_user}', 'gists_url': 'https://api.github.com/users/Saduf2019/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/Saduf2019/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/Saduf2019/subscriptions', 'organizations_url': 'https://api.github.com/users/Saduf2019/orgs', 'repos_url': 'https://api.github.com/users/Saduf2019/repos', 'events_url': 'https://api.github.com/users/Saduf2019/events{/privacy}', 'received_events_url': 'https://api.github.com/users/Saduf2019/received_events', 'type': 'User', 'site_admin': False}, 'assignees': [{'login': 'Saduf2019', 'id': 59822926, 'node_id': 'MDQ6VXNlcjU5ODIyOTI2', 'avatar_url': 'https://avatars.githubusercontent.com/u/59822926?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/Saduf2019', 'html_url': 'https://github.com/Saduf2019', 'followers_url': 'https://api.github.com/users/Saduf2019/followers', 'following_url': 'https://api.github.com/users/Saduf2019/following{/other_user}', 'gists_url': 'https://api.github.com/users/Saduf2019/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/Saduf2019/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/Saduf2019/subscriptions', 'organizations_url': 'https://api.github.com/users/Saduf2019/orgs', 'repos_url': 'https://api.github.com/users/Saduf2019/repos', 'events_url': 'https://api.github.com/users/Saduf2019/events{/privacy}', 'received_events_url': 'https://api.github.com/users/Saduf2019/received_events', 'type': 'User', 'site_admin': False}], 'milestone': None, 'comments': 8, 'created_at': '2021-03-02T07:50:26Z', 'updated_at': '2021-03-02T11:25:38Z', 'closed_at': '2021-03-02T11:25:36Z', 'author_association': 'NONE', 'active_lock_reason': None, 'body': '<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\\r\\n\\r\\n**System information**\\r\\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\\r\\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\\r\\n- TensorFlow installed from (source or binary):\\r\\n- TensorFlow version:\\r\\n- Python version:\\r\\n- Installed using virtualenv? pip? conda?:\\r\\n- Bazel version (if compiling from source):\\r\\n- GCC/Compiler version (if compiling from source):\\r\\n- CUDA/cuDNN version:\\r\\n- GPU model and memory:\\r\\n\\r\\n\\r\\n\\r\\nImportError: Traceback (most recent call last):\\r\\n  File \"C:\\\\Users\\\\Junaid\\\\Anaconda3\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\pywrap_tensorflow.py\", line 58, in <module>\\r\\n    from tensorflow.python.pywrap_tensorflow_internal import *\\r\\n  File \"C:\\\\Users\\\\Junaid\\\\Anaconda3\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\pywrap_tensorflow_internal.py\", line 28, in <module>\\r\\n    _pywrap_tensorflow_internal = swig_import_helper()\\r\\n  File \"C:\\\\Users\\\\Junaid\\\\Anaconda3\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\\r\\n    _mod = imp.load_module(\\'_pywrap_tensorflow_internal\\', fp, pathname, description)\\r\\n  File \"C:\\\\Users\\\\Junaid\\\\Anaconda3\\\\lib\\\\imp.py\", line 242, in load_module\\r\\n    return load_dynamic(name, filename, file)\\r\\n  File \"C:\\\\Users\\\\Junaid\\\\Anaconda3\\\\lib\\\\imp.py\", line 342, in load_dynamic\\r\\n    return _load(spec)\\r\\nImportError: DLL load failed with error code -1073741795\\r\\n\\r\\n\\r\\nFailed to load the native TensorFlow runtime.\\r\\n\\r\\n- first I run the command line pip install tensorflow==2.2.0\\r\\n- then while importing the tensorflow it throws the above error\\r\\n\\r\\n![image](https://user-images.githubusercontent.com/72540582/109615543-079dfe80-7b5a-11eb-82d9-9e33fd4b8909.png)\\r\\n\\r\\n', 'performed_via_github_app': None}\n"
     ]
    }
   ],
   "source": [
    "first_issue=issues_list[0]\n",
    "print(first_issue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1073741795\n"
     ]
    }
   ],
   "source": [
    "print(first_issue[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 386191887, 'node_id': 'MDU6TGFiZWwzODYxOTE4ODc=', 'url': 'https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response', 'name': 'stat:awaiting response', 'color': 'f4b400', 'default': False, 'description': 'Status  - Awaiting response from author'}, {'id': 473173351, 'node_id': 'MDU6TGFiZWw0NzMxNzMzNTE=', 'url': 'https://api.github.com/repos/tensorflow/tensorflow/labels/type:build/install', 'name': 'type:build/install', 'color': '159b2e', 'default': False, 'description': 'Build and install issues'}]\n"
     ]
    }
   ],
   "source": [
    "print(first_issue[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 386191887, 'node_id': 'MDU6TGFiZWwzODYxOTE4ODc=', 'url': 'https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response', 'name': 'stat:awaiting response', 'color': 'f4b400', 'default': False, 'description': 'Status  - Awaiting response from author'}, {'id': 473173351, 'node_id': 'MDU6TGFiZWw0NzMxNzMzNTE=', 'url': 'https://api.github.com/repos/tensorflow/tensorflow/labels/type:build/install', 'name': 'type:build/install', 'color': '159b2e', 'default': False, 'description': 'Build and install issues'}]\n"
     ]
    }
   ],
   "source": [
    "labels = first_issue[\"labels\"]\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stat:awaiting response\n",
      "type:build/install\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(labels)):\n",
    "    label=labels[i]\n",
    "    label_name=label[\"name\"]\n",
    "    print(label_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'issues_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c6e16e706fc1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0missues_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0missue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0missues_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0missue_title\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0missue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"title\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0missue_title\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'issues_list' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(len(issues_list)):\n",
    "    issue=issues_list[i]\n",
    "    issue_title=issue[\"title\"]\n",
    "    print(issue_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "issue title -  -1073741795\n",
      "issue body -  <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "- TensorFlow installed from (source or binary):\r\n",
      "- TensorFlow version:\r\n",
      "- Python version:\r\n",
      "- Installed using virtualenv? pip? conda?:\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version:\r\n",
      "- GPU model and memory:\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "ImportError: Traceback (most recent call last):\r\n",
      "  File \"C:\\Users\\Junaid\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n",
      "    from tensorflow.python.pywrap_tensorflow_internal import *\r\n",
      "  File \"C:\\Users\\Junaid\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n",
      "    _pywrap_tensorflow_internal = swig_import_helper()\r\n",
      "  File \"C:\\Users\\Junaid\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n",
      "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n",
      "  File \"C:\\Users\\Junaid\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\n",
      "    return load_dynamic(name, filename, file)\r\n",
      "  File \"C:\\Users\\Junaid\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n",
      "    return _load(spec)\r\n",
      "ImportError: DLL load failed with error code -1073741795\r\n",
      "\r\n",
      "\r\n",
      "Failed to load the native TensorFlow runtime.\r\n",
      "\r\n",
      "- first I run the command line pip install tensorflow==2.2.0\r\n",
      "- then while importing the tensorflow it throws the above error\r\n",
      "\r\n",
      "![image](https://user-images.githubusercontent.com/72540582/109615543-079dfe80-7b5a-11eb-82d9-9e33fd4b8909.png)\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "stat:awaiting response\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  zero gradient for higher-order derivatives when using tf.function and tf.scan\n",
      "issue body -  **System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n",
      "- TensorFlow version (use command below): v2.4.1-0-g85c8b2a817f\r\n",
      "- Python version: 3.0\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "When using tf.function decorator on functions involving tf.scan, the second-order derivative goes to zero.\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "I would expect the gradient not to be zero when tf.function is used.\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "https://colab.research.google.com/drive/1M5-ua3LXgrc8QpwtcwY5tEsQ1gSkY9TD\n",
      "issue labels - \n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  keras lstm layer produces different results in tf 1 and tf 2 due to different default activation\n",
      "issue body -  <em>Please make sure that this is a bug. As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:bug_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- TensorFlow version:\r\n",
      "1. TF 1.0: v1.15.4-39-g3db52be7be8 1.15.5\r\n",
      "2. TF 2.0: v2.4.0-49-g85c8b2a817f 2.4.1\r\n",
      "\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "The LSTM layer in tf 1.0 uses \"hard_sigmoid\" as the default recurrent activation: https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/python/keras/layers/recurrent.py#L2489\r\n",
      "\r\n",
      "while it uses  \"sigmoid\" in tf 2.0: https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/python/keras/layers/recurrent_v2.py#L1077\r\n",
      "\r\n",
      "As a result, when I try to execute the same code (without specifying recurrent_activation flag explicitly) in tf 1 and 2, I get slightly different results and took quite a while to find the cause.\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "Same code should produce the same result in both versions. Or some prompt should be given for easier debugging.\r\n",
      "\n",
      "issue labels - \n",
      "comp:keras\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Can't save/load a Keras model's optimizer weights when using SavedModel format\n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n",
      "- TensorFlow installed from (source or binary): No\r\n",
      "- TensorFlow version (use command below): 2.4.1\r\n",
      "- Python version: 3.7.10\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "When saving a keras model in TensorFlow SavedModel format using `model.save` optimizer weights are not saved, even when specifying `include_optimizer=True` in the `model.save` call  and `compiled=True` in the `tf.keras.models.load_model` call. This is crucial for continuing training from a checkpoint with adaptive optimizers.\r\n",
      "Looks like the optimizer object gets saved and loaded along with its parameters. Only the weights are missing.\r\n",
      "\r\n",
      "Weights **are** saved as expected when saving in `h5` format.\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "Optimizer weights should be saved when specifying `include_optimizer=True`.\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "Reproduced in [this Colab](https://colab.research.google.com/drive/1bEZif1c6xzHdcGO4SQQMB9q0DMDo8jbI?usp=sharing)\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:keras\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Convert Pack to Reshape when there is only one operand to be packed.\n",
      "issue body -  This can improve optimization as there are more patterns about Reshape. Take `MultiHeadAttention` for example,\r\n",
      "\r\n",
      "```python\r\n",
      "layer = tf.keras.layers.MultiHeadAttention(num_heads=3, key_dim=5)\r\n",
      "target = tf.keras.Input(shape=[8, 16], batch_size=1)\r\n",
      "source = tf.keras.Input(shape=[4, 16], batch_size=1)\r\n",
      "output_tensor = layer(target, source, return_attention_scores=False)\r\n",
      "model = tf.keras.Model([target, source], output_tensor)\r\n",
      "```\r\n",
      "\r\n",
      "Converting Pack to Reshape can help the bias of the last EinsumDense fuse into FullyConnected.\n",
      "issue labels - \n",
      "TFLiteConverter\n",
      "cla: yes\n",
      "comp:lite\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  Tensorflow Lite Custom Object detection Model Error in Android app\n",
      "issue body -  Could you please help to solve this error?\r\n",
      "\r\n",
      "I am testing a custom Object Detection model using TensorFlow Lite in Android App according to the documentation, but I have an error when the library tries to recognize an image.\r\n",
      "\r\n",
      "I am using the Tensorflow lite sample app: `https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android`\r\n",
      "\r\n",
      "**Using Task Library:** `https://www.tensorflow.org/lite/inference_with_metadata/task_library/object_detector`\r\n",
      "\r\n",
      "**dependency version implementation:** `'org.tensorflow:tensorflow-lite-task-vision:0.1.0'`\r\n",
      "\r\n",
      "When this method is executed, this error is obtained::\r\n",
      "\r\n",
      "**method**\r\n",
      "` List<Detection> results = objectDetector.detect(TensorImage.fromBitmap(bitmap));`\r\n",
      "\r\n",
      "error\r\n",
      "```\r\n",
      "Abort message: 'JNI DETECTED ERROR IN APPLICATION: JNI NewStringUTF called with pending exception java.lang.NoSuchMethodError: no static method Lorg/tensorflow/lite/support/label/Category;.create(Ljava/lang/String;Ljava/lang/String;F)Lorg/tensorflow/lite/support/label/Category;\"\r\n",
      "        at java.util.List org.tensorflow.lite.task.vision.detector.ObjectDetector.detectNative(long, java.nio.ByteBuffer, int, int, int) (ObjectDetector.java:-2)\r\n",
      "        at java.util.List org.tensorflow.lite.task.vision.detector.ObjectDetector.detect(org.tensorflow.lite.support.image.TensorImage, org.tensorflow.lite.task.core.vision.ImageProcessingOptions) (ObjectDetector.java:312)\r\n",
      "        at java.util.List org.tensorflow.lite.task.vision.detector.ObjectDetector.detect(org.tensorflow.lite.support.image.TensorImage) (ObjectDetector.java:292)\r\n",
      "        at java.util.List org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.recognizeImage(android.graphics.Bitmap) (TFLiteObjectDetectionAPIModel.java:87)\r\n",
      "        at void org.tensorflow.lite.examples.detection.DetectorActivity$2.run() (DetectorActivity.java:187)\r\n",
      "        at void android.os.Handler.handleCallback(android.os.Message) (Handler.java:938)\r\n",
      "        at void android.os.Handler.dispatchMessage(android.os.Message) (Handler.java:99)\r\n",
      "        at void android.os.Looper.loop() (Looper.java:223)\r\n",
      "        at void android.os.HandlerThread.run() (HandlerThread.java:67)\r\n",
      "    \r\n",
      "        in call to NewStringUTF\r\n",
      "        from java.util.List org.tensorflow.lite.task.vision.detector.ObjectDetector.detectNative(long, java.nio.ByteBuffer, int, int, int)'\r\n",
      "\r\n",
      "```\r\n",
      "\n",
      "issue labels - \n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  how to Training .TextGrid file by tensorflow\n",
      "issue body -  now, i have get some audio label file , that  read .wav file and use Annotate -TextGrid(sentence) button operator by Praat tool;\r\n",
      "open file  :\r\n",
      "\r\n",
      "`File type = \"ooTextFile\"\r\n",
      "Object class = \"TextGrid\"\r\n",
      "\r\n",
      "xmin = 0 \r\n",
      "xmax = 17.275351473922903 \r\n",
      "tiers? <exists> \r\n",
      "size = 1 \r\n",
      "item []: \r\n",
      "    item [1]:\r\n",
      "        class = \"IntervalTier\" \r\n",
      "        name = \"silences\" \r\n",
      "        xmin = 0 \r\n",
      "        xmax = 17.275351473922903 \r\n",
      "        intervals: size = 19 \r\n",
      "        intervals [1]:\r\n",
      "            xmin = 0 \r\n",
      "            xmax = 1.9536757369614515 \r\n",
      "            text = \"silent\" \r\n",
      "        intervals [2]:\r\n",
      "            xmin = 1.9536757369614515 \r\n",
      "            xmax = 3.4176757369614514 \r\n",
      "            text = \"sounding\" \r\n",
      "        intervals [3]:\r\n",
      "            xmin = 3.4176757369614514 \r\n",
      "            xmax = 4.281675736961452 \r\n",
      "            text = \"silent\" \r\n",
      "        intervals [4]:\r\n",
      "            xmin = 4.281675736961452 \r\n",
      "            xmax = 5.081675736961452 \r\n",
      "            text = \"sounding\" `\r\n",
      "\r\n",
      "---------\r\n",
      "but  i do not how to use the file with tensorflow , \r\n",
      "give me a dir,   3Q;\n",
      "issue labels - \n",
      "stat:awaiting response\n",
      "\n",
      "\n",
      "issue title -  Correct range of input values for MobileNet\n",
      "issue body -  Hello,\r\n",
      "\r\n",
      "I want to use the implementation of MobileNetV3 (either the MobileNet_large or MobileNet_small version) in my project. I'm using the TensorFlow v2.4.1.\r\n",
      "\r\n",
      " I get confused when I read the documentation about the range of input values for MobileNet-based models. According to this [link](https://tfhub.dev/google/imagenet/mobilenet_v3_large_100_224/classification/5)  the expected range of values for the model is [0,1]. However, according to this other [link](https://www.tensorflow.org/tutorials/images/transfer_learning), the correct range of values is [-1, 1]. A similar issue was already reported at this [link](https://github.com/tensorflow/hub/issues/637).\r\n",
      "\r\n",
      "On the other hand, to my surprise, the function preprocess_input of the module [MobileNetV3](https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/python/keras/applications/mobilenet_v3.py#L556-L558) does not apply any change over the input; i.e. the function is returning directly the input.\r\n",
      "\r\n",
      "```\r\n",
      "@keras_export('keras.applications.mobilenet_v3.preprocess_input')\r\n",
      "def preprocess_input(x, data_format=None):  # pylint: disable=unused-argument\r\n",
      "  return x\r\n",
      "\r\n",
      "```\r\n",
      "\r\n",
      "So, what should be the correct range of input values for MobileNetV3?\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:keras\n",
      "stat:awaiting tensorflower\n",
      "type:docs-bug\n",
      "\n",
      "\n",
      "issue title -  Addition of TanhExp Activation function\n",
      "issue body -  Changes to tensorflow\\python\\keras\\activations.py and \\tensorflow\\python\\ops\\nn_impl.py files by adding TanhExp function for the issue #45929\n",
      "issue labels - \n",
      "cla: no\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  Batch Normalization fails as kernel constraint for Conv layers when using mixed precision\n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n",
      "- TensorFlow installed from (source or binary): source\r\n",
      "- TensorFlow version (use command below): 2.4.1\r\n",
      "- Python version: 3.7\r\n",
      "\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "When using `tf.keras.layers.BatchNormalization()` as a constraint in a conv layer using mixed precision, the model cannot train\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "Using `tf.keras.layers.BatchNormalization()` as a constraint in a conv layer behaves the same regardless of using mixed precision or not.\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "Provide a reproducible test case that is the bare minimum necessary to generate\r\n",
      "the problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n",
      "[Colab link](https://colab.research.google.com/drive/1IFWFwRrYUQx7Kw0I_KdtrKtVhvZbI7hy?usp=sharing).\r\n",
      "\r\n",
      "I've included a few notes in comments to show that this issue is isolated to conv layers when using mixed precision.\r\n",
      "\r\n",
      "**Other info / logs** Include any logs or source code that would be helpful to\r\n",
      "diagnose the problem. If including tracebacks, please include the full\r\n",
      "traceback. Large logs and files should be attached.\r\n",
      "\r\n",
      "It looks like the issue is in the loss_scale_optimizer.\r\n",
      "\r\n",
      "```txt\r\n",
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\r\n",
      "        return step_function(self, iterator)\r\n",
      "    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:795 step_function  **\r\n",
      "        outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n",
      "    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\r\n",
      "        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n",
      "    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\r\n",
      "        return self._call_for_each_replica(fn, args, kwargs)\r\n",
      "    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\r\n",
      "        return fn(*args, **kwargs)\r\n",
      "    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:788 run_step  **\r\n",
      "        outputs = model.train_step(data)\r\n",
      "    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:757 train_step\r\n",
      "        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\r\n",
      "    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:498 minimize\r\n",
      "        return self.apply_gradients(grads_and_vars, name=name)\r\n",
      "    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/mixed_precision/loss_scale_optimizer.py:712 apply_gradients\r\n",
      "        args=(grads_and_vars, name, experimental_aggregate_gradients))\r\n",
      "    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2941 merge_call\r\n",
      "        return self._merge_call(merge_fn, args, kwargs)\r\n",
      "    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2948 _merge_call\r\n",
      "        return merge_fn(self._strategy, *args, **kwargs)\r\n",
      "    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/mixed_precision/loss_scale_optimizer.py:745 _apply_gradients_cross_replica  **\r\n",
      "        do_not_apply_fn)\r\n",
      "    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/smart_cond.py:59 smart_cond\r\n",
      "        name=name)\r\n",
      "    /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\r\n",
      "        return target(*args, **kwargs)\r\n",
      "    /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/deprecation.py:538 new_func\r\n",
      "        return func(*args, **kwargs)\r\n",
      "    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/control_flow_ops.py:1180 cond\r\n",
      "        return cond_v2.cond_v2(pred, true_fn, false_fn, name)\r\n",
      "    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/cond_v2.py:89 cond_v2\r\n",
      "        op_return_value=pred)\r\n",
      "    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py:990 func_graph_from_py_func\r\n",
      "        func_outputs = python_func(*func_args, **func_kwargs)\r\n",
      "    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/mixed_precision/loss_scale_optimizer.py:732 apply_fn\r\n",
      "        args=(grads, wrapped_vars, name, experimental_aggregate_gradients))\r\n",
      "    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\r\n",
      "        return self._call_for_each_replica(fn, args, kwargs)\r\n",
      "    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\r\n",
      "        return fn(*args, **kwargs)\r\n",
      "    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/mixed_precision/loss_scale_optimizer.py:755 _apply_gradients\r\n",
      "        experimental_aggregate_gradients=experimental_aggregate_gradients)\r\n",
      "    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:635 apply_gradients\r\n",
      "        \"name\": name,\r\n",
      "    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2941 merge_call\r\n",
      "        return self._merge_call(merge_fn, args, kwargs)\r\n",
      "    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2948 _merge_call\r\n",
      "        return merge_fn(self._strategy, *args, **kwargs)\r\n",
      "    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:683 _distributed_apply  **\r\n",
      "        var, apply_grad_to_update_var, args=(grad,), group=False))\r\n",
      "    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2494 update\r\n",
      "        return self._update(var, fn, args, kwargs, group)\r\n",
      "    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:3431 _update\r\n",
      "        return self._update_non_slot(var, fn, (var,) + tuple(args), kwargs, group)\r\n",
      "    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:3437 _update_non_slot\r\n",
      "        result = fn(*args, **kwargs)\r\n",
      "    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:661 apply_grad_to_update_var  **\r\n",
      "        return var.assign(var.constraint(var))\r\n",
      "    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/mixed_precision/autocast_variable.py:237 assign\r\n",
      "        name, read_value)\r\n",
      "    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/mixed_precision/autocast_variable.py:209 _apply_assign_update\r\n",
      "        assign_op = update_fn(value, use_locking, name, False)\r\n",
      "    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/resource_variable_ops.py:882 assign\r\n",
      "        value_tensor = ops.convert_to_tensor(value, dtype=self.dtype)\r\n",
      "    /usr/local/lib/python3.7/dist-packages/tensorflow/python/profiler/trace.py:163 wrapped\r\n",
      "        return func(*args, **kwargs)\r\n",
      "    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py:1509 convert_to_tensor\r\n",
      "        (dtype.name, value.dtype.name, value))\r\n",
      "\r\n",
      "    ValueError: Tensor conversion requested dtype float32 for Tensor with dtype float16: <tf.Tensor 'cond_1/SGD/SGD/update/batch_normalization_2/FusedBatchNormV3:0' shape=(3, 3, 1, 32) dtype=float16>\r\n",
      "```\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:keras\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Correctly close file descriptors when loading saved models\n",
      "issue body -  This PR uses the `FileIO` contextmanager in order to ensure that filedescriptors are correctly closed after use when loading saved models.\n",
      "issue labels - \n",
      "cla: yes\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  Add link to presentation, minor fixes\n",
      "issue body -  Signed-off-by: Michael Gielda <mgielda@antmicro.com>\r\n",
      "\r\n",
      "Adding link to slides as proposed by TF Lite Micro team\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  In EXPO load coco-ssd model speed very slow\n",
      "issue body -  Hello, I'm running a demo test in expo, but I found coco-ssh model loading is really slow or nothing happens. By the way I'm in China, is it caused by internet restrictions? Please give some help, thanks. \r\n",
      "this.model = await cocossd.load() // loading, no response\r\n",
      "\r\n",
      "My code:\r\n",
      "import React from 'react'\r\n",
      "import {\r\n",
      "  StyleSheet,\r\n",
      "  Text,\r\n",
      "  View,\r\n",
      "  ActivityIndicator,\r\n",
      "  StatusBar,\r\n",
      "  Image,\r\n",
      "  TouchableOpacity\r\n",
      "} from 'react-native'\r\n",
      "import * as tf from '@tensorflow/tfjs'\r\n",
      "import { fetch } from '@tensorflow/tfjs-react-native'\r\n",
      "import * as cocossd from '@tensorflow-models/coco-ssd'\r\n",
      "import * as jpeg from 'jpeg-js'\r\n",
      "import * as ImagePicker from 'expo-image-picker'\r\n",
      "import Constants from 'expo-constants'\r\n",
      "import * as Permissions from 'expo-permissions'\r\n",
      "\r\n",
      "class App extends React.Component {\r\n",
      "  state = {\r\n",
      "    isTfReady: false,\r\n",
      "    isModelReady: false,\r\n",
      "    predictions: null,\r\n",
      "    image: null\r\n",
      "  }\r\n",
      "\r\n",
      "  async componentDidMount() {\r\n",
      "    await tf.ready()\r\n",
      "    this.setState({\r\n",
      "      isTfReady: true\r\n",
      "    })\r\n",
      "    this.model = await cocossd.load()\r\n",
      "    this.setState({ isModelReady: true })\r\n",
      "    this.getPermissionAsync()\r\n",
      "  }\r\n",
      "\r\n",
      "  getPermissionAsync = async () => {\r\n",
      "    if (Constants.platform.ios) {\r\n",
      "      const { status } = await Permissions.askAsync(Permissions.CAMERA_ROLL)\r\n",
      "      if (status !== 'granted') {\r\n",
      "        alert('Sorry, we need camera roll permissions to make this work!')\r\n",
      "      }\r\n",
      "    }\r\n",
      "  }\r\n",
      "\r\n",
      "  imageToTensor(rawImageData) {\r\n",
      "    const TO_UINT8ARRAY = true\r\n",
      "    const { width, height, data } = jpeg.decode(rawImageData, TO_UINT8ARRAY)\r\n",
      "    // Drop the alpha channel info for mobilenet\r\n",
      "    const buffer = new Uint8Array(width * height * 3)\r\n",
      "    let offset = 0 // offset into original data\r\n",
      "    for (let i = 0; i < buffer.length; i += 3) {\r\n",
      "      buffer[i] = data[offset]\r\n",
      "      buffer[i + 1] = data[offset + 1]\r\n",
      "      buffer[i + 2] = data[offset + 2]\r\n",
      "\r\n",
      "      offset += 4\r\n",
      "    }\r\n",
      "\r\n",
      "    return tf.tensor3d(buffer, [height, width, 3])\r\n",
      "  }\r\n",
      "\r\n",
      "  classifyImage = async () => {\r\n",
      "    try {\r\n",
      "      const imageAssetPath = Image.resolveAssetSource(this.state.image)\r\n",
      "      const response = await fetch(imageAssetPath.uri, {}, { isBinary: true })\r\n",
      "      const rawImageData = await response.arrayBuffer()\r\n",
      "      const imageTensor = this.imageToTensor(rawImageData)\r\n",
      "      const predictions = await this.model.classify(imageTensor)\r\n",
      "      this.setState({ predictions })\r\n",
      "      console.log(predictions)\r\n",
      "    } catch (error) {\r\n",
      "      console.log(error)\r\n",
      "    }\r\n",
      "  }\r\n",
      "\r\n",
      "  selectImage = async () => {\r\n",
      "    try {\r\n",
      "      let response = await ImagePicker.launchImageLibraryAsync({\r\n",
      "        mediaTypes: ImagePicker.MediaTypeOptions.All,\r\n",
      "        allowsEditing: true,\r\n",
      "        aspect: [4, 3]\r\n",
      "      })\r\n",
      "\r\n",
      "      if (!response.cancelled) {\r\n",
      "        const source = { uri: response.uri }\r\n",
      "        this.setState({ image: source })\r\n",
      "        this.classifyImage()\r\n",
      "      }\r\n",
      "    } catch (error) {\r\n",
      "      console.log(error)\r\n",
      "    }\r\n",
      "  }\r\n",
      "\r\n",
      "  renderPrediction = prediction => {\r\n",
      "    return (\r\n",
      "      <Text key={prediction.className} style={styles.text}>\r\n",
      "        {prediction.className}\r\n",
      "      </Text>\r\n",
      "    )\r\n",
      "  }\r\n",
      "\r\n",
      "  render() {\r\n",
      "    const { isTfReady, isModelReady, predictions, image } = this.state\r\n",
      "\r\n",
      "    return (\r\n",
      "      <View style={styles.container}>\r\n",
      "        <StatusBar barStyle='light-content' />\r\n",
      "        <View style={styles.loadingContainer}>\r\n",
      "          <Text style={styles.text}>\r\n",
      "            TFJS ready? {isTfReady ? <Text>✅</Text> : ''}\r\n",
      "          </Text>\r\n",
      "\r\n",
      "          <View style={styles.loadingModelContainer}>\r\n",
      "            <Text style={styles.text}>Model ready? </Text>\r\n",
      "            {isModelReady ? (\r\n",
      "              <Text style={styles.text}>✅</Text>\r\n",
      "            ) : (\r\n",
      "              <ActivityIndicator size='small' />\r\n",
      "            )}\r\n",
      "          </View>\r\n",
      "        </View>\r\n",
      "        <TouchableOpacity\r\n",
      "          style={styles.imageWrapper}\r\n",
      "          onPress={isModelReady ? this.selectImage : undefined}>\r\n",
      "          {image && <Image source={image} style={styles.imageContainer} />}\r\n",
      "\r\n",
      "          {isModelReady && !image && (\r\n",
      "            <Text style={styles.transparentText}>Tap to choose image</Text>\r\n",
      "          )}\r\n",
      "        </TouchableOpacity>\r\n",
      "        <View style={styles.predictionWrapper}>\r\n",
      "          {isModelReady && image && (\r\n",
      "            <Text style={styles.text}>\r\n",
      "              Predictions: {predictions ? '' : 'Predicting...'}\r\n",
      "            </Text>\r\n",
      "          )}\r\n",
      "          {isModelReady &&\r\n",
      "            predictions &&\r\n",
      "            predictions.map(p => this.renderPrediction(p))}\r\n",
      "        </View>\r\n",
      "        <View style={styles.footer}>\r\n",
      "          <Text style={styles.poweredBy}>Powered by:</Text>\r\n",
      "          <Image source={require('./assets/tfjs.jpg')} style={styles.tfLogo} />\r\n",
      "        </View>\r\n",
      "      </View>\r\n",
      "    )\r\n",
      "  }\r\n",
      "}\r\n",
      "\r\n",
      "const styles = StyleSheet.create({\r\n",
      "  container: {\r\n",
      "    flex: 1,\r\n",
      "    backgroundColor: '#171f24',\r\n",
      "    alignItems: 'center'\r\n",
      "  },\r\n",
      "  loadingContainer: {\r\n",
      "    marginTop: 80,\r\n",
      "    justifyContent: 'center'\r\n",
      "  },\r\n",
      "  text: {\r\n",
      "    color: '#ffffff',\r\n",
      "    fontSize: 16\r\n",
      "  },\r\n",
      "  loadingModelContainer: {\r\n",
      "    flexDirection: 'row',\r\n",
      "    marginTop: 10\r\n",
      "  },\r\n",
      "  imageWrapper: {\r\n",
      "    width: 280,\r\n",
      "    height: 280,\r\n",
      "    padding: 10,\r\n",
      "    borderColor: '#cf667f',\r\n",
      "    borderWidth: 5,\r\n",
      "    borderStyle: 'dashed',\r\n",
      "    marginTop: 40,\r\n",
      "    marginBottom: 10,\r\n",
      "    position: 'relative',\r\n",
      "    justifyContent: 'center',\r\n",
      "    alignItems: 'center'\r\n",
      "  },\r\n",
      "  imageContainer: {\r\n",
      "    width: 250,\r\n",
      "    height: 250,\r\n",
      "    position: 'absolute',\r\n",
      "    top: 10,\r\n",
      "    left: 10,\r\n",
      "    bottom: 10,\r\n",
      "    right: 10\r\n",
      "  },\r\n",
      "  predictionWrapper: {\r\n",
      "    height: 100,\r\n",
      "    width: '100%',\r\n",
      "    flexDirection: 'column',\r\n",
      "    alignItems: 'center'\r\n",
      "  },\r\n",
      "  transparentText: {\r\n",
      "    color: '#ffffff',\r\n",
      "    opacity: 0.7\r\n",
      "  },\r\n",
      "  footer: {\r\n",
      "    marginTop: 40\r\n",
      "  },\r\n",
      "  poweredBy: {\r\n",
      "    fontSize: 20,\r\n",
      "    color: '#e69e34',\r\n",
      "    marginBottom: 6\r\n",
      "  },\r\n",
      "  tfLogo: {\r\n",
      "    width: 125,\r\n",
      "    height: 70\r\n",
      "  }\r\n",
      "})\r\n",
      "\r\n",
      "export default App\r\n",
      "\r\n",
      "\r\n",
      "package.json:\r\n",
      "{\r\n",
      "  \"dependencies\": {\r\n",
      "    \"expo-gl\": \"~9.2.0\",\r\n",
      "    \"jpeg-js\": \"0.3.6\",\r\n",
      "    \"expo-constants\": \"~9.3.3\",\r\n",
      "    \"@tensorflow/tfjs\": \"1.2.11\",\r\n",
      "    \"expo-permissions\": \"~10.0.0\",\r\n",
      "    \"expo-image-picker\": \"~9.2.0\",\r\n",
      "    \"@tensorflow/tfjs-core\": \"^1.2.8\",\r\n",
      "    \"react-native-image-picker\": \"1.1.0\",\r\n",
      "    \"@tensorflow/tfjs-converter\": \"1.2.1\",\r\n",
      "    \"@react-native-community/blur\": \"3.3.1\",\r\n",
      "    \"@tensorflow-models/mobilenet\": \"2.0.4\",\r\n",
      "    \"@tensorflow/tfjs-react-native\": \"0.1.0-alpha.2\",\r\n",
      "    \"@react-native-community/async-storage\": \"~1.12.0\",\r\n",
      "    \"@tensorflow-models/coco-ssd\": \"^2.2.1\"\r\n",
      "  }\r\n",
      "}\r\n",
      "\n",
      "issue labels - \n",
      "stat:awaiting response\n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  conflict numpy version between tensorflow and tf-nighty\n",
      "issue body -  When I install tf-nighty as below command:\r\n",
      "`pip3 install tf-nightly`\r\n",
      "\r\n",
      "\r\n",
      "the error appear as below:\r\n",
      "\r\n",
      "```\r\n",
      "Requirement already satisfied: zipp>=0.5 in /home/hoaphan/.local/lib/python3.6/site-packages (from importlib-metadata->markdown>=2.6.8->tb-nightly~=2.4.0.a->tf-nightly) (3.4.0)\r\n",
      "Installing collected packages: numpy\r\n",
      "  Attempting uninstall: numpy\r\n",
      "    Found existing installation: numpy 1.18.5\r\n",
      "    Uninstalling numpy-1.18.5:\r\n",
      "      Successfully uninstalled numpy-1.18.5\r\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "tensorflow 2.3.1 requires numpy<1.19.0,>=1.16.0, but you have numpy 1.19.5 which is incompatible.\r\n",
      "Successfully installed numpy-1.19.5\r\n",
      "```\r\n",
      "this mean:\r\n",
      "the tensorflow requires numpy <1.19.0\r\n",
      "but the tf-nighty requires numpy 1.19.5\r\n",
      "\r\n",
      "so I cannot install tensorflow and tf-nighty at same time,\r\n",
      "so please help me solve this issue.\n",
      "issue labels - \n",
      "TF 2.5\n",
      "stat:awaiting response\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  evaluate\n",
      "issue body -  Please go to Stack Overflow for help and support:\r\n",
      "\r\n",
      "https://stackoverflow.com/questions/tagged/tensorflow\r\n",
      "\r\n",
      "If you open a GitHub issue, here is our policy:\r\n",
      "\r\n",
      "1.  It must be a bug, a feature request, or a significant problem with the\r\n",
      "    documentation (for small docs fixes please send a PR instead).\r\n",
      "2.  The form below must be filled out.\r\n",
      "3.  It shouldn't be a TensorBoard issue. Those go\r\n",
      "    [here](https://github.com/tensorflow/tensorboard/issues).\r\n",
      "\r\n",
      "**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n",
      "\r\n",
      "------------------------\r\n",
      "\r\n",
      "### System information\r\n",
      "\r\n",
      "-   **Have I written custom code (as opposed to using a stock example script\r\n",
      "    provided in TensorFlow)**:\r\n",
      "-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n",
      "-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n",
      "    happens on a mobile device**:\r\n",
      "-   **TensorFlow installed from (source or binary)**:\r\n",
      "-   **TensorFlow version (use command below)**:\r\n",
      "-   **Python version**:\r\n",
      "-   **Bazel version (if compiling from source)**:\r\n",
      "-   **GCC/Compiler version (if compiling from source)**:\r\n",
      "-   **CUDA/cuDNN version**:\r\n",
      "-   **GPU model and memory**:\r\n",
      "-   **Exact command to reproduce**:\r\n",
      "\r\n",
      "You can collect some of this information using our environment capture script:\r\n",
      "\r\n",
      "https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n",
      "\r\n",
      "You can obtain the TensorFlow version with:\r\n",
      "\r\n",
      "```bash\r\n",
      "python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n",
      "```\r\n",
      "\r\n",
      "### Describe the problem\r\n",
      "Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n",
      "\r\n",
      "### Source code / logs\r\n",
      "Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n",
      "\n",
      "issue labels - \n",
      "\n",
      "\n",
      "issue title -  [INTEL MKL] Fixed mkl unit test build error due to benchmark test api change\n",
      "issue body -  \n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:core\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  Use zeros_like(x) in favour of zeros(shape(x)) when computing gradients\n",
      "issue body -  This PR uses `zeros_like(x)` in favour of `zeros(shape(x))` in gradient definitions.\r\n",
      "\r\n",
      "This simplifies the code and slightly improves performance since it can use the fused `ZerosLike` kernel instead of requiring both `Shape` and `Zeros`.\n",
      "issue labels - \n",
      "cla: yes\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  dummy\n",
      "issue body -  \n",
      "issue labels - \n",
      "cla: no\n",
      "invalid\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Add int8_t support for micro op zeros_like\n",
      "issue body -  Continue to work on issue #46049. Two changes were made:\r\n",
      "1. Added int8_t support. Note the TFLite counterpart does not yet support int8_t;\r\n",
      "2. Consolidated the test code with the typename template.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  lite: nnapi: Fix fd leak in NNMemory's destructor\n",
      "issue body -  Valid fd values are non-negative, including zero.\n",
      "issue labels - \n",
      "cla: yes\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Login loop caused by unmet packages in gpu Ubuntu16.04 installation guide\n",
      "issue body -  **System information**\r\n",
      "- OS Platform and Distribution Linux Ubuntu 16.04:\r\n",
      "- GeForce GTX TITAN 2070\r\n",
      "Other inapplicable cause I following a guide: \r\n",
      "https://www.tensorflow.org/install/gpu?hl=ur#ubuntu_1604_cuda_110\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "23th line \r\n",
      "`sudo apt-get install --no-install-recommends     cuda-11-0     libcudnn8=8.0.4.30-1+cuda11.0      libcudnn8-dev=8.0.4.30-1+cuda11.0`\r\n",
      "\r\n",
      "leads to\r\n",
      "`Reading package lists... Done\r\n",
      "Building dependency tree       \r\n",
      "Reading state information... Done\r\n",
      "libcudnn8-dev is already the newest version (8.0.4.30-1+cuda11.0).\r\n",
      "libcudnn8 is already the newest version (8.0.4.30-1+cuda11.0).\r\n",
      "The following additional packages will be installed:\r\n",
      "  cuda-demo-suite-11-0 cuda-drivers cuda-drivers-460 cuda-runtime-11-0\r\n",
      "  nvidia-460 nvidia-460-dev\r\n",
      "Recommended packages:\r\n",
      "  nvidia-prime | bumblebee\r\n",
      "The following packages will be REMOVED:\r\n",
      "  nvidia-450\r\n",
      "The following NEW packages will be installed:\r\n",
      "  cuda-11-0 cuda-demo-suite-11-0 cuda-drivers cuda-drivers-460\r\n",
      "  cuda-runtime-11-0 nvidia-460 nvidia-460-dev\r\n",
      "0 upgraded, 7 newly installed, 1 to remove and 66 not upgraded.\r\n",
      "Need to get 0 B/168 MB of archives.\r\n",
      "After this operation, 93,2 MB of additional disk space will be used.\r\n",
      "Do you want to continue? [Y/n] `\r\n",
      "\r\n",
      "If 'yes' is choosen next reboot leads to login loop because of\r\n",
      "\"NVIDIA NVML Driver/library version mismatch\"\r\n",
      "nvidia 450 was installed at 19th line and nvidia 460 after 23th line\r\n",
      "\r\n",
      "Deleting nvidia 460 allowing to login normally,with nvidia smi giving 450 version,\r\n",
      "but \r\n",
      "```\r\n",
      "from tensorflow.python.client import device_lib\r\n",
      "device_lib.list_local_devices()\r\n",
      "```\r\n",
      "Still don't show accessible GPU.\r\n",
      "\r\n",
      "**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n",
      "https://www.tensorflow.org/install/gpu?hl=ur#ubuntu_1604_cuda_110\r\n",
      "except 19th line\r\n",
      "`sudo apt-get install --no-install-recommends nvidia-driver-450`\r\n",
      "due to\r\n",
      "https://github.com/tensorflow/tensorflow/issues/47402\r\n",
      "instead of it \r\n",
      "`sudo apt-get install --no-install-recommends nvidia-450`\r\n",
      "And\r\n",
      "`sudo apt-get update --allow-unauthenticated` before 2nd line\r\n",
      "due to\r\n",
      "`W: Failed to fetch https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/InRelease  Could not resolve host: developer.download.nvidia.com`\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "stat:awaiting response\n",
      "subtype: ubuntu/linux\n",
      "type:build/install\n",
      "type:docs-bug\n",
      "\n",
      "\n",
      "issue title -  Install gpu Ubuntu16.04 misses crucial steps: \"unable to locate package nvidia-driver-450\"\n",
      "issue body -  ## URL(s) with the issue:\r\n",
      "https://www.tensorflow.org/install/gpu?hl=ur#ubuntu_1604_cuda_110\r\n",
      "## Description of issue (what needs changing):\r\n",
      "\r\n",
      "19th line:\r\n",
      "`sudo apt-get install --no-install-recommends nvidia-driver-450`\r\n",
      "leads to error: \r\n",
      "`E: Unable to locate package nvidia-driver-450`\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "stat:awaiting response\n",
      "subtype: ubuntu/linux\n",
      "type:build/install\n",
      "type:docs-bug\n",
      "\n",
      "\n",
      "issue title -  Burst mode execution in NNAPI\n",
      "issue body -  How can we use the burst mode of execution while using the NNAPI delegate? There seems to be no API exposed to select the mode of execution and by default, the NNAPI delegate selects only between async and sync mode based on version. If we want to select a mode of execution through an android application, what should be the approach? \n",
      "issue labels - \n",
      "TFLiteNNAPIDelegate\n",
      "comp:lite\n",
      "stat:awaiting response\n",
      "type:feature\n",
      "\n",
      "\n",
      "issue title -  tflite api doesnt return input shape in c++\n",
      "issue body -  I am using a functional model which  I converted it to tflite  in tf2.4.0 but I dont get shape of input tensor in c++ but Its  visible in python\r\n",
      "\r\n",
      "```\r\n",
      "interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\r\n",
      "interpreter.allocate_tensors()\r\n",
      "input_details = interpreter.get_input_details()\r\n",
      "print(input_details)\r\n",
      "print(input_details[0]['shape'])  ##Returns  shape of input tensor [1  100 100 1]\r\n",
      "```\r\n",
      "\r\n",
      "```\r\n",
      "std::unique_ptr<tflite::FlatBufferModel> model = tflite::FlatBufferModel::BuildFromFile(\"model.tflite\");\r\n",
      "tflite::ops::builtin::BuiltinOpResolver resolver;\r\n",
      "tflite::InterpreterBuilder builder(*model, resolver);\r\n",
      "std::unique_ptr<tflite::Interpreter> interpreter;\r\n",
      "builder(&interpreter);\r\n",
      "interpreter->AllocateTensors();\r\n",
      "std::cout <<\"Input variable name: \"<<interpreter->GetInputName(0)<<std::endl; // Works\r\n",
      "std::vector<int> inputs_vec = interpreter->inputs();  // Returns [0]. Size of vector is 1\r\n",
      "\r\n",
      "\r\n",
      "```\r\n",
      "\n",
      "issue labels - \n",
      "comp:lite\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Simple Example with CIFAR-10. ImageDataGenerator + 'sparse_categorical_crossentropy'\n",
      "issue body -  **System information**\r\n",
      "- TensorFlow version (use command below): 2.4\r\n",
      "\r\n",
      "I try example CIFAR-10 with ImageDataGenerator. But it doesn't works. \r\n",
      "\r\n",
      "My custom CNN Model is not training when I use **ImageDataGenerator + sparse_categorical_crossentropy**.\r\n",
      "However, when I use **ImageDataGenerator + categorical_crossentropy**, it works well.\r\n",
      "\r\n",
      "what happen in this code?\r\n",
      "\r\n",
      "Thank you and my code is below.\r\n",
      "\r\n",
      "------------------------------------------\r\n",
      "\r\n",
      "```\r\n",
      "from tensorflow.keras.datasets import cifar10\r\n",
      "import numpy as np\r\n",
      "\r\n",
      "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\r\n",
      "\r\n",
      "x_mean = np.mean(x_train, axis = (0, 1, 2))\r\n",
      "x_std = np.std(x_train, axis = (0, 1, 2))\r\n",
      "\r\n",
      "x_train = (x_train - x_mean) / x_std\r\n",
      "x_test = (x_test - x_mean) / x_std\r\n",
      "\r\n",
      "from sklearn.model_selection import train_test_split\r\n",
      "\r\n",
      "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, \r\n",
      "                                                  test_size = 0.3, random_state = 777)\r\n",
      "\r\n",
      "print('data ready~')\r\n",
      "\r\n",
      "from tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n",
      "\r\n",
      "train_datagen = ImageDataGenerator(horizontal_flip = True,\r\n",
      "                                   zoom_range = 0.2,\r\n",
      "                                   width_shift_range = 0.1,\r\n",
      "                                   height_shift_range = 0.1,\r\n",
      "                                   rotation_range = 30,\r\n",
      "                                   fill_mode = 'nearest'\r\n",
      "                                  )\r\n",
      "val_datagen = ImageDataGenerator()\r\n",
      "\r\n",
      "batch_size = 32\r\n",
      "\r\n",
      "train_generator = train_datagen.flow(x_train, y_train,\r\n",
      "                                    batch_size = batch_size)\r\n",
      "val_generator = val_datagen.flow(x_val, y_val,\r\n",
      "                                batch_size = batch_size)\r\n",
      "\r\n",
      "\r\n",
      "from tensorflow.keras.models import Sequential\r\n",
      "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Activation, BatchNormalization\r\n",
      "from tensorflow.keras.optimizers import Adam\r\n",
      "\r\n",
      "model = Sequential()\r\n",
      "\r\n",
      "model.add(Conv2D(filters = 32, kernel_size = 3, padding = 'same', input_shape = (32, 32, 3)))\r\n",
      "model.add(BatchNormalization())\r\n",
      "model.add(Activation('relu'))\r\n",
      "model.add(Conv2D(filters = 32, kernel_size = 3, padding = 'same'))\r\n",
      "model.add(BatchNormalization())\r\n",
      "model.add(Activation('relu'))\r\n",
      "model.add(MaxPool2D(pool_size = (2, 2), strides = 2, padding = 'same'))\r\n",
      "\r\n",
      "model.add(Conv2D(filters = 64, kernel_size = 3, padding = 'same'))\r\n",
      "model.add(BatchNormalization())\r\n",
      "model.add(Activation('relu'))\r\n",
      "model.add(Conv2D(filters = 64, kernel_size = 3, padding = 'same'))\r\n",
      "model.add(BatchNormalization())\r\n",
      "model.add(Activation('relu'))\r\n",
      "model.add(MaxPool2D(pool_size = (2, 2), strides = 2, padding = 'same'))\r\n",
      "\r\n",
      "model.add(Conv2D(filters = 128, kernel_size = 3, padding = 'same'))\r\n",
      "model.add(BatchNormalization())\r\n",
      "model.add(Activation('relu'))\r\n",
      "model.add(Conv2D(filters = 128, kernel_size = 3, padding = 'same'))\r\n",
      "model.add(BatchNormalization())\r\n",
      "model.add(Activation('relu'))\r\n",
      "model.add(MaxPool2D(pool_size = (2, 2), strides = 2, padding = 'same'))\r\n",
      "\r\n",
      "model.add(Flatten())\r\n",
      "model.add(Dense(256))\r\n",
      "model.add(Activation('relu'))\r\n",
      "model.add(Dense(10, activation = 'softmax'))\r\n",
      "\r\n",
      "model.compile(optimizer = Adam(1e-4),\r\n",
      "             loss = 'sparse_categorical_crossentropy',\r\n",
      "             metrics = ['acc'])\r\n",
      "\r\n",
      "def get_step(train_len, batch_size):\r\n",
      "    if(train_len % batch_size > 0):\r\n",
      "        return train_len // batch_size + 1\r\n",
      "    else:\r\n",
      "        return train_len // batch_size\r\n",
      "\r\n",
      "history = model.fit(train_generator,\r\n",
      "                    epochs = 100,\r\n",
      "                    steps_per_epoch = get_step(len(x_train), batch_size),\r\n",
      "                    validation_data = val_generator,\r\n",
      "                    validation_steps = get_step(len(x_val), batch_size))\r\n",
      "```\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:keras\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Fix describe error in keil readme template\n",
      "issue body -  Fix describe error in keil readme template\r\n",
      "\r\n",
      "# TensorFlow Lite Micro Mbed Project \r\n",
      "-> \r\n",
      "# TensorFlow Lite Micro Keil Project \n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Unable to migrate TF1 code to TF2, tape.gradient returns None\n",
      "issue body -  <em>Please make sure that this is a bug. As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:bug_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code: yes\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macos big sur\r\n",
      "- TensorFlow installed from (source or binary): binary, pip\r\n",
      "- TensorFlow version (use command below): v2.4.0-49-g85c8b2a817f 2.4.1\r\n",
      "- Python version: 3.8.7\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "I'm migrating code from TF1 that uses `tf.gradients()` for doing a custom gradient calculation. I'm trying to get the same results I get with TF1 in TF2 using `tf.GradientTape()` however, no matter what I do including:\r\n",
      "- I tried to use `tape.watch()` and the issue persists.\r\n",
      "- I tried manually creating a `tf.Variable()` with `trainable=True`, watch the variable and the issue persists.\r\n",
      "- I tried using `tf.gradients()` within a `tf.function` and `tf.compat.v1.gradients()` if there is any difference at all, and the issue persists\r\n",
      "\r\n",
      "Here's a jupyter [notebook](https://colab.research.google.com/drive/1eeEc9eIlvMVAJbueIfhpgh569pi-lGx4?usp=sharing) with the full code to be able to reproduce the issue.\r\n",
      "\r\n",
      "Here's the [code](https://github.com/openai/baselines/blob/master/baselines/acer/acer.py) I'm migrating. Check lines [156 - 176]. Below is the part of interest:\r\n",
      "\r\n",
      "```\r\n",
      "    g = tf.gradients(-loss, f)  # loss being a float and f being a (m, n) tensor\r\n",
      "    k = -f_pol / (f + eps)  # f_pol another (m, n) tensor and eps a float\r\n",
      "    k_dot_g = tf.reduce_sum(k * g, axis=-1)\r\n",
      "    adj = tf.maximum(\r\n",
      "        0.0,\r\n",
      "        (tf.reduce_sum(k * g, axis=-1) - delta)\r\n",
      "        / (tf.reduce_sum(tf.square(k), axis=-1) + eps),\r\n",
      "    )\r\n",
      "    g = g - tf.reshape(adj, [nenvs * nsteps, 1]) * k\r\n",
      "    grads_f = -g / (nenvs * nsteps)\r\n",
      "    grads_policy = tf.gradients(f, params, grads_f)  # params being the model parameters\r\n",
      "```\r\n",
      "\r\n",
      "and here's a simplified version of what I'm trying to do:\r\n",
      "\r\n",
      "    with tf.GradientTape() as tape:\r\n",
      "        f = calculate_f()\r\n",
      "        f_pol = calculate_f_pol()\r\n",
      "        others = do_further_calculations()\r\n",
      "        loss = calculate_loss()\r\n",
      "    g = tape.gradient(-loss, f)\r\n",
      "    print(g)\r\n",
      "\r\n",
      "results in:\r\n",
      "\r\n",
      "    None\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "\r\n",
      "As far as I understand `tf.GradientTape()` is the TF2 alternative to `tf.gradients()`. I'm trying to replicate the exact same results in TF2 and it doesn't work, so this implies either there is something wrong with my code or it is a bug. This is not the first time it happens with someone, I found numerous other complains including closed issues and neither includes a solution I have not tried.\r\n",
      " \r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "the problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n",
      "\r\n",
      "jupyter [notebook](https://colab.research.google.com/drive/1eeEc9eIlvMVAJbueIfhpgh569pi-lGx4?usp=sharing) \r\n",
      "\r\n",
      "**Other info / logs** Include any logs or source code that would be helpful to\r\n",
      "diagnose the problem. If including tracebacks, please include the full\r\n",
      "traceback. Large logs and files should be attached.\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:ops\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  H fusion sharing opnd with user upstream again\n",
      "issue body -  \r\n",
      "Re-submitting the PR from https://github.com/tensorflow/tensorflow/pull/46614.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:xla\n",
      "ready to pull\n",
      "size:L\n",
      "\n",
      "\n",
      "issue title -  Update STM32 Bare Lib for zero initialization of the bss section\n",
      "issue body -  With google/stm32_bare_lib@aaabdeb STM32 Bare Lib zero-initializes the bss section.\r\n",
      "\r\n",
      "This change is also pulling out the download into a standalone bash script.\r\n",
      "\r\n",
      "See #46937 for more discussion on this.\r\n",
      "\r\n",
      "Fixes #46937\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  Use xa_nnlib for depthwise_conv for Fusion F1\n",
      "issue body -  The code in this change is the subset of functionality needed for int8 svdf for Hifi4 copied from pnikam-cad/tensorflow@a737c1e/tensorflow/lite/micro/kernels/xtensa_hifi/depthwise_conv.cc\r\n",
      "\r\n",
      "Note that the current change has not pulled in the floating point, uint8 implementation or the Hifi5 implementation.\r\n",
      "\r\n",
      "Profiled the person_detection_benchmark with the following command:\r\n",
      "\r\n",
      "make -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=fusion_f1 XTENSA_CORE=F1_190305_swupgrade run_person_detection_benchmark -j8\r\n",
      "gives a latency of 9.661M ticks with this change vs 73.761M ticks without this change.\r\n",
      "\r\n",
      "Per OP latency with this change:\r\n",
      "```\r\n",
      "WithPersonDataIterations(1) took 9661310 ticks (9661 ms)\r\n",
      "DEPTHWISE_CONV_2D took 1156157 ticks (1156 ms).\r\n",
      "DEPTHWISE_CONV_2D took 628525 ticks (628 ms).\r\n",
      "CONV_2D took 987374 ticks (987 ms).\r\n",
      "DEPTHWISE_CONV_2D took 395420 ticks (395 ms).\r\n",
      "CONV_2D took 554630 ticks (554 ms).\r\n",
      "DEPTHWISE_CONV_2D took 545252 ticks (545 ms).\r\n",
      "CONV_2D took 665222 ticks (665 ms).\r\n",
      "DEPTHWISE_CONV_2D took 172412 ticks (172 ms).\r\n",
      "CONV_2D took 334262 ticks (334 ms).\r\n",
      "DEPTHWISE_CONV_2D took 283280 ticks (283 ms).\r\n",
      "CONV_2D took 444854 ticks (444 ms).\r\n",
      "DEPTHWISE_CONV_2D took 88394 ticks (88 ms).\r\n",
      "CONV_2D took 225302 ticks (225 ms).\r\n",
      "DEPTHWISE_CONV_2D took 158090 ticks (158 ms).\r\n",
      "CONV_2D took 335894 ticks (335 ms).\r\n",
      "DEPTHWISE_CONV_2D took 158090 ticks (158 ms).\r\n",
      "CONV_2D took 335894 ticks (335 ms).\r\n",
      "DEPTHWISE_CONV_2D took 158090 ticks (158 ms).\r\n",
      "CONV_2D took 335894 ticks (335 ms).\r\n",
      "DEPTHWISE_CONV_2D took 158090 ticks (158 ms).\r\n",
      "CONV_2D took 335894 ticks (335 ms).\r\n",
      "DEPTHWISE_CONV_2D took 158090 ticks (158 ms).\r\n",
      "CONV_2D took 335894 ticks (335 ms).\r\n",
      "DEPTHWISE_CONV_2D took 59525 ticks (59 ms).\r\n",
      "CONV_2D took 173270 ticks (173 ms).\r\n",
      "DEPTHWISE_CONV_2D took 112424 ticks (112 ms).\r\n",
      "CONV_2D took 283862 ticks (283 ms).\r\n",
      "AVERAGE_POOL_2D took 75604 ticks (75 ms).\r\n",
      "CONV_2D took 3398 ticks (3 ms).\r\n",
      "RESHAPE took 290 ticks (0 ms).\r\n",
      "SOFTMAX took 1933 ticks (1 ms).\r\n",
      "```\r\n",
      "\r\n",
      "Without this change:\r\n",
      "```\r\n",
      "KeywordRunNIerations(1) took 38516 ticks (38 ms)\r\n",
      "DEPTHWISE_CONV_2D took 11961939 ticks (11961 ms).\r\n",
      "DEPTHWISE_CONV_2D took 12296923 ticks (12296 ms).\r\n",
      "CONV_2D took 987358 ticks (987 ms).\r\n",
      "DEPTHWISE_CONV_2D took 6138259 ticks (6138 ms).\r\n",
      "CONV_2D took 554614 ticks (554 ms).\r\n",
      "DEPTHWISE_CONV_2D took 12063331 ticks (12063 ms).\r\n",
      "CONV_2D took 665206 ticks (665 ms).\r\n",
      "DEPTHWISE_CONV_2D took 3018615 ticks (3018 ms).\r\n",
      "CONV_2D took 334246 ticks (334 ms).\r\n",
      "DEPTHWISE_CONV_2D took 5837463 ticks (5837 ms).\r\n",
      "CONV_2D took 444838 ticks (444 ms).\r\n",
      "DEPTHWISE_CONV_2D took 1462009 ticks (1462 ms).\r\n",
      "CONV_2D took 225286 ticks (225 ms).\r\n",
      "DEPTHWISE_CONV_2D took 2734009 ticks (2734 ms).\r\n",
      "CONV_2D took 335878 ticks (335 ms).\r\n",
      "DEPTHWISE_CONV_2D took 2734009 ticks (2734 ms).\r\n",
      "CONV_2D took 335878 ticks (335 ms).\r\n",
      "DEPTHWISE_CONV_2D took 2734009 ticks (2734 ms).\r\n",
      "CONV_2D took 335878 ticks (335 ms).\r\n",
      "DEPTHWISE_CONV_2D took 2734009 ticks (2734 ms).\r\n",
      "CONV_2D took 335878 ticks (335 ms).\r\n",
      "DEPTHWISE_CONV_2D took 2734009 ticks (2734 ms).\r\n",
      "CONV_2D took 335878 ticks (335 ms).\r\n",
      "DEPTHWISE_CONV_2D took 685980 ticks (685 ms).\r\n",
      "CONV_2D took 173254 ticks (173 ms).\r\n",
      "DEPTHWISE_CONV_2D took 1197084 ticks (1197 ms).\r\n",
      "CONV_2D took 283846 ticks (283 ms).\r\n",
      "AVERAGE_POOL_2D took 75604 ticks (75 ms).\r\n",
      "CONV_2D took 3382 ticks (3 ms).\r\n",
      "RESHAPE took 290 ticks (0 ms).\r\n",
      "SOFTMAX took 1933 ticks (1 ms).\r\n",
      "```\r\n",
      "\r\n",
      "Confirmed that the kernel_conv_test passes with:\r\n",
      "```\r\n",
      "make -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=fusion_f1 XTENSA_CORE=F1_190305_swupgrade test_kernel_depthwise_conv_test -j8\r\n",
      "```\r\n",
      "Progress towards http://b/177457688\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:L\n",
      "\n",
      "\n",
      "issue title -  Update callbacks.py\n",
      "issue body -  Mirrored cl/358287618\r\n",
      "Fixes GitHub #40604\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:keras\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Use xa_nnlib for conv for Fusion F1\n",
      "issue body -  The code in this change is the subset of functionality needed for int8 svdf for Hifi4 copied from pnikam-cad/tensorflow@a737c1e/tensorflow/lite/micro/kernels/xtensa_hifi/conv.cc\r\n",
      "\r\n",
      "Note that the current change has not pulled in the floating point, uint8 implementation or the Hifi5 implementation.\r\n",
      "\r\n",
      "Profiled the person_detection_benchmark with the following command:\r\n",
      "\r\n",
      "make -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=fusion_f1 XTENSA_CORE=F1_190305_swupgrade run_person_detection_benchmark -j8\r\n",
      "gives a latency of 73.761M ticks with this change vs 212.980M ticks without this change.\r\n",
      "\r\n",
      "Per OP latency with this change:\r\n",
      "```\r\n",
      "KeywordRunNIerations(1) took 38516 ticks (38 ms)\r\n",
      "DEPTHWISE_CONV_2D took 11961939 ticks (11961 ms).\r\n",
      "DEPTHWISE_CONV_2D took 12296923 ticks (12296 ms).\r\n",
      "CONV_2D took 987358 ticks (987 ms).\r\n",
      "DEPTHWISE_CONV_2D took 6138259 ticks (6138 ms).\r\n",
      "CONV_2D took 554614 ticks (554 ms).\r\n",
      "DEPTHWISE_CONV_2D took 12063331 ticks (12063 ms).\r\n",
      "CONV_2D took 665206 ticks (665 ms).\r\n",
      "DEPTHWISE_CONV_2D took 3018615 ticks (3018 ms).\r\n",
      "CONV_2D took 334246 ticks (334 ms).\r\n",
      "DEPTHWISE_CONV_2D took 5837463 ticks (5837 ms).\r\n",
      "CONV_2D took 444838 ticks (444 ms).\r\n",
      "DEPTHWISE_CONV_2D took 1462009 ticks (1462 ms).\r\n",
      "CONV_2D took 225286 ticks (225 ms).\r\n",
      "DEPTHWISE_CONV_2D took 2734009 ticks (2734 ms).\r\n",
      "CONV_2D took 335878 ticks (335 ms).\r\n",
      "DEPTHWISE_CONV_2D took 2734009 ticks (2734 ms).\r\n",
      "CONV_2D took 335878 ticks (335 ms).\r\n",
      "DEPTHWISE_CONV_2D took 2734009 ticks (2734 ms).\r\n",
      "CONV_2D took 335878 ticks (335 ms).\r\n",
      "DEPTHWISE_CONV_2D took 2734009 ticks (2734 ms).\r\n",
      "CONV_2D took 335878 ticks (335 ms).\r\n",
      "DEPTHWISE_CONV_2D took 2734009 ticks (2734 ms).\r\n",
      "CONV_2D took 335878 ticks (335 ms).\r\n",
      "DEPTHWISE_CONV_2D took 685980 ticks (685 ms).\r\n",
      "CONV_2D took 173254 ticks (173 ms).\r\n",
      "DEPTHWISE_CONV_2D took 1197084 ticks (1197 ms).\r\n",
      "CONV_2D took 283846 ticks (283 ms).\r\n",
      "AVERAGE_POOL_2D took 75604 ticks (75 ms).\r\n",
      "CONV_2D took 3382 ticks (3 ms).\r\n",
      "RESHAPE took 290 ticks (0 ms).\r\n",
      "SOFTMAX took 1933 ticks (1 ms).\r\n",
      "```\r\n",
      "Without this change:\r\n",
      "```\r\n",
      "WithPersonDataIterations(1) took 212980371 ticks (212980 ms)\r\n",
      "DEPTHWISE_CONV_2D took 11961939 ticks (11961 ms).\r\n",
      "DEPTHWISE_CONV_2D took 12296923 ticks (12296 ms).\r\n",
      "CONV_2D took 13604549 ticks (13604 ms).\r\n",
      "DEPTHWISE_CONV_2D took 6138259 ticks (6138 ms).\r\n",
      "CONV_2D took 9585893 ticks (9585 ms).\r\n",
      "DEPTHWISE_CONV_2D took 12063331 ticks (12063 ms).\r\n",
      "CONV_2D took 15189221 ticks (15189 ms).\r\n",
      "DEPTHWISE_CONV_2D took 3018615 ticks (3018 ms).\r\n",
      "CONV_2D took 7590389 ticks (7590 ms).\r\n",
      "DEPTHWISE_CONV_2D took 5837463 ticks (5837 ms).\r\n",
      "CONV_2D took 13193717 ticks (13193 ms).\r\n",
      "DEPTHWISE_CONV_2D took 1462009 ticks (1462 ms).\r\n",
      "CONV_2D took 6596093 ticks (6596 ms).\r\n",
      "DEPTHWISE_CONV_2D took 2734009 ticks (2734 ms).\r\n",
      "CONV_2D took 12199421 ticks (12199 ms).\r\n",
      "DEPTHWISE_CONV_2D took 2734009 ticks (2734 ms).\r\n",
      "CONV_2D took 12199421 ticks (12199 ms).\r\n",
      "DEPTHWISE_CONV_2D took 2734009 ticks (2734 ms).\r\n",
      "CONV_2D took 12199421 ticks (12199 ms).\r\n",
      "DEPTHWISE_CONV_2D took 2734009 ticks (2734 ms).\r\n",
      "CONV_2D took 12199421 ticks (12199 ms).\r\n",
      "DEPTHWISE_CONV_2D took 2734009 ticks (2734 ms).\r\n",
      "CONV_2D took 12199421 ticks (12199 ms).\r\n",
      "DEPTHWISE_CONV_2D took 685980 ticks (685 ms).\r\n",
      "CONV_2D took 6099809 ticks (6099 ms).\r\n",
      "DEPTHWISE_CONV_2D took 1197084 ticks (1197 ms).\r\n",
      "CONV_2D took 11703137 ticks (11703 ms).\r\n",
      "AVERAGE_POOL_2D took 75604 ticks (75 ms).\r\n",
      "CONV_2D took 10983 ticks (10 ms).\r\n",
      "RESHAPE took 290 ticks (0 ms).\r\n",
      "SOFTMAX took 1933 ticks (1 ms).\r\n",
      "```\r\n",
      "Confirmed that the kernel_conv_test passes with:\r\n",
      "\r\n",
      "```\r\n",
      "make -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=fusion_f1 XTENSA_CORE=F1_190305_swupgrade test_kernel_conv_test -j8\r\n",
      "```\r\n",
      "Progress towards http://b/177457688\n",
      "issue labels - \n",
      "cla: yes\n",
      "ready to pull\n",
      "size:L\n",
      "\n",
      "\n",
      "issue title -  Fix TFLM github CI bazel build\n",
      "issue body -  To keep the bazel build short, we maintain a copy of the subset of packages that are needed for the TFLM (+ shared TfLite) bazel targets.\r\n",
      "\r\n",
      "Eigen was updated for TF with 0effd3dc59621d79c2535e2d928d0cf41db94d3c and we make the corresponding change in TFLM's pared down version of workspace.bzl with this change.\r\n",
      "\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  No version of tensorflow will install on python 3.7.9 x64 / pip 20.1\n",
      "issue body -  - OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `OS X 11.1 (PC)`\r\n",
      "- TensorFlow installed from (source or binary): `pip`\r\n",
      "- TensorFlow version: `1.15 or any other`\r\n",
      "- Python version: `3.7.9`\r\n",
      "- Installed using virtualenv? pip? conda?: `pip`\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "\r\n",
      "installed python 3.7.9 as compatible platform with TF 1.15 via `pyenv`\r\n",
      "no version of tf will install at all:\r\n",
      "\r\n",
      "```\r\n",
      ":~$ python3 -VV\r\n",
      "Python 3.7.9 (default, Feb 24 2021, 13:04:10)\r\n",
      "[Clang 12.0.0 (clang-1200.0.32.29)]\r\n",
      ":~$ python3 -c 'import sys;print(\"%x\" % sys.maxsize, sys.maxsize > 2**32)'\r\n",
      "7fffffffffffffff True\r\n",
      ":~$ python3 -m pip install tensorflow==1.15\r\n",
      "ERROR: Could not find a version that satisfies the requirement tensorflow==1.15 (from versions: none)\r\n",
      "ERROR: No matching distribution found for tensorflow==1.15\r\n",
      "```\r\n",
      "same output with just `pip3 install tensorflow`\r\n",
      "\r\n",
      "Is there something else I am overlooking as far the python/pip version/build type?\r\n",
      "\n",
      "issue labels - \n",
      "TF 1.15\n",
      "stat:awaiting response\n",
      "subtype:macOS\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  [INTEL MKL] Remove unnecessary MKL  build macros and bug fixes\n",
      "issue body -  Remove the following MKL build options (and usage of related MACRO's):\r\n",
      "\r\n",
      "1. ENABLE_INTEL_MKL_BFLOAT16  (treat as \"TRUE\" value)\r\n",
      "2. INTEL_MKL_DNN_ONLY              (treat same as \"INTEL_MKL\")\r\n",
      "3. ENABLE_MKLDNN_V1                 (DNN 0.x code cleanup; cleanup was overwritten by later PRs).\r\n",
      "\r\n",
      "#3 fixes some bugs caused by the merge of final DNN 0.x code cleanup PR (https://github.com/tensorflow/tensorflow/pull/46370/), most MKL unit test failure.\r\n",
      "\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:mkl\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  Fixed docstring formatting for api_docs\n",
      "issue body -  The api_docs website formatting of [leaky_relu](https://www.tensorflow.org/api_docs/python/tf/nn/leaky_relu) was wrong due to a missing newline\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:ops\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  AttributeError: Can't set the attribute \"name\" when building bidirectional layer with stackedrnncell\n",
      "issue body -  <em>Please make sure that this is a bug. As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:bug_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "On Jupyter Notebook, Python 3.6, Tensorflow Version 2.4\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "I am currently trying to build a bidirectional layer using stackedrnncell. However, I am running to an attribute error but the error message is not helping me in debugging this error. \r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "Would expect the model to be build. \r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "Provide a reproducible test case that is the bare minimum necessary to generate\r\n",
      "the problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n",
      "\r\n",
      "lstm_1 = tf.keras.layers.LSTMCell(128)\r\n",
      "lstm_2 = tf.keras.layers.LSTMCell(128)\r\n",
      "lstm_fw_cell = tf.keras.layers.StackedRNNCells([lstm_1, lstm_2])\r\n",
      "lstm_fw = tf.keras.layers.RNN(lstm_fw_cell,unroll=True)\r\n",
      "\r\n",
      "lstm_3 = tf.keras.layers.LSTMCell(128)\r\n",
      "lstm_4 = tf.keras.layers.LSTMCell(128)\r\n",
      "lstm_bw_cell= tf.keras.layers.StackedRNNCells([lstm_3, lstm_4])\r\n",
      "lstm_bw = tf.keras.layers.LSTM(lstm_bw_cell,go_backwards=True,unroll=True)\r\n",
      "\r\n",
      "model = Sequential()\r\n",
      "model.add(Bidirectional(lstm_fw, backward_layer=lstm_bw))\r\n",
      "model.add(Dense(4, activation=\"softmax\"))\r\n",
      "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
      "\r\n",
      "model.summary()\r\n",
      "\r\n",
      "**Other info / logs** Include any logs or source code that would be helpful to\r\n",
      "diagnose the problem. If including tracebacks, please include the full\r\n",
      "traceback. Large logs and files should be attached.\r\n",
      "\r\n",
      "![image](https://user-images.githubusercontent.com/41911024/109015802-7a3d5300-76f0-11eb-9ed7-b6a7b3f34f04.png)\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  TFLM: Enable pooling tests for FVP build\n",
      "issue body -  Enabling pooling and bumping CMSIS version as the problem has been fixed.\r\n",
      "\r\n",
      "This progress towards: https://github.com/tensorflow/tensorflow/issues/47070\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "comp:micro:arm\n",
      "prtype:bugfix\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  sorry for mistake reference issue, please delete it\n",
      "issue body -  \n",
      "issue labels - \n",
      "\n",
      "\n",
      "issue title -  Add a status badge for Arduino examples\n",
      "issue body -  This PR adds a status badge to TFLite Micro Readme.\r\n",
      "\r\n",
      "The graphic style is slightly different to the current badges, but it's auto-generated by GitHub, so it requires no additional handling on the CI side.\r\n",
      "\r\n",
      "cc @advaitjain \n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  ssd mobilenet model size is not correct using tflite converter python api\n",
      "issue body -  ### 1. System information\r\n",
      "\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n",
      "- TensorFlow installation (pip package or built from source): pip package\r\n",
      "- TensorFlow library (version, if pip package or github SHA, if built from source): 2.4.1\r\n",
      "\r\n",
      "### 2. Code\r\n",
      "\r\n",
      "My script [SSD mobilenet convert to TF lite colab script](https://colab.research.google.com/drive/1qaGW7ViN0fW2z4bqRKL4sd8X-HIq5Vm2?usp=sharing)\r\n",
      "\r\n",
      "\r\n",
      "### 3. Failure after conversion\r\n",
      "\r\n",
      "convert offical SSD MobileNet v2 320x320 model using python api from [tf2_detection_zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md), conversion is successful, but tflite file size is too small (only 6.4 MB) and run on mobile extremely slow (900 ms).\r\n",
      "\r\n",
      "And I found following result:\r\n",
      "\r\n",
      "* if **convert to saved model and tflite both using tf 2.4**\r\n",
      "   * using tflite converter python api is 6.4M\r\n",
      "   * using command line tool (tflite_converter) is 324 bytes\r\n",
      "   * **both file is NOT correct**\r\n",
      "* if **convert to saved model using TF 2.3 and convert to tflite using tf 2.4**\r\n",
      "   * using tflite converter python api is 6.4M\r\n",
      "   * using command line tool (tflite_converter) is 24M\r\n",
      "   * **the command line tool transform result is correct** which run on mobile very fast (100 ms)!\r\n",
      "\r\n",
      "saved model convert script\r\n",
      "\r\n",
      "```bash=\r\n",
      "python object_detection/export_tflite_graph_tf2.py --pipeline_config_path ./ssd_mobilenet_v2_320x320_coco17_tpu-8/pipeline.config --trained_checkpoint_dir ./ssd_mobilenet_v2_320x320_coco17_tpu-8/checkpoint/ --output_directory ./output/\r\n",
      "```\r\n",
      "\r\n",
      "tflite converter python api convert code\r\n",
      "\r\n",
      "```python=\r\n",
      "converter = tf.lite.TFLiteConverter.from_saved_model('output/saved_model/',signature_keys=[\"serving_default\"])\r\n",
      "converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n",
      "converter.experimental_new_converter = True\r\n",
      "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\n",
      "\r\n",
      "tflite_model = converter.convert()\r\n",
      "\r\n",
      "with tf.io.gfile.GFile('output/detect_using_api.tflite', 'wb') as f:\r\n",
      "  f.write(tflite_model)\r\n",
      "```\r\n",
      "\r\n",
      "command line tool convert script\r\n",
      "\r\n",
      "```bash=\r\n",
      "tflite_convert --output_file output/detect_using_command.tflite --saved_model_dir output/saved_model/ --experimental_new_converter True\r\n",
      "```\r\n",
      "\r\n",
      "My question is\r\n",
      "\r\n",
      "Why output result is different between using tflite converter python api and command line tool?\r\n",
      "\r\n",
      "### 5. (optional) Any other info / logs\r\n",
      "N/A\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "TFLiteConverter\n",
      "comp:lite\n",
      "type:performance\n",
      "\n",
      "\n",
      "issue title -  Having issue with NotImplemented Tensor to NumPy\n",
      "issue body -  <em>Please make sure that this is a bug. As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:bug_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "Arch Linux\r\n",
      "Tensorflow version 2.4.1\r\n",
      "Python version 3.9.1\r\n",
      "\r\n",
      "I am doing the [TensorFlow tutorial](https://www.tensorflow.org/tutorials/images/classification#data_augmentation) and am getting NotImplemented errors at the Data Augmentation stage:\r\n",
      "\r\n",
      "```\r\n",
      "data_augmentation = keras.Sequential(\r\n",
      "    [\r\n",
      "        layers.experimental.preprocessing.RandomFlip(\"horizontal\",\r\n",
      "                                                     input_shape=(img_height,\r\n",
      "                                                                  img_width,\r\n",
      "                                                                  3)),\r\n",
      "        layers.experimental.preprocessing.RandomRotation(0.1),\r\n",
      "        layers.experimental.preprocessing.RandomZoom(0.1),\r\n",
      "    ]\r\n",
      ")\r\n",
      "```\r\n",
      "\r\n",
      "\r\n",
      "Error Trace:\r\n",
      "```\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/yathavan/Documents/tensorflow/flowers/flowers.py\", line 136, in <module>\r\n",
      "    data_augmentation = keras.Sequential(\r\n",
      "  File \"/usr/lib/python3.9/site-packages/tensorflow/python/training/tracking/base.py\", line 517, in _method_wrapper\r\n",
      "    result = method(self, *args, **kwargs)\r\n",
      "  File \"/usr/lib/python3.9/site-packages/tensorflow/python/keras/engine/sequential.py\", line 144, in __init__\r\n",
      "    self.add(layer)\r\n",
      "  File \"/usr/lib/python3.9/site-packages/tensorflow/python/training/tracking/base.py\", line 517, in _method_wrapper\r\n",
      "    result = method(self, *args, **kwargs)\r\n",
      "  File \"/usr/lib/python3.9/site-packages/tensorflow/python/keras/engine/sequential.py\", line 223, in add\r\n",
      "    output_tensor = layer(self.outputs[0])\r\n",
      "  File \"/usr/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 951, in __call__\r\n",
      "    return self._functional_construction_call(inputs, args, kwargs,\r\n",
      "  File \"/usr/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 1090, in _functional_construction_call\r\n",
      "    outputs = self._keras_tensor_symbolic_call(\r\n",
      "  File \"/usr/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 822, in _keras_tensor_symbolic_call\r\n",
      "    return self._infer_output_signature(inputs, args, kwargs, input_masks)\r\n",
      "  File \"/usr/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 863, in _infer_output_signature\r\n",
      "    outputs = call_fn(inputs, *args, **kwargs)\r\n",
      "  File \"/usr/lib/python3.9/site-packages/tensorflow/python/keras/layers/preprocessing/image_preprocessing.py\", line 866, in call\r\n",
      "    output = control_flow_util.smart_cond(training, random_rotated_inputs,\r\n",
      "  File \"/usr/lib/python3.9/site-packages/tensorflow/python/keras/utils/control_flow_util.py\", line 114, in smart_cond\r\n",
      "    return smart_module.smart_cond(\r\n",
      "  File \"/usr/lib/python3.9/site-packages/tensorflow/python/framework/smart_cond.py\", line 54, in smart_cond\r\n",
      "    return true_fn()\r\n",
      "  File \"/usr/lib/python3.9/site-packages/tensorflow/python/keras/layers/preprocessing/image_preprocessing.py\", line 861, in random_rotated_inputs\r\n",
      "    get_rotation_matrix(angles, img_hd, img_wd),\r\n",
      "  File \"/usr/lib/python3.9/site-packages/tensorflow/python/keras/layers/preprocessing/image_preprocessing.py\", line 757, in get_rotation_matrix\r\n",
      "    array_ops.zeros((num_angles, 2), dtypes.float32),\r\n",
      "  File \"/usr/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\r\n",
      "    return target(*args, **kwargs)\r\n",
      "  File \"/usr/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py\", line 2819, in wrapped\r\n",
      "    tensor = fun(*args, **kwargs)\r\n",
      "  File \"/usr/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py\", line 2868, in zeros\r\n",
      "    output = _constant_if_small(zero, shape, dtype, name)\r\n",
      "  File \"/usr/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py\", line 2804, in _constant_if_small\r\n",
      "    if np.prod(shape) < 1000:\r\n",
      "  File \"<__array_function__ internals>\", line 5, in prod\r\n",
      "  File \"/usr/lib/python3.9/site-packages/numpy/core/fromnumeric.py\", line 3030, in prod\r\n",
      "    return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out,\r\n",
      "  File \"/usr/lib/python3.9/site-packages/numpy/core/fromnumeric.py\", line 87, in _wrapreduction\r\n",
      "    return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\r\n",
      "  File \"/usr/lib/python3.9/site-packages/tensorflow/python/framework/ops.py\", line 852, in __array__\r\n",
      "    raise NotImplementedError(\r\n",
      "NotImplementedError: Cannot convert a symbolic Tensor (random_rotation/rotation_matrix/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported\r\n",
      "```\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:ops\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Im trying to list my gpus, but only show gpu0  and do not show gpu1 that is my nvidia\n",
      "issue body -  This template is for miscellaneous issues not covered by the other issue categories.\r\n",
      "\r\n",
      "For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n",
      "\r\n",
      "If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n",
      "\r\n",
      "For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n",
      "\n",
      "issue labels - \n",
      "stat:awaiting response\n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  Properly support complex in SparseTensorDenseMatMul\n",
      "issue body -  This adds gradient support for complex, and fixes a bug with adjoints (https://github.com/tensorflow/tensorflow/issues/47337).\r\n",
      "\r\n",
      "~My laptop can't build, so I haven't been able to properly test this yet. Hopefully CI will do it?~\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:ops\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  TensorFlow 2.4 Contains References to nocopts, which is No Longer Compatible with Bazel\n",
      "issue body -  In e5f8043742f927ed0e1711bb48f3e1a153b7a997 and ddde447e792231cdf83b435b0eeb59dd59bf4044, among other commits, support for `nocopts` was removed to enable the upgrade of Bazel to 1.0 and above.\r\n",
      "\r\n",
      "However, there are still a few references to `nocopts` in the r2.4 branch:\r\n",
      "```\r\n",
      "$ git grep nocopts\r\n",
      "tensorflow/lite/micro/testing/micro_test.bzl:        nocopts = \"\",\r\n",
      "tensorflow/lite/micro/testing/micro_test.bzl:        nocopts: list of gcc compilation flags to remove for this rule\r\n",
      "tensorflow/lite/micro/testing/micro_test.bzl:        nocopts = nocopts,\r\n",
      "tensorflow/tensorflow.bzl:    # -fno-exceptions in nocopts breaks compilation if header modules are enabled.\r\n",
      "tensorflow/tensorflow.bzl:    # -fno-exceptions in nocopts breaks compilation if header modules are enabled.\r\n",
      "```\r\n",
      "\r\n",
      "These should probably be removed for consistency and to avoid confusion.\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:lite\n",
      "comp:micro\n",
      "stat:awaiting tensorflower\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  When will tensorflow be compatible with CUDA 11.2?\n",
      "issue body -  - TensorFlow version: 2.5 (nightly)\r\n",
      "- GPU: RTX 3070\r\n",
      "- CUDA version: 11.2\r\n",
      "- cuDNN version: 8\r\n",
      "\r\n",
      "Right now, tensorflow is unable to recognize my GPU, so I'm not able to take advantage of the hardware. I read that others have been having the same issue with the newer RTX cards and it's due to tensorflow not supporting the latest CUDA versions. Please make this update, so we can leverage these newer cards. They're pretty hard to get right now, so it was disappointing to find out I couldn't use it.\r\n",
      "\n",
      "issue labels - \n",
      "comp:gpu\n",
      "stat:awaiting response\n",
      "type:feature\n",
      "\n",
      "\n",
      "issue title -  fix Windows debug build\n",
      "issue body -  PDB file format has internal 32-bit limits, which doesn't allow PDB files to grow beyond 4GB even on x64 builds\r\n",
      "\r\n",
      "thus use reduced debug symbols set in order not to exceed 4GB limit\n",
      "issue labels - \n",
      "cla: yes\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  fix Linux build error\n",
      "issue body -  fix some more issues of the kind already addressed in #42745\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:core\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Download from https://mirror.bazel.build/github.com/aws/aws-sdk-cpp/archive/1.7.336.tar.gz failed\n",
      "issue body -  ### System information\r\n",
      "\r\n",
      "-   **Have I written custom code (as opposed to using a stock example script\r\n",
      "    provided in TensorFlow)**: no\r\n",
      "-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: windows 10\r\n",
      "-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n",
      "    happens on a mobile device**:\r\n",
      "-   **TensorFlow installed from (source or binary)**: source\r\n",
      "-   **TensorFlow version (use command below)**: 2.4.0\r\n",
      "-   **Python version**: 3.8.8\r\n",
      "-   **Bazel version (if compiling from source)**: 3.1.0\r\n",
      "-   **GCC/Compiler version (if compiling from source)**: MSVC 2019\r\n",
      "-   **CUDA/cuDNN version**: 11.0.3/8.0.5\r\n",
      "-   **GPU model and memory**: GTX 1650 Ti, 4GB\r\n",
      "-   **Exact command to reproduce**:\r\n",
      "\r\n",
      "### Describe the problem\r\n",
      "Bazel fails to download 1.7.336.tar.gz, then unable to unzip an empty simple_console_for_windows.zip (size is 0b).\r\n",
      "\r\n",
      "### Source code / logs\r\n",
      "```PS C:\\tensorflow> bazel build //tensorflow/tools/pip_package:build_pip_package\r\n",
      "PS C:\\tensorflow> bazel build //tensorflow/tools/pip_package:build_pip_package\r\n",
      "Starting local Bazel server and connecting to it...\r\n",
      "INFO: Options provided by the client:\r\n",
      "  Inherited 'common' options: --isatty=1 --terminal_columns=120\r\n",
      "INFO: Reading rc options for 'build' from c:\\tensorflow\\.bazelrc:\r\n",
      "  Inherited 'common' options: --experimental_repo_remote_exec\r\n",
      "INFO: Options provided by the client:\r\n",
      "  'build' options: --python_path=C:/Python38/python.exe\r\n",
      "INFO: Reading rc options for 'build' from c:\\tensorflow\\.bazelrc:\r\n",
      "  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2\r\n",
      "INFO: Reading rc options for 'build' from c:\\tensorflow\\.tf_configure.bazelrc:\r\n",
      "  'build' options: --action_env PYTHON_BIN_PATH=C:/Python38/python.exe --action_env PYTHON_LIB_PATH=C:/Python38/lib/site-packages --python_path=C:/Python38/python.exe --config=xla --action_env CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.0 --action_env TF_CUDA_COMPUTE_CAPABILITIES=7.0 --config=cuda --define=override_eigen_strong_inline=true --action_env TF_CONFIGURE_IOS=0\r\n",
      "INFO: Found applicable config definition build:short_logs in file c:\\tensorflow\\.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\n",
      "INFO: Found applicable config definition build:v2 in file c:\\tensorflow\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\n",
      "INFO: Found applicable config definition build:xla in file c:\\tensorflow\\.bazelrc: --define=with_xla_support=true\r\n",
      "INFO: Found applicable config definition build:cuda in file c:\\tensorflow\\.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\n",
      "INFO: Found applicable config definition build:using_cuda in file c:\\tensorflow\\.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1\r\n",
      "INFO: Found applicable config definition build:windows in file c:\\tensorflow\\.bazelrc: --copt=/W0 --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --experimental_strict_action_env=true --verbose_failures --distinct_host_configuration=false\r\n",
      "INFO: Found applicable config definition build:monolithic in file c:\\tensorflow\\.bazelrc: --define framework_shared_object=false\r\n",
      "**WARNING: Download from https://mirror.bazel.build/github.com/aws/aws-sdk-cpp/archive/1.7.336.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found**\r\n",
      "INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (410 packages loaded, 26438 targets configured).\r\n",
      "INFO: Found 1 target...\r\n",
      "Target //tensorflow/tools/pip_package:build_pip_package up-to-date:\r\n",
      "  bazel-bin/tensorflow/tools/pip_package/build_pip_package\r\n",
      "  bazel-bin/tensorflow/tools/pip_package/build_pip_package.exe\r\n",
      "INFO: Elapsed time: 6455.931s, Critical Path: 785.27s\r\n",
      "INFO: 10307 processes: 10307 local.\r\n",
      "INFO: Build completed successfully, 14648 total actions\r\n",
      "PS C:\\tensorflow> bazel-bin\\tensorflow\\tools\\pip_package\\build_pip_package C:/tmp/tensorflow_pkg\r\n",
      "Tue Feb 23 16:54:05 CEST 2021 : === Preparing sources in dir: /tmp/tmp.ZkbeuBDQsm\r\n",
      "Unzipping simple_console_for_windows.zip to create runfiles tree...\r\n",
      "[./bazel-bin/tensorflow/tools/pip_package/simple_console_for_windows.zip]\r\n",
      "  End-of-central-directory signature not found.  Either this file is not\r\n",
      "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\r\n",
      "  latter case the central directory and zipfile comment will be found on\r\n",
      "  the last disk(s) of this archive.\r\n",
      "unzip:  cannot find zipfile directory in one of ./bazel-bin/tensorflow/tools/pip_package/simple_console_for_windows.zip or\r\n",
      "        ./bazel-bin/tensorflow/tools/pip_package/simple_console_for_windows.zip.zip, and cannot find ./bazel-bin/tensorflow/tools/pip_package/simple_console_for_windows.zip.ZIP, period.```\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "stat:awaiting response\n",
      "subtype:windows\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  Proposal to use newer oneDNN version in the core\n",
      "issue body -  **System information**\r\n",
      "- TensorFlow version (you are using): v2.4.1/r2.4\r\n",
      "- Are you willing to contribute it: It depends on TF Community plans\r\n",
      "\r\n",
      "**Describe the feature and the current behavior/state.**\r\n",
      "At the moment TensorFlow employs [oneDNN v1.6.4](https://github.com/tensorflow/tensorflow/blob/r2.4/tensorflow/workspace.bzl#L172-L181) for core computationally intensive routines. Meanwhile oneDNN is approaching [to v2.2](https://github.com/oneapi-src/oneDNN/milestones) release. May you please clarify if there are any plans for the upcoming TF releases to move to a newer oneDNN version?\r\n",
      "\r\n",
      "**Will this change the current api? How?**\r\n",
      "It should not change the current API, I built TensorFlow with oneDNN master and it worked fine.\r\n",
      "\r\n",
      "**Who will benefit with this feature?**\r\n",
      "Newer versions of oneDNN have better support for and performance on AArch64, the main benefits will be for the users running TensorFlow on AArch64-based machines.\r\n",
      "\r\n",
      "**Any Other info.**\r\n",
      "This issue is mainly a question to understand TensorFlow Community plans for this topic.\n",
      "issue labels - \n",
      "comp:apis\n",
      "stat:awaiting tensorflower\n",
      "type:feature\n",
      "\n",
      "\n",
      "issue title -  support of TFlite of Snapdragon888 dsp\n",
      "issue body -  Hi @shuki-k \r\n",
      "Sadly this is not possible.\r\n",
      "These new chips uses a newer version of HVX that is different than what Hexagon delegate supports.\r\n",
      "\r\n",
      "_Originally posted by @karimnosseir in https://github.com/tensorflow/tensorflow/issues/47246#issuecomment-783807697_\n",
      "issue labels - \n",
      "\n",
      "\n",
      "issue title -  How to convert a TF1 model with custom_getter to TF2\n",
      "issue body -  I need to do this very same calculation in TF2 which ends up by calculating `v1` and `v2` which are softmax output and the same softmax output but calculated using a \r\n",
      "`tf.train.ExponentialMovingAverage` `average()` method. The original example can be found [here](https://github.com/openai/baselines/blob/master/baselines/acer/acer.py) at line 88 but I simplified the code in the example here for readability.\r\n",
      "\r\n",
      "**TF1 code:**\r\n",
      "\r\n",
      "    import tensorflow as tf\r\n",
      "    \r\n",
      "    \r\n",
      "    with tf.variable_scope('my_model', reuse=tf.AUTO_REUSE):\r\n",
      "        step_model = StepModel(step_param)  # step_param is a placeholder\r\n",
      "        train_model = TrainModel(train_param)  # train_param is a placeholder\r\n",
      "    \r\n",
      "    params = tf.trainable_variables('my_model')\r\n",
      "    ema = tf.train.ExponentialMovingAverage(0.99)\r\n",
      "    ema_apply_op = ema.apply(params)\r\n",
      "    \r\n",
      "    def custom_getter(getter, *args, **kwargs):\r\n",
      "        return ema.average(getter(*args, **kwargs))\r\n",
      "    \r\n",
      "    with tf.variable_scope(\"my_model\", custom_getter=custom_getter, reuse=True):\r\n",
      "        avg_model = AveragingModel(avg_param)  # avg_param is a placeholder\r\n",
      "    \r\n",
      "    train_model_p = tf.nn.softmax(train_model.pi)\r\n",
      "    avg_model_p = tf.nn.softmax(avg_model.pi)\r\n",
      "    common_param = some_param\r\n",
      "    with tf.get_default_session() as sess:\r\n",
      "        feed_dict = {\r\n",
      "            train_model.train_param: common_param,\r\n",
      "            avg_model.avg_param: common_param,\r\n",
      "        }\r\n",
      "        ops = [train_model_p, avg_model_p]\r\n",
      "        v1, v2 = sess.run(ops, feed_dict)[1:]\r\n",
      "\r\n",
      "If I want to keep a moving average of the trainable variables in TF2 without worrying about `v1` and `v2`, I'd do:\r\n",
      "\r\n",
      "\r\n",
      "    from tensorflow.keras.optimizers import Adam\r\n",
      "    from tensorflow_addons.optimizers import MovingAverage\r\n",
      "    from tensorflow.keras.models import Model\r\n",
      "\r\n",
      "\r\n",
      "and then I'd wrap the `tf.keras.optimizers.Optimizer()` being used:\r\n",
      "\r\n",
      "    \r\n",
      "    model = Model(...)\r\n",
      "    optim = MovingAverage(Adam())\r\n",
      "    model.compile(optimizer=optim)\r\n",
      "    model.fit(...)\r\n",
      "\r\n",
      "But I need to get the values of `v1` and `v2` for further calculations.\r\n",
      "    \n",
      "issue labels - \n",
      "comp:keras\n",
      "stat:awaiting response\n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  ModelCheckPoint callback creates \".png\" file instead of \".h5\" file \n",
      "issue body -  <em>Please make sure that this is a bug. As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:bug_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "- TensorFlow installed from (source or binary):\r\n",
      "- TensorFlow version (use command below):\r\n",
      "- Python version:\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version:\r\n",
      "- GPU model and memgory:\r\n",
      "\r\n",
      "You can collect some of this information using our environment capture\r\n",
      "[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n",
      "You can also obtain the TensorFlow version with:\r\n",
      "1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n",
      "2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n",
      "\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "if we already have a saved model say \"model_name.h5\" in models directory and then we use ModelCheckPoint callback and provide the same name(\"model_name.h5\") to save the model. After the training is finished we get \"model_name.png\" file instead of \"model_name.h5\" file. \r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "The user should get \"model_name.h5\" file instead of \"model_name.png\" after training.\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "Provide a reproducible test case that is the bare minimum necessary to generate\r\n",
      "the problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n",
      "\r\n",
      "https://colab.research.google.com/drive/1rBD6SZwjU9wgxfsgNn5COeaDmCpGI9IQ#scrollTo=0qqWjf-v1lVw\r\n",
      "\r\n",
      "Note: run the training cell twice. first time to generate \"model_name.h5\" file then when you run the training cell second time, we can observe \"model_name.png\" is generated \r\n",
      "\r\n",
      "**Other info / logs** Include any logs or source code that would be helpful to\r\n",
      "diagnose the problem. If including tracebacks, please include the full\r\n",
      "traceback. Large logs and files should be attached.\r\n",
      "\n",
      "issue labels - \n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  TFLM: Add FVP script to CI script\n",
      "issue body -  This is a fix for: https://github.com/tensorflow/tensorflow/issues/46829\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "comp:micro:arm\n",
      "prtype:bugfix\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  SparseTensorDenseMatMul adjoint_a takes transpose, not adjoint\n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.15.5\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n",
      "- TensorFlow installed from (source or binary): binary\r\n",
      "- TensorFlow version (use command below): v2.4.0-49-g85c8b2a817f 2.4.1\r\n",
      "- Python version: 3.7.6\r\n",
      "- Bazel version (if compiling from source): N/A\r\n",
      "- GCC/Compiler version (if compiling from source): N/A\r\n",
      "- CUDA/cuDNN version: N/A\r\n",
      "- GPU model and memory: N/A\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "On CPU (haven't checked GPU), the `adjoint_a` argument to `SparseTensorDenseMatMul` op (accessed via `tf.sparse.sparse_dense_matmul`) takes the transpose of the argument but not the conjugate.\r\n",
      "\r\n",
      "Looks like the issue is here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/sparse_tensor_dense_matmul_op.h#L55; shouldn't that be taking a conjugate?\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "`adjoint_a` takes the adjoint of `a`.\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "```\r\n",
      "import tensorflow as tf\r\n",
      "x = tf.sparse.from_dense([[1j]])\r\n",
      "y = tf.constant([[1]], dtype=tf.complex128)\r\n",
      "print(tf.sparse.sparse_dense_matmul(x, y, adjoint_a=True).numpy()) # Gives [[1j]]\r\n",
      "```\r\n",
      "See gist here: https://colab.research.google.com/drive/1qRZo4q1qwwt_pt6EfxRJPVohXmDUrdvN?usp=sharing\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:ops\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  How to extend self attention in transformer with local attention\n",
      "issue body -  Hello everyone.. I'm trying (Vaswani et al, 2017) for machine translation task. Some of the recent findings (\"https://www.aclweb.org/anthology/P19-1295/\" and \"https://www.aclweb.org/anthology/D19-5622/\" ) show that number of heads can extract redundant features as original work by Vaswani et al, 2017 used global attention. These papers suggested to use local attention along with global attention. So, I'm trying to extent Vaswani et al, 2017 with local attention as suggested by \"https://www.aclweb.org/anthology/D19-5622/\". \r\n",
      "In the paper, suggested global mask as follows:\r\n",
      "![global mask](https://user-images.githubusercontent.com/60576100/108811216-df9b2200-75d2-11eb-9ca4-55c3e4a398c2.png)\r\n",
      "\r\n",
      "and local mask as: \r\n",
      "![local mask](https://user-images.githubusercontent.com/60576100/108811141-b67a9180-75d2-11eb-89af-2aed43ace7a1.png)\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "I have created global and local mask as follows: \r\n",
      "        seq_len = tf.shape(seq)[1]\r\n",
      "        global_mask = tf.zeros((tf.shape(seq)[0], tf.shape(seq)[1]))\r\n",
      "                \r\n",
      "        local_mask = global_mask\r\n",
      "        local_mask=tf.Variable(local_mask)\r\n",
      "        for i in range(0, tf.shape(seq)[0]):\r\n",
      "          for j in range(0,tf.shape(seq)[1]):\r\n",
      "            if not (i-w<=j and j<=i+w):\r\n",
      "              local_mask[i,j].assign(-1e9) \r\n",
      "        local_mask = tf.convert_to_tensor(local_mask)\r\n",
      "\r\n",
      "       global_mask=global_mask[:,tf.newaxis,tf.newaxis,:]\r\n",
      "       local_mask=local_mask[:,tf.newaxis,tf.newaxis,:]\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "comp:apis\n",
      "stat:awaiting response\n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  Refactor depthwise_conv to share code between reference and optimized kernels\n",
      "issue body -  Move shared structs / helper functions into depthwise_conv_common.cc\r\n",
      "Clean up some of the existing code to directly call the reference implementations (made possible by the refactor of the helper functions).\r\n",
      "\r\n",
      "Tested with:\r\n",
      "make -j8 -f tensorflow/lite/micro/tools/make/Makefile test_kernel_depthwise_conv_test\r\n",
      "\r\n",
      "make -j8 -f tensorflow/lite/micro/tools/make/Makefile OPTIMIZED_KERNEL_DIR=cmsis_nn TARGET=stm32f4 test_kernel_depthwise_conv_test\r\n",
      "\r\n",
      "make -f tensorflow/lite/micro/tools/make/Makefile -j8 TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=hifimini XTENSA_CORE=mini1m1m_RG test_kernel_depthwise_conv_test\r\n",
      "\r\n",
      "Progress towards http://b/177457688\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:lite\n",
      "ready to pull\n",
      "size:XL\n",
      "\n",
      "\n",
      "issue title -  [TFL] Support I32 for OptimizeSlice pattern\n",
      "issue body -  Somehow, some int32 `begin` will be generated. This PR extends the original pattern to support i32 `begin` because we have type constraint of `TFL_I32OrI64Tensor` on `begin` in ODS. Pattern is found in `tf.keras.layers.MultiHeadAttention`, for example.\r\n",
      "\r\n",
      "```python\r\n",
      "import tensorflow as tf\r\n",
      "\r\n",
      "layer = tf.keras.layers.MultiHeadAttention(num_heads=2, key_dim=2)\r\n",
      "target = tf.keras.Input(shape=[8, 16], batch_size=1)\r\n",
      "source = tf.keras.Input(shape=[4, 16], batch_size=1)\r\n",
      "output_tensor = layer(target, source, return_attention_scores=False)\r\n",
      "model = tf.keras.Model([target, source], output_tensor)\r\n",
      "```\r\n",
      "\r\n",
      "Also fix:\r\n",
      "- when input has dynamic shape.\r\n",
      "- when shape has -1.\r\n",
      "\r\n",
      "It turns out that we have to check if `shape` is the shape of `input`. Otherwise, given `input: tensor<?xf32>` and `output: tensor<?xf32>`, the original pattern is not going to work.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:lite\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  ImportError: cannot import name 'keras_modules_injection'\n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code and  used `from tensorflow.python.keras.applications import keras_modules_injection` in Tensorflow 2.0, it worked, now I changed to Tensorflow 2.4, and facing error `ImportError: cannot import name 'keras_modules_injection'`\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n",
      "- TensorFlow installed from (source or binary):binary\r\n",
      "- TensorFlow version (use command below): 2.4.1\r\n",
      "- Python version: Python 3.6.9\r\n",
      "- CUDA/cuDNN version: CUDA 11\r\n",
      "- GPU model and memory: Titan XP\r\n",
      "\r\n",
      "Checking TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n",
      "\r\n",
      "`2021-02-22 20:44:00.220774: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "v2.4.0-49-g85c8b2a817f 2.4.1`\r\n",
      "\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "python test_tf_imports.py \r\n",
      "2021-02-22 20:24:25.627138: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"test_tf_imports.py\", line 2, in <module>\r\n",
      "    from tensorflow.python.keras.applications import keras_modules_injection\r\n",
      "ImportError: cannot import name 'keras_modules_injection'\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "With the line of code `from tensorflow.python.keras.applications import keras_modules_injection` which was running in TF2.0, should run in TF2.4.x\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "from tensorflow.python.keras.applications import keras_modules_injection\r\n",
      "from tensorflow.python.util.tf_export import keras_export\r\n",
      "\r\n",
      "@keras_export('keras.applications.resnet50.ResNet50',\r\n",
      "              'keras.applications.ResNet50')\r\n",
      "@keras_modules_injection\r\n",
      "def ResNet50(*args, **kwargs):\r\n",
      "  return resnet50.ResNet50(*args, **kwargs)\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:keras\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Add some more notes for running a binary with renode.\n",
      "issue body -  The additional notes were helpful when I was working on https://github.com/tensorflow/tensorflow/pull/47276\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string\n",
      "issue body -  Opening a new bug per @nikitamaia's suggestion. See discussion in #28007, in which she confirmed the bug. Bug only presents when using GPU, not CPU.\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.2 LTS\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "- TensorFlow installed from (source or binary): binary\r\n",
      "- TensorFlow version (use command below): 2.4.1\r\n",
      "- Python version: 3.8.5\r\n",
      "- Bazel version (if compiling from source): n/a\r\n",
      "- GCC/Compiler version (if compiling from source): n/a\r\n",
      "- CUDA/cuDNN version: 2.4.1/8\r\n",
      "- GPU model and memory: GeForce GTX 1050 Ti computeCapability: 6.1, 3.95GiB\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "Error during function call for compiled function:\r\n",
      "`Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string`\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "Compiled function call should succeed like uncompiled function call.\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "```python\r\n",
      "import tensorflow as tf\r\n",
      "\r\n",
      "\r\n",
      "def _get_char_flags(char_tensor):\r\n",
      "    char = char_tensor.numpy().decode()\r\n",
      "    return char.isalpha(), char.isspace()\r\n",
      "\r\n",
      "\r\n",
      "@tf.function  # <--------------- Runs fine with this line commented out.\r\n",
      "def tf_split_text(text):\r\n",
      "    tf.assert_rank(text, 0)\r\n",
      "    tf.debugging.assert_type(text, tf.string)\r\n",
      "\r\n",
      "    chars = tf.strings.unicode_split(text, input_encoding='UTF-8')\r\n",
      "    is_alpha, is_space = tf.map_fn(lambda char: tf.py_function(_get_char_flags, char, Tout=[tf.bool, tf.bool]),\r\n",
      "                                   (chars,), dtype=tf.bool, parallel_iterations=True,\r\n",
      "                                   fn_output_signature=[tf.bool, tf.bool])\r\n",
      "\r\n",
      "    is_alpha = tf.concat([is_alpha, [False]], axis=0)\r\n",
      "    is_space = tf.concat([is_space, [True]], axis=0)\r\n",
      "\r\n",
      "    is_special = ~(is_alpha | is_space)\r\n",
      "    is_non_alpha = ~is_alpha\r\n",
      "    is_non_space = ~is_space\r\n",
      "\r\n",
      "    was_special = tf.concat([[False], is_special[:-1]], axis=0)\r\n",
      "    was_non_alpha = tf.concat([[True], is_non_alpha[:-1]], axis=0)\r\n",
      "    was_non_space = tf.concat([[False], is_non_space[:-1]], axis=0)\r\n",
      "\r\n",
      "    any_to_special = is_special\r\n",
      "    non_alpha_to_non_space = was_non_alpha & is_non_space\r\n",
      "    token_start_flags = any_to_special | non_alpha_to_non_space\r\n",
      "    token_start_indices = tf.where(token_start_flags)[:, 0]\r\n",
      "\r\n",
      "    special_to_any = was_special\r\n",
      "    non_space_to_non_alpha = was_non_space & is_non_alpha\r\n",
      "    token_end_flags = special_to_any | non_space_to_non_alpha\r\n",
      "    token_end_indices = tf.where(token_end_flags)[:, 0]\r\n",
      "\r\n",
      "    tf.debugging.assert_equal(tf.size(token_start_indices), tf.size(token_end_indices))\r\n",
      "\r\n",
      "    preceding_space = tf.concat([[False], is_space[:-1]], axis=0)\r\n",
      "\r\n",
      "    tokens = tf.strings.substr(text, token_start_indices, token_end_indices - token_start_indices, unit='UTF8_CHAR')\r\n",
      "    has_preceding_space = tf.gather(preceding_space, token_start_indices)\r\n",
      "\r\n",
      "    tf.assert_rank(has_preceding_space, 1)\r\n",
      "    tf.assert_rank(tokens, 1)\r\n",
      "    tf.assert_equal(tf.reduce_sum(tf.map_fn(tf.strings.length, tokens, fn_output_signature=tf.int32)) +\r\n",
      "                    tf.reduce_sum(tf.cast(has_preceding_space, tf.int32)),\r\n",
      "                    tf.strings.length(text))\r\n",
      "    return has_preceding_space, tokens\r\n",
      "\r\n",
      "\r\n",
      "text = tf.constant('hi there', tf.string)\r\n",
      "preceding_spaces, words = tf_split_text(text)\r\n",
      "print(words)\r\n",
      "```\r\n",
      "\r\n",
      "**Other info / logs**\r\n",
      "\r\n",
      "Traceback:\r\n",
      "```python\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/hosford42/PycharmProjects/ImageParser/error.py\", line 56, in <module>\r\n",
      "    preceding_spaces, words = tf_split_text(text)\r\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\", line 828, in __call__\r\n",
      "    result = self._call(*args, **kwds)\r\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\", line 894, in _call\r\n",
      "    return self._concrete_stateful_fn._call_flat(\r\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\", line 1918, in _call_flat\r\n",
      "    return self._build_call_outputs(self._inference_function.call(\r\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\", line 555, in call\r\n",
      "    outputs = execute.execute(\r\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\r\n",
      "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.\r\n",
      "  (0) Invalid argument:  2 root error(s) found.\r\n",
      "  (0) Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string\r\n",
      "  (1) Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string\r\n",
      "0 successful operations.\r\n",
      "0 derived errors ignored.\r\n",
      "\t [[{{node map_1/TensorArrayUnstack/TensorListFromTensor/_96}}]]\r\n",
      "\t [[map_1/while/loop_body_control/_61/_107]]\r\n",
      "  (1) Invalid argument:  2 root error(s) found.\r\n",
      "  (0) Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string\r\n",
      "  (1) Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string\r\n",
      "0 successful operations.\r\n",
      "0 derived errors ignored.\r\n",
      "\t [[{{node map_1/TensorArrayUnstack/TensorListFromTensor/_96}}]]\r\n",
      "0 successful operations.\r\n",
      "0 derived errors ignored. [Op:__inference_tf_split_text_265]\r\n",
      "\r\n",
      "Function call stack:\r\n",
      "tf_split_text -> tf_split_text\r\n",
      "```\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:gpu\n",
      "stat:awaiting tensorflower\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Using GridSearchCV in a regression with Keras:  TypeError: cannot pickle '_thread.RLock' object\n",
      "issue body -  I'm trying to use GridSearchCV in a regression with a Keras neural network.\r\n",
      "The data I'm using is the Boston Housing Price dataset, which was loaded directly from keras `boston_housing.load_data()`. The following is a code snippet of I'm trying to do.\r\n",
      "\r\n",
      "    def build_model():\r\n",
      "        model=models.Sequential()\r\n",
      "        model.add(layers.Dense(64,activation=\"relu\",\r\n",
      "                              input_shape=(train_data_norm.shape[1],)))\r\n",
      "        model.add(layers.Dense(64,activation=\"relu\"))\r\n",
      "        model.add(layers.Dense(1))\r\n",
      "        model.compile(optimizer='rmsprop',loss=\"mse\",metrics=[\"mae\"])\r\n",
      "        return model \r\n",
      "    \r\n",
      "    from sklearn.model_selection import GridSearchCV\r\n",
      "    \r\n",
      "    from keras.wrappers.scikit_learn import KerasRegressor\r\n",
      "    \r\n",
      "    model=KerasRegressor(build_fn=build_model(),epochs=30)\r\n",
      "    \r\n",
      "    param_grid = {\"epochs\":list(range(1,51))}\r\n",
      "    \r\n",
      "    grid_model=GridSearchCV(model,param_grid,cv=4)\r\n",
      "    \r\n",
      "    grid_model.fit(train_data_norm, train_targets)\r\n",
      "\r\n",
      "And I get the following error message: \r\n",
      "\r\n",
      "    TypeError Traceback (most recent call last)\r\n",
      "    <ipython-input-185-3fa5fc34b6b0> in <module>\r\n",
      "          9 grid_model=GridSearchCV(model,param_grid,cv=4)\r\n",
      "         10 \r\n",
      "    ---> 11 grid_model.fit(train_data_norm, train_targets)\r\n",
      "    \r\n",
      "    ~\\anaconda3\\envs\\PythonCPU\\lib\\site-packages\\sklearn\\utils\\validation.py in inner_f(*args, **kwargs)\r\n",
      "         70                           FutureWarning)\r\n",
      "         71         kwargs.update({k: arg for k, arg in zip(sig.parameters, args)})\r\n",
      "    ---> 72         return f(**kwargs)\r\n",
      "         73     return inner_f\r\n",
      "         74 \r\n",
      "    \r\n",
      "    ~\\anaconda3\\envs\\PythonCPU\\lib\\site-packages\\sklearn\\model_selection\\_search.py in fit(self, X, y, groups, **fit_params)\r\n",
      "        679         n_splits = cv.get_n_splits(X, y, groups)\r\n",
      "        680 \r\n",
      "    --> 681         base_estimator = clone(self.estimator)\r\n",
      "        682 \r\n",
      "        683         parallel = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\r\n",
      "    \r\n",
      "    ~\\anaconda3\\envs\\PythonCPU\\lib\\site-packages\\sklearn\\utils\\validation.py in inner_f(*args, **kwargs)\r\n",
      "         70                           FutureWarning)\r\n",
      "         71         kwargs.update({k: arg for k, arg in zip(sig.parameters, args)})\r\n",
      "    ---> 72         return f(**kwargs)\r\n",
      "         73     return inner_f\r\n",
      "         74 \r\n",
      "    \r\n",
      "    ~\\anaconda3\\envs\\PythonCPU\\lib\\site-packages\\sklearn\\base.py in clone(estimator, safe)\r\n",
      "         85     new_object_params = estimator.get_params(deep=False)\r\n",
      "         86     for name, param in new_object_params.items():\r\n",
      "    ---> 87         new_object_params[name] = clone(param, safe=False)\r\n",
      "         88     new_object = klass(**new_object_params)\r\n",
      "         89     params_set = new_object.get_params(deep=False)\r\n",
      "    \r\n",
      "    ~\\anaconda3\\envs\\PythonCPU\\lib\\site-packages\\sklearn\\utils\\validation.py in inner_f(*args, **kwargs)\r\n",
      "         70                           FutureWarning)\r\n",
      "         71         kwargs.update({k: arg for k, arg in zip(sig.parameters, args)})\r\n",
      "    ---> 72         return f(**kwargs)\r\n",
      "         73     return inner_f\r\n",
      "         74 \r\n",
      "    \r\n",
      "    ~\\anaconda3\\envs\\PythonCPU\\lib\\site-packages\\sklearn\\base.py in clone(estimator, safe)\r\n",
      "         69     elif not hasattr(estimator, 'get_params') or isinstance(estimator, type):\r\n",
      "         70         if not safe:\r\n",
      "    ---> 71             return copy.deepcopy(estimator)\r\n",
      "         72         else:\r\n",
      "         73             if isinstance(estimator, type):\r\n",
      "    \r\n",
      "    ~\\anaconda3\\envs\\PythonCPU\\lib\\copy.py in deepcopy(x, memo, _nil)\r\n",
      "        170                     y = x\r\n",
      "        171                 else:\r\n",
      "    --> 172                     y = _reconstruct(x, memo, *rv)\r\n",
      "        173 \r\n",
      "        174     # If is its own copy, don't memoize.\r\n",
      "    \r\n",
      "    ~\\anaconda3\\envs\\PythonCPU\\lib\\copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)\r\n",
      "        268     if state is not None:\r\n",
      "        269         if deep:\r\n",
      "    --> 270             state = deepcopy(state, memo)\r\n",
      "        271         if hasattr(y, '__setstate__'):\r\n",
      "        272             y.__setstate__(state)\r\n",
      "    \r\n",
      "    ~\\anaconda3\\envs\\PythonCPU\\lib\\copy.py in deepcopy(x, memo, _nil)\r\n",
      "        144     copier = _deepcopy_dispatch.get(cls)\r\n",
      "        145     if copier is not None:\r\n",
      "    --> 146         y = copier(x, memo)\r\n",
      "        147     else:\r\n",
      "        148         if issubclass(cls, type):\r\n",
      "    \r\n",
      "    ~\\anaconda3\\envs\\PythonCPU\\lib\\copy.py in _deepcopy_dict(x, memo, deepcopy)\r\n",
      "        228     memo[id(x)] = y\r\n",
      "        229     for key, value in x.items():\r\n",
      "    --> 230         y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n",
      "        231     return y\r\n",
      "        232 d[dict] = _deepcopy_dict\r\n",
      "    \r\n",
      "    ~\\anaconda3\\envs\\PythonCPU\\lib\\copy.py in deepcopy(x, memo, _nil)\r\n",
      "        144     copier = _deepcopy_dispatch.get(cls)\r\n",
      "        145     if copier is not None:\r\n",
      "    --> 146         y = copier(x, memo)\r\n",
      "        147     else:\r\n",
      "        148         if issubclass(cls, type):\r\n",
      "    \r\n",
      "    ~\\anaconda3\\envs\\PythonCPU\\lib\\copy.py in _deepcopy_list(x, memo, deepcopy)\r\n",
      "        203     append = y.append\r\n",
      "        204     for a in x:\r\n",
      "    --> 205         append(deepcopy(a, memo))\r\n",
      "        206     return y\r\n",
      "        207 d[list] = _deepcopy_list\r\n",
      "    \r\n",
      "    ~\\anaconda3\\envs\\PythonCPU\\lib\\copy.py in deepcopy(x, memo, _nil)\r\n",
      "        170                     y = x\r\n",
      "        171                 else:\r\n",
      "    --> 172                     y = _reconstruct(x, memo, *rv)\r\n",
      "        173 \r\n",
      "        174     # If is its own copy, don't memoize.\r\n",
      "    \r\n",
      "    ~\\anaconda3\\envs\\PythonCPU\\lib\\copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)\r\n",
      "        268     if state is not None:\r\n",
      "        269         if deep:\r\n",
      "    --> 270             state = deepcopy(state, memo)\r\n",
      "        271         if hasattr(y, '__setstate__'):\r\n",
      "        272             y.__setstate__(state)\r\n",
      "    \r\n",
      "    ~\\anaconda3\\envs\\PythonCPU\\lib\\copy.py in deepcopy(x, memo, _nil)\r\n",
      "        144     copier = _deepcopy_dispatch.get(cls)\r\n",
      "        145     if copier is not None:\r\n",
      "    --> 146         y = copier(x, memo)\r\n",
      "        147     else:\r\n",
      "        148         if issubclass(cls, type):\r\n",
      "    \r\n",
      "    ~\\anaconda3\\envs\\PythonCPU\\lib\\copy.py in _deepcopy_dict(x, memo, deepcopy)\r\n",
      "        228     memo[id(x)] = y\r\n",
      "        229     for key, value in x.items():\r\n",
      "    --> 230         y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n",
      "        231     return y\r\n",
      "        232 d[dict] = _deepcopy_dict\r\n",
      "    \r\n",
      "    ~\\anaconda3\\envs\\PythonCPU\\lib\\copy.py in deepcopy(x, memo, _nil)\r\n",
      "        144     copier = _deepcopy_dispatch.get(cls)\r\n",
      "        145     if copier is not None:\r\n",
      "    --> 146         y = copier(x, memo)\r\n",
      "        147     else:\r\n",
      "        148         if issubclass(cls, type):\r\n",
      "    \r\n",
      "    ~\\anaconda3\\envs\\PythonCPU\\lib\\copy.py in _deepcopy_list(x, memo, deepcopy)\r\n",
      "        203     append = y.append\r\n",
      "        204     for a in x:\r\n",
      "    --> 205         append(deepcopy(a, memo))\r\n",
      "        206     return y\r\n",
      "        207 d[list] = _deepcopy_list\r\n",
      "    \r\n",
      "    ~\\anaconda3\\envs\\PythonCPU\\lib\\copy.py in deepcopy(x, memo, _nil)\r\n",
      "        170                     y = x\r\n",
      "        171                 else:\r\n",
      "    --> 172                     y = _reconstruct(x, memo, *rv)\r\n",
      "        173 \r\n",
      "        174     # If is its own copy, don't memoize.\r\n",
      "    \r\n",
      "    ~\\anaconda3\\envs\\PythonCPU\\lib\\copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)\r\n",
      "        268     if state is not None:\r\n",
      "        269         if deep:\r\n",
      "    --> 270             state = deepcopy(state, memo)\r\n",
      "        271         if hasattr(y, '__setstate__'):\r\n",
      "        272             y.__setstate__(state)\r\n",
      "    \r\n",
      "    ~\\anaconda3\\envs\\PythonCPU\\lib\\copy.py in deepcopy(x, memo, _nil)\r\n",
      "        144     copier = _deepcopy_dispatch.get(cls)\r\n",
      "        145     if copier is not None:\r\n",
      "    --> 146         y = copier(x, memo)\r\n",
      "        147     else:\r\n",
      "        148         if issubclass(cls, type):\r\n",
      "    \r\n",
      "    ~\\anaconda3\\envs\\PythonCPU\\lib\\copy.py in _deepcopy_dict(x, memo, deepcopy)\r\n",
      "        228     memo[id(x)] = y\r\n",
      "        229     for key, value in x.items():\r\n",
      "    --> 230         y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n",
      "        231     return y\r\n",
      "        232 d[dict] = _deepcopy_dict\r\n",
      "    \r\n",
      "    ~\\anaconda3\\envs\\PythonCPU\\lib\\copy.py in deepcopy(x, memo, _nil)\r\n",
      "        170                     y = x\r\n",
      "        171                 else:\r\n",
      "    --> 172                     y = _reconstruct(x, memo, *rv)\r\n",
      "        173 \r\n",
      "        174     # If is its own copy, don't memoize.\r\n",
      "    \r\n",
      "    ~\\anaconda3\\envs\\PythonCPU\\lib\\copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)\r\n",
      "        268     if state is not None:\r\n",
      "        269         if deep:\r\n",
      "    --> 270             state = deepcopy(state, memo)\r\n",
      "        271         if hasattr(y, '__setstate__'):\r\n",
      "        272             y.__setstate__(state)\r\n",
      "    \r\n",
      "    ~\\anaconda3\\envs\\PythonCPU\\lib\\copy.py in deepcopy(x, memo, _nil)\r\n",
      "        144     copier = _deepcopy_dispatch.get(cls)\r\n",
      "        145     if copier is not None:\r\n",
      "    --> 146         y = copier(x, memo)\r\n",
      "        147     else:\r\n",
      "        148         if issubclass(cls, type):\r\n",
      "    \r\n",
      "    ~\\anaconda3\\envs\\PythonCPU\\lib\\copy.py in _deepcopy_dict(x, memo, deepcopy)\r\n",
      "        228     memo[id(x)] = y\r\n",
      "        229     for key, value in x.items():\r\n",
      "    --> 230         y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n",
      "        231     return y\r\n",
      "        232 d[dict] = _deepcopy_dict\r\n",
      "    \r\n",
      "    ~\\anaconda3\\envs\\PythonCPU\\lib\\copy.py in deepcopy(x, memo, _nil)\r\n",
      "        170                     y = x\r\n",
      "        171                 else:\r\n",
      "    --> 172                     y = _reconstruct(x, memo, *rv)\r\n",
      "        173 \r\n",
      "        174     # If is its own copy, don't memoize.\r\n",
      "    \r\n",
      "    ~\\anaconda3\\envs\\PythonCPU\\lib\\copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)\r\n",
      "        268     if state is not None:\r\n",
      "        269         if deep:\r\n",
      "    --> 270             state = deepcopy(state, memo)\r\n",
      "        271         if hasattr(y, '__setstate__'):\r\n",
      "        272             y.__setstate__(state)\r\n",
      "    \r\n",
      "    ~\\anaconda3\\envs\\PythonCPU\\lib\\copy.py in deepcopy(x, memo, _nil)\r\n",
      "        144     copier = _deepcopy_dispatch.get(cls)\r\n",
      "        145     if copier is not None:\r\n",
      "    --> 146         y = copier(x, memo)\r\n",
      "        147     else:\r\n",
      "        148         if issubclass(cls, type):\r\n",
      "    \r\n",
      "    ~\\anaconda3\\envs\\PythonCPU\\lib\\copy.py in _deepcopy_dict(x, memo, deepcopy)\r\n",
      "        228     memo[id(x)] = y\r\n",
      "        229     for key, value in x.items():\r\n",
      "    --> 230         y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n",
      "        231     return y\r\n",
      "        232 d[dict] = _deepcopy_dict\r\n",
      "    \r\n",
      "    ~\\anaconda3\\envs\\PythonCPU\\lib\\copy.py in deepcopy(x, memo, _nil)\r\n",
      "        170                     y = x\r\n",
      "        171                 else:\r\n",
      "    --> 172                     y = _reconstruct(x, memo, *rv)\r\n",
      "        173 \r\n",
      "        174     # If is its own copy, don't memoize.\r\n",
      "    \r\n",
      "    ~\\anaconda3\\envs\\PythonCPU\\lib\\copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)\r\n",
      "        268     if state is not None:\r\n",
      "        269         if deep:\r\n",
      "    --> 270             state = deepcopy(state, memo)\r\n",
      "        271         if hasattr(y, '__setstate__'):\r\n",
      "        272             y.__setstate__(state)\r\n",
      "    \r\n",
      "    ~\\anaconda3\\envs\\PythonCPU\\lib\\copy.py in deepcopy(x, memo, _nil)\r\n",
      "        144     copier = _deepcopy_dispatch.get(cls)\r\n",
      "        145     if copier is not None:\r\n",
      "    --> 146         y = copier(x, memo)\r\n",
      "        147     else:\r\n",
      "        148         if issubclass(cls, type):\r\n",
      "    \r\n",
      "    ~\\anaconda3\\envs\\PythonCPU\\lib\\copy.py in _deepcopy_dict(x, memo, deepcopy)\r\n",
      "        228     memo[id(x)] = y\r\n",
      "        229     for key, value in x.items():\r\n",
      "    --> 230         y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n",
      "        231     return y\r\n",
      "        232 d[dict] = _deepcopy_dict\r\n",
      "    \r\n",
      "    ~\\anaconda3\\envs\\PythonCPU\\lib\\copy.py in deepcopy(x, memo, _nil)\r\n",
      "        159                     reductor = getattr(x, \"__reduce_ex__\", None)\r\n",
      "        160                     if reductor is not None:\r\n",
      "    --> 161                         rv = reductor(4)\r\n",
      "        162                     else:\r\n",
      "        163                         reductor = getattr(x, \"__reduce__\", None)\r\n",
      "    \r\n",
      "    TypeError: cannot pickle '_thread.RLock' object\r\n",
      "\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "comp:keras\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Value error with DELF\n",
      "issue body -  I use the following code to compute delf,\r\n",
      "\r\n",
      "```\r\n",
      "import argparse\r\n",
      "from glob import glob\r\n",
      "\r\n",
      "import tensorflow as tf\r\n",
      "import tensorflow_hub as hub\r\n",
      "from tensorflow.python.framework import ops\r\n",
      "from tensorflow.python.framework.ops import disable_eager_execution\r\n",
      "from tqdm import tqdm\r\n",
      "\r\n",
      "disable_eager_execution()\r\n",
      "tf.compat.v1.disable_v2_behavior()\r\n",
      "\r\n",
      "class DeepDELF:\r\n",
      "\r\n",
      "    def __init__(self, input_path):\r\n",
      "        ops.reset_default_graph()\r\n",
      "\r\n",
      "        m = hub.Module('https://tfhub.dev/google/delf/1')\r\n",
      "\r\n",
      "        # The module operates on a single image at a time, so define a placeholder to\r\n",
      "        # feed an arbitrary image in.\r\n",
      "        self.image_placeholder = tf.compat.v1.placeholder(\r\n",
      "            tf.float32, shape=(None, None, 3), name='input_image')\r\n",
      "\r\n",
      "        module_inputs = {\r\n",
      "            'image': self.image_placeholder,\r\n",
      "            'score_threshold': 100.0,\r\n",
      "            'image_scales': [0.25, 0.3536, 0.5, 0.7071, 1.0, 1.4142, 2.0],\r\n",
      "            'max_feature_num': 1000,\r\n",
      "        }\r\n",
      "\r\n",
      "        self.module_outputs = m(module_inputs, as_dict=True)\r\n",
      "        self.image_tf = self.image_input_fn(glob(input_path + '/*'))\r\n",
      "        self.path_list = glob(input_path + '/*')\r\n",
      "\r\n",
      "    def extract(self):\r\n",
      "        with tf.compat.v1.train.MonitoredSession() as sess:\r\n",
      "            results_dict = {}  # Stores the locations and their descriptors for each image\r\n",
      "            for image_path in tqdm(self.path_list):\r\n",
      "                image = sess.run(self.image_tf)\r\n",
      "                print('Extracting locations and descriptors from %s' % image_path)\r\n",
      "                results_dict[image_path] = sess.run(\r\n",
      "                    [self.module_outputs['locations'], self.module_outputs['descriptors']],\r\n",
      "                    feed_dict={self.image_placeholder: image})\r\n",
      "            return results_dict\r\n",
      "\r\n",
      "    def image_input_fn(self, image_files):\r\n",
      "        filename_queue = tf.compat.v1.train.string_input_producer(\r\n",
      "            image_files, shuffle=False)\r\n",
      "        reader = tf.compat.v1.WholeFileReader()\r\n",
      "        _, value = reader.read(filename_queue)\r\n",
      "        image_tf = tf.image.decode_jpeg(value, channels=3)\r\n",
      "        return tf.image.convert_image_dtype(image_tf, tf.float32)\r\n",
      "\r\n",
      "\r\n",
      "def main(args):\r\n",
      "    path = args['input_path']\r\n",
      "    extrator = None\r\n",
      "    extractor = DeepDELF(path)\r\n",
      "    results_dict = extractor.extract()\r\n",
      "    results_dict2 = extractor.extract()\r\n",
      "    print(\"Shape feature: \", results_dict.keys())\r\n",
      "    print(\"Shape feature 2: \", results_dict2.keys())\r\n",
      "\r\n",
      "\r\n",
      "def args_parser():\r\n",
      "    parser = argparse.ArgumentParser(description=\"Methods extract image.\")\r\n",
      "    parser.add_argument('-i', '--input_path', \r\n",
      "                        help=\"The path of the input image.\")\r\n",
      "    return vars(parser.parse_args())\r\n",
      "\r\n",
      "\r\n",
      "if __name__ == \"__main__\":\r\n",
      "    args = args_parser()\r\n",
      "    # End default optional arguments\r\n",
      "    # Print info arguments\r\n",
      "    print(\"Extract feature from image.\".upper().center(100))\r\n",
      "    print(str(\"-\" * 63).center(100))\r\n",
      "    print(\"|{:<30}:\\n|{:<30}|\".format(\"Image path\", args['input_path']).center(100))\r\n",
      "    print(str(\"-\" * 63).center(100))\r\n",
      "\r\n",
      "    main(args)\r\n",
      "\r\n",
      "```\r\n",
      "\r\n",
      "Unfortunately, it throws the following error\r\n",
      "\r\n",
      "```\r\n",
      "    raise ValueError(not_null_err)\r\n",
      "ValueError: string_input_producer requires a non-null input tensor\r\n",
      "```\r\n",
      "\r\n",
      "How can I fix this?\n",
      "issue labels - \n",
      "stat:awaiting response\n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  Keras fit with generators not executing in the main thread when setting workers=0\n",
      "issue body -  Can be reproduced on colab with TensorFlow (`v2.4.1-0-g85c8b2a817f 2.4.1`)\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "\r\n",
      "Generator code is executed in other threads when using `model.fit`.\r\n",
      "\r\n",
      "From the docs, it looks like that setting workers=0 would execute the generator code in the main thread.\r\n",
      "\r\n",
      "> workers Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. **If 0, will execute the generator on the main thread**.\r\n",
      "\r\n",
      "This doesn't seem to work though as only the first iterations seems to be executed in the main thread.\r\n",
      "\r\n",
      "For example:\r\n",
      "\r\n",
      "```\r\n",
      "import tensorflow as tf\r\n",
      "import threading\r\n",
      "model = tf.keras.Sequential([tf.keras.layers.Dense(1)])\r\n",
      "model.compile(loss = \"mse\", optimizer = \"adam\")\r\n",
      "\r\n",
      "def gen ():\r\n",
      "  for i in range(100):\r\n",
      "    print(threading.current_thread())\r\n",
      "    yield (tf.random.normal(shape=(100,1)), tf.random.normal(shape = (100,)))\r\n",
      "\r\n",
      "model.fit(gen(), epochs = 1, workers = 0, verbose = 0, steps_per_epoch = 3, max_queue_size=0)\r\n",
      "```\r\n",
      "\r\n",
      "I get:\r\n",
      "\r\n",
      "```\r\n",
      "<_MainThread(MainThread, started 140516450817920)>\r\n",
      "<_DummyThread(Dummy-4, started daemon 140514717599488)>\r\n",
      "<_DummyThread(Dummy-5, started daemon 140514709206784)>\r\n",
      "<tensorflow.python.keras.callbacks.History at 0x7fcc1d7223c8>\r\n",
      "```\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "\r\n",
      "I'd expect that all calls from the generator to be executed in the main thread. This is problematic because in my use case the generator is not thread safe and crashes the program when executed from other threads. \r\n",
      "\r\n",
      "Note: I have opened a [SO question](https://stackoverflow.com/questions/66320198/keras-fit-with-generator-function-always-execute-in-the-main-thread) but I feel this behavior should be treated as a bug.\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:keras\n",
      "stat:awaiting tensorflower\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Prefer generator expressions over list comprehensions\n",
      "issue body -  This PR replaces list comprehensions that are only used as input to `any()` or `all()` with generator expressions. This removes the need to instantiate unnecessary lists if the expression is only consumed as an iterator and in the case of any allows the loop to potentially exit early which can improve performance for long iterations.\r\n",
      "\r\n",
      "Most of the changes are not in a hot code path so this won't noticeably improve performance but since the change don't hurt readability I think they are still useful to include.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:data\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  BUG: gfile.rmtree is raises a NotFound error for ram:// filesystems\n",
      "issue body -  Tested via tf-nightly on 2020-02-22 (YYYY-MM-DD).\r\n",
      "\r\n",
      "Code to reproduce:\r\n",
      "\r\n",
      "```python\r\n",
      "from tensorflow.io import gfile\r\n",
      "\r\n",
      "gfile.mkdir(\"ram://deletethisdir\")\r\n",
      "gfile.rmtree(\"ram://deletethisdir\")  # raises a NotFoundError\r\n",
      "```\r\n",
      "\r\n",
      "Colab notebook: https://colab.research.google.com/drive/13bWmaJ40aeQKd2pUfNtnfwaaUP1M_IVL?authuser=1#\n",
      "issue labels - \n",
      "TF 2.5\n",
      "comp:ops\n",
      "regression issue\n",
      "stat:awaiting tensorflower\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  BUG: gfile.walk return full paths instead of filenames for ram:// filesystems\n",
      "issue body -  Tested via tf-nightly on 2020-02-22 (YYYY-MM-DD).\r\n",
      "\r\n",
      "Code to reproduce:\r\n",
      "\r\n",
      "```python3\r\n",
      "from tensorflow.io import gfile\r\n",
      "\r\n",
      "gfile.mkdir(\"ram://testdir\")\r\n",
      "with gfile.GFile(\"ram://testdir/file.txt\", \"w\") as f:\r\n",
      "    f.write(\"test\")\r\n",
      "\r\n",
      "print(list(gfile.walk(\"ram://testdir\")))\r\n",
      "```\r\n",
      "\r\n",
      "The third element is `['ram://testdir/file.txt']`, but it should be just `['file.txt']`\r\n",
      "\r\n",
      "Colab notebook: https://colab.research.google.com/drive/13bWmaJ40aeQKd2pUfNtnfwaaUP1M_IVL?authuser=1#\n",
      "issue labels - \n",
      "comp:ops\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Update audio recognition tutorial link\n",
      "issue body -  Updated audio recognition tutorial link.\r\n",
      "\r\n",
      "Fixes issue [#47305](https://github.com/tensorflow/tensorflow/issues/47305)\n",
      "issue labels - \n",
      "cla: yes\n",
      "ready to pull\n",
      "size:XS\n",
      "type:docs-bug\n",
      "\n",
      "\n",
      "issue title -  Need a easy way to let different weights have different LR in Keras.\n",
      "issue body -  **System information**\r\n",
      "- TensorFlow version (you are using): 2.4.1\r\n",
      "- Are you willing to contribute it (Yes/No): Yes, may be\r\n",
      "\r\n",
      "\r\n",
      "**Describe the feature and the current behavior/state.**\r\n",
      "At nowtimes, if I want to train model and use defferent LR for weights, I need to give up `Model.Fit()`.\r\n",
      "\r\n",
      "\r\n",
      "**Will this change the current api? How?**\r\n",
      "Yes, probably:\r\n",
      "- Layer.add_weight() may have extra param \"Init_LR\"\r\n",
      "- Weights may add attribute “Init_LR”\r\n",
      "- Some others.\r\n",
      "\r\n",
      "\r\n",
      "**Who will benefit with this feature?**\r\n",
      "Anyone who want to use this.\r\n",
      "Developer who also use PyTorch.\n",
      "issue labels - \n",
      "comp:keras\n",
      "stat:awaiting tensorflower\n",
      "type:feature\n",
      "\n",
      "\n",
      "issue title -  tutorial URL outdated, should probably be \"www.tensorflow.org/tutorials/audio/simple_audio\"\n",
      "issue body -  The url \r\n",
      "`https://www.tensorflow.org/tutorials/audio_recognition`\r\n",
      "in line 21 of `tensorflow/tensorflow/examples/speech_commands/train.py` is outdated and produces an 404 error. \r\n",
      "\r\n",
      "It should probably be:\r\n",
      "`https://www.tensorflow.org/tutorials/audio/simple_audio`\r\n",
      "\n",
      "issue labels - \n",
      "type:docs-bug\n",
      "\n",
      "\n",
      "issue title -  Hide subprocess windows for windows os\n",
      "issue body -  Unlike Linux-based operating systems, Windows will create a new console for a child process if the parent is a GUI application.\r\n",
      "\r\n",
      "This causes console window flashes when `ptxas` is called if the process that loaded `tensorflow.dll` is running in GUI mode.\r\n",
      "\r\n",
      "This PR makes children's windows hidden by default. \r\n",
      "\r\n",
      "As tensorflow does not start any GUI programs, it should not cause any bad effect.\r\n",
      "\r\n",
      "Console mode is not affected.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:core\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  TFLM / example: micro_speech / mbed compile -m DISCO_F746NG -t GCC_ARM collect2: error: ld returned 1 exit status\n",
      "issue body -  @tensorflow/micro\r\n",
      "\r\n",
      "**System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.1\r\n",
      "- TensorFlow installed from (source or binary): source\r\n",
      "- TensorFlow version: TensorFlow/Lite/Micro\r\n",
      "- Python version: Python 2.7.17 and Python3 3.6.9\r\n",
      "- Installed using virtualenv? pip? conda?: pip, and no virtualenv, no conda\r\n",
      "- GCC/Compiler version (if compiling from source): gcc 7.5.0\r\n",
      "- mbed version: 1.10.5\r\n",
      "- Target platform: DISCO_F746NG\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "TensorFlow Lite Micro cross compile for micro speech example at DISCO_F746NG platform. It was 100% completed in compile, but report link error.\r\n",
      "I follow the advice from [issue 46721](https://github.com/tensorflow/tensorflow/issues/46721) and modify the tensorflow/tensorflow/lite/micro/examples/micro_speech/disco_f746ng/Makefile.inc as the following, see also [link](https://github.com/marconi1964/tensorflow/blob/example/tensorflow/lite/micro/examples/micro_speech/disco_f746ng/Makefile.inc)\r\n",
      "\r\n",
      "`# Settings for the Discovery STM32F746NG board.`\r\n",
      "`ifeq ($(TARGET),$(filter $(TARGET),mbed))`\r\n",
      "\r\n",
      "`  micro_speech_MBED_PROJECT_FILES += \\`\r\n",
      "`    AUDIO_DISCO_F746NG.lib \\`\r\n",
      "`    BSP_DISCO_F746NG.lib \\`\r\n",
      "`    SDRAM_DISCO_F746NG.lib \\`\r\n",
      "`    LCD_DISCO_F746NG.lib`\r\n",
      "`  MICRO_SPEECH_SRCS += \\`\r\n",
      "`\ttensorflow/lite/micro/examples/micro_speech/disco_f746ng/audio_provider.cc \\`\r\n",
      "`\ttensorflow/lite/micro/examples/micro_speech/disco_f746ng/command_responder.cc`\r\n",
      "\r\n",
      "`endif`\r\n",
      "\r\n",
      "**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n",
      "The procedures are\r\n",
      "`$ cd tensorflow`\r\n",
      "`$ make -f tensorflow/lite/micro/tools/make/Makefile TARGET=mbed generate_micro_speech_mbed_project`\r\n",
      "`$ mbed config root .`\r\n",
      "`$ mbed deploy`\r\n",
      "`$ mbed compile -m DISCO_F746NG -t GCC_ARM`\r\n",
      "collect2: error: ld returned 1 exit status\r\n",
      "\r\n",
      "[mbed] ERROR: \"/usr/bin/python\" returned error.\r\n",
      "       Code: 1\r\n",
      "       Path: \"/home/ubuntu/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/micro_speech/mbed\"\r\n",
      "       Command: \"/usr/bin/python -u /home/ubuntu/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/micro_speech/mbed/mbed-os/tools/make.py -t GCC_ARM -m DISCO_F746NG --source . --build ./BUILD/DISCO_F746NG/GCC_ARM\"\r\n",
      "       Tip: You could retry the last command with \"-v\" flag for verbose output\r\n",
      "\r\n",
      "Below is the complete log of last command with -v flag.\r\n",
      "\r\n",
      "**Any other info / logs**\r\n",
      "`ubuntu@ubuntu:~/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/micro_speech/mbed$ /usr/bin/python -u /home/ubuntu/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/micro_speech/mbed/mbed-os/tools/make.py -t GCC_ARM -m DISCO_F746NG --source . --build ./BUILD/DISCO_F746NG/GCC_ARM -v`\r\n",
      "\r\n",
      "[Warning] @,: Compiler version mismatch: Have 7.3.1; expected version >= 9.0.0 and < 10.0.0\r\n",
      "Building project mbed (DISCO_F746NG, GCC_ARM)\r\n",
      "Scan: mbed\r\n",
      "Macros: -DDEVICE_CRC=1 -DTARGET_STM32F746xG -DDEVICE_I2C_ASYNCH=1 -DDEVICE_EMAC=1 -D__MBED__=1 -DDEVICE_I2CSLAVE=1 -D__FPU_PRESENT=1 -DDEVICE_PORTOUT=1 -DDEVICE_PORTINOUT=1 -D__MBED_CMSIS_RTOS_CM -DTARGET_DISCO_F746NG -DCOMPONENT_FLASHIAP=1 -DTARGET_STM32F7 -DDEVICE_MPU=1 -DDEVICE_SERIAL_ASYNCH=1 -D__CMSIS_RTOS -DTARGET_N25Q128A -DTOOLCHAIN_GCC -DDEVICE_CAN=1 -DARM_MATH_CM7 -DTARGET_CORTEX_M -DTARGET_LIKE_CORTEX_M7 -DDEVICE_RTC=1 -DDEVICE_ANALOGOUT=1 -DDEVICE_QSPI=1 -DTARGET_M7 -DCOMPONENT_PSA_SRV_IMPL=1 -DEXTRA_IDLE_STACK_REQUIRED -DDEVICE_LPTICKER=1 -DDEVICE_PWMOUT=1 -DDEVICE_SPI_ASYNCH=1 -DMBED_TICKLESS -DUSE_FULL_LL_DRIVER -DCOMPONENT_QSPIF=1 -DTARGET_CORTEX -DDEVICE_I2C=1 -DTRANSACTION_QUEUE_SIZE_SPI=2 -DDEVICE_USBDEVICE=1 -DDEVICE_STDIO_MESSAGES=1 -D__CORTEX_M7 -DTARGET_FAMILY_STM32 -DUSE_HAL_DRIVER -DTARGET_FF_ARDUINO -DHSE_VALUE=25000000 -DTARGET_RELEASE -DTARGET_STM -DTARGET_NAME=DISCO_F746NG -DDEVICE_SERIAL_FC=1 -DCOMPONENT_PSA_SRV_EMUL=1 -DDEVICE_USTICKER=1 -DDEVICE_WATCHDOG=1 -DDEVICE_TRNG=1 -DTARGET_LIKE_MBED -DTARGET_RTOS_M4_M7 -DDEVICE_SLEEP=1 -DTOOLCHAIN_GCC_ARM -DMBED_BUILD_TIMESTAMP=1613922146.79 -DDEVICE_RESET_REASON=1 -DDEVICE_SPI=1 -DCOMPONENT_NSPE=1 -DDEVICE_INTERRUPTIN=1 -DDEVICE_SPISLAVE=1 -DDEVICE_ANALOGIN=1 -DDEVICE_SERIAL=1 -DDEVICE_FLASH=1 -DDEVICE_PORTIN=1 -DSTM32F746xx\r\n",
      "Link: mbed\r\n",
      "Preproc: arm-none-eabi-cpp -E -P ./mbed-os/targets/TARGET_STM/TARGET_STM32F7/TARGET_STM32F746xG/device/TOOLCHAIN_GCC_ARM/STM32F746xG.ld -Wl,--gc-sections -Wl,--wrap,main -Wl,--wrap,_malloc_r -Wl,--wrap,_free_r -Wl,--wrap,_realloc_r -Wl,--wrap,_memalign_r -Wl,--wrap,_calloc_r -Wl,--wrap,exit -Wl,--wrap,atexit -Wl,-n -Wl,--wrap,printf -Wl,--wrap,sprintf -Wl,--wrap,snprintf -Wl,--wrap,vprintf -Wl,--wrap,vsprintf -Wl,--wrap,vsnprintf -Wl,--wrap,fprintf -Wl,--wrap,vfprintf -mcpu=cortex-m7 -mthumb -mfpu=fpv5-sp-d16 -mfloat-abi=softfp -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -fmessage-length=0 -fno-exceptions -ffunction-sections -fdata-sections -funsigned-char -MMD -fomit-frame-pointer -Os -g -DMBED_TRAP_ERRORS_ENABLED=1 -DMBED_MINIMAL_PRINTF -mcpu=cortex-m7 -mthumb -mfpu=fpv5-sp-d16 -mfloat-abi=softfp -DMBED_ROM_START=0x8000000 -DMBED_ROM_SIZE=0x100000 -DMBED_ROM1_START=0x200000 -DMBED_ROM1_SIZE=0x100000 -DMBED_RAM_START=0x20010000 -DMBED_RAM_SIZE=0x40000 -DMBED_RAM1_START=0x20000000 -DMBED_RAM1_SIZE=0x10000 -DMBED_BOOT_STACK_SIZE=4096 -DXIP_ENABLE=0 -o ./BUILD/DISCO_F746NG/GCC_ARM/.link_script.ld -DDEVICE_CRC=1 -DTARGET_STM32F746xG -DDEVICE_I2C_ASYNCH=1 -DDEVICE_EMAC=1 -D__MBED__=1 -DDEVICE_I2CSLAVE=1 -D__FPU_PRESENT=1 -DDEVICE_PORTOUT=1 -DDEVICE_PORTINOUT=1 -D__MBED_CMSIS_RTOS_CM -DTARGET_DISCO_F746NG -DCOMPONENT_FLASHIAP=1 -DTARGET_STM32F7 -DDEVICE_MPU=1 -DDEVICE_SERIAL_ASYNCH=1 -D__CMSIS_RTOS -DTARGET_N25Q128A -DTOOLCHAIN_GCC -DDEVICE_CAN=1 -DARM_MATH_CM7 -DTARGET_CORTEX_M -DTARGET_LIKE_CORTEX_M7 -DDEVICE_RTC=1 -DDEVICE_ANALOGOUT=1 -DDEVICE_QSPI=1 -DTARGET_M7 -DCOMPONENT_PSA_SRV_IMPL=1 -DEXTRA_IDLE_STACK_REQUIRED -DDEVICE_LPTICKER=1 -DDEVICE_PWMOUT=1 -DDEVICE_SPI_ASYNCH=1 -DMBED_TICKLESS -DUSE_FULL_LL_DRIVER -DCOMPONENT_QSPIF=1 -DTARGET_CORTEX -DDEVICE_I2C=1 -DTRANSACTION_QUEUE_SIZE_SPI=2 -DDEVICE_USBDEVICE=1 -DDEVICE_STDIO_MESSAGES=1 -D__CORTEX_M7 -DTARGET_FAMILY_STM32 -DUSE_HAL_DRIVER -DTARGET_FF_ARDUINO -DHSE_VALUE=25000000 -DTARGET_RELEASE -DTARGET_STM -DTARGET_NAME=DISCO_F746NG -DDEVICE_SERIAL_FC=1 -DCOMPONENT_PSA_SRV_EMUL=1 -DDEVICE_USTICKER=1 -DDEVICE_WATCHDOG=1 -DDEVICE_TRNG=1 -DTARGET_LIKE_MBED -DTARGET_RTOS_M4_M7 -DDEVICE_SLEEP=1 -DTOOLCHAIN_GCC_ARM -DMBED_BUILD_TIMESTAMP=1613922146.79 -DDEVICE_RESET_REASON=1 -DDEVICE_SPI=1 -DCOMPONENT_NSPE=1 -DDEVICE_INTERRUPTIN=1 -DDEVICE_SPISLAVE=1 -DDEVICE_ANALOGIN=1 -DDEVICE_SERIAL=1 -DDEVICE_FLASH=1 -DDEVICE_PORTIN=1 -DSTM32F746xx @./BUILD/DISCO_F746NG/GCC_ARM/.includes_d41d8cd98f00b204e9800998ecf8427e.txt -include ./BUILD/DISCO_F746NG/GCC_ARM/mbed_config.h\r\n",
      "[DEBUG] Return: 0\r\n",
      "Link: arm-none-eabi-gcc @./BUILD/DISCO_F746NG/GCC_ARM/.link_options.txt\r\n",
      "[DEBUG] Return: 1\r\n",
      "[DEBUG] Errors: BUILD/DISCO_F746NG/GCC_ARM/tensorflow/lite/micro/examples/micro_speech/disco_f746ng/audio_provider.o: In function `GetAudioSamples(tflite::ErrorReporter*, int, int, int*, short**)':\r\n",
      "[DEBUG] Errors: /home/ubuntu/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/micro_speech/mbed/./tensorflow/lite/micro/examples/micro_speech/disco_f746ng/audio_provider.cc:154: multiple definition of `GetAudioSamples(tflite::ErrorReporter*, int, int, int*, short**)'\r\n",
      "[DEBUG] Errors: BUILD/DISCO_F746NG/GCC_ARM/tensorflow/lite/micro/examples/micro_speech/audio_provider.o:/home/ubuntu/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/micro_speech/mbed/./tensorflow/lite/micro/examples/micro_speech/audio_provider.cc:28: first defined here\r\n",
      "[DEBUG] Errors: BUILD/DISCO_F746NG/GCC_ARM/tensorflow/lite/micro/examples/micro_speech/disco_f746ng/audio_provider.o: In function `LatestAudioTimestamp()':\r\n",
      "[DEBUG] Errors: /home/ubuntu/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/micro_speech/mbed/./tensorflow/lite/micro/examples/micro_speech/disco_f746ng/audio_provider.cc:181: multiple definition of `LatestAudioTimestamp()'\r\n",
      "[DEBUG] Errors: BUILD/DISCO_F746NG/GCC_ARM/tensorflow/lite/micro/examples/micro_speech/audio_provider.o:/home/ubuntu/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/micro_speech/mbed/./tensorflow/lite/micro/examples/micro_speech/audio_provider.cc:37: first defined here\r\n",
      "[DEBUG] Errors: BUILD/DISCO_F746NG/GCC_ARM/tensorflow/lite/micro/examples/micro_speech/disco_f746ng/command_responder.o: In function `RespondToCommand(tflite::ErrorReporter*, long, char const*, unsigned char, bool)':\r\n",
      "[DEBUG] Errors: /home/ubuntu/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/micro_speech/mbed/./tensorflow/lite/micro/examples/micro_speech/disco_f746ng/command_responder.cc:26: multiple definition of `RespondToCommand(tflite::ErrorReporter*, long, char const*, unsigned char, bool)'\r\n",
      "[DEBUG] Errors: BUILD/DISCO_F746NG/GCC_ARM/tensorflow/lite/micro/examples/micro_speech/command_responder.o:/home/ubuntu/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/micro_speech/mbed/./tensorflow/lite/micro/examples/micro_speech/command_responder.cc:23: first defined here\r\n",
      "[DEBUG] Errors: collect2: error: ld returned 1 exit status\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/ubuntu/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/micro_speech/mbed/mbed-os/tools/make.py\", line 78, in wrapped_build_project\r\n",
      "    *args, **kwargs\r\n",
      "  File \"/home/ubuntu/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/micro_speech/mbed/mbed-os/tools/build_api.py\", line 610, in build_project\r\n",
      "    res = toolchain.link_program(resources, build_path, name)\r\n",
      "  File \"/home/ubuntu/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/micro_speech/mbed/mbed-os/tools/toolchains/mbed_toolchain.py\", line 778, in link_program\r\n",
      "    self.link(elf, objects, libraries, lib_dirs, linker_script)\r\n",
      "  File \"/home/ubuntu/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/micro_speech/mbed/mbed-os/tools/toolchains/gcc.py\", line 357, in link\r\n",
      "    self.default_cmd(cmd)\r\n",
      "  File \"/home/ubuntu/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/micro_speech/mbed/mbed-os/tools/toolchains/mbed_toolchain.py\", line 830, in default_cmd\r\n",
      "    raise ToolException(stderr)\r\n",
      "ToolException: BUILD/DISCO_F746NG/GCC_ARM/tensorflow/lite/micro/examples/micro_speech/disco_f746ng/audio_provider.o: In function `GetAudioSamples(tflite::ErrorReporter*, int, int, int*, short**)':\r\n",
      "/home/ubuntu/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/micro_speech/mbed/./tensorflow/lite/micro/examples/micro_speech/disco_f746ng/audio_provider.cc:154: multiple definition of `GetAudioSamples(tflite::ErrorReporter*, int, int, int*, short**)'\r\n",
      "BUILD/DISCO_F746NG/GCC_ARM/tensorflow/lite/micro/examples/micro_speech/audio_provider.o:/home/ubuntu/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/micro_speech/mbed/./tensorflow/lite/micro/examples/micro_speech/audio_provider.cc:28: first defined here\r\n",
      "BUILD/DISCO_F746NG/GCC_ARM/tensorflow/lite/micro/examples/micro_speech/disco_f746ng/audio_provider.o: In function `LatestAudioTimestamp()':\r\n",
      "/home/ubuntu/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/micro_speech/mbed/./tensorflow/lite/micro/examples/micro_speech/disco_f746ng/audio_provider.cc:181: multiple definition of `LatestAudioTimestamp()'\r\n",
      "BUILD/DISCO_F746NG/GCC_ARM/tensorflow/lite/micro/examples/micro_speech/audio_provider.o:/home/ubuntu/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/micro_speech/mbed/./tensorflow/lite/micro/examples/micro_speech/audio_provider.cc:37: first defined here\r\n",
      "BUILD/DISCO_F746NG/GCC_ARM/tensorflow/lite/micro/examples/micro_speech/disco_f746ng/command_responder.o: In function `RespondToCommand(tflite::ErrorReporter*, long, char const*, unsigned char, bool)':\r\n",
      "/home/ubuntu/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/micro_speech/mbed/./tensorflow/lite/micro/examples/micro_speech/disco_f746ng/command_responder.cc:26: multiple definition of `RespondToCommand(tflite::ErrorReporter*, long, char const*, unsigned char, bool)'\r\n",
      "BUILD/DISCO_F746NG/GCC_ARM/tensorflow/lite/micro/examples/micro_speech/command_responder.o:/home/ubuntu/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/micro_speech/mbed/./tensorflow/lite/micro/examples/micro_speech/command_responder.cc:23: first defined here\r\n",
      "collect2: error: ld returned 1 exit status\n",
      "issue labels - \n",
      "comp:lite\n",
      "comp:micro\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  [TFLM] CEVA-BX1: Changed compiler to use O4 and fixed a typo in the Makefile\n",
      "issue body -  Fixed a couple of typos and changed optimization level to O4.\r\n",
      "\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "comp:micro:ceva\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Load Train Model From Checkpoint - NotFoundError\n",
      "issue body -  <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64bit\r\n",
      "- TensorFlow installed from (source or binary):\r\n",
      "- https://github.com/tensorflow/models\r\n",
      "- TensorFlow version: 2\r\n",
      "- Python version: 3.7.3\r\n",
      "- Installed using virtualenv? pip? conda?: conda and pip\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "I have downloaded and installed tensorflow, and I'm attempting to train a custom model, but keep getting runtime errors or notfounderrors to do with tenorflow lib files.\r\n",
      "**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n",
      "`WORKSPACE_PATH = 'Tensorflow/workspace'\r\n",
      "SCRIPTS_PATH = 'Tensorflow/scripts'\r\n",
      "APIMODEL_PATH = 'Tensorflow/models'\r\n",
      "ANNOTATION_PATH = WORKSPACE_PATH+'/annotations'\r\n",
      "IMAGE_PATH = WORKSPACE_PATH+'/images'\r\n",
      "MODEL_PATH = WORKSPACE_PATH+'/models'\r\n",
      "PRETRAINED_MODEL_PATH = WORKSPACE_PATH+'/pre-trained-models'\r\n",
      "CONFIG_PATH = MODEL_PATH+'/my_ssd_mobnet/pipeline.config'\r\n",
      "CHECKPOINT_PATH = MODEL_PATH+'/my_ssd_mobnet/'\r\n",
      "\r\n",
      "labels = [{'name':'title', 'id':1}, {'name':'xaxis', 'id':2}, {'name':'yaxis', 'id':3}, {'name':'bar', 'id':4}, {'name':'key', 'id':5}]\r\n",
      "\r\n",
      "with open(ANNOTATION_PATH + '\\label_map.pbtxt', 'w') as f:\r\n",
      "    for label in labels:\r\n",
      "        f.write('item { \\n')\r\n",
      "        f.write('\\tname:\\'{}\\'\\n'.format(label['name']))\r\n",
      "        f.write('\\tid:{}\\n'.format(label['id']))\r\n",
      "        f.write('}\\n')\r\n",
      "\r\n",
      "!python {SCRIPTS_PATH + '/generate_tfrecord.py'} -x {IMAGE_PATH + '/train'} -l {ANNOTATION_PATH + '/label_map.pbtxt'} -o {ANNOTATION_PATH + '/train.record'}\r\n",
      "!python {SCRIPTS_PATH + '/generate_tfrecord.py'} -x{IMAGE_PATH + '/test'} -l {ANNOTATION_PATH + '/label_map.pbtxt'} -o {ANNOTATION_PATH + '/test.record'}\r\n",
      "\r\n",
      "!cd Tensorflow && git clone https://github.com/tensorflow/models\r\n",
      "\r\n",
      "CUSTOM_MODEL_NAME = 'my_ssd_mobnet' \r\n",
      "!mkdir {'Tensorflow\\workspace\\models\\\\'+CUSTOM_MODEL_NAME}\r\n",
      "!cp {PRETRAINED_MODEL_PATH+'/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/pipeline.config'} {MODEL_PATH+'/'+CUSTOM_MODEL_NAME}\r\n",
      "\r\n",
      "import tensorflow as tf\r\n",
      "from object_detection.utils import config_util\r\n",
      "from object_detection.protos import pipeline_pb2\r\n",
      "from google.protobuf import text_format\r\n",
      "\r\n",
      "CONFIG_PATH = MODEL_PATH+'/'+CUSTOM_MODEL_NAME+'/pipeline.config'\r\n",
      "\r\n",
      "config = config_util.get_configs_from_pipeline_file(CONFIG_PATH)\r\n",
      "config\r\n",
      "{'model': ssd {\r\n",
      "   num_classes: 90\r\n",
      "   image_resizer {\r\n",
      "     fixed_shape_resizer {\r\n",
      "       height: 320\r\n",
      "       width: 320\r\n",
      "     }\r\n",
      "   }\r\n",
      "   feature_extractor {\r\n",
      "     type: \"ssd_mobilenet_v2_fpn_keras\"\r\n",
      "     depth_multiplier: 1.0\r\n",
      "     min_depth: 16\r\n",
      "     conv_hyperparams {\r\n",
      "       regularizer {\r\n",
      "         l2_regularizer {\r\n",
      "           weight: 3.9999998989515007e-05\r\n",
      "         }\r\n",
      "       }\r\n",
      "       initializer {\r\n",
      "         random_normal_initializer {\r\n",
      "           mean: 0.0\r\n",
      "           stddev: 0.009999999776482582\r\n",
      "         }\r\n",
      "       }\r\n",
      "       activation: RELU_6\r\n",
      "       batch_norm {\r\n",
      "         decay: 0.996999979019165\r\n",
      "         scale: true\r\n",
      "         epsilon: 0.0010000000474974513\r\n",
      "       }\r\n",
      "     }\r\n",
      "     use_depthwise: true\r\n",
      "     override_base_feature_extractor_hyperparams: true\r\n",
      "     fpn {\r\n",
      "       min_level: 3\r\n",
      "       max_level: 7\r\n",
      "       additional_layer_depth: 128\r\n",
      "     }\r\n",
      "   }\r\n",
      "   box_coder {\r\n",
      "     faster_rcnn_box_coder {\r\n",
      "       y_scale: 10.0\r\n",
      "       x_scale: 10.0\r\n",
      "       height_scale: 5.0\r\n",
      "       width_scale: 5.0\r\n",
      "     }\r\n",
      "   }\r\n",
      "   matcher {\r\n",
      "     argmax_matcher {\r\n",
      "       matched_threshold: 0.5\r\n",
      "       unmatched_threshold: 0.5\r\n",
      "       ignore_thresholds: false\r\n",
      "       negatives_lower_than_unmatched: true\r\n",
      "       force_match_for_each_row: true\r\n",
      "       use_matmul_gather: true\r\n",
      "     }\r\n",
      "   }\r\n",
      "   similarity_calculator {\r\n",
      "     iou_similarity {\r\n",
      "     }\r\n",
      "   }\r\n",
      "   box_predictor {\r\n",
      "     weight_shared_convolutional_box_predictor {\r\n",
      "       conv_hyperparams {\r\n",
      "         regularizer {\r\n",
      "           l2_regularizer {\r\n",
      "             weight: 3.9999998989515007e-05\r\n",
      "           }\r\n",
      "         }\r\n",
      "         initializer {\r\n",
      "           random_normal_initializer {\r\n",
      "             mean: 0.0\r\n",
      "             stddev: 0.009999999776482582\r\n",
      "           }\r\n",
      "         }\r\n",
      "         activation: RELU_6\r\n",
      "         batch_norm {\r\n",
      "           decay: 0.996999979019165\r\n",
      "           scale: true\r\n",
      "           epsilon: 0.0010000000474974513\r\n",
      "         }\r\n",
      "       }\r\n",
      "       depth: 128\r\n",
      "       num_layers_before_predictor: 4\r\n",
      "       kernel_size: 3\r\n",
      "       class_prediction_bias_init: -4.599999904632568\r\n",
      "       share_prediction_tower: true\r\n",
      "       use_depthwise: true\r\n",
      "     }\r\n",
      "   }\r\n",
      "   anchor_generator {\r\n",
      "     multiscale_anchor_generator {\r\n",
      "       min_level: 3\r\n",
      "       max_level: 7\r\n",
      "       anchor_scale: 4.0\r\n",
      "       aspect_ratios: 1.0\r\n",
      "       aspect_ratios: 2.0\r\n",
      "       aspect_ratios: 0.5\r\n",
      "       scales_per_octave: 2\r\n",
      "     }\r\n",
      "   }\r\n",
      "   post_processing {\r\n",
      "     batch_non_max_suppression {\r\n",
      "       score_threshold: 9.99999993922529e-09\r\n",
      "       iou_threshold: 0.6000000238418579\r\n",
      "       max_detections_per_class: 100\r\n",
      "       max_total_detections: 100\r\n",
      "       use_static_shapes: false\r\n",
      "     }\r\n",
      "     score_converter: SIGMOID\r\n",
      "   }\r\n",
      "   normalize_loss_by_num_matches: true\r\n",
      "   loss {\r\n",
      "     localization_loss {\r\n",
      "       weighted_smooth_l1 {\r\n",
      "       }\r\n",
      "     }\r\n",
      "     classification_loss {\r\n",
      "       weighted_sigmoid_focal {\r\n",
      "         gamma: 2.0\r\n",
      "         alpha: 0.25\r\n",
      "       }\r\n",
      "     }\r\n",
      "     classification_weight: 1.0\r\n",
      "     localization_weight: 1.0\r\n",
      "   }\r\n",
      "   encode_background_as_zeros: true\r\n",
      "   normalize_loc_loss_by_codesize: true\r\n",
      "   inplace_batchnorm_update: true\r\n",
      "   freeze_batchnorm: false\r\n",
      " }, 'train_config': batch_size: 128\r\n",
      " data_augmentation_options {\r\n",
      "   random_horizontal_flip {\r\n",
      "   }\r\n",
      " }\r\n",
      " data_augmentation_options {\r\n",
      "   random_crop_image {\r\n",
      "     min_object_covered: 0.0\r\n",
      "     min_aspect_ratio: 0.75\r\n",
      "     max_aspect_ratio: 3.0\r\n",
      "     min_area: 0.75\r\n",
      "     max_area: 1.0\r\n",
      "     overlap_thresh: 0.0\r\n",
      "   }\r\n",
      " }\r\n",
      " sync_replicas: true\r\n",
      " optimizer {\r\n",
      "   momentum_optimizer {\r\n",
      "     learning_rate {\r\n",
      "       cosine_decay_learning_rate {\r\n",
      "         learning_rate_base: 0.07999999821186066\r\n",
      "         total_steps: 50000\r\n",
      "         warmup_learning_rate: 0.026666000485420227\r\n",
      "         warmup_steps: 1000\r\n",
      "       }\r\n",
      "     }\r\n",
      "     momentum_optimizer_value: 0.8999999761581421\r\n",
      "   }\r\n",
      "   use_moving_average: false\r\n",
      " }\r\n",
      " fine_tune_checkpoint: \"PATH_TO_BE_CONFIGURED\"\r\n",
      " num_steps: 50000\r\n",
      " startup_delay_steps: 0.0\r\n",
      " replicas_to_aggregate: 8\r\n",
      " max_number_of_boxes: 100\r\n",
      " unpad_groundtruth_tensors: false\r\n",
      " fine_tune_checkpoint_type: \"classification\"\r\n",
      " fine_tune_checkpoint_version: V2, 'train_input_config': label_map_path: \"PATH_TO_BE_CONFIGURED\"\r\n",
      " tf_record_input_reader {\r\n",
      "   input_path: \"PATH_TO_BE_CONFIGURED\"\r\n",
      " }, 'eval_config': metrics_set: \"coco_detection_metrics\"\r\n",
      " use_moving_averages: false, 'eval_input_configs': [label_map_path: \"PATH_TO_BE_CONFIGURED\"\r\n",
      " shuffle: false\r\n",
      " num_epochs: 1\r\n",
      " tf_record_input_reader {\r\n",
      "   input_path: \"PATH_TO_BE_CONFIGURED\"\r\n",
      " }\r\n",
      " ], 'eval_input_config': label_map_path: \"PATH_TO_BE_CONFIGURED\"\r\n",
      " shuffle: false\r\n",
      " num_epochs: 1\r\n",
      " tf_record_input_reader {\r\n",
      "   input_path: \"PATH_TO_BE_CONFIGURED\"\r\n",
      " }}\r\n",
      "pipeline_config = pipeline_p\r\n",
      "\r\n",
      "pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\r\n",
      "with tf.io.gfile.GFile(CONFIG_PATH, \"r\") as f:                                                                                                                                                                                                                     \r\n",
      "    proto_str = f.read()                                                                                                                                                                                                                                          \r\n",
      "    text_format.Merge(proto_str, pipeline_config)  \r\n",
      "pipeline_config.model.ssd.num_classes = 2\r\n",
      "pipeline_config.train_config.batch_size = 4\r\n",
      "pipeline_config.train_config.fine_tune_checkpoint = PRETRAINED_MODEL_PATH+'/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/checkpoint/ckpt-0'\r\n",
      "pipeline_config.train_config.fine_tune_checkpoint_type = \"detection\"\r\n",
      "pipeline_config.train_input_reader.label_map_path= ANNOTATION_PATH + '/label_map.pbtxt'\r\n",
      "pipeline_config.train_input_reader.tf_record_input_reader.input_path[:] = [ANNOTATION_PATH + '/train.record']\r\n",
      "pipeline_config.eval_input_reader[0].label_map_path = ANNOTATION_PATH + '/label_map.pbtxt'\r\n",
      "pipeline_config.eval_input_reader[0].tf_record_input_reader.input_path[:] = [ANNOTATION_PATH + '/test.record']\r\n",
      "\r\n",
      "config_text = text_format.MessageToString(pipeline_config)                                                                                                                                                                                                        \r\n",
      "with tf.io.gfile.GFile(CONFIG_PATH, \"wb\") as f:                                                                                                                                                                                                                     \r\n",
      "    f.write(config_text)   \r\n",
      "\r\n",
      "print(\"\"\"python {}/research/object_detection/model_main_tf2.py --model_dir={}/{} --pipeline_config_path={}/{}/pipeline.config --num_train_steps=5000\"\"\".format(APIMODEL_PATH, MODEL_PATH,CUSTOM_MODEL_NAME,MODEL_PATH,CUSTOM_MODEL_NAME))\r\n",
      "\r\n",
      "import os\r\n",
      "from object_detection.utils import label_map_util\r\n",
      "from object_detection.utils import visualization_utils as viz_utils\r\n",
      "from object_detection.builders import model_builder\r\n",
      "\r\n",
      "# Load pipeline config and build a detection model\r\n",
      "configs = config_util.get_configs_from_pipeline_file(CONFIG_PATH)\r\n",
      "detection_model = model_builder.build(model_config=configs['model'], is_training=False)\r\n",
      "\r\n",
      "# Restore checkpoint\r\n",
      "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\r\n",
      "ckpt.restore(os.path.join(CHECKPOINT_PATH, 'ckpt-6')).expect_partial()\r\n",
      "\r\n",
      "@tf.function\r\n",
      "def detect_fn(image):\r\n",
      "    image, shapes = detection_model.preprocess(image)\r\n",
      "    prediction_dict = detection_model.predict(image, shapes)\r\n",
      "    detections = detection_model.postprocess(prediction_dict, shapes)\r\n",
      "    return detections\r\n",
      "#---------------------This Is Where The error Happens ----------------------------------------`\r\n",
      "\r\n",
      "\r\n",
      "**Error Message**\r\n",
      "`---------------------------------------------------------------------------\r\n",
      "RuntimeError                              Traceback (most recent call last)\r\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\py_checkpoint_reader.py in NewCheckpointReader(filepattern)\r\n",
      "     94   try:\r\n",
      "---> 95     return CheckpointReader(compat.as_bytes(filepattern))\r\n",
      "     96   # TODO(b/143319754): Remove the RuntimeError casting logic once we resolve the\r\n",
      "\r\n",
      "RuntimeError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for Tensorflow/workspace/models/my_ssd_mobnet/ckpt-6\r\n",
      "\r\n",
      "During handling of the above exception, another exception occurred:\r\n",
      "\r\n",
      "NotFoundError                             Traceback (most recent call last)\r\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\util.py in restore(self, save_path, options)\r\n",
      "   2259     try:\r\n",
      "-> 2260       status = self.read(save_path, options=options)\r\n",
      "   2261     except errors_impl.NotFoundError:\r\n",
      "\r\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\util.py in read(self, save_path, options)\r\n",
      "   2147     options = options or checkpoint_options.CheckpointOptions()\r\n",
      "-> 2148     return self._saver.restore(save_path=save_path, options=options)\r\n",
      "   2149 \r\n",
      "\r\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\util.py in restore(self, save_path, options)\r\n",
      "   1291       return InitializationOnlyStatus(self._graph_view, ops.uid())\r\n",
      "-> 1292     reader = py_checkpoint_reader.NewCheckpointReader(save_path)\r\n",
      "   1293     graph_building = not context.executing_eagerly()\r\n",
      "\r\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\py_checkpoint_reader.py in NewCheckpointReader(filepattern)\r\n",
      "     98   except RuntimeError as e:\r\n",
      "---> 99     error_translator(e)\r\n",
      "\r\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\py_checkpoint_reader.py in error_translator(e)\r\n",
      "     34       'matching files for') in error_message:\r\n",
      "---> 35     raise errors_impl.NotFoundError(None, None, error_message)\r\n",
      "     36   elif 'Sliced checkpoints are not supported' in error_message or (\r\n",
      "\r\n",
      "NotFoundError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for Tensorflow/workspace/models/my_ssd_mobnet/ckpt-6\r\n",
      "\r\n",
      "During handling of the above exception, another exception occurred:\r\n",
      "\r\n",
      "NotFoundError                             Traceback (most recent call last)\r\n",
      "<ipython-input-17-f5bf27ce595e> in <module>\r\n",
      "      5 # Restore checkpoint\r\n",
      "      6 ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\r\n",
      "----> 7 ckpt.restore(os.path.join(CHECKPOINT_PATH, 'ckpt-6')).expect_partial()\r\n",
      "      8 \r\n",
      "      9 @tf.function\r\n",
      "\r\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\util.py in restore(self, save_path, options)\r\n",
      "   2263           None, None,\r\n",
      "   2264           \"Could not find checkpoint or SavedModel at {}.\"\r\n",
      "-> 2265           .format(orig_save_path))\r\n",
      "   2266     # Create the save counter now so it gets initialized with other variables\r\n",
      "   2267     # when graph building. Creating it earlier would lead to errors when using,\r\n",
      "\r\n",
      "NotFoundError: Could not find checkpoint or SavedModel at Tensorflow/workspace/models/my_ssd_mobnet/ckpt-6.`\n",
      "issue labels - \n",
      "TF 2.0\n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  Remove unnecessary np_bool conversion in basic test.\n",
      "issue body -  Add a tie breaking tests along axis=1.\n",
      "issue labels - \n",
      "cla: no\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Update license to 2021\n",
      "issue body -  Update the license year from 2019 to 2021\n",
      "issue labels - \n",
      "cla: yes\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Update License year to 2021\n",
      "issue body -  Update license year from 2019 to 2021\n",
      "issue labels - \n",
      "cla: no\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -      ValueError: No gradients provided for any variable: ['conv2d/kernel:0', 'conv2d/bias:0', ...]\n",
      "issue body -  This is not the first time I encounter this weird error, I keep getting every now and then and I don't know what causes it / how to fix it. I always end up trying alternative approaches until it's gone. Here's a colab [notebook](https://colab.research.google.com/drive/1eeEc9eIlvMVAJbueIfhpgh569pi-lGx4?usp=sharing) with the full code.\r\n",
      "\r\n",
      "Training function:\r\n",
      "\r\n",
      "```\r\n",
      "    def train_step(self):\r\n",
      "        with tf.GradientTape() as tape:\r\n",
      "            (\r\n",
      "                _,\r\n",
      "                rewards,\r\n",
      "                actions,\r\n",
      "                value_logits,\r\n",
      "                dones,\r\n",
      "                _,\r\n",
      "                entropies,\r\n",
      "                actor_logits,\r\n",
      "            ) = tf.numpy_function(\r\n",
      "                self.np_train_step, [], [tf.float32 for _ in range(8)]\r\n",
      "            )\r\n",
      "            action_probs = tf.nn.softmax(actor_logits)\r\n",
      "            values = tf.reduce_sum(action_probs * value_logits, axis=-1)\r\n",
      "            action_indices = self.get_action_indices(self.batch_indices, actions)\r\n",
      "            selected_probs = tf.gather_nd(action_probs, action_indices)\r\n",
      "            selected_logits = tf.gather_nd(value_logits, action_indices)\r\n",
      "            importance_ratio = action_probs / (action_probs + self.epsilon)\r\n",
      "            action_importance = tf.gather_nd(importance_ratio, action_indices)\r\n",
      "            returns = tf.numpy_function(\r\n",
      "                self.calculate_returns,\r\n",
      "                [rewards, values, dones, selected_logits, action_importance],\r\n",
      "                tf.float32,\r\n",
      "            )\r\n",
      "            loss = self.compute_loss(\r\n",
      "                returns,\r\n",
      "                values,\r\n",
      "                entropies,\r\n",
      "                action_importance,\r\n",
      "                value_logits,\r\n",
      "                importance_ratio,\r\n",
      "                action_probs,\r\n",
      "                selected_probs,\r\n",
      "                selected_logits,\r\n",
      "            )\r\n",
      "        grads = tape.gradient(loss, self.model.trainable_variables)\r\n",
      "        if self.grad_norm is not None:\r\n",
      "            grads, _ = tf.clip_by_global_norm(grads, self.grad_norm)\r\n",
      "        self.model.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\r\n",
      "```\r\n",
      "Results in:\r\n",
      "\r\n",
      "```\r\n",
      "---------------------------------------------------------------------------\r\n",
      "ValueError                                Traceback (most recent call last)\r\n",
      "<ipython-input-13-65446c8a838c> in <module>()\r\n",
      "      7 o = MovingAverage(Adam(7e-4))\r\n",
      "      8 agn = ACER(env, m, optimizer=o)\r\n",
      "----> 9 agn.fit(19)\r\n",
      "\r\n",
      "10 frames\r\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)\r\n",
      "    975           except Exception as e:  # pylint:disable=broad-except\r\n",
      "    976             if hasattr(e, \"ag_error_metadata\"):\r\n",
      "--> 977               raise e.ag_error_metadata.to_exception(e)\r\n",
      "    978             else:\r\n",
      "    979               raise\r\n",
      "\r\n",
      "ValueError: in user code:\r\n",
      "\r\n",
      "    <ipython-input-11-cc50c030ee4d>:151 train_step  *\r\n",
      "        self.model.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\r\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow_addons/optimizers/average_wrapper.py:56 apply_gradients  *\r\n",
      "        return super().apply_gradients(grads_and_vars, name, **kwargs)\r\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:598 apply_gradients  **\r\n",
      "        grads_and_vars = optimizer_utils.filter_empty_gradients(grads_and_vars)\r\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/utils.py:79 filter_empty_gradients\r\n",
      "        ([v.name for _, v in grads_and_vars],))\r\n",
      "\r\n",
      "    ValueError: No gradients provided for any variable: ['conv2d/kernel:0', 'conv2d/bias:0', 'conv2d_1/kernel:0', 'conv2d_1/bias:0', 'conv2d_2/kernel:0', 'conv2d_2/bias:0', 'dense/kernel:0', 'dense/bias:0'].\r\n",
      "```\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:ops\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -   OSError: SavedModel file does not exist at: model_resnet152V2.h5/{saved_model.pbtxt|saved_model.pb}\n",
      "issue body -  Please go to Stack Overflow for help and support:\r\n",
      "\r\n",
      "https://stackoverflow.com/questions/tagged/tensorflow\r\n",
      "\r\n",
      "If you open a GitHub issue, here is our policy:\r\n",
      "\r\n",
      "1.  It must be a bug, a feature request, or a significant problem with the\r\n",
      "    documentation (for small docs fixes please send a PR instead).\r\n",
      "2.  The form below must be filled out.\r\n",
      "3.  It shouldn't be a TensorBoard issue. Those go\r\n",
      "    [here](https://github.com/tensorflow/tensorboard/issues).\r\n",
      "\r\n",
      "**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n",
      "\r\n",
      "------------------------\r\n",
      "\r\n",
      "### System information\r\n",
      "\r\n",
      "-   **Have I written custom code (as opposed to using a stock example script\r\n",
      "    provided in TensorFlow)**:\r\n",
      "-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n",
      "-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n",
      "    happens on a mobile device**:\r\n",
      "-   **TensorFlow installed from (source or binary)**:\r\n",
      "-   **TensorFlow version (use command below)**:\r\n",
      "-   **Python version**:\r\n",
      "-   **Bazel version (if compiling from source)**:\r\n",
      "-   **GCC/Compiler version (if compiling from source)**:\r\n",
      "-   **CUDA/cuDNN version**:\r\n",
      "-   **GPU model and memory**:\r\n",
      "-   **Exact command to reproduce**:\r\n",
      "\r\n",
      "You can collect some of this information using our environment capture script:\r\n",
      "\r\n",
      "https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n",
      "\r\n",
      "You can obtain the TensorFlow version with:\r\n",
      "\r\n",
      "```bash\r\n",
      "python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n",
      "```\r\n",
      "\r\n",
      "### Describe the problem\r\n",
      "File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\loader_impl.py\", line 113, in parse_saved_model\r\n",
      "    constants.SAVED_MODEL_FILENAME_PB))\r\n",
      "\r\n",
      "OSError: SavedModel file does not exist at: model_resnet152V2.h5/{saved_model.pbtxt|saved_model.pb}\r\n",
      "\r\n",
      "### Source code / logs\r\n",
      "\r\n",
      "from __future__ import division, print_function\r\n",
      "# coding=utf-8\r\n",
      "import sys\r\n",
      "import os\r\n",
      "import glob\r\n",
      "import re\r\n",
      "import numpy as np\r\n",
      "import tensorflow as tf\r\n",
      "import tensorflow as tf\r\n",
      "\r\n",
      "from tensorflow.compat.v1 import ConfigProto\r\n",
      "from tensorflow.compat.v1 import InteractiveSession\r\n",
      "\r\n",
      "config = ConfigProto()\r\n",
      "config.gpu_options.per_process_gpu_memory_fraction = 0.2\r\n",
      "config.gpu_options.allow_growth = True\r\n",
      "session = InteractiveSession(config=config)\r\n",
      "# Keras\r\n",
      "from tensorflow.keras.applications.resnet50 import preprocess_input\r\n",
      "from tensorflow.keras.models import load_model\r\n",
      "from tensorflow.keras.preprocessing import image\r\n",
      "\r\n",
      "# Flask utils\r\n",
      "from flask import Flask, redirect, url_for, request, render_template\r\n",
      "from werkzeug.utils import secure_filename\r\n",
      "#from gevent.pywsgi import WSGIServer\r\n",
      "\r\n",
      "# Define a flask app\r\n",
      "app = Flask(__name__)\r\n",
      "\r\n",
      "# Model saved with Keras model.save()\r\n",
      "MODEL_PATH ='model_resnet152V2.h5'\r\n",
      "\r\n",
      "# Load your trained model\r\n",
      "model = load_model(MODEL_PATH)\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "def model_predict(img_path, model):\r\n",
      "    print(img_path)\r\n",
      "    img = image.load_img(img_path, target_size=(224, 224))\r\n",
      "\r\n",
      "    # Preprocessing the image\r\n",
      "    x = image.img_to_array(img)\r\n",
      "    # x = np.true_divide(x, 255)\r\n",
      "    ## Scaling\r\n",
      "    x=x/255\r\n",
      "    x = np.expand_dims(x, axis=0)\r\n",
      "   \r\n",
      "\r\n",
      "    # Be careful how your trained model deals with the input\r\n",
      "    # otherwise, it won't make correct prediction!\r\n",
      "   # x = preprocess_input(x)\r\n",
      "\r\n",
      "    preds = model.predict(x)\r\n",
      "    preds=np.argmax(preds, axis=1)\r\n",
      "    if preds==0:\r\n",
      "        preds=\"The leaf is diseased cotton leaf\"\r\n",
      "    elif preds==1:\r\n",
      "        preds=\"The leaf is diseased cotton plant\"\r\n",
      "    elif preds==2:\r\n",
      "        preds=\"The leaf is fresh cotton leaf\"\r\n",
      "    else:\r\n",
      "        preds=\"The leaf is fresh cotton plant\"\r\n",
      "        \r\n",
      "    \r\n",
      "    \r\n",
      "    return preds\r\n",
      "\r\n",
      "\r\n",
      "@app.route('/', methods=['GET'])\r\n",
      "def index():\r\n",
      "    # Main page\r\n",
      "    return render_template('index.html')\r\n",
      "\r\n",
      "\r\n",
      "@app.route('/predict', methods=['GET', 'POST'])\r\n",
      "def upload():\r\n",
      "    if request.method == 'POST':\r\n",
      "        # Get the file from post request\r\n",
      "        f = request.files['file']\r\n",
      "\r\n",
      "        # Save the file to ./uploads\r\n",
      "        basepath = os.path.dirname(__file__)\r\n",
      "        file_path = os.path.join(\r\n",
      "            basepath, 'uploads', secure_filename(f.filename))\r\n",
      "        f.save(file_path)\r\n",
      "\r\n",
      "        # Make prediction\r\n",
      "        preds = model_predict(file_path, model)\r\n",
      "        result=preds\r\n",
      "        return result\r\n",
      "    return None\r\n",
      "\r\n",
      "\r\n",
      "if __name__ == '__main__':\r\n",
      "    app.run(port=5001,debug=True)\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "comp:keras\n",
      "stat:awaiting response\n",
      "\n",
      "\n",
      "issue title -  micro: port operator ELU kernel from lite with test\n",
      "issue body -  Complete implementation of TFLM operator ELU and associated TFLM test code.\r\n",
      "\r\n",
      "PR step 5 of the work to port operator ELU as tracked in Issue #46323\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:lite\n",
      "comp:micro\n",
      "prtype:bugfix\n",
      "ready to pull\n",
      "size:L\n",
      "\n",
      "\n",
      "issue title -  tf-mlir-translate can not handle the value of tf.const node with float_value\n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n",
      "- TensorFlow installed from (source or binary): source\r\n",
      "- TensorFlow version (use command below): libtensorflow_framework.so.2.5.0\r\n",
      "- Python version: Python 3.7.0\r\n",
      "- Bazel version (if compiling from source): bazel-3.7.2-linux-x86_64\r\n",
      "- GCC/Compiler version (if compiling from source):  10.1.0\r\n",
      "- CUDA/cuDNN version: 11.0\r\n",
      "- GPU model and memory: TITAN V 12066MiB\r\n",
      "\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "***Build command***\r\n",
      "In $LLVM_SRC_DIR:\r\n",
      "`git reset --hard 2bfe27da171e8a6dddac6c444c4bca003103941a`\r\n",
      "Then:\r\n",
      "`bazel build --distdir=$build_dir --override_repository=llvm-project=$LLVM_SRC_DIR   -c opt tensorflow/compiler/mlir:tf-mlir-translate`\r\n",
      "***Run command***\r\n",
      "`./tf-mlir-translate --graphdef-to-mlir ~/tem_new_graph_def.graphdef -o ~/model.mlir`\r\n",
      "or\r\n",
      "`./tf-mlir-translate --savedmodel-signaturedefs-to-mlir ~/model.savedmodel/ -o ~/model.mlir`\r\n",
      "\r\n",
      "When the mlir translator meet a float_val tensor, It just generated some messy hex. For example:\r\n",
      "**raw graphdef:**\r\n",
      "`node {\r\n",
      "  name: \"model/addf0/BiasAdd/ReadVariableOp\"\r\n",
      "  op: \"Const\"\r\n",
      "  attr {\r\n",
      "    key: \"dtype\"\r\n",
      "    value {\r\n",
      "      type: DT_FLOAT\r\n",
      "    }\r\n",
      "  }\r\n",
      "  attr {\r\n",
      "    key: \"value\"\r\n",
      "    value {\r\n",
      "      tensor {\r\n",
      "        dtype: DT_FLOAT\r\n",
      "        tensor_shape {\r\n",
      "          dim {\r\n",
      "            size: 256\r\n",
      "          }\r\n",
      "        }\r\n",
      "        float_val: 0.0051481337286531925\r\n",
      "        float_val: -0.004853762686252594\r\n",
      "        float_val: -0.0063857887871563435\r\n",
      "        float_val: -0.005939189810305834\r\n",
      "        float_val: -0.007605542428791523\r\n",
      "        float_val: -0.014029058627784252\r\n",
      "        float_val: 6.364182627294213e-05\r\n",
      "        float_val: 0.0026220008730888367\r\n",
      "        float_val: -0.006861857604235411\r\n",
      "        float_val: 0.011504308320581913\r\n",
      "        float_val: 0.00323308352380991\r\n",
      "        float_val: 0.004311520606279373\r\n",
      "        float_val: -0.00712333619594574\r\n",
      "        float_val: 0.0016613556072115898\r\n",
      "        float_val: -0.001945714931935072\r\n",
      "        float_val: -5.2431416406761855e-05\r\n",
      "        float_val: -0.0072508323937654495\r\n",
      "        float_val: 0.007718958426266909\r\n",
      "        float_val: -0.005794827360659838\r\n",
      "        float_val: -0.00043485683272592723\r\n",
      "        float_val: -0.011029565706849098\r\n",
      "        float_val: 0.00399430375546217\r\n",
      "        float_val: 0.009735933504998684\r\n",
      "        float_val: -0.005351892672479153\r\n",
      "        float_val: 0.0004388465313240886\r\n",
      "        float_val: 0.006414614152163267\r\n",
      "        float_val: -0.0023052822798490524\r\n",
      "        float_val: 0.0028045496437698603\r\n",
      "        float_val: 0.0013200563844293356\r\n",
      "        float_val: -0.008794997818768024\r\n",
      "        float_val: -0.0009779843967407942\r\n",
      "        float_val: -0.010262236930429935\r\n",
      "        float_val: 0.0024026455357670784\r\n",
      "        float_val: 0.008377766236662865\r\n",
      "        float_val: 0.0016489957924932241\r\n",
      "        float_val: 0.008051405660808086\r\n",
      "        float_val: -0.0026613050140440464\r\n",
      "        float_val: 0.000884789798874408\r\n",
      "        float_val: -0.00757144670933485\r\n",
      "        float_val: -0.00411588978022337\r\n",
      "        float_val: -0.006682147271931171\r\n",
      "        float_val: -0.0025500720366835594\r\n",
      "        float_val: 0.003163927234709263\r\n",
      "        float_val: -0.0024716767948120832\r\n",
      "        float_val: -0.011840238235890865\r\n",
      "        float_val: 0.002221206435933709\r\n",
      "        float_val: 0.010188544169068336\r\n",
      "        float_val: -0.0005134445964358747\r\n",
      "        float_val: 0.014506646431982517\r\n",
      "        float_val: 0.008010786958038807\r\n",
      "        float_val: 0.00045173970283940434\r\n",
      "        float_val: -0.006965363398194313\r\n",
      "        float_val: -0.0003679479705169797\r\n",
      "        float_val: -0.008194392547011375\r\n",
      "        float_val: -0.01170737948268652\r\n",
      "        float_val: -0.0030496888794004917\r\n",
      "        float_val: -0.005231104791164398\r\n",
      "        float_val: -0.006195568945258856\r\n",
      "        float_val: -0.010564603842794895\r\n",
      "        float_val: -0.007049794774502516\r\n",
      "        float_val: -0.0033882728312164545\r\n",
      "        float_val: -0.0076379235833883286\r\n",
      "        float_val: -0.0052634309977293015\r\n",
      "        float_val: 0.0020512063056230545\r\n",
      "        float_val: -0.010911918245255947\r\n",
      "        float_val: -0.0014321575872600079\r\n",
      "        float_val: -0.004976712167263031\r\n",
      "        float_val: -0.005636582151055336\r\n",
      "        float_val: 0.0057461741380393505\r\n",
      "        float_val: -0.0050669782795012\r\n",
      "        float_val: -0.005440800916403532\r\n",
      "        float_val: 0.016398049890995026\r\n",
      "        float_val: 0.005827216897159815\r\n",
      "        float_val: -0.0025648120790719986\r\n",
      "        float_val: -0.00967519823461771\r\n",
      "        float_val: -0.004590281751006842\r\n",
      "        float_val: 0.0002168136416003108\r\n",
      "        float_val: -0.009099945425987244\r\n",
      "        float_val: -0.007822085171937943\r\n",
      "        float_val: -0.004594044294208288\r\n",
      "        float_val: -0.00698000006377697\r\n",
      "        float_val: 0.002748528029769659\r\n",
      "        float_val: -0.006815034430474043\r\n",
      "        float_val: 0.008094011805951595\r\n",
      "        float_val: -0.005640577524900436\r\n",
      "        float_val: -0.005020929034799337\r\n",
      "        float_val: -0.009470895864069462\r\n",
      "        float_val: -0.002705982653424144\r\n",
      "        float_val: 0.004372714087367058\r\n",
      "        float_val: -0.002115572802722454\r\n",
      "        float_val: -0.008014006540179253\r\n",
      "        float_val: -0.00045102433068677783\r\n",
      "        float_val: 0.005882886704057455\r\n",
      "        float_val: -0.0012644800590351224\r\n",
      "        float_val: 0.005815275013446808\r\n",
      "        float_val: -0.009082064032554626\r\n",
      "        float_val: -0.004126546438783407\r\n",
      "        float_val: 0.015735354274511337\r\n",
      "        float_val: -0.00870969332754612\r\n",
      "        float_val: -0.011116635985672474\r\n",
      "        float_val: -0.0030493661761283875\r\n",
      "        float_val: -0.003578674979507923\r\n",
      "        float_val: 0.006664385553449392\r\n",
      "        float_val: 0.005230343900620937\r\n",
      "        float_val: -0.00039703131187707186\r\n",
      "        float_val: -0.0038723910693079233\r\n",
      "        float_val: -0.0011821255320683122\r\n",
      "        float_val: 0.006628826260566711\r\n",
      "        float_val: -0.0008820746443234384\r\n",
      "        float_val: -0.01215388160198927\r\n",
      "        float_val: -0.005438085645437241\r\n",
      "        float_val: -0.0014401013031601906\r\n",
      "        float_val: -0.0031687782611697912\r\n",
      "        float_val: 0.004439719021320343\r\n",
      "        float_val: -0.0010851756669580936\r\n",
      "        float_val: 0.00455262279137969\r\n",
      "        float_val: 0.01028736773878336\r\n",
      "        float_val: -0.01316842157393694\r\n",
      "        float_val: -0.007040669210255146\r\n",
      "        float_val: 0.005985334049910307\r\n",
      "        float_val: 0.001018555136397481\r\n",
      "        float_val: -0.006859580520540476\r\n",
      "        float_val: -0.0012270506704226136\r\n",
      "        float_val: 0.00039293334702961147\r\n",
      "        float_val: 0.00015758314111735672\r\n",
      "        float_val: -0.007701294496655464\r\n",
      "        float_val: -0.005402510054409504\r\n",
      "        float_val: 0.0027941293083131313\r\n",
      "        float_val: -0.00476961862295866\r\n",
      "        float_val: 0.013895658776164055\r\n",
      "        float_val: 0.0048489863984286785\r\n",
      "        float_val: -0.01240287534892559\r\n",
      "        float_val: 0.00227100751362741\r\n",
      "        float_val: 0.0004433704598341137\r\n",
      "        float_val: -0.00233777379617095\r\n",
      "        float_val: -0.004992697387933731\r\n",
      "        float_val: 0.0029940051026642323\r\n",
      "        float_val: -0.0024685945827513933\r\n",
      "        float_val: -0.004253252409398556\r\n",
      "        float_val: -0.010474653914570808\r\n",
      "        float_val: 0.016132526099681854\r\n",
      "        float_val: 0.007510742638260126\r\n",
      "        float_val: 0.011006304062902927\r\n",
      "        float_val: 0.020926298573613167\r\n",
      "        float_val: -0.0018459537532180548\r\n",
      "        float_val: -0.00524304062128067\r\n",
      "        float_val: -0.006811123341321945\r\n",
      "        float_val: -0.004207645542919636\r\n",
      "        float_val: -0.0026161535643041134\r\n",
      "        float_val: -0.00533562945201993\r\n",
      "        float_val: -0.003053524997085333\r\n",
      "        float_val: 0.012821650132536888\r\n",
      "        float_val: -0.00304988631978631\r\n",
      "        float_val: -0.004115841817110777\r\n",
      "        float_val: -0.0026411768049001694\r\n",
      "        float_val: 0.004588124807924032\r\n",
      "        float_val: -0.006611636374145746\r\n",
      "        float_val: -0.007796596735715866\r\n",
      "        float_val: -0.0005695070140063763\r\n",
      "        float_val: -0.005537876393646002\r\n",
      "        float_val: -0.0021315556950867176\r\n",
      "        float_val: 0.00860658660531044\r\n",
      "        float_val: 0.008062107488512993\r\n",
      "        float_val: -0.007444465532898903\r\n",
      "        float_val: 0.0004948594723828137\r\n",
      "        float_val: 0.013905920088291168\r\n",
      "        float_val: 0.01084441039711237\r\n",
      "        float_val: -0.006462818011641502\r\n",
      "        float_val: -0.007946974597871304\r\n",
      "        float_val: -0.00013547498383559287\r\n",
      "        float_val: 0.0029342391062527895\r\n",
      "        float_val: 0.0032939868979156017\r\n",
      "        float_val: -0.006928297225385904\r\n",
      "        float_val: 0.004277899395674467\r\n",
      "        float_val: -0.004152035806328058\r\n",
      "        float_val: 0.0007484358502551913\r\n",
      "        float_val: -0.006124483421444893\r\n",
      "        float_val: -0.0001521199446870014\r\n",
      "        float_val: -0.006839861627668142\r\n",
      "        float_val: 0.004457100760191679\r\n",
      "        float_val: -0.009015962481498718\r\n",
      "        float_val: 0.0029336384031921625\r\n",
      "        float_val: 0.0015798307722434402\r\n",
      "        float_val: 0.0014634429244324565\r\n",
      "        float_val: -0.007801746483892202\r\n",
      "        float_val: 0.0010955692268908024\r\n",
      "        float_val: -0.007367278914898634\r\n",
      "        float_val: -0.00868617556989193\r\n",
      "        float_val: -0.00434133131057024\r\n",
      "        float_val: -0.002788239624351263\r\n",
      "        float_val: -0.004975281655788422\r\n",
      "        float_val: -0.004753256682306528\r\n",
      "        float_val: -0.0012826395686715841\r\n",
      "        float_val: -0.0036673436406999826\r\n",
      "        float_val: -0.0059289475902915\r\n",
      "        float_val: 0.016369104385375977\r\n",
      "        float_val: -0.00635050144046545\r\n",
      "        float_val: 0.004774186760187149\r\n",
      "        float_val: 0.0037566213868558407\r\n",
      "        float_val: -0.008517655543982983\r\n",
      "        float_val: -0.004843609873205423\r\n",
      "        float_val: -0.007144990377128124\r\n",
      "        float_val: 0.016293589025735855\r\n",
      "        float_val: -0.007814271375536919\r\n",
      "        float_val: 0.011035293340682983\r\n",
      "        float_val: -0.007298213895410299\r\n",
      "        float_val: -0.009457477368414402\r\n",
      "        float_val: 0.004397874232381582\r\n",
      "        float_val: 0.017384257167577744\r\n",
      "        float_val: 0.007743040099740028\r\n",
      "        float_val: -0.00099231640342623\r\n",
      "        float_val: -0.005199010483920574\r\n",
      "        float_val: 0.003438242245465517\r\n",
      "        float_val: 0.008518864400684834\r\n",
      "        float_val: 0.0025021492037922144\r\n",
      "        float_val: -0.006360295228660107\r\n",
      "        float_val: 0.004405853804200888\r\n",
      "        float_val: 0.0017105545848608017\r\n",
      "        float_val: 0.001984659116715193\r\n",
      "        float_val: -0.006431774701923132\r\n",
      "        float_val: -0.006175287999212742\r\n",
      "        float_val: -0.007920660078525543\r\n",
      "        float_val: 0.010450722649693489\r\n",
      "        float_val: -0.005794399883598089\r\n",
      "        float_val: -0.007095997221767902\r\n",
      "        float_val: -0.005131132435053587\r\n",
      "        float_val: 0.001255601178854704\r\n",
      "        float_val: -0.003194880671799183\r\n",
      "        float_val: -0.008282691240310669\r\n",
      "        float_val: 0.02166000008583069\r\n",
      "        float_val: 0.007440856657922268\r\n",
      "        float_val: -0.0023202376905828714\r\n",
      "        float_val: 0.012448390014469624\r\n",
      "        float_val: 0.0006656379555352032\r\n",
      "        float_val: -0.0024139166343957186\r\n",
      "        float_val: -0.003526628715917468\r\n",
      "        float_val: -0.0010204833233729005\r\n",
      "        float_val: -0.007021935656666756\r\n",
      "        float_val: 0.010514422319829464\r\n",
      "        float_val: 0.005775094032287598\r\n",
      "        float_val: -0.010260146111249924\r\n",
      "        float_val: 0.004442145116627216\r\n",
      "        float_val: 0.002659732010215521\r\n",
      "        float_val: -0.0016509274719282985\r\n",
      "        float_val: -0.001772205694578588\r\n",
      "        float_val: 0.0015684027457609773\r\n",
      "        float_val: 0.001310656196437776\r\n",
      "        float_val: 0.0006843036971986294\r\n",
      "        float_val: -0.00796379055827856\r\n",
      "        float_val: -0.0076110996305942535\r\n",
      "        float_val: 0.0022425760980695486\r\n",
      "        float_val: -0.0026748101226985455\r\n",
      "        float_val: 0.004271205514669418\r\n",
      "        float_val: -0.0023577818647027016\r\n",
      "        float_val: -0.010191367007791996\r\n",
      "        float_val: -0.00014487635053228587\r\n",
      "      }\r\n",
      "    }\r\n",
      "  }\r\n",
      "}`\r\n",
      "\r\n",
      "**TF milr generated by tf-mlir-translate:**\r\n",
      "`%outputs_408, %control_409 = tf_executor.island wraps \"tf.Const\"() {device = \"\", value = dense<\"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tensor<256xf32>} : () -> tensor<256xf32>`\r\n",
      "\r\n",
      "**when change all float_val to one, the result shown like this:**\r\n",
      "` %outputs_408, %control_409 = tf_executor.island wraps \"tf.Const\"() {device = \"\", value = dense<1.000000e+00> :tensor<256xf32>} : () -> tensor<256xf32>`\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "It should be outputted like this:\r\n",
      "`%outputs_408, %control_409 = tf_executor.island wraps \"tf.Const\"() {device = \"\", value = dense<\"0.0051481337286531925,-0.004853762686252594,……,-0.00014487635053228587\"> : tensor<256xf32>} : () -> tensor<256xf32>`\n",
      "issue labels - \n",
      "TF 2.5\n",
      "comp:apis\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Simple reproduction of issue decribed in #46937\n",
      "issue body -  Simple reproduction of issue decribed in #46937\r\n",
      "\r\n",
      "Expected output (i.e. what I get on x86):\r\n",
      "```\r\n",
      "make -f tensorflow/lite/micro/tools/make/Makefile run_test_renode -j8\r\n",
      "```\r\n",
      "gives:\r\n",
      "```\r\n",
      "a: 45\r\n",
      "init_to_false: 0\r\n",
      "Was initialized to false\r\n",
      "init_to_false: 1\r\n",
      "init_to_true: 1\r\n",
      "init_to_nullptr: 0\r\n",
      "```\r\n",
      "\r\n",
      "What I get with Renode + bluepill:\r\n",
      "```\r\n",
      "make -f tensorflow/lite/micro/tools/make/Makefile TARGET=bluepill test_renode -j8\r\n",
      "tensorflow/lite/micro/tools/make/downloads/renode/renode\r\n",
      "```\r\n",
      "and in the renode terminal:\r\n",
      "```\r\n",
      "Clear; include @tensorflow/lite/micro/testing/bluepill_nontest.resc; sysbus LoadELF @tensorflow/lite/micro/tools/make/gen/bluepill_cortex-m3_default/bin/test_renode; start\r\n",
      "```\r\n",
      "\r\n",
      "gives:\r\n",
      "```\r\n",
      "a: 45\r\n",
      "init_to_false: 239\r\n",
      "Was not initialized to false\r\n",
      "init_to_false: 1\r\n",
      "init_to_true: 1\r\n",
      "init_to_nullptr: -559038737\r\n",
      "```\r\n",
      "\r\n",
      "Need to debug further, but creating a PR in case @PiotrZierhoffer has any ideas.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "prtype:bugfix\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  Note string optimizer name is case-insensitive\n",
      "issue body -  A minor documentation addition. For clarity, it can be noted that the optimizer string can be put in any casing in `model.compile`, i.e. case-insensitive. e.g.\r\n",
      "\r\n",
      "```python\r\n",
      "model.compile(optimizer='RMSprop')\r\n",
      "model.compile(optimizer='rmsprop')\r\n",
      "model.compile(optimizer='RMSPROP')\r\n",
      "```\r\n",
      "\r\n",
      "→ Are all the same. It makes sense to tell the user they do not have to mind the casing.\r\n",
      "\r\n",
      "This is because whatever the user inputs as an optimizer string is converted to a lowercase string in [optimizers.py#L81](https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/python/keras/optimizers.py#L81); and then compared to other lower-cased optimizer strings.\r\n",
      "\r\n",
      "Have a great day still ☀️\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:keras\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Non-deterministic graph when custom_gradient has watched variables\n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux 7.8.2003\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n",
      "- TensorFlow installed from (source or binary): binary\r\n",
      "- TensorFlow version (use command below): 2.5.0\r\n",
      "- Python version: 3.7.4\r\n",
      "- Bazel version (if compiling from source): N/A\r\n",
      "- GCC/Compiler version (if compiling from source): N/A\r\n",
      "- CUDA/cuDNN version: N/A\r\n",
      "- GPU model and memory: N/A\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "\r\n",
      "`tf.custom_gradient()` keeps track of variables that are not part of the inputs and returns gradients for them. When there are multiple such variables, the created graph (in non-eager mode) is non-deterministic. The reason is that watched variable references are stored in a frozenset and since these references are essentially ids that have different ordering from one python invocation to another, the generated graph is different across different runs.\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "\r\n",
      "The generated graph in the aforementioned case should be deterministic across different runs.\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "\r\n",
      "Run the python script multiple times and diff the generated graphdefs. They are going to be different.\r\n",
      "\r\n",
      "```bash\r\n",
      "$ python run.py 1  # creates tf_graph.pbtxt.1\r\n",
      "$ python run.py 2  # creates tf_graph.pbtxt.2\r\n",
      "$ python run.py 3  # creates tf_graph.pbtxt.3\r\n",
      "```\r\n",
      "\r\n",
      "```python\r\n",
      "# run.py\r\n",
      "import sys\r\n",
      "import tensorflow as tf\r\n",
      "import tensorflow.compat.v1 as v1\r\n",
      "\r\n",
      "\r\n",
      "g = v1.Graph()\r\n",
      "with g.as_default():\r\n",
      "    # Create a bunch of variables that are captured in custom_gradient\r\n",
      "    captured = [tf.Variable(float(i)) for i in range(1, 20)]\r\n",
      "\r\n",
      "    @tf.custom_gradient\r\n",
      "    def FuncMult(x):\r\n",
      "        def GradMult(*dys, variables=None):\r\n",
      "            return (\r\n",
      "                4. * sum(captured) * dys[0],\r\n",
      "                [(i + 1) * x * y for i in range(len(variables))]\r\n",
      "            )\r\n",
      "\r\n",
      "        return x * sum(captured), GradMult\r\n",
      "\r\n",
      "    x = tf.Variable(6.)\r\n",
      "    y = FuncMult(x)\r\n",
      "    grad = tf.gradients(y, [x])\r\n",
      "\r\n",
      "graph_def = g.as_graph_def(add_shapes=True)\r\n",
      "with open(f\"tf_graph.pbtxt.{sys.argv[1]}\", \"w\") as f:\r\n",
      "    f.write(str(graph_def))\r\n",
      "```\r\n",
      "\r\n",
      "**Fix**\r\n",
      "\r\n",
      "Please see #47266 for a potential fix of this issue.\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.5\n",
      "comp:ops\n",
      "stat:awaiting tensorflower\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Sort variables by name in custom gradient for determinism\n",
      "issue body -  This PR is the fix for issue https://github.com/tensorflow/tensorflow/issues/47267.\r\n",
      "\r\n",
      "`tf.custom_gradient()` keeps track of variables that are not part of the inputs and returns gradients for them. When there are multiple such variables, the created graph (in non-eager mode) is non-deterministic. The reason is that watched variable references are stored in a frozenset and since these references are essentially ids that have different ordering from one python invocation to another, the generated graph is different across different runs.\r\n",
      "\r\n",
      "This change deterministically orders the watched variables by their name to avoid that issue.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:ops\n",
      "prtype:bugfix\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  [Code provided] Weird tf.function behaviour with boolean inputs after trace\n",
      "issue body -  When passed a tensor (dtype = tf.bool) from an tf.function to another tf.function, its behaviour become weird after first time call (trace).\r\n",
      "\r\n",
      "https://colab.research.google.com/drive/1id1t9L8xypGbavcA2hqtNEDQ4VFwJ40I?usp=sharing\n",
      "issue labels - \n",
      "Fixed in Nightly\n",
      "TF 2.4\n",
      "comp:tf.function\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  tflite building is taking hours\n",
      "issue body -  <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): yocto\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "- TensorFlow installed from (source or binary): source \r\n",
      "- TensorFlow version: 2.4 \r\n",
      "- Python version: 3.6\r\n",
      "- Installed using virtualenv? pip? conda?:\r\n",
      "- Bazel version (if compiling from source): 3.7\r\n",
      "- GCC/Compiler version (if compiling from source): 10.2\r\n",
      "- CUDA/cuDNN version:\r\n",
      "- GPU model and memory:\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "I am trying to compile tensorflow-lite and benchmark_model from source using yocto. I already managed to do it using a yocto recipe based on the Makefile. But the Makefile support is limited, so I need to compile tensorflow-lite and benchmark_model using bazel. But bazel build takes several hours!  \r\n",
      "\r\n",
      "**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n",
      "I started from http://git.yoctoproject.org/cgit/cgit.cgi/meta-tensorflow/ and slightly modified http://git.yoctoproject.org/cgit/cgit.cgi/meta-tensorflow/tree/recipes-framework/tensorflow/tensorflow_2.4.0.bb to compile tensorflow-lite instead of tensorflow. Basically I just modified the do_compile so now it is:\r\n",
      "```\r\n",
      "do_compile () {\r\n",
      "    export CT_NAME=$(echo ${HOST_PREFIX} | rev | cut -c 2- | rev)\r\n",
      "    unset CC\r\n",
      "    ${BAZEL} build \\\r\n",
      "        --jobs=40 \\\r\n",
      "        ${TF_ARGS_EXTRA} \\\r\n",
      "        -c opt \\\r\n",
      "        --cpu=${BAZEL_TARGET_CPU} \\\r\n",
      "        --crosstool_top=@local_config_yocto_compiler//:toolchain \\\r\n",
      "        --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \\\r\n",
      "        --copt -DTF_LITE_DISABLE_X86_NEON --copt -DCL_DELEGATE_NO_GL \\\r\n",
      "        //tensorflow/lite:libtensorflowlite.so \\\r\n",
      "        //tensorflow/lite/tools/benchmark:benchmark_model \\\r\n",
      "        //tensorflow/tools/pip_package:build_pip_package \\\r\n",
      "        ${TF_TARGET_EXTRA}\r\n",
      "}\r\n",
      "```\r\n",
      "I also updated the do_install function with the correct path (tensorflow --> tensorflow-lite)\r\n",
      "\r\n",
      "And the recipe works. I was able to install `benchmark_model` on the target and run it. \r\n",
      "The issue here is that, the build takes literally 3 hours to complete while with the Makefile it tooks only few minutes. \r\n",
      "I am building on a very powerfull machine with 40 cores and 189G of RAM. When using `htop` to monitor cpu load, I see only a few processes although I added `--jobs=40` on the bazel build command. \r\n",
      "\r\n",
      "Am I missing something here? \r\n",
      "Thank you for your help\r\n",
      "\r\n",
      "**Any other info / logs**\r\n",
      "Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:lite\n",
      "stat:awaiting response\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  [TFL] Remove unnecessary Reshape before FullyConnected.\n",
      "issue body -  Remove unnecessary Reshape before FullyConnected. TFLite support N-D input of shape [batch_size, ..., channels] for FullyConnected. When keep_num_dims is false, all dimensions except the last one are collapsed into a single dimension. Therefore, it's safe to remove Reshape when it does not alter the last dimension.\r\n",
      "\r\n",
      "The pattern is found in the official [mobilebert](https://www.tensorflow.org/lite/models/bert_qa/overview). Reshape op is not expensive but will block the folding of two consecutive FullyConnected Op (unimplemented yet).\r\n",
      "\r\n",
      "I benchmark with simple model here, which is similar to the one found in provided tflite mobilebert model `bert/encoder/layer_0/bottleneck/attention/FakeLayerNorm/add`.\r\n",
      "\r\n",
      "```python\r\n",
      "model = tf.keras.Sequential([\r\n",
      "  tf.keras.layers.Dense(128, batch_input_shape=(1, 384, 128), use_bias=True),\r\n",
      "])\r\n",
      "```\r\n",
      "\r\n",
      "It contains a pattern of Reshape-FullyConnected-Reshape. This PR remove the first Reshape so it becomes FullyConnected-Reshape. The performance on Pixel3a is in the following\r\n",
      "\r\n",
      "-----\r\n",
      "Before\r\n",
      "```\r\n",
      "count=1035 first=987 curr=923 min=913 max=1070 avg=939.102 std=27\r\n",
      "```\r\n",
      "-----\r\n",
      "After\r\n",
      "```\r\n",
      "count=1049 first=991 curr=910 min=894 max=1069 avg=923.888 std=31\r\n",
      "```\r\n",
      "-----\r\n",
      "\r\n",
      "For end2end testing, I download the savedmodel from [google-research/mobilebert](https://github.com/google-research/google-research/tree/master/mobilebert) (floating point) and convert to tflite.\r\n",
      "\r\n",
      "-----\r\n",
      "Before\r\n",
      "```\r\n",
      "count=94 first=1612190 curr=1611814 min=1601869 max=1623227 avg=1.61185e+06 std=4439\r\n",
      "```\r\n",
      "-----\r\n",
      "After\r\n",
      "```\r\n",
      "count=94 first=1601583 curr=1600427 min=1590411 max=1614870 avg=1.60146e+06 std=5378\r\n",
      "```\r\n",
      "-----\r\n",
      "\r\n",
      "There is no significant performance regression or improvement as FullyConnected ops dominate the running time.\r\n",
      "\r\n",
      "I have to reword that the Reshape that will block the folding of two consecutive FullyConnected disappears if I convert the model  manually with tf-nightly, but the pattern still happens very often in mobilebert as well as using `tf.keras.layers.Dense` with 3D or higher input. Also, I cannot really find the pattern stated in my previous response https://github.com/tensorflow/tensorflow/pull/47258#issuecomment-782819838 if I convert the model manually with tf-nightly.\r\n",
      "\r\n",
      "Moreover, even with the simple two consecutive dense layers with 3D input, for example,\r\n",
      "\r\n",
      "```python\r\n",
      "model = tf.keras.Sequential([\r\n",
      "  tf.keras.layers.Dense(2, batch_input_shape=(1, 384, 512)),\r\n",
      "  tf.keras.layers.Dense(3)\r\n",
      "])\r\n",
      "```\r\n",
      "\r\n",
      "the Reshape ops between two dense layers are optimized out.\n",
      "issue labels - \n",
      "cla: yes\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  MultiWorkerMirroredStrategy rendezvous Did not find key \n",
      "issue body -  Hi, I want to use rendezvous to transfer tensor between two machines within the scope of MultiWorkerMirroredStrategy, but it complains an Internal error `Did not find key my_tensor_name`.\r\n",
      "\r\n",
      "What is the device name different machines used to identify each other in the cluster? Can two machines identify each other with such device name? \r\n",
      "```c++\r\n",
      "const char* dst_device_name = \"/job:worker/replica:0/task:1/device:CPU:0\";\r\n",
      "const char* src_device_name= \"/job:worker/replica:0/task:0/device:CPU:0\";\r\n",
      "```\r\n",
      "\r\n",
      "BTW, `typeid(*OpKernelContext->rendezvous()).name()` show `SimpleRendezvous` in one machine and `IntraProcessRendezvous` for another machine. Why not `*Remote*Rendezvous` is created? \r\n",
      "\r\n",
      "Here is my code snippet:\r\n",
      "1. my custom op\r\n",
      "```c++\r\n",
      "#include \"tensorflow/core/framework/op_kernel.h\"\r\n",
      "#include <chrono>\r\n",
      "#include <thread>\r\n",
      "\r\n",
      "namespace tensorflow {\r\n",
      "\r\n",
      "using GPUDevice = Eigen::GpuDevice;\r\n",
      "using CPUDevice = Eigen::ThreadPoolDevice; \r\n",
      "\r\n",
      "class TestRendezvousMultiWorkerOp : public AsyncOpKernel {\r\n",
      "public:\r\n",
      "    explicit TestRendezvousMultiWorkerOp(OpKernelConstruction* ctx) : AsyncOpKernel(ctx) {\r\n",
      "        OP_REQUIRES_OK(ctx, ctx->GetAttr(\"task_id\", &task_id_));\r\n",
      "        OP_REQUIRES(ctx, task_id_ >= 0, \r\n",
      "            errors::Internal(\"task_id should be >= 0, but got \", task_id_));\r\n",
      "    }\r\n",
      "    void ComputeAsync(OpKernelContext* ctx, DoneCallback done) override {\r\n",
      "        OP_REQUIRES_ASYNC(ctx, ctx->rendezvous() != nullptr,\r\n",
      "            errors::Internal(\"Op kernel context needs to provide a rendezvous.\"), done);\r\n",
      "\r\n",
      "        const char* dst_device_name = \"/job:worker/replica:0/task:1/device:CPU:0\";\r\n",
      "        const char* src_device_name = \"/job:worker/replica:0/task:0/device:CPU:0\";\r\n",
      "\r\n",
      "        std::string send_key = Rendezvous::CreateKey(\r\n",
      "                            src_device_name /*src_device*/,\r\n",
      "                            123/*src_incarnation*/,\r\n",
      "                            dst_device_name /*dst_device*/,\r\n",
      "                            \"test_rendezvous_string\"/*name*/,\r\n",
      "                            FrameAndIter(0, 0)/*frame_and_iter*/);\r\n",
      "\r\n",
      "        Rendezvous::ParsedKey parsed_key;\r\n",
      "        OP_REQUIRES_OK_ASYNC(ctx, Rendezvous::ParseKey(send_key, &parsed_key), done);\r\n",
      "        \r\n",
      "        Rendezvous::Args args;\r\n",
      "\r\n",
      "        const Tensor* input_tensor = nullptr;\r\n",
      "        OP_REQUIRES_OK_ASYNC(ctx, ctx->input(\"value\", &input_tensor), done);\r\n",
      "        Tensor* out_tensor = nullptr;\r\n",
      "        OP_REQUIRES_OK_ASYNC(ctx, ctx->allocate_output(0, input_tensor->shape(), &out_tensor), done);\r\n",
      "\r\n",
      "        if (task_id_ == 0) {\r\n",
      "            OP_REQUIRES_OK_ASYNC(ctx, ctx->rendezvous()->Send(parsed_key, args, *input_tensor, ctx->is_input_dead()), done);\r\n",
      "            std::memcpy(out_tensor->data(), input_tensor->data(), input_tensor->TotalBytes());\r\n",
      "            std::this_thread::sleep_for(std::chrono::seconds(5));\r\n",
      "        } else {\r\n",
      "            bool is_dead = false;\r\n",
      "            OP_REQUIRES_OK_ASYNC(ctx, ctx->rendezvous()->Recv(parsed_key, args, out_tensor, &is_dead), done);\r\n",
      "        }\r\n",
      "\r\n",
      "        done();\r\n",
      "        \r\n",
      "    }\r\n",
      "private:\r\n",
      "    int task_id_;\r\n",
      "};\r\n",
      "\r\n",
      "REGISTER_KERNEL_BUILDER(Name(\"TestRendezvousMultiWorker\").Device(DEVICE_CPU), \r\n",
      "                        TestRendezvousMultiWorkerOp);\r\n",
      "\r\n",
      "} // namespace tensorflow\r\n",
      "```\r\n",
      "\r\n",
      "2. my python script\r\n",
      "```python\r\n",
      "import tensorflow as tf\r\n",
      "import os, json\r\n",
      "\r\n",
      "os.environ['CUDA_VISIBLE_DEVICES'] = \"\"\r\n",
      "\r\n",
      "os.environ[\"TF_CONFIG\"] = json.dumps({\r\n",
      "    \"cluster\": {\"worker\": [\"host1:port\", \"host2:port\"]},\r\n",
      "    \"task\": {\"type\": \"worker\", \"index\": args.task_id}\r\n",
      "})\r\n",
      "\r\n",
      "resolver = tf.distribute.cluster_resolver.TFConfigClusterResolver()\r\n",
      "strategy = tf.distribute.MultiWorkerMirroredStrategy(resolver)\r\n",
      "\r\n",
      "import my_custom_lib\r\n",
      "\r\n",
      "@tf.function\r\n",
      "def _test_step(task_id):\r\n",
      "    value = tf.constant([1.0, 2.0])\r\n",
      "    out = my_custom_lib.test_rendezvous_multi_worker(value, task_id=task_id)\r\n",
      "    return out\r\n",
      "\r\n",
      "out = strategy.run(_test_step, args=(args.task_id,))\r\n",
      "print(out)\r\n",
      "```\r\n",
      "\r\n",
      "3. the python3 command\r\n",
      "```python\r\n",
      "python3 file.py --task_id=0 # on machine 1\r\n",
      "python3 file.py --task_id=1 # on machine 2\r\n",
      "```\n",
      "issue labels - \n",
      "comp:dist-strat\n",
      "stat:awaiting tensorflower\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  First Commit for issue #47216\n",
      "issue body -  I have made some changes to address issue #47216 - https://github.com/tensorflow/tensorflow/issues/47216\r\n",
      "This is my first pull request to TensorFlow, so I would appreciate complete feedback including the coding style too.\r\n",
      "\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:keras\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  Why TF do not support strides = [1,0] or [0,1] for conv2d?\n",
      "issue body -  It will mean the kernel moves on only one direction. \r\n",
      "Are there alternative way to do it?\r\n",
      "Thank you very much.\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:apis\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Add GPU implementation of SparseReshape\n",
      "issue body -  This follows https://github.com/tensorflow/tensorflow/pull/46275.\r\n",
      "\r\n",
      "cc @nluehr \n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:core\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  Please Do not merge. This is a dummy PR for checking my accesses.\n",
      "issue body -  \n",
      "issue labels - \n",
      "cla: no\n",
      "size:XL\n",
      "\n",
      "\n",
      "issue title -  ValueError: Expected `model` argument to be a functional `Model` instance, but got a subclass model instead.\n",
      "issue body -  <em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n",
      "\r\n",
      "\r\n",
      "**System information**\r\n",
      "- TensorFlow version (you are using): 2.4.1\r\n",
      "- Are you willing to contribute it (Yes/No): Maybe, unless there's some way to achieve the same thing already\r\n",
      "\r\n",
      "**Describe the feature and the current behavior/state.**\r\n",
      "\r\n",
      "I need to copy a keras model and there is no way that I know of which can be done unless the model **is not** a `tf.keras.models.Model()` subclass. \r\n",
      "\r\n",
      "**Note:** The use `copy.deepcopy()` will work without giving any errors however it will result in another error whenever the copy is used:\r\n",
      "\r\n",
      "Results in:\r\n",
      "\r\n",
      "    /usr/local/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py:569 _run_internal_graph  **\r\n",
      "        assert x_id in tensor_dict, 'Could not compute output ' + str(x)\r\n",
      "    \r\n",
      "    AssertionError: Could not compute output KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.int64, name=None), name='tf.math.argmax/ArgMax:0', description=\"created by layer 'tf.math.argmax'\")\r\n",
      "\r\n",
      "Example:\r\n",
      "\r\n",
      "    import tensorflow as tf\r\n",
      "    \r\n",
      "    class MyModel(tf.keras.Model):\r\n",
      "    \r\n",
      "      def __init__(self):\r\n",
      "        super(MyModel, self).__init__()\r\n",
      "        self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)\r\n",
      "        self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)\r\n",
      "        self.dropout = tf.keras.layers.Dropout(0.5)\r\n",
      "    \r\n",
      "      def call(self, inputs, training=False):\r\n",
      "        x = self.dense1(inputs)\r\n",
      "        if training:\r\n",
      "          x = self.dropout(x, training=training)\r\n",
      "        return self.dense2(x)\r\n",
      "    \r\n",
      "    \r\n",
      "    if __name__ == '__main__':\r\n",
      "        model1 = MyModel()\r\n",
      "        model2 = tf.keras.models.clone_model(model1)\r\n",
      "\r\n",
      "Results in:\r\n",
      "\r\n",
      "    Traceback (most recent call last):\r\n",
      "      File \"/Users/emadboctor/Library/Application Support/JetBrains/PyCharm2020.3/scratches/scratch.py\", line 600, in <module>\r\n",
      "        model2 = tf.keras.models.clone_model(model1)\r\n",
      "      File \"/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/models.py\", line 430, in clone_model\r\n",
      "        return _clone_functional_model(\r\n",
      "      File \"/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/models.py\", line 171, in _clone_functional_model\r\n",
      "        raise ValueError('Expected `model` argument '\r\n",
      "    ValueError: Expected `model` argument to be a functional `Model` instance, but got a subclass model instead.\r\n",
      "\r\n",
      "**Will this change the current api? How?** I don't know\r\n",
      "\r\n",
      "**Who will benefit with this feature?** Whoever wants to create n copies of a custom model without writing n lines for achieving the same thing.\r\n",
      "\r\n",
      "**Any Other info.**\r\n",
      "\n",
      "issue labels - \n",
      "comp:keras\n",
      "stat:awaiting response\n",
      "type:feature\n",
      "\n",
      "\n",
      "issue title -  TensorFlow lite support for snapdragon 888/875 dsp\n",
      "issue body -  <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Mobile device Snapdragon 875/888\r\n",
      "- TensorFlow version 2.4.0:\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "Problem description:\r\n",
      "\r\n",
      "when running my tesorflow lite app that need to use the hexagon dsp delegate.\r\n",
      "I get the following error:\r\n",
      "\r\n",
      "\"WARNING: Failed to fetch Hexagon NN version. This might be because you're using incompatible versions of libhexagon_interface and libhexagon_nn_skel. You must use compatible versions. Refer to Tensorflow Lite Hexagon Delegate Guide.\r\n",
      "INFO: Hexagon Delegate is not supported\"\r\n",
      "\r\n",
      "from https://www.tensorflow.org/lite/performance/hexagon_delegate I can't see that you support the snapdragon version I'm using 875/888 , is the Hexagon delegate doesn't support this new snapdragon device or am I'm missing something ?\r\n",
      "When will it be supported ?\r\n",
      "\r\n",
      "\r\n",
      "Thanks\r\n",
      "Shuki\r\n",
      "\n",
      "issue labels - \n",
      "comp:lite\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  Cannot convert a symbolic Tensor (gru/strided_slice:0) to a numpy array.\n",
      "issue body -  **System information**\r\n",
      "- OS Platform and Distribution: Arch Linux\r\n",
      "- TensorFlow installed from package: python-tensorflow\r\n",
      "- TensorFlow version: unknown 2.4.1\r\n",
      "- Python version: 3.9.1\r\n",
      "- CUDA/cuDNN version: No GPU\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "```python\r\n",
      "from tensorflow.keras.layers import Embedding, Input, GRU\r\n",
      "\r\n",
      "x = Input(shape=(None,))\r\n",
      "x = Embedding(input_dim=50, output_dim=16, mask_zero=True)(x)\r\n",
      "x = GRU(units=256)(x)\r\n",
      "```\r\n",
      "\r\n",
      "**Other info / logs**\r\n",
      "```\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/jnphilipp/Nextcloud/Code/jnphilipp/deep_learning/test3.py\", line 5, in <module>\r\n",
      "    x = GRU(units=256)(x)\r\n",
      "  File \"/usr/lib/python3.9/site-packages/tensorflow/python/keras/layers/recurrent.py\", line 660, in __call__\r\n",
      "    return super(RNN, self).__call__(inputs, **kwargs)\r\n",
      "  File \"/usr/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 951, in __call__\r\n",
      "    return self._functional_construction_call(inputs, args, kwargs,\r\n",
      "  File \"/usr/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 1090, in _functional_construction_call\r\n",
      "    outputs = self._keras_tensor_symbolic_call(\r\n",
      "  File \"/usr/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 822, in _keras_tensor_symbolic_call\r\n",
      "    return self._infer_output_signature(inputs, args, kwargs, input_masks)\r\n",
      "  File \"/usr/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 863, in _infer_output_signature\r\n",
      "    outputs = call_fn(inputs, *args, **kwargs)\r\n",
      "  File \"/usr/lib/python3.9/site-packages/tensorflow/python/keras/layers/recurrent_v2.py\", line 439, in call\r\n",
      "    inputs, initial_state, _ = self._process_inputs(inputs, initial_state, None)\r\n",
      "  File \"/usr/lib/python3.9/site-packages/tensorflow/python/keras/layers/recurrent.py\", line 859, in _process_inputs\r\n",
      "    initial_state = self.get_initial_state(inputs)\r\n",
      "  File \"/usr/lib/python3.9/site-packages/tensorflow/python/keras/layers/recurrent.py\", line 642, in get_initial_state\r\n",
      "    init_state = get_initial_state_fn(\r\n",
      "  File \"/usr/lib/python3.9/site-packages/tensorflow/python/keras/layers/recurrent.py\", line 1948, in get_initial_state\r\n",
      "    return _generate_zero_filled_state_for_cell(self, inputs, batch_size, dtype)\r\n",
      "  File \"/usr/lib/python3.9/site-packages/tensorflow/python/keras/layers/recurrent.py\", line 2987, in _generate_zero_filled_state_for_cell\r\n",
      "    return _generate_zero_filled_state(batch_size, cell.state_size, dtype)\r\n",
      "  File \"/usr/lib/python3.9/site-packages/tensorflow/python/keras/layers/recurrent.py\", line 3005, in _generate_zero_filled_state\r\n",
      "    return create_zeros(state_size)\r\n",
      "  File \"/usr/lib/python3.9/site-packages/tensorflow/python/keras/layers/recurrent.py\", line 3000, in create_zeros\r\n",
      "    return array_ops.zeros(init_state_size, dtype=dtype)\r\n",
      "  File \"/usr/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\r\n",
      "    return target(*args, **kwargs)\r\n",
      "  File \"/usr/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py\", line 2819, in wrapped\r\n",
      "    tensor = fun(*args, **kwargs)\r\n",
      "  File \"/usr/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py\", line 2868, in zeros\r\n",
      "    output = _constant_if_small(zero, shape, dtype, name)\r\n",
      "  File \"/usr/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py\", line 2804, in _constant_if_small\r\n",
      "    if np.prod(shape) < 1000:\r\n",
      "  File \"<__array_function__ internals>\", line 5, in prod\r\n",
      "  File \"/usr/lib/python3.9/site-packages/numpy/core/fromnumeric.py\", line 3030, in prod\r\n",
      "    return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out,\r\n",
      "  File \"/usr/lib/python3.9/site-packages/numpy/core/fromnumeric.py\", line 87, in _wrapreduction\r\n",
      "    return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\r\n",
      "  File \"/usr/lib/python3.9/site-packages/tensorflow/python/framework/ops.py\", line 852, in __array__\r\n",
      "    raise NotImplementedError(\r\n",
      "NotImplementedError: Cannot convert a symbolic Tensor (gru/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported\r\n",
      "```\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:keras\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  TFLM: Support null pointer bias in CMSIS-NN fully_connected\n",
      "issue body -  Remove bias null pointer checks as arm_fully_connected_s8 now supports it.\r\n",
      "\r\n",
      "Fix for: https://github.com/tensorflow/tensorflow/issues/47237\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  Edited as it was spam\n",
      "issue body -  Edited as it was spam\n",
      "issue labels - \n",
      "invalid\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  Fix the Xtensa Vision P6 build.\n",
      "issue body -  https://github.com/tensorflow/tensorflow/pull/47199 broke the Vision P6 build with the following error message:\r\n",
      "```\r\n",
      "tensorflow/lite/micro/kernels/xtensa/quantize.cc:144:5: error: static_assert failed \"Unsupported xtensa architecture.\"\r\n",
      "    static_assert(false, \"Unsupported xtensa architecture.\");\r\n",
      "```\r\n",
      "\r\n",
      "Manually confirmed that with this change, the following command passes:\r\n",
      "```\r\n",
      "make -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=vision_p6 XTENSA_CORE=P6_200528 test -j8\r\n",
      "```\r\n",
      "\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Update bazel command to `bazel run` instead of `bazel test`.\n",
      "issue body -  `bazel test` puts the error logs into a file instead of on the terminal, whereas `bazel run` will show all the logs on the terminal, which is nicer when debugging locally.\n",
      "\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Documentation for how to add new optimized kernels to TFLM.\n",
      "issue body -  \n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  [Grappler] Add support for QuantizeAndDequantizeV4\n",
      "issue body -  52df91c5634e6c666843849a1c6ff29b3d2676be added `QuantizeAndDequantizeV4` which is used in `tf.quantization.quantize_and_dequantize_v2` and deprecated `QuantizeAndDequantizeV2`.\r\n",
      "\r\n",
      "This PR adds  `QuantizeAndDequantizeV4` support to grappler which can be a simple alias to `QuantizeAndDequantizeV2` since they share the same forward kernel implementation:\r\n",
      "https://github.com/tensorflow/tensorflow/blob/e43be76009614be88454d2fdf2fe702acc5bab77/tensorflow/core/kernels/quantize_and_dequantize_op.cc#L419-L422\r\n",
      "\r\n",
      "These changes should be covered by the existing unittests.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:grappler\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Update avgpooling_op.cc for GPU kernel\n",
      "issue body -  This pull request fixes the similar issue as in pr [#46838](https://github.com/tensorflow/tensorflow/pull/46838) but for the GPU kernel.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:core\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Does TensorFlow do some optimization (like merge a conv and BN into one conv) when converting to TFLite? If yes, is there any other layers merge optimization Tensorflow did?\n",
      "issue body -  <em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n",
      "\r\n",
      "\r\n",
      "**System information**\r\n",
      "- TensorFlow version (you are using): 2.4.1\r\n",
      "- Are you willing to contribute it (Yes/No):Yes\r\n",
      "\r\n",
      "\r\n",
      "**Describe the feature and the current behavior/state.**\r\n",
      "What kind of layers merge does Tensorflow do when converting to TFLite? (Like merge consecutive conv and BN into a new conv). I am working on some optimization part of TFLite and If Tensorflow already did some layers merge optimization part then I do not need to redo this part.\r\n",
      "\r\n",
      "**Will this change the current api? How?**\r\n",
      "No\r\n",
      "**Who will benefit with this feature?**\r\n",
      "Anyone who would like to do optimizations to TFLite\r\n",
      "**Any Other info.**\r\n",
      "\n",
      "issue labels - \n",
      "TFLiteConverter\n",
      "comp:lite\n",
      "type:feature\n",
      "\n",
      "\n",
      "issue title -  TF-TRT Improve matrix multiplication conversion and enable dynamic shape mode\n",
      "issue body -  This PR improves the MatMul and BatchMatMul converters.\r\n",
      "\r\n",
      "- Unnecessary transpose of weights are removed. Transposing weights for `IMatrixMultiplyLayer` is not necessary, because `IMatrixMultiplyLayer` can directly pass the transpose flags to the underlaying GEMM call, which can use it to access elements with the correct stride without any actual transposition. \r\n",
      "- Restrictions caused by previous weight transpose ops eliminated.\r\n",
      "- Enabled explicit batch and dynamic shape input.\r\n",
      "- `IFullyConnectedLayer` (FC) usage fixed:\r\n",
      "  - FC layer is preferred over `IMatrixMultiply` because it is expected to give better performance. Moreover, currently only FC layer supprorts INT8 precision.\r\n",
      "  - Fixed and relaxed FC layer conversion condition.\r\n",
      "  - Fixed input tensor_a shaped handling. In dynamic shape mode care has to be taken to retain static dim where available and not to confuse unknown dims with -1 wildcard.\r\n",
      "  - Enabled rank > 2 for weights.\r\n",
      "  - Fixed conversion of `BatchMatMul` to FC: broadcast now preserves the information whether the input is tensor or weight, so that we can correctly check FC compatibility condition.\r\n",
      "\r\n",
      "\r\n",
      "`BatchMatMul` involves a potential broadcast step. TRT requires that the input tensors have the same rank, with 1 values filled in the dimensions which need to be broadcasted. A helper function `BroadcastTensors` was added to make the tensors match in rank. In dynamic shape mode we need shape inference for this step. The `DynamicReshape` function was modified to allow insertion of multiple singleton dimensions.\r\n",
      "\r\n",
      "Tagging @bixia1 for review and @DEKHTIARJonathan for visibility.\r\n",
      "Tracker: #45481\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:gpu:tensorrt\n",
      "ready to pull\n",
      "size:L\n",
      "\n",
      "\n",
      "issue title -  Use of tensorflow_addons instead of tf.contrib\n",
      "issue body -  I am using tensorflow2. As we know tensorflow2 is not supporting tf.contrib and tf.contrib is move to tensorflow_addons.\r\n",
      "But I don't know how to use tesorflow_addons instead of tf.contrib\r\n",
      "\r\n",
      "```\r\n",
      "import tensorflow as tf\r\n",
      "import tensorflow_addons as tfa\r\n",
      "\r\n",
      "slim_example_decoder = tf.contrib.slim.tfexample_decoder\r\n",
      "```\r\n",
      "\r\n",
      "I want to write `slim_example_decoder = tf.contrib.slim.tfexample_decoder` using **tesorflow_addons**.\r\n",
      "can anyone have any idea?\n",
      "issue labels - \n",
      "\n",
      "\n",
      "issue title -  [XLA] Add Nx to XLA documentation frontends\n",
      "issue body -  Hi everyone, for the last 3 months we've been working on a numerical computing library for the Elixir programming language. We support pluggable compilers, but we started by writing a compiler using XLA first. Nx supports JIT compilation using XLA to CPU and GPU right now. We hope to add TPU support in the future. We we're hoping we could get Nx added to the list of supported XLA frontends. Thank you!\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:xla\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Replacing activation in a pretrained network\n",
      "issue body -  I'm trying to replace swish activation with relu activation in pretrained TF model EfficientNetB0. EfficientNetB0 uses swish activation in Conv2D and Activation layers. I could not find any relevant resources on how to do this. Is there any TF API to do this? Below is the code that I'm working on to replace swish activations with relu activations. Any pointers/help on replacing the activation is much appreciated.\r\n",
      "\r\n",
      "```\r\n",
      "import tensorflow as tf\r\n",
      "from tensorflow.keras.layers import ReLU\r\n",
      "\r\n",
      "def replace_swish_with_relu(model):\r\n",
      "    '''\r\n",
      "    Modify passed model by replacing swish activation with relu\r\n",
      "    '''\r\n",
      "    for layer in tuple(model.layers):\r\n",
      "        layer_type = type(layer).__name__\r\n",
      "        if hasattr(layer, 'activation') and layer.activation.__name__ == 'swish':\r\n",
      "            print(layer_type, layer.activation.__name__)\r\n",
      "            if layer_type == \"Conv2D\":\r\n",
      "                # conv layer with swish activation.\r\n",
      "                # Do something\r\n",
      "                layer.activation = ReLU() # This didn't work\r\n",
      "            else:\r\n",
      "                # activation layer\r\n",
      "                # Do something\r\n",
      "                layer = tf.keras.layers.Activation('relu', name=layer.name + \"_relu\") # This didn't work\r\n",
      "    return model\r\n",
      "\r\n",
      "# load pretrained efficientNet\r\n",
      "model = tf.keras.applications.EfficientNetB0(\r\n",
      "    include_top=True, weights='imagenet', input_tensor=None,\r\n",
      "    input_shape=(224, 224, 3), pooling=None, classes=1000,\r\n",
      "    classifier_activation='softmax')\r\n",
      "\r\n",
      "# convert swish activation to relu activation\r\n",
      "model = replace_swish_with_relu(model)\r\n",
      "model.save(\"efficientNet-relu\")\r\n",
      "```\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:keras\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Prevent BackupAndRestore cleanup from throwing if dir doesn't exist\n",
      "issue body -  If `model.fit()` is called with `epochs=0` (which is allowed by Keras) [`BackupAndRestore.on_train_end`](https://github.com/tensorflow/tensorflow/blob/9663abe4c9037030b0b497c68cc4b2ba991967dd/tensorflow/python/keras/callbacks.py#L1648-L1656) would throw due to the fact that the checkpoint directory doesn't exist since [`on_epoch_end`](https://github.com/tensorflow/tensorflow/blob/9663abe4c9037030b0b497c68cc4b2ba991967dd/tensorflow/python/keras/callbacks.py#L1658-L1660) never get's called. This PR makes sure that `BackupAndRestore` doesn't throw in cases where no cleanup is necessary.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:keras\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  AttributeError: module 'tensorflow' has no attribute 'contrib'\n",
      "issue body -  I am learning object-detection from [this link](https://pythonprogramming.net/training-custom-objects-tensorflow-object-detection-api-tutorial/).\r\n",
      "\r\n",
      "But I **got an error** while run command in **anaconda prompt**:\r\n",
      "\r\n",
      "(base) R:\\SEMESTER 8\\SC\\eye-detection\\models\\research\\object_detection\\legacy>python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/ssd_mobilenet_v1_pets.config\r\n",
      "2021-02-17 15:30:15.419573: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\r\n",
      "2021-02-17 15:30:15.419775: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"train.py\", line 49, in <module>\r\n",
      "    from object_detection.builders import dataset_builder\r\n",
      "  File \"R:\\Anaconda\\anaconda\\lib\\site-packages\\object_detection\\builders\\dataset_builder.py\", line 27, in <module>\r\n",
      "    from object_detection.data_decoders import tf_example_decoder\r\n",
      "  File \"R:\\Anaconda\\anaconda\\lib\\site-packages\\object_detection\\data_decoders\\tf_example_decoder.py\", line 27, in <module>\r\n",
      "    slim_example_decoder = tf.contrib.slim.tfexample_decoder\r\n",
      "AttributeError: module 'tensorflow' has no attribute 'contrib'\r\n",
      "\r\n",
      "### What I have tried:\r\n",
      "\r\n",
      "- I tried to automatically upgrade script to tensorflow2 using [single python file command](https://www.tensorflow.org/guide/upgrade#single_file). But didn't work for me.\r\n",
      "(base) R:\\Anaconda\\anaconda\\Lib\\site-packages\\object_detection\\data_decoders>**tf_upgrade_v2 --infile tf_example_decoder.py --outfile tf_example_decoder.py**\r\n",
      "2021-02-17 15:14:35.610773: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\r\n",
      "2021-02-17 15:14:35.611176: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n",
      "ERROR line 27:23: Using member tf.contrib.slim.tfexample_decoder in deprecated module tf.contrib. tf.contrib.slim.tfexample_decoder cannot be converted automatically. tf.contrib will not be distributed with TensorFlow 2.0, please consider an alternative in non-contrib TensorFlow, a community-maintained repository such as tensorflow/addons, or fork the required code.\r\n",
      "ERROR line 62:23: Using member tf.contrib.lookup.HashTable in deprecated module tf.contrib. tf.contrib.lookup.HashTable cannot be converted automatically. tf.contrib will not be distributed with TensorFlow 2.0, please consider an alternative in non-contrib TensorFlow, a community-maintained repository such as tensorflow/addons, or fork the required code.\r\n",
      "ERROR line 63:20: Using member tf.contrib.lookup.KeyValueTensorInitializer in deprecated module tf.contrib. tf.contrib.lookup.KeyValueTensorInitializer cannot be converted automatically. tf.contrib will not be distributed with TensorFlow 2.0, please consider an alternative in non-contrib TensorFlow, a community-maintained repository such as tensorflow/addons, or fork the required code.\r\n",
      "ERROR line 71:31: Using member tf.contrib.lookup.HashTable in deprecated module tf.contrib. tf.contrib.lookup.HashTable cannot be converted automatically. tf.contrib will not be distributed with TensorFlow 2.0, please consider an alternative in non-contrib TensorFlow, a community-maintained repository such as tensorflow/addons, or fork the required code.\r\n",
      "ERROR line 72:20: Using member tf.contrib.lookup.KeyValueTensorInitializer in deprecated module tf.contrib. tf.contrib.lookup.KeyValueTensorInitializer cannot be converted automatically. tf.contrib will not be distributed with TensorFlow 2.0, please consider an alternative in non-contrib TensorFlow, a community-maintained repository such as tensorflow/addons, or fork the required code.\r\n",
      "TensorFlow 2.0 Upgrade Script\r\n",
      "**Converted 1 files\r\n",
      "Detected 5 issues that require attention\r\n",
      "File: tf_example_decoder.py**\r\n",
      "tf_example_decoder.py:27:23: ERROR: Using member tf.contrib.slim.tfexample_decoder in deprecated module tf.contrib. tf.contrib.slim.tfexample_decoder cannot be converted automatically. tf.contrib will not be distributed with TensorFlow 2.0, please consider an alternative in non-contrib TensorFlow, a community-maintained repository such as tensorflow/addons, or fork the required code.\r\n",
      "tf_example_decoder.py:62:23: ERROR: Using member tf.contrib.lookup.HashTable in deprecated module tf.contrib. tf.contrib.lookup.HashTable cannot be converted automatically. tf.contrib will not be distributed with TensorFlow 2.0, please consider an alternative in non-contrib TensorFlow, a community-maintained repository such as tensorflow/addons, or fork the required code.\r\n",
      "tf_example_decoder.py:63:20: ERROR: Using member tf.contrib.lookup.KeyValueTensorInitializer in deprecated module tf.contrib. tf.contrib.lookup.KeyValueTensorInitializer cannot be converted automatically. tf.contrib will not be distributed with TensorFlow 2.0, please consider an alternative in non-contrib TensorFlow, a community-maintained repository such as tensorflow/addons, or fork the required code.\r\n",
      "tf_example_decoder.py:71:31: ERROR: Using member tf.contrib.lookup.HashTable in deprecated module tf.contrib. tf.contrib.lookup.HashTable cannot be converted automatically. tf.contrib will not be distributed with TensorFlow 2.0, please consider an alternative in non-contrib TensorFlow, a community-maintained repository such as tensorflow/addons, or fork the required code.\r\n",
      "tf_example_decoder.py:72:20: ERROR: Using member tf.contrib.lookup.KeyValueTensorInitializer in deprecated module tf.contrib. tf.contrib.lookup.KeyValueTensorInitializer cannot be converted automatically. tf.contrib will not be distributed with TensorFlow 2.0, please consider an alternative in non-contrib TensorFlow, a community-maintained repository such as tensorflow/addons, or fork the required code.\r\n",
      "Make sure to read the detailed log 'report.txt'\r\n",
      "\r\n",
      "- tried to install downgrade tesorflow version but not supported in python 3.9.1\r\n",
      "- tried to run this code in cmd also but got same error.\r\n",
      "\r\n",
      "**_Tensorflow Version: 2.4.1\r\n",
      "python version: 3.9.1\r\n",
      "windows: 10\r\n",
      "conda 4.9.2_**\r\n",
      "\r\n",
      "Can anyone know proper solution for this?\n",
      "issue labels - \n",
      "stat:awaiting response\n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  RE: Some questions\n",
      "issue body -  Hi I have some questions in regards to this project:\r\n",
      "\r\n",
      "1. How does this differ to OpenCV?\r\n",
      "2. Am I able to also code in C++?\r\n",
      "3. Does it support AMD OpenCL?\n",
      "issue labels - \n",
      "stat:awaiting response\n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  How can I get min/max information from TFlite Intepreter?\n",
      "issue body -  <em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n",
      "\r\n",
      "\r\n",
      "**System information**\r\n",
      "- TensorFlow version (you are using):2.4.1\r\n",
      "- Are you willing to contribute it (Yes/No):Yes\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**Describe the feature and the current behavior/state.**\r\n",
      "I can see the min/max information by using netron open .tflite file. I need those two information to calculate radix. However, I cannot get min/max from any API provided in TFLite Intepreter. I guess to use get_tensor_details(). However I can only get the scale and zero point. Is there any way I can access min/max?\r\n",
      "\r\n",
      "**Will this change the current api? How?**\r\n",
      "Yes, adding one or two items in get_tensor_details()\r\n",
      "**Who will benefit with this feature?**\r\n",
      "Anyone that wants to calculate radix from quantized models in TFLite\r\n",
      "**Any Other info.**\r\n",
      "\n",
      "issue labels - \n",
      "ModelOptimizationToolkit\n",
      "comp:lite\n",
      "type:feature\n",
      "\n",
      "\n",
      "issue title -  Use xa_nnlib for quantize for Fusion F1.\n",
      "issue body -  Copied the relevant function call from https://github.com/pnikam-cad/tensorflow/blob/a737c1e3945bc70022259479ad24133a343ec906/tensorflow/lite/micro/kernels/xtensa_hifi/quantize.cc\r\n",
      "\r\n",
      "Latency for the first quantize op (int16->int8) in the keyword_benchmark went from 3758 ticks to 800 ticks. Overall latency went from 38516 ticks to 34253 ticks.\r\n",
      "\r\n",
      "Tested with:\r\n",
      "```\r\n",
      "make -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa TARGET_ARCH=fusion_f1 XTENSA_CORE=F1_190305_swupgrade OPTIMIZED_KERNEL_DIR=xtensa run_keyword_benchmark -j8\r\n",
      "```\r\n",
      "\r\n",
      "Full output (for completeness):\r\n",
      "```\r\n",
      "InitializeKeywordRunner took 160568 ticks (160 ms).\r\n",
      "\r\n",
      "KeywordRunNIerations(1) took 34253 ticks (34 ms)\r\n",
      "QUANTIZE took 800 ticks (0 ms).\r\n",
      "SVDF took 4753 ticks (4 ms).\r\n",
      "FULLY_CONNECTED took 1353 ticks (1 ms).\r\n",
      "SVDF took 4211 ticks (4 ms).\r\n",
      "FULLY_CONNECTED took 1353 ticks (1 ms).\r\n",
      "SVDF took 3145 ticks (3 ms).\r\n",
      "FULLY_CONNECTED took 1353 ticks (1 ms).\r\n",
      "SVDF took 4211 ticks (4 ms).\r\n",
      "FULLY_CONNECTED took 1353 ticks (1 ms).\r\n",
      "SVDF took 2890 ticks (2 ms).\r\n",
      "SVDF took 3583 ticks (3 ms).\r\n",
      "SVDF took 3054 ticks (3 ms).\r\n",
      "FULLY_CONNECTED took 1091 ticks (1 ms).\r\n",
      "SOFTMAX took 749 ticks (0 ms).\r\n",
      "QUANTIZE took 354 ticks (0 ms).\r\n",
      "```\r\n",
      "\r\n",
      "Also tested that the kernel test passes with:\r\n",
      "```\r\n",
      "make -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa TARGET_ARCH=fusion_f1 XTENSA_CORE=F1_190305_swupgrade OPTIMIZED_KERNEL_DIR=xtensa test_kernel_quantize_test -j8\r\n",
      "```\r\n",
      "\r\n",
      "Progress towards http://b/177457688\r\n",
      "\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  pip Install Error: don't could find version that satisfies the requirement tensorflow\n",
      "issue body -  ------------------------\r\n",
      "\r\n",
      "### System information\r\n",
      "\r\n",
      "-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n",
      "-   **Python version**: 3.9.1 64x\r\n",
      "\r\n",
      "I tried to \r\n",
      "\r\n",
      "```bash\r\n",
      "pip install https://github.com/tensorflow/tensorflow/archive/v2.4.1.tar.gz\r\n",
      "```\r\n",
      "or\r\n",
      "```bash\r\n",
      "pip install tensorflow\r\n",
      "```\r\n",
      "on VirtualEnv created via VSCode and I am getting the following errors:\r\n",
      "\r\n",
      "```bash\r\n",
      "ERROR: Could not find a version that satisfies the requirement tensorflow\r\n",
      "ERROR: No matching distribution found for tensorflow\r\n",
      "```\r\n",
      "or\r\n",
      "\r\n",
      "```bash\r\n",
      "Collecting https://github.com/tensorflow/tensorflow/archive/v2.4.1.tar.gz\r\n",
      "  Using cached https://github.com/tensorflow/tensorflow/archive/v2.4.1.tar.gz\r\n",
      "    ERROR: Command errored out with exit status 1:\r\n",
      "     command: 'c:\\users\\venic\\onedrive\\kaggle\\fashion_ai\\.venv\\scripts\\python.exe' -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\venic\\\\AppData\\\\Local\\\\Temp\\\\pip-req-build-72uc4pr3\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\venic\\\\AppData\\\\Local\\\\Temp\\\\pip-req-build-72uc4pr3\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base 'C:\\Users\\venic\\AppData\\Local\\Temp\\pip-pip-egg-info-z0d8x7xg'\r\n",
      "    Complete output (5 lines):\r\n",
      "      File \"C:\\Users\\venic\\AppData\\Local\\Programs\\Python\\Python39\\lib\\tokenize.py\", line 392, in open\r\n",
      "        buffer = _builtin_open(filename, 'rb')\r\n",
      "    FileNotFoundError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\venic\\\\AppData\\\\Local\\\\Temp\\\\pip-req-build-72uc4pr3\\\\setup.py'\r\n",
      "    ----------------------------------------\r\n",
      "```\r\n",
      "\r\n",
      "Pip version = 21.0.1\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "subtype:windows\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  I have problem with python code in google colab\n",
      "issue body -  I write this code:\r\n",
      "basemodel.fit(X_train,y_train,epochs=20,validation_split=.1,callbacks=call_back)\r\n",
      "but don't work this error:\r\n",
      "Epoch 1/20\r\n",
      "---------------------------------------------------------------------------\r\n",
      "ValueError                                Traceback (most recent call last)\r\n",
      "<ipython-input-34-337a90849ebd> in <module>()\r\n",
      "----> 1 basemodel.fit(X_train,y_train,epochs=20,validation_split=0.1,callbacks=call_back)\r\n",
      "\r\n",
      "9 frames\r\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)\r\n",
      "    975           except Exception as e:  # pylint:disable=broad-except\r\n",
      "    976             if hasattr(e, \"ag_error_metadata\"):\r\n",
      "--> 977               raise e.ag_error_metadata.to_exception(e)\r\n",
      "    978             else:\r\n",
      "    979               raise\n",
      "issue labels - \n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  Error while starting the training for custom dataset using Tensor flow object detection API\n",
      "issue body -  Trying to train a model with custom dataset using the link :\r\n",
      "https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html\r\n",
      "this Object detection API. \r\n",
      "I followed all the steps mentioned in the documentation and the training should have been stated after running the command:\r\n",
      ">python model_main_tf2.py --model_dir=models/my_ssd_resnet50_v1_fpn --pipeline_config_path=models/my_ssd_resnet50_v1_fpn/pipeline.config\r\n",
      "but instead this error is coming up. \r\n",
      "Please help \r\n",
      "\r\n",
      "**System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n",
      "- TensorFlow installed from (source or binary): pip installed\r\n",
      "- TensorFlow version (use command below): 2.4\r\n",
      "- Python version: 3.8\r\n",
      "- Bazel version (if compiling from source): NA\r\n",
      "- GCC/Compiler version (if compiling from source): NA\r\n",
      "- CUDA/cuDNN version: 11.0.2, CUDnn 8.0.5\r\n",
      "- GPU model and memory: Nvidia Quadro T2000\r\n",
      "\r\n",
      "\r\n",
      "Error log from cmd: \r\n",
      "\r\n",
      "\r\n",
      "(tensorflow) C:\\Saurav\\Work\\SESA\\YardHealthStick\\tensorflow\\workspace\\training_demo>python model_main_tf2.py --model_dir=models/my_ssd_resnet50_v1_fpn --pipeline_config_path=models/my_ssd_resnet50_v1_fpn/pipeline.config\r\n",
      "2021-02-16 21:41:58.726218: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n",
      "2021-02-16 21:42:03.204172: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n",
      "2021-02-16 21:42:03.208849: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n",
      "2021-02-16 21:42:03.255184: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:\r\n",
      "pciBusID: 0000:01:00.0 name: Quadro T2000 computeCapability: 7.5\r\n",
      "coreClock: 1.785GHz coreCount: 16 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 119.24GiB/s\r\n",
      "2021-02-16 21:42:03.262012: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n",
      "2021-02-16 21:42:03.275486: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n",
      "2021-02-16 21:42:03.279686: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n",
      "2021-02-16 21:42:03.287813: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n",
      "2021-02-16 21:42:03.294632: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n",
      "2021-02-16 21:42:03.312945: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n",
      "2021-02-16 21:42:03.322076: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n",
      "2021-02-16 21:42:03.328164: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n",
      "2021-02-16 21:42:03.330916: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n",
      "2021-02-16 21:42:03.333579: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\r\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n",
      "2021-02-16 21:42:03.346264: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:\r\n",
      "pciBusID: 0000:01:00.0 name: Quadro T2000 computeCapability: 7.5\r\n",
      "coreClock: 1.785GHz coreCount: 16 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 119.24GiB/s\r\n",
      "2021-02-16 21:42:03.353121: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n",
      "2021-02-16 21:42:03.356164: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n",
      "2021-02-16 21:42:03.358468: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n",
      "2021-02-16 21:42:03.361200: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n",
      "2021-02-16 21:42:03.365114: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n",
      "2021-02-16 21:42:03.367316: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n",
      "2021-02-16 21:42:03.370458: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n",
      "2021-02-16 21:42:03.373566: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n",
      "2021-02-16 21:42:03.376127: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n",
      "2021-02-16 21:42:04.480554: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n",
      "2021-02-16 21:42:04.484849: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0\r\n",
      "2021-02-16 21:42:04.487720: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N\r\n",
      "2021-02-16 21:42:04.490792: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2903 MB memory) -> physical GPU (device: 0, name: Quadro T2000, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n",
      "2021-02-16 21:42:04.499549: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\r\n",
      "I0216 21:42:04.510176 15720 mirrored_strategy.py:350] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\r\n",
      "INFO:tensorflow:Maybe overwriting train_steps: None\r\n",
      "I0216 21:42:04.516160 15720 config_util.py:552] Maybe overwriting train_steps: None\r\n",
      "INFO:tensorflow:Maybe overwriting use_bfloat16: False\r\n",
      "I0216 21:42:04.517184 15720 config_util.py:552] Maybe overwriting use_bfloat16: False\r\n",
      "WARNING:tensorflow:From C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\object_detection\\model_lib_v2.py:530: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "rename to distribute_datasets_from_function\r\n",
      "W0216 21:42:04.714892 15720 deprecation.py:333] From C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\object_detection\\model_lib_v2.py:530: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "rename to distribute_datasets_from_function\r\n",
      "INFO:tensorflow:Reading unweighted datasets: ['annotations/train.record']\r\n",
      "I0216 21:42:04.721902 15720 dataset_builder.py:163] Reading unweighted datasets: ['annotations/train.record']\r\n",
      "INFO:tensorflow:Reading record datasets for input file: ['annotations/train.record']\r\n",
      "I0216 21:42:04.723216 15720 dataset_builder.py:80] Reading record datasets for input file: ['annotations/train.record']\r\n",
      "INFO:tensorflow:Number of filenames to read: 1\r\n",
      "I0216 21:42:04.723867 15720 dataset_builder.py:81] Number of filenames to read: 1\r\n",
      "WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\r\n",
      "W0216 21:42:04.724863 15720 dataset_builder.py:87] num_readers has been reduced to 1 to match input file shards.\r\n",
      "WARNING:tensorflow:From C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\object_detection\\builders\\dataset_builder.py:101: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`.\r\n",
      "W0216 21:42:04.730878 15720 deprecation.py:333] From C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\object_detection\\builders\\dataset_builder.py:101: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`.\r\n",
      "WARNING:tensorflow:From C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\object_detection\\builders\\dataset_builder.py:236: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use `tf.data.Dataset.map()\r\n",
      "W0216 21:42:04.758770 15720 deprecation.py:333] From C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\object_detection\\builders\\dataset_builder.py:236: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use `tf.data.Dataset.map()\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"model_main_tf2.py\", line 113, in <module>\r\n",
      "    tf.compat.v1.app.run()\r\n",
      "  File \"C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 40, in run\r\n",
      "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n",
      "  File \"C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\absl\\app.py\", line 300, in run\r\n",
      "    _run_main(main, args)\r\n",
      "  File \"C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\absl\\app.py\", line 251, in _run_main\r\n",
      "    sys.exit(main(argv))\r\n",
      "  File \"model_main_tf2.py\", line 104, in main\r\n",
      "    model_lib_v2.train_loop(\r\n",
      "  File \"C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\object_detection\\model_lib_v2.py\", line 530, in train_loop\r\n",
      "    train_input = strategy.experimental_distribute_datasets_from_function(\r\n",
      "  File \"C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 340, in new_func\r\n",
      "    return func(*args, **kwargs)\r\n",
      "  File \"C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\", line 1143, in experimental_distribute_datasets_from_function\r\n",
      "    return self.distribute_datasets_from_function(dataset_fn, options)\r\n",
      "  File \"C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\", line 1134, in distribute_datasets_from_function\r\n",
      "    return self._extended._distribute_datasets_from_function(  # pylint: disable=protected-access\r\n",
      "  File \"C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\distribute\\mirrored_strategy.py\", line 545, in _distribute_datasets_from_function\r\n",
      "    return input_lib.get_distributed_datasets_from_function(\r\n",
      "  File \"C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\distribute\\input_lib.py\", line 161, in get_distributed_datasets_from_function\r\n",
      "    return DistributedDatasetsFromFunction(dataset_fn, input_workers,\r\n",
      "  File \"C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\distribute\\input_lib.py\", line 1272, in __init__\r\n",
      "    _create_datasets_from_function_with_input_context(\r\n",
      "  File \"C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\distribute\\input_lib.py\", line 1936, in _create_datasets_from_function_with_input_context\r\n",
      "    dataset = dataset_fn(ctx)\r\n",
      "  File \"C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\object_detection\\model_lib_v2.py\", line 521, in train_dataset_fn\r\n",
      "    train_input = inputs.train_input(\r\n",
      "  File \"C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\object_detection\\inputs.py\", line 893, in train_input\r\n",
      "    dataset = INPUT_BUILDER_UTIL_MAP['dataset_build'](\r\n",
      "  File \"C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\object_detection\\builders\\dataset_builder.py\", line 251, in build\r\n",
      "    dataset = dataset_map_fn(dataset, decoder.decode, batch_size,\r\n",
      "  File \"C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\object_detection\\builders\\dataset_builder.py\", line 236, in dataset_map_fn\r\n",
      "    dataset = dataset.map_with_legacy_function(\r\n",
      "  File \"C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 340, in new_func\r\n",
      "    return func(*args, **kwargs)\r\n",
      "  File \"C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 2679, in map_with_legacy_function\r\n",
      "    ParallelMapDataset(\r\n",
      "  File \"C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 4242, in __init__\r\n",
      "    self._map_func = StructuredFunctionWrapper(\r\n",
      "  File \"C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 3493, in __init__\r\n",
      "    self._function.add_to_graph(ops.get_default_graph())\r\n",
      "  File \"C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\function.py\", line 546, in add_to_graph\r\n",
      "    self._create_definition_if_needed()\r\n",
      "  File \"C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\function.py\", line 378, in _create_definition_if_needed\r\n",
      "    self._create_definition_if_needed_impl()\r\n",
      "  File \"C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\function.py\", line 400, in _create_definition_if_needed_impl\r\n",
      "    temp_graph = func_graph_from_py_func(\r\n",
      "  File \"C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\function.py\", line 971, in func_graph_from_py_func\r\n",
      "    outputs = func(*func_graph.inputs)\r\n",
      "  File \"C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 3485, in wrapper_fn\r\n",
      "    ret = _wrapper_helper(*args)\r\n",
      "  File \"C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 3453, in _wrapper_helper\r\n",
      "    ret = autograph.tf_convert(func, ag_ctx)(*nested_args)\r\n",
      "  File \"C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 670, in wrapper\r\n",
      "    raise e.ag_error_metadata.to_exception(e)\r\n",
      "NotImplementedError: in user code:\r\n",
      "\r\n",
      "    C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\object_detection\\data_decoders\\tf_example_decoder.py:524 default_groundtruth_weights  *\r\n",
      "        [tf.shape(tensor_dict[fields.InputDataFields.groundtruth_boxes])[0]],\r\n",
      "    C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper  **\r\n",
      "        return target(*args, **kwargs)\r\n",
      "    C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:3120 ones\r\n",
      "        output = _constant_if_small(one, shape, dtype, name)\r\n",
      "    C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:2804 _constant_if_small\r\n",
      "        if np.prod(shape) < 1000:\r\n",
      "    <__array_function__ internals>:5 prod\r\n",
      "\r\n",
      "    C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3030 prod\r\n",
      "        return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out,\r\n",
      "    C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\core\\fromnumeric.py:87 _wrapreduction\r\n",
      "        return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\r\n",
      "    C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:852 __array__\r\n",
      "        raise NotImplementedError(\r\n",
      "\r\n",
      "    NotImplementedError: Cannot convert a symbolic Tensor (cond_2/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "stat:awaiting response\n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  Bug related to Martin Görnes Attend Tutorial\n",
      "issue body -  I know that TF 1 is no longer supported but if I test this notebook related to Martin Görner Attention Based Classification Tutorial: https://github.com/conversationai/conversationai-models/blob/master/attention-tutorial/Attention_Model_Tutorial.ipynb\r\n",
      "\r\n",
      "And add the following lines to the bi_rnn_model Function after the encoding was calculated:\r\n",
      "\r\n",
      "Original Code\r\n",
      "```\r\n",
      "encoding, alphas = attend(outputs, \r\n",
      "                            hparams['attention_size'], \r\n",
      "                            hparams['attention_depth'])\r\n",
      "```\r\n",
      "To add code.\r\n",
      "\r\n",
      "```\r\n",
      "  with tf.Session() as sess:\r\n",
      "    sess.run(tf.global_variables_initializer())\r\n",
      "    \r\n",
      "    \r\n",
      "    xev = encoding.eval()\r\n",
      "\r\n",
      "```\r\n",
      "The eval Method will hang forever. This happens in session.py:\r\n",
      "\r\n",
      "```\r\n",
      "def _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list,\r\n",
      "                          run_metadata):\r\n",
      "    return tf_session.TF_SessionRun_wrapper(\r\n",
      "        self._session, options, feed_dict, fetch_list, target_list,\r\n",
      "        run_metadata)\r\n",
      "\r\n",
      "```\r\n",
      "\r\n",
      "\r\n",
      "Am I doing it completely wrong or is it an old TF1 Bug???\r\n",
      "\r\n",
      "Kind regards,\r\n",
      "\r\n",
      "Dirk\n",
      "issue labels - \n",
      "TF 1.15\n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  CUDA devices are not recognized\n",
      "issue body -  <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n",
      "- TensorFlow installed from (source or binary): binary\r\n",
      "- TensorFlow version: 2.3.0\r\n",
      "- Python version: 3.8.5\r\n",
      "- Installed using virtualenv? pip? conda?: Anaconda 4.8.4\r\n",
      "- Bazel version (if compiling from source): N/A\r\n",
      "- GCC/Compiler version (if compiling from source): N/A\r\n",
      "- CUDA/cuDNN version: 10.1.243/7.6.5\r\n",
      "- GPU model and memory: NVIDIA Quadro P2000 4GB\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "- The list of available GPU is empty!\r\n",
      "- TensorFlow runs only on CPU\r\n",
      "\r\n",
      "**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n",
      "`conda env create -f tf-gpu-test.yaml`\r\n",
      "`conda activate tf-gpu-test`\r\n",
      "`python tf-gpu-test.py`\r\n",
      "\r\n",
      "**Any other info / logs**\r\n",
      "Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n",
      "Files: [tf-gpu-test.zip](https://github.com/tensorflow/tensorflow/files/5989857/tf-gpu-test.zip)\r\n",
      "CUDA: []\r\n",
      "CPU: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
      "issue labels - \n",
      "TF 2.3\n",
      "comp:gpu\n",
      "subtype:windows\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  Removed deprecated TAGS option and fixed few READMEs for ARC.\n",
      "issue body -  This pull request removes deprecated functionality and fixes build for ARC targets.\r\n",
      "\r\n",
      "Fixes #42932\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "prtype:bugfix\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  test ExtractVolumePatches with float types that GPU supports\n",
      "issue body -  The original test case tests int64 data type only, which GPU does not register at all.\r\n",
      "The padding issue on GPU was fixed already on eigen side: https://gitlab.com/libeigen/eigen/-/merge_requests/362, https://gitlab.com/libeigen/eigen/-/merge_requests/367 .\r\n",
      "So update the case to test data types that GPU supports here.\r\n",
      "\r\n",
      "We may also need to remove the specialization of ExtractVolumePatches op for CPU on TensorFlow side later(as discussed in eigen's MRs)\n",
      "issue labels - \n",
      "cla: yes\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  TF repo preprocessing for EfficientNet differs from that contained in Keras repo.\n",
      "issue body -  Hi all,\r\n",
      "\r\n",
      "I've noticed that the code for EfficientNet image preprocessing contained [here](https://github.com/keras-team/keras-applications/blob/master/keras_applications/efficientnet.py#L530) is different from the one contained [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/applications/efficientnet.py#L740).\r\n",
      "In the first link (from keras-team) standard ImageNet preprocessing is performed, while in the second link (from tensorflow) nothing is done.\r\n",
      "\r\n",
      "Which version of the code should I trust for reusing the EfficientNet checkpoints publicly released by Google?\r\n",
      "\r\n",
      "Thanks for your time!\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:keras\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Updated the equation for FTLP as per the paper\n",
      "issue body -  Used this https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41159.pdf to update the description.\r\n",
      "![image](https://user-images.githubusercontent.com/16112670/108043678-6433f080-7067-11eb-9e00-9d7d56ae6e22.png)\r\n",
      "\r\n",
      "Currenly, in the exisiting website:\r\n",
      "![image](https://user-images.githubusercontent.com/16112670/108043964-b6751180-7067-11eb-94d7-e7f7ef60a85c.png)\r\n",
      "\r\n",
      "The \\abs function is errornious, Lambda_2 is doubled but as per the paper it is not multiplied by 2.\r\n",
      "\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:keras\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  [TFL] Remove (log-)softmax before arg-minmax as (log-)softmax is monotonic\n",
      "issue body -  It's a common pattern for classification task. Most models in `tf.keras.applications` set `classifier_activation=\"softmax\"` by default. This PR optimizes out that softmax op when users want to get prediction ids only instead of probabilities.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:lite\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  tensorflow_model_server  taking a lot of time in deploying model\n",
      "issue body -  Command is running from hours \r\n",
      "nohup tensorflow_model_server \\\r\n",
      "  --rest_api_port=9090 \\\r\n",
      "  --model_name=seman_model \\\r\n",
      "  --model_base_path=\"${Model_Directory}\" >server1.log 2>&1\n",
      "issue labels - \n",
      "stalled\n",
      "stat:awaiting response\n",
      "\n",
      "\n",
      "issue title -  Model benchmark binary throws \"unconsumed cmdline flag: --graph\" error\n",
      "issue body -  **System information**\r\n",
      "- Windows 10\r\n",
      "- ADB\r\n",
      "- Binary downloaded from [here](https://storage.googleapis.com/tensorflow-nightly-public/prod/tensorflow/release/lite/tools/nightly/latest/android_aarch64_benchmark_model_performance_options)\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "ADB Shell input and output:\r\n",
      "```\r\n",
      "CPH1920:/data/local/tmp $ ./android_aarch64_benchmark_model_performance_options --graph test\r\n",
      "STARTING!\r\n",
      "Unconsumed cmdline flags: --graph test\r\n",
      "WARNING: unrecognized commandline flag: --graph\r\n",
      "WARNING: unrecognized commandline flag: test\r\n",
      "The list of TFLite runtime options to be benchmarked: [all]\r\n",
      "Please specify the name of your TF Lite input file with --graph\r\n",
      "Please specify the name of your TF Lite input file with --graph\r\n",
      "Please specify the name of your TF Lite input file with --graph\r\n",
      "Please specify the name of your TF Lite input file with --graph\r\n",
      "Please specify the name of your TF Lite input file with --graph\r\n",
      "Please specify the name of your TF Lite input file with --graph\r\n",
      "Please specify the name of your TF Lite input file with --graph\r\n",
      "Please specify the name of your TF Lite input file with --graph\r\n",
      "Please specify the name of your TF Lite input file with --graph\r\n",
      "Please specify the name of your TF Lite input file with --graph\r\n",
      "\r\n",
      "==============Summary of All Runs w/ Different Performance Options==============\r\n",
      "                  gpu-fp16:  failed!\r\n",
      "          cpu w/ 1 threads:  failed!\r\n",
      "cpu w/ 2 threads (xnnpack):  failed!\r\n",
      "cpu w/ 4 threads (xnnpack):  failed!\r\n",
      "               gpu-default:  failed!\r\n",
      "     nnapi(w/o accel name):  failed!\r\n",
      "          cpu w/ 4 threads:  failed!\r\n",
      "cpu w/ 1 threads (xnnpack):  failed!\r\n",
      "          cpu w/ 2 threads:  failed!\r\n",
      "            dsp w/ hexagon:  failed!\r\n",
      "```\r\n",
      "**Describe the expected behavior**\r\n",
      "Should not be asking for `--graph` flag if it is already provided.\n",
      "issue labels - \n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Add axis argument in losses.categorical_crossentropy and losses.binary_crossentropy\n",
      "issue body -  Fix tensorflow/tensorflow/issues/39230.\r\n",
      "This pull request will add the axis argument in tensorflow.keras.losses.categorical_crossentropy() and in tensorflow.keras.losses.binary_crossentropy() as in tensorflow.keras.losses.sparse_categorical_crossentropy().\n",
      "issue labels - \n",
      "awaiting review\n",
      "cla: yes\n",
      "comp:keras\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  [XLA] More readable emitted LLVM code.\n",
      "issue body -  Sometimes use named llvm variable that match (or are close) to the name used in the hlo file. This greatly help find the relevant part of the code we want to investigate.\r\n",
      "\r\n",
      "@cheshire \n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:xla\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  Same dataset(180 GB), why training 1 epoch on 2 nodes(16 gpus) takes around 10 minutes, training on 4 nodes(32 gpus) only take 33 seconds?\n",
      "issue body -  # System Information:\r\n",
      "TensorFlow version (use command below): 2.1.1\r\n",
      "Python version: 3.7\r\n",
      "mpi/openmpi: 4.0.3-gnu-9.2.0\r\n",
      "compiler/gnu/: 9.2.0\r\n",
      "CUDA version: 10.1.243\r\n",
      "GPU model and memory:  Tesla V100-SXM2-32GB\r\n",
      "\r\n",
      "\r\n",
      "# Description:\r\n",
      "\r\n",
      "I tested the code with multiple nodes in a cluster, and the training time for one epoch has sudden decrease when use 4 nodes, and I have no idea why it happens. The codes tries to read from 343 csv files(around 180 gb in total), and each csv file has 14406 records, each record has 4860 features, and 8 labels to be predicted. The 8 labels are numbers, thus it is a regression problem.\r\n",
      "\r\n",
      "Here is the code: \r\n",
      "\r\n",
      "\r\n",
      "# Code: \r\n",
      "```\r\n",
      "import time\r\n",
      "import pathlib\r\n",
      "import numpy as np\r\n",
      "import tensorflow as tf\r\n",
      "from tensorflow import keras\r\n",
      "from tensorflow.keras.layers import Conv2D, AveragePooling2D, MaxPool2D, Flatten, Dense, BatchNormalization, Dropout, Activation\r\n",
      "from tensorflow.keras.optimizers import SGD, RMSprop\r\n",
      "import sys\r\n",
      "import json\r\n",
      "import os\r\n",
      "import socket\r\n",
      "import datetime\r\n",
      "import math\r\n",
      "import pathlib\r\n",
      "\r\n",
      "tf.random.set_seed(22)\r\n",
      "\r\n",
      "hostlist=sys.argv[1].split(',')\r\n",
      "current_host=socket.gethostname()\r\n",
      "index=hostlist.index(current_host)\r\n",
      "\r\n",
      "simsInFile=14406\r\n",
      "\r\n",
      "verbose = index < 1\r\n",
      "nodes=[]\r\n",
      "\r\n",
      "for host in hostlist:\r\n",
      "  nodes.append(host + ':2001')\r\n",
      "\r\n",
      "os.environ['TF_CONFIG'] = json.dumps({\r\n",
      "  'cluster': {\r\n",
      "    'worker': nodes\r\n",
      "  },\r\n",
      "  'task': {'type': 'worker', 'index': index}\r\n",
      "})\r\n",
      "print(os.environ['TF_CONFIG'])\r\n",
      "\r\n",
      "strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(tf.distribute.experimental.CollectiveCommunication.AUTO)\r\n",
      "\r\n",
      "AUTOTUNE = tf.data.experimental.AUTOTUNE\r\n",
      "\r\n",
      "per_worker_batch_size = 512\r\n",
      "num_workers = strategy.num_replicas_in_sync\r\n",
      "global_batch_size = per_worker_batch_size * num_workers\r\n",
      "\r\n",
      "num_epochs = 1\r\n",
      "\r\n",
      "def read_csv(line):\r\n",
      "  fields = tf.io.decode_csv(line, record_defaults=[tf.constant([], dtype=tf.float32)]*4869, field_delim=\",\")\r\n",
      "  label = tf.stack(fields[4861:4869])\r\n",
      "  label = tf.dtypes.cast(label, tf.dtypes.float32)\r\n",
      "  label = tf.reshape(label,[8])\r\n",
      "\r\n",
      "  features = tf.stack(fields[1:4861])\r\n",
      "  features = tf.dtypes.cast(features, tf.dtypes.float32)\r\n",
      "  features = tf.reshape(features,[10,486])\r\n",
      "\r\n",
      "  return features, label\r\n",
      "\r\n",
      "def create_train_dataset(ds_files_, batch_size=global_batch_size, nr_epochs=num_epochs, buffer_size=10000):\r\n",
      "  dataset = ds_files_.interleave(lambda x:tf.data.TextLineDataset(x).skip(1))\r\n",
      "  dataset = dataset.map(read_csv, num_parallel_calls=AUTOTUNE)\r\n",
      "  dataset = dataset.cache()\r\n",
      "  dataset = dataset.shuffle(buffer_size=buffer_size)\r\n",
      "  dataset = dataset.repeat()\r\n",
      "  dataset = dataset.batch(batch_size)\r\n",
      "  dataset = dataset.prefetch(1)\r\n",
      "  return dataset\r\n",
      "\r\n",
      "learning_rate = 1e-3\r\n",
      "total_epoch = 1\r\n",
      "ds_files = tf.data.Dataset.list_files(xxx/*.csv')\r\n",
      "ds_files = ds_files.shard(num_workers, index)\r\n",
      "train_dataset = create_train_dataset(ds_files)\r\n",
      "\r\n",
      "def build_model():\r\n",
      "  return tf.keras.models.Sequential([\r\n",
      "    tf.keras.layers.Conv1D(64,3,input_shape=(10, 486)),\r\n",
      "    tf.keras.layers.Conv1D(64,3),\r\n",
      "    tf.keras.layers.Dropout(0.3),\r\n",
      "    tf.keras.layers.MaxPooling1D(pool_size=2),\r\n",
      "    tf.keras.layers.Flatten(),\r\n",
      "    tf.keras.layers.Dense(100, activation='relu'),\r\n",
      "    tf.keras.layers.Dense(8)\r\n",
      "  ])\r\n",
      "\r\n",
      "with strategy.scope():\r\n",
      "  model = build_model()\r\n",
      "  model.compile(optimizer=RMSprop(learning_rate), loss='mse')\r\n",
      "  start_time = datetime.datetime.now()\r\n",
      "  steps_epoch = 9651//num_workers\r\n",
      "  history = model.fit(train_dataset,steps_per_epoch = steps_epoch,epochs = num_epochs, verbose = 1)\r\n",
      "  print(\"Training Finished!\")\r\n",
      "\r\n",
      "os.environ.pop('TF_CONFIG', None)\r\n",
      "```\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.1\n",
      "comp:dist-strat\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  TFL: Update detection_postprocess kernel\n",
      "issue body -  This change uses std::stable_sort instead of std::partial_sort for the case where std::partial_sort use the full range.\r\n",
      "\r\n",
      "This is the case preventing bit-exactness between TFL and TFLM. Problem with std::partial_sort is that the order of equal elements is not guaranteed to be preserved. Potentially std::partial_sort should be totally replaced but leaving that for now.\r\n",
      "\r\n",
      "Next step would be to add a custom stable sort to TFLM, as std::stable_sort uses heap.\r\n",
      "\r\n",
      "This progress towards: https://github.com/tensorflow/tensorflow/issues/47158\r\n",
      "\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "comp:micro:arm\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  Add the ability load specific weights with the tf.keras.Model.load_weights method\n",
      "issue body -  **System information**\r\n",
      "- **TensorFlow version (you are using):** 2.4.1\r\n",
      "**Are you willing to contribute it (~Yes~/No):** At the moment I'm a bit short on time, I might have time to do this in the future. This feature request is a way to check if there is enough support to implement it.\r\n",
      "\r\n",
      "\r\n",
      "**Describe the feature and the current behaviour/state.**\r\n",
      "\r\n",
      "Currently the [tf.keras.Model.load_weights](https://www.tensorflow.org/api_docs/python/tf/keras/Model#load_weights) method loads all the weights that are in a checkpoint. In my project, I want the user to have the ability to load only certain layers (weights) that are in a checkpoint. The user should be able to do this by supplying CLI arguments.\r\n",
      "\r\n",
      "Currently, I achieve this behaviour by overloading the [tf.keras.Model.load_weights](https://www.tensorflow.org/api_docs/python/tf/keras/Model#load_weights) method. I first store the old model state in this overloaded method before I call the `super().load_weights` method. After the weights are restored, I then overwrite the weights the user didn't want to load based on the old mode state. \r\n",
      "\r\n",
      "Giving Tensorflow users the ability to supply an `ignore_list`, for example, in the [CheckpointOptions](https://www.tensorflow.org/api_docs/python/tf/train/CheckpointOptions) object would ease this process.\r\n",
      "\r\n",
      "**Will this change the current api? How?**\r\n",
      "\r\n",
      "It would add a extra parameter to the [CheckpointOptions](https://www.tensorflow.org/api_docs/python/tf/train/CheckpointOptions) object and some additional code to the [tf.keras.Model.load_weights](https://www.tensorflow.org/api_docs/python/tf/keras/Model#load_weights) method in order to filter based on parameter names.\r\n",
      "\r\n",
      "**Who will benefit with this feature?**\r\n",
      "\r\n",
      "I think it will ease the loading process for people that are using transfer learning. It would stop them from relying on workarounds as the one explained above. I am not sure how much work it is to implement such an addition, and if the work outweighs the benefits. I, therefore, opened this issue to see if other people also think this is helpful.\n",
      "issue labels - \n",
      "comp:keras\n",
      "stat:awaiting tensorflower\n",
      "type:feature\n",
      "\n",
      "\n",
      "issue title -  RuntimeError: Cannot use a constraint function on a sparse variable in google colab\n",
      "issue body -  I am trying to train my model using Keras and TensorFlow 2.x, while using the model.fit() method I ran into this error\r\n",
      "\r\n",
      "Error\r\n",
      "\r\n",
      "```\r\n",
      "Epoch 1/10\r\n",
      "---------------------------------------------------------------------------\r\n",
      "RuntimeError                              Traceback (most recent call last)\r\n",
      "\r\n",
      "<ipython-input-73-4871a80f91a3> in <module>()\r\n",
      "      2 \r\n",
      "      3 for i in range(N_epoch):\r\n",
      "----> 4     model.fit(x=train_X,y=train_Y,batch_size=32,epochs=10,verbose=1, validation_data=(val_X,val_Y))\r\n",
      "      5     output = model.predict_proba(val_X, batch_size=10, verbose=1)\r\n",
      "      6     # find validation accuracy using the best threshold value t\r\n",
      "\r\n",
      "9 frames\r\n",
      "\r\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)\r\n",
      "    975           except Exception as e:  # pylint:disable=broad-except\r\n",
      "    976             if hasattr(e, \"ag_error_metadata\"):\r\n",
      "--> 977               raise e.ag_error_metadata.to_exception(e)\r\n",
      "    978             else:\r\n",
      "    979               raise\r\n",
      "\r\n",
      "RuntimeError: in user code:\r\n",
      "\r\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\r\n",
      "        return step_function(self, iterator)\r\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:795 step_function  **\r\n",
      "        outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\r\n",
      "        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\r\n",
      "        return self._call_for_each_replica(fn, args, kwargs)\r\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\r\n",
      "        return fn(*args, **kwargs)\r\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:788 run_step  **\r\n",
      "        outputs = model.train_step(data)\r\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:757 train_step\r\n",
      "        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\r\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:498 minimize\r\n",
      "        return self.apply_gradients(grads_and_vars, name=name)\r\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:635 apply_gradients\r\n",
      "        \"name\": name,\r\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2941 merge_call\r\n",
      "        return self._merge_call(merge_fn, args, kwargs)\r\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2948 _merge_call\r\n",
      "        return merge_fn(self._strategy, *args, **kwargs)\r\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:683 _distributed_apply  **\r\n",
      "        var, apply_grad_to_update_var, args=(grad,), group=False))\r\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2494 update\r\n",
      "        return self._update(var, fn, args, kwargs, group)\r\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:3431 _update\r\n",
      "        return self._update_non_slot(var, fn, (var,) + tuple(args), kwargs, group)\r\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:3437 _update_non_slot\r\n",
      "        result = fn(*args, **kwargs)\r\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:650 apply_grad_to_update_var  **\r\n",
      "        \"Cannot use a constraint function on a sparse variable.\")\r\n",
      "\r\n",
      "    RuntimeError: Cannot use a constraint function on a sparse variable.\r\n",
      "```\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code (Please find the code below)\r\n",
      "- OS Platform and Distribution: Mac OSX Big Sur\r\n",
      "- TensorFlow installed from (source or binary): binary\r\n",
      "- TensorFlow version (use command below): Tensorflow 2.2\r\n",
      "- Python version: Python 3.9.1\r\n",
      "\r\n",
      "Code is given below. [Link](https://colab.research.google.com/drive/1nxjfPUWFWDW9jtoVK7AzKWuTT-C2o-Cb?usp=sharing) to the colab for full code. \r\n",
      "```\r\n",
      "\r\n",
      "# Train data preparation\r\n",
      "N = datasets[0].shape[0]\r\n",
      "conv_input_width = W.shape[1]\r\n",
      "conv_input_height = int(datasets[0].shape[1]-1)\r\n",
      "\r\n",
      "# For each word write a word index (not vector) to X tensor\r\n",
      "train_X = np.zeros((N, conv_input_height), dtype=np.int)\r\n",
      "train_Y = np.zeros((N, 2), dtype=np.int)\r\n",
      "for i in range(N):\r\n",
      "    for j in range(conv_input_height):\r\n",
      "        train_X[i, j] = datasets[0][i, j]\r\n",
      "    \r\n",
      "print ('train_X.shape = {}'.format(train_X.shape))\r\n",
      "print ('train_Y.shape = {}'.format(train_Y.shape))\r\n",
      "\r\n",
      "# Validation data preparation\r\n",
      "Nv = datasets[1].shape[0]\r\n",
      "\r\n",
      "# For each word write a word index (not vector) to X tensor\r\n",
      "val_X = np.zeros((Nv, conv_input_height), dtype=np.int)\r\n",
      "val_Y = np.zeros((Nv, 2), dtype=np.int)\r\n",
      "for i in range(Nv):\r\n",
      "    for j in range(conv_input_height):\r\n",
      "        val_X[i, j] = datasets[1][i, j]\r\n",
      "print('val_X.shape = {}'.format(val_X.shape))\r\n",
      "print('val_Y.shape = {}'.format(val_Y.shape))\r\n",
      "for i in range(Nv):\r\n",
      "    val_Y[i,data_train.iloc[i,3]] = 1\r\n",
      "\r\n",
      "from keras.optimizers import RMSprop\r\n",
      "from keras import backend\r\n",
      "backend.set_image_data_format('channels_first')\r\n",
      "import keras\r\n",
      "\r\n",
      "\r\n",
      "# Number of feature maps (outputs of convolutional layer)\r\n",
      "N_fm = 200\r\n",
      "# kernel size of convolutional layer\r\n",
      "kernel_size = 5\r\n",
      "\r\n",
      "model = Sequential()\r\n",
      "# Embedding layer (lookup table of trainable word vectors)\r\n",
      "model.add(Embedding(input_dim=W.shape[0], \r\n",
      "                    output_dim=W.shape[1], \r\n",
      "                    input_length=conv_input_height,\r\n",
      "                    weights=[W], \r\n",
      "                    embeddings_constraint=UnitNorm,\r\n",
      "                    name = 'e_l'))\r\n",
      "# Reshape word vectors from Embedding to tensor format suitable for Convolutional layer\r\n",
      "model.add(Reshape((1, conv_input_height, conv_input_width)))\r\n",
      "\r\n",
      "# first convolutional layer\r\n",
      "model.add(Convolution2D(N_fm,\r\n",
      "                        kernel_size, \r\n",
      "                        conv_input_width,\r\n",
      "                        kernel_initializer='random_uniform',\r\n",
      "                        padding='valid',\r\n",
      "                        kernel_regularizer=l2(0.001)))\r\n",
      "# ReLU activation\r\n",
      "model.add(Activation('relu'))\r\n",
      "\r\n",
      "# aggregate data in every feature map to scalar using MAX operation\r\n",
      "model.add(MaxPooling2D(pool_size=(conv_input_height+kernel_size+1,1), padding='same'))\r\n",
      "\r\n",
      "model.add(Flatten())\r\n",
      "model.add(Dropout(0.4))\r\n",
      "model.add(Dense(128,kernel_initializer='random_uniform'))\r\n",
      "model.add(Activation('relu'))\r\n",
      "model.add(Dropout(0.4))\r\n",
      "# Inner Product layer (as in regular neural network, but without non-linear activation function)\r\n",
      "model.add(Dense(2))\r\n",
      "# SoftMax activation; actually, Dense+SoftMax works as Multinomial Logistic Regression\r\n",
      "model.add(Activation('softmax'))\r\n",
      "\r\n",
      "# Custom optimizers could be used, though right now standard adadelta is employed\r\n",
      "opt = RMSprop(lr=0.001, rho=0.9, epsilon=None)\r\n",
      "model.compile(loss='mean_squared_error', \r\n",
      "              optimizer=opt,\r\n",
      "              metrics=['accuracy'])\r\n",
      "\r\n",
      "```\r\n",
      "\r\n",
      "**The line that throws the error**\r\n",
      "\r\n",
      "```\r\n",
      "N_epoch = 3\r\n",
      "\r\n",
      "for i in range(N_epoch):\r\n",
      "    model.fit(x=train_X,y=train_Y,batch_size=32,epochs=10,verbose=1, validation_data=(val_X,val_Y))\r\n",
      "    output = model.predict_proba(val_X, batch_size=10, verbose=1)\r\n",
      "    # find validation accuracy using the best threshold value t\r\n",
      "    vacc = np.max([np.sum((output[:,1]>t)==(val_Y[:,1]>0.5))*1.0/len(output) for t in np.arange(0.0, 1.0, 0.01)])\r\n",
      "    # find validation AUC\r\n",
      "    vauc = roc_auc_score(val_Y, output)\r\n",
      "    val_acc.append(vacc)\r\n",
      "    val_auc.append(vauc)\r\n",
      "    print('Epoch {}: validation accuracy = {:.3%}, validation AUC = {:.3%}'.format(epoch, vacc, vauc))\r\n",
      "    epoch += 1\r\n",
      "    \r\n",
      "print('{} epochs passed'.format(epoch))\r\n",
      "print('Accuracy on validation dataset:')\r\n",
      "print(val_acc)\r\n",
      "print('AUC on validation dataset:')\r\n",
      "print(val_auc)\r\n",
      "```\r\n",
      "\r\n",
      "**Tweaks I tried**\r\n",
      "1. I have tried changing the UnitNorm in the embedding layer\r\n",
      "2. Verified the embedding layer doesn't use sparse data. Instead, it uses a dense matrix to store that data.\r\n",
      "3.  Referred this [link](https://github.com/tensorflow/tensorflow/issues/33755) but couldn't solve my error.\r\n",
      "\r\n",
      "\r\n",
      "Please can anyone suggest a solution? Thanks\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:keras\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  [TFL] Optimize add/sub\n",
      "issue body -  - Assign to `Eigen::Map` will cause forced evaluation. Chain `cwiseMin` and `cwiseMax` to avoid read/write memory twice. One can verify with\r\n",
      "\r\n",
      "```cpp\r\n",
      "#include <Eigen/Core>\r\n",
      "#include <algorithm>\r\n",
      "\r\n",
      "void foo(int* a, int mn, int mx, int n) {\r\n",
      "  Eigen::Map<Eigen::Matrix<int, Eigen::Dynamic, 1>> vector_map(a, n, 1);\r\n",
      "  vector_map = vector_map.cwiseMin(mx);\r\n",
      "  vector_map = vector_map.cwiseMax(mn);\r\n",
      "}\r\n",
      "\r\n",
      "void bar(int* a, int mn, int mx, int n) {\r\n",
      "  Eigen::Map<Eigen::Matrix<int, Eigen::Dynamic, 1>> vector_map(a, n, 1);\r\n",
      "  vector_map = vector_map.cwiseMin(mx).cwiseMax(mn);\r\n",
      "}\r\n",
      "\r\n",
      "void zoo(int* a, int mn, int mx, int n) {\r\n",
      "  for (int i = 0; i < n; ++i) {\r\n",
      "    a[i] = std::min(std::max(a[i], mn), mx);\r\n",
      "  }\r\n",
      "}\r\n",
      "```\r\n",
      "\r\n",
      "- Optimize non broadcast sub with Eigen\r\n",
      "   - `SetActivationMinMax` is the same with `GetActivationParams` in `tensorflow/lite/kernels/internal/types.h`\r\n",
      "   - I cannot see any usage of  `SubNonBroadcast` across TensorFlow. It's just an alias of `SubWithActivation`, which is the real function being dispatched.\r\n",
      "   - Eigen's vectorization for neon looks pretty good. Do we need to write intrinsics like some of optimized kernels in tflite?\r\n",
      "\r\n",
      "On pixel 3a, benchmark with single op\r\n",
      "\r\n",
      "-----\r\n",
      "\r\n",
      "Sub two (224 * 224 * 32) float32 \r\n",
      "\r\n",
      "Before \r\n",
      "```\r\n",
      "Timings (microseconds): count=191 first=2281 curr=2278 min=2250 max=3229 avg=2445.86 std=251\r\n",
      "```\r\n",
      "After\r\n",
      "```\r\n",
      "Timings (microseconds): count=208 first=1769 curr=1757 min=1715 max=3050 avg=2083.2 std=397\r\n",
      "```\r\n",
      "\r\n",
      "-----\r\n",
      "\r\n",
      "\r\n",
      "Add two (224 * 224 * 32) int32 (I modify int32 kernel only)\r\n",
      "\r\n",
      "Before\r\n",
      "```\r\n",
      "Timings (microseconds): count=133 first=4698 curr=4344 min=4201 max=6049 avg=4628.22 std=429\r\n",
      "```\r\n",
      "Commit 6fc655b\r\n",
      "```\r\n",
      "Timings (microseconds): count=158 first=3307 curr=3015 min=2917 max=4429 avg=3335.49 std=419\r\n",
      "```\r\n",
      "Commit 86c1a09 -  fuse add and clip\r\n",
      "```\r\n",
      "Timings (microseconds): count=221 first=1985 curr=1706 min=1596 max=3085 avg=1901.17 std=404\r\n",
      "```\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:lite\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  Add reminder on MacOS Catalina\n",
      "issue body -  ARM mbed MacOS installer (mbed-cli-v0.0.10.dmg from https://github.com/ARMmbed/mbed-cli-osx-installer/releases/tag/v0.0.10) creates error during installation.\r\n",
      "\r\n",
      "It is caused by MacOS Catalina moving path of terminal app from _/Applications/Utilities/Terminal.app_ to _/System/Applications/Utilities/Terminal.app_. ) ARM mbed installer scripts still use _/Applications/Utilities/Terminal.app_.\r\n",
      "\r\n",
      "So add a reminder, and a [link](https://github.com/ARMmbed/mbed-cli/issues/930#issuecomment-660550734) to a solution from ARM mbed github.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Update README.md\n",
      "issue body -  \n",
      "issue labels - \n",
      "cla: yes\n",
      "invalid\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  I tried filling in all of your \"form\" & you closed off the issue so, were going to do this EVERY DAY until you act like grown ups\n",
      "issue body -  Backend terminated or disconnected.Fatal Python error: Illegal instruction\r\n",
      "\r\n",
      "Current thread 0x00007f0f5412a740 (most recent call first):\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 219 in _call_with_frames_removed\r\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1101 in create_module\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 556 in module_from_spec\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 657 in _load_unlocked\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 975 in _find_and_load_unlocked\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 991 in _find_and_load\r\n",
      "  File \"/usr/lib/python3/dist-packages/thonny/backend.py\", line 285 in _custom_import\r\n",
      "  File \"/home/ian/.local/lib/python3.8/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 64 in <module>\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 219 in _call_with_frames_removed\r\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 783 in exec_module\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 671 in _load_unlocked\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 975 in _find_and_load_unlocked\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 991 in _find_and_load\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 219 in _call_with_frames_removed\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 1042 in _handle_fromlist\r\n",
      "  File \"/usr/lib/python3/dist-packages/thonny/backend.py\", line 285 in _custom_import\r\n",
      "  File \"/home/ian/.local/lib/python3.8/site-packages/tensorflow/python/__init__.py\", line 39 in <module>\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 219 in _call_with_frames_removed\r\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 783 in exec_module\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 671 in _load_unlocked\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 975 in _find_and_load_unlocked\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 991 in _find_and_load\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 219 in _call_with_frames_removed\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 961 in _find_and_load_unlocked\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 991 in _find_and_load\r\n",
      "  File \"/usr/lib/python3/dist-packages/thonny/backend.py\", line 285 in _custom_import\r\n",
      "  File \"/home/ian/.local/lib/python3.8/site-packages/tensorflow/__init__.py\", line 41 in <module>\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 219 in _call_with_frames_removed\r\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 783 in exec_module\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 671 in _load_unlocked\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 975 in _find_and_load_unlocked\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 991 in _find_and_load\r\n",
      "  File \"/usr/lib/python3/dist-packages/thonny/backend.py\", line 285 in _custom_import\r\n",
      "  File \"<pyshell>\", line 1 in <module>\r\n",
      "  File \"/usr/lib/python3/dist-packages/thonny/backend.py\", line 1272 in _execute_prepared_user_code\r\n",
      "  File \"/usr/lib/python3/dist-packages/thonny/backend.py\", line 1200 in wrapper\r\n",
      "  File \"/usr/lib/python3/dist-packages/thonny/backend.py\", line 1213 in wrapper\r\n",
      "  File \"/usr/lib/python3/dist-packages/thonny/backend.py\", line 1259 in execute_source\r\n",
      "  File \"/usr/lib/python3/dist-packages/thonny/backend.py\", line 814 in _execute_source\r\n",
      "  File \"/usr/lib/python3/dist-packages/thonny/backend.py\", line 444 in _cmd_execute_source\r\n",
      "  File \"/usr/lib/python3/dist-packages/thonny/backend.py\", line 204 in handle_command\r\n",
      "  File \"/usr/lib/python3/dist-packages/thonny/backend.py\", line 146 in mainloop\r\n",
      "  File \"/usr/lib/python3/dist-packages/thonny/backend_launcher.py\", line 87 in <module> Use 'Stop/Restart' to restart.\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "stat:awaiting response\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  [MLIR] Add conversion of `tf.leakyRelu` and `tf.leakyReluGrad` from t…\n",
      "issue body -  …f to mhlo.\r\n",
      "\r\n",
      "This commits implements conversion of tf.leakyRelu and tf.leakyReluGrad\r\n",
      "operations from tf to mhlo. The changes have been made as a part of\r\n",
      "xla-legalize-tf pass.\r\n",
      "\r\n",
      "Signed-off-by: Prashant Kumar <prashantk@polymagelabs.com>\n",
      "issue labels - \n",
      "cla: yes\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  GPU not detected / tf.test.is_built_with_cuda returns False\n",
      "issue body -  Please go to Stack Overflow for help and support:\r\n",
      "\r\n",
      "https://stackoverflow.com/questions/tagged/tensorflow\r\n",
      "\r\n",
      "If you open a GitHub issue, here is our policy:\r\n",
      "\r\n",
      "1.  It must be a bug, a feature request, or a significant problem with the\r\n",
      "    documentation (for small docs fixes please send a PR instead).\r\n",
      "2.  The form below must be filled out.\r\n",
      "3.  It shouldn't be a TensorBoard issue. Those go\r\n",
      "    [here](https://github.com/tensorflow/tensorboard/issues).\r\n",
      "\r\n",
      "**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n",
      "\r\n",
      "------------------------\r\n",
      "\r\n",
      "### System information\r\n",
      "\r\n",
      "-   **Have I written custom code (as opposed to using a stock example script\r\n",
      "    provided in TensorFlow)**: N/A\r\n",
      "-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n",
      "-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n",
      "    happens on a mobile device**: N/A\r\n",
      "-   **TensorFlow installed from (source or binary)**: conda\r\n",
      "-   **TensorFlow version (use command below)**: 2.3.0\r\n",
      "-   **Python version**: 3.8.5 [Anaconda 2020.11]\r\n",
      "-   **Bazel version (if compiling from source)**:\r\n",
      "-   **GCC/Compiler version (if compiling from source)**:\r\n",
      "-   **CUDA/cuDNN version**: 10.1 / 7.6.4 [graphics driver 461.40]\r\n",
      "-   **GPU model and memory**: GeForce RTX 3080 / 10GB [EVGA Black]\r\n",
      "-   **Exact command to reproduce**: import tensorflow as tf, tf.test.is_built_with_cuda()\r\n",
      "\r\n",
      "You can collect some of this information using our environment capture script:\r\n",
      "\r\n",
      "https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n",
      "\r\n",
      "You can obtain the TensorFlow version with:\r\n",
      "\r\n",
      "```bash\r\n",
      "python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n",
      "```\r\n",
      "\r\n",
      "### Describe the problem\r\n",
      "-Installed MSVS 2019 Community Edition\r\n",
      "-Installed Cuda Toolkit 10.1 [in preparation for TensorFlow 2.3.0 from conda]. NOTE: Did custom installation, not installing the graphics or physics drivers as I already have the most up to date drivers.\r\n",
      "-Installed / copied drivers across from cuDNN 7.6.4\r\n",
      "-Installed Anaconda 2020.11\r\n",
      "-Created environment and installed TensorFlow-GPU [conda create -n tf-gpu tensorflow-gpu]\r\n",
      "-Activated tf-gpu environment [conda activate tf-gpu]\r\n",
      "-started python [python]\r\n",
      "-imported tensorflow [import tensorflow as tf]\r\n",
      "-checked installation [tf.test.is_built_with_cuda()]\r\n",
      "returns False\r\n",
      "-checked for GPU [tf.config.list_physical_devices('GPU')\r\n",
      "returns []\r\n",
      "\r\n",
      "I'm not sure what I'm missing or doing wrong during the installation. I don't have the CPU only tensorflow installed. Is it because I am not installing the graphics drivers with CUDA Toolkit? Any help is much appreciated.\r\n",
      "\r\n",
      "\r\n",
      "### Source code / logs\r\n",
      "Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.3\n",
      "comp:gpu\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Python import failure\n",
      "issue body -  **System information**\r\n",
      "- OS Platform and Distribution (Linux Ubuntu 20 latest update):\r\n",
      "- Mobile device ()Starlabs Star Lite laptop with 8Gb of RAM 113Gb of SSD space:\r\n",
      "- TensorFlow installed from (pip install tensorflow==2.4.1):\r\n",
      "- TensorFlow version: see above\r\n",
      "- Python version: Python 3.8.5 (/usr/bin/python3)\r\n",
      "- Installed using virtualenv? pip? conda?: See above\r\n",
      "- Bazel version (if compiling from source): NA NA\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version: Dunno\r\n",
      "- GPU model and memory: (description: VGA compatible controller\r\n",
      "       product: UHD Graphics 605\r\n",
      "       vendor: Intel Corporation\r\n",
      "       physical id: 2\r\n",
      "       bus info: pci@0000:00:02.0\r\n",
      "       version: 03\r\n",
      "       width: 64 bits\r\n",
      "       clock: 33MHz\r\n",
      "       capabilities: pciexpress msi pm vga_controller bus_master cap_list rom\r\n",
      "       configuration: driver=i915 latency=0\r\n",
      "       resources: irq:132 memory:a0000000-a0ffffff memory:90000000-9fffffff ioport:f000(size=64) memory:c0000-dffff)\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "when I import tensorflow into the Thonny shell I get:\r\n",
      "\r\n",
      "Backend terminated or disconnected.Fatal Python error: Illegal instruction\r\n",
      "\r\n",
      "Current thread 0x00007f30949d8740 (most recent call first):\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 219 in _call_with_frames_removed\r\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1101 in create_module\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 556 in module_from_spec\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 657 in _load_unlocked\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 975 in _find_and_load_unlocked\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 991 in _find_and_load\r\n",
      "  File \"/usr/lib/python3/dist-packages/thonny/backend.py\", line 285 in _custom_import\r\n",
      "  File \"/home/ian/.local/lib/python3.8/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 64 in <module>\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 219 in _call_with_frames_removed\r\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 783 in exec_module\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 671 in _load_unlocked\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 975 in _find_and_load_unlocked\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 991 in _find_and_load\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 219 in _call_with_frames_removed\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 1042 in _handle_fromlist\r\n",
      "  File \"/usr/lib/python3/dist-packages/thonny/backend.py\", line 285 in _custom_import\r\n",
      "  File \"/home/ian/.local/lib/python3.8/site-packages/tensorflow/python/__init__.py\", line 39 in <module>\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 219 in _call_with_frames_removed\r\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 783 in exec_module\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 671 in _load_unlocked\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 975 in _find_and_load_unlocked\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 991 in _find_and_load\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 219 in _call_with_frames_removed\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 961 in _find_and_load_unlocked\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 991 in _find_and_load\r\n",
      "  File \"/usr/lib/python3/dist-packages/thonny/backend.py\", line 285 in _custom_import\r\n",
      "  File \"/home/ian/.local/lib/python3.8/site-packages/tensorflow/__init__.py\", line 41 in <module>\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 219 in _call_with_frames_removed\r\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 783 in exec_module\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 671 in _load_unlocked\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 975 in _find_and_load_unlocked\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 991 in _find_and_load\r\n",
      "  File \"/usr/lib/python3/dist-packages/thonny/backend.py\", line 285 in _custom_import\r\n",
      "  File \"<pyshell>\", line 1 in <module>\r\n",
      "  File \"/usr/lib/python3/dist-packages/thonny/backend.py\", line 1272 in _execute_prepared_user_code\r\n",
      "  File \"/usr/lib/python3/dist-packages/thonny/backend.py\", line 1200 in wrapper\r\n",
      "  File \"/usr/lib/python3/dist-packages/thonny/backend.py\", line 1213 in wrapper\r\n",
      "  File \"/usr/lib/python3/dist-packages/thonny/backend.py\", line 1259 in execute_source\r\n",
      "  File \"/usr/lib/python3/dist-packages/thonny/backend.py\", line 814 in _execute_source\r\n",
      "  File \"/usr/lib/python3/dist-packages/thonny/backend.py\", line 444 in _cmd_execute_source\r\n",
      "  File \"/usr/lib/python3/dist-packages/thonny/backend.py\", line 204 in handle_command\r\n",
      "  File \"/usr/lib/python3/dist-packages/thonny/backend.py\", line 146 in mainloop\r\n",
      "  File \"/usr/lib/python3/dist-packages/thonny/backend_launcher.py\", line 87 in <module> Use 'Stop/Restart' to restart.Backend terminated or disconnected.Fatal Python error: Illegal instruction\r\n",
      "\r\n",
      "Current thread 0x00007f30949d8740 (most recent call first):\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 219 in _call_with_frames_removed\r\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1101 in create_module\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 556 in module_from_spec\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 657 in _load_unlocked\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 975 in _find_and_load_unlocked\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 991 in _find_and_load\r\n",
      "  File \"/usr/lib/python3/dist-packages/thonny/backend.py\", line 285 in _custom_import\r\n",
      "  File \"/home/ian/.local/lib/python3.8/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 64 in <module>\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 219 in _call_with_frames_removed\r\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 783 in exec_module\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 671 in _load_unlocked\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 975 in _find_and_load_unlocked\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 991 in _find_and_load\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 219 in _call_with_frames_removed\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 1042 in _handle_fromlist\r\n",
      "  File \"/usr/lib/python3/dist-packages/thonny/backend.py\", line 285 in _custom_import\r\n",
      "  File \"/home/ian/.local/lib/python3.8/site-packages/tensorflow/python/__init__.py\", line 39 in <module>\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 219 in _call_with_frames_removed\r\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 783 in exec_module\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 671 in _load_unlocked\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 975 in _find_and_load_unlocked\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 991 in _find_and_load\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 219 in _call_with_frames_removed\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 961 in _find_and_load_unlocked\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 991 in _find_and_load\r\n",
      "  File \"/usr/lib/python3/dist-packages/thonny/backend.py\", line 285 in _custom_import\r\n",
      "  File \"/home/ian/.local/lib/python3.8/site-packages/tensorflow/__init__.py\", line 41 in <module>\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 219 in _call_with_frames_removed\r\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 783 in exec_module\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 671 in _load_unlocked\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 975 in _find_and_load_unlocked\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 991 in _find_and_load\r\n",
      "  File \"/usr/lib/python3/dist-packages/thonny/backend.py\", line 285 in _custom_import\r\n",
      "  File \"<pyshell>\", line 1 in <module>\r\n",
      "  File \"/usr/lib/python3/dist-packages/thonny/backend.py\", line 1272 in _execute_prepared_user_code\r\n",
      "  File \"/usr/lib/python3/dist-packages/thonny/backend.py\", line 1200 in wrapper\r\n",
      "  File \"/usr/lib/python3/dist-packages/thonny/backend.py\", line 1213 in wrapper\r\n",
      "  File \"/usr/lib/python3/dist-packages/thonny/backend.py\", line 1259 in execute_source\r\n",
      "  File \"/usr/lib/python3/dist-packages/thonny/backend.py\", line 814 in _execute_source\r\n",
      "  File \"/usr/lib/python3/dist-packages/thonny/backend.py\", line 444 in _cmd_execute_source\r\n",
      "  File \"/usr/lib/python3/dist-packages/thonny/backend.py\", line 204 in handle_command\r\n",
      "  File \"/usr/lib/python3/dist-packages/thonny/backend.py\", line 146 in mainloop\r\n",
      "  File \"/usr/lib/python3/dist-packages/thonny/backend_launcher.py\", line 87 in <module> Use 'Stop/Restart' to restart.\r\n",
      "\r\n",
      "**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n",
      "pip install tensorflow==2.4.1 # into the terminal\r\n",
      "# followed by\r\n",
      "\r\n",
      "import tensorflow # into Thonny's shell\r\n",
      "\r\n",
      "# the error appears in the shell\r\n",
      "\r\n",
      "**Any other info / logs**\r\n",
      "Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n",
      "NA\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "stat:awaiting response\n",
      "subtype: ubuntu/linux\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  tensorflow\n",
      "issue body -  Please go to Stack Overflow for help and support:\r\n",
      "\r\n",
      "https://stackoverflow.com/questions/tagged/tensorflow\r\n",
      "\r\n",
      "If you open a GitHub issue, here is our policy:\r\n",
      "\r\n",
      "1.  It must be a bug, a feature request, or a significant problem with the\r\n",
      "    documentation (for small docs fixes please send a PR instead).\r\n",
      "2.  The form below must be filled out.\r\n",
      "3.  It shouldn't be a TensorBoard issue. Those go\r\n",
      "    [here](https://github.com/tensorflow/tensorboard/issues).\r\n",
      "\r\n",
      "**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n",
      "\r\n",
      "------------------------\r\n",
      "\r\n",
      "### System information\r\n",
      "\r\n",
      "-   **Have I written custom code (as opposed to using a stock example script\r\n",
      "    provided in TensorFlow)**:\r\n",
      "-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n",
      "-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n",
      "    happens on a mobile device**:\r\n",
      "-   **TensorFlow installed from (source or binary)**:\r\n",
      "-   **TensorFlow version (use command below)**:\r\n",
      "-   **Python version**:\r\n",
      "-   **Bazel version (if compiling from source)**:\r\n",
      "-   **GCC/Compiler version (if compiling from source)**:\r\n",
      "-   **CUDA/cuDNN version**:\r\n",
      "-   **GPU model and memory**:\r\n",
      "-   **Exact command to reproduce**:\r\n",
      "\r\n",
      "You can collect some of this information using our environment capture script:\r\n",
      "\r\n",
      "https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n",
      "\r\n",
      "You can obtain the TensorFlow version with:\r\n",
      "\r\n",
      "```bash\r\n",
      "python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n",
      "```\r\n",
      "\r\n",
      "### Describe the problem\r\n",
      "Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n",
      "\r\n",
      "### Source code / logs\r\n",
      "Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n",
      "\n",
      "issue labels - \n",
      "invalid\n",
      "\n",
      "\n",
      "issue title -  EdgeTPU compilation fails for multi-head networks\n",
      "issue body -  ### 1. System information\r\n",
      "\r\n",
      "- OS Platform and Distribution  Linux Ubuntu 20.04:\r\n",
      "- TensorFlow installation pip installation 2.3.1:\r\n",
      "\r\n",
      "### 2. Code\r\n",
      "\r\n",
      "Model Architecture:\r\n",
      "```\r\n",
      "TARGET_SIZE = 224\r\n",
      "inputs = tf.keras.layers.Input(shape=(TARGET_SIZE, TARGET_SIZE, 3))\r\n",
      "base_model = tf.keras.applications.MobileNetV2(input_shape=(TARGET_SIZE, TARGET_SIZE, 3), alpha=1.0, include_top=False, weights='imagenet', input_tensor=inputs)\r\n",
      "base_out = base_model.output\r\n",
      "base_out = tf.keras.layers.Conv2D(filters=512, kernel_size=3, activation='relu')(base_out)\r\n",
      "base_out = tf.keras.layers.Dropout(0.2)(base_out)\r\n",
      "base_out = tf.keras.layers.GlobalAveragePooling2D()(base_out)\r\n",
      "#base_out = tf.keras.layers.Flatten()(base_out)\r\n",
      "\r\n",
      "# construct a fully-connected layer header to output the predicted\r\n",
      "# bounding box coordinates\r\n",
      "bboxHead = tf.keras.layers.Dense(256, activation=\"relu\")(base_out)\r\n",
      "bboxHead = tf.keras.layers.Dense(128, activation=\"relu\")(bboxHead)\r\n",
      "bboxHead = tf.keras.layers.Dense(64, activation=\"relu\")(bboxHead)\r\n",
      "bboxHead = tf.keras.layers.Dense(32, activation=\"relu\")(bboxHead)\r\n",
      "bboxHead = tf.keras.layers.Dense(4, activation=\"sigmoid\", name=\"bounding_box\")(bboxHead)\r\n",
      "\r\n",
      "# construct a second fully-connected layer head, this one to predict\r\n",
      "# the class label\r\n",
      "softmaxHead = tf.keras.layers.Dense(256, activation=\"relu\")(base_out)\r\n",
      "softmaxHead = tf.keras.layers.Dense(128, activation=\"relu\")(softmaxHead)\r\n",
      "softmaxHead = tf.keras.layers.Dense(64, activation=\"relu\")(softmaxHead)\r\n",
      "softmaxHead = tf.keras.layers.Dense(8, activation=\"relu\")(softmaxHead)\r\n",
      "softmaxHead = tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"class_label\")(softmaxHead)\r\n",
      "\r\n",
      "# put together our model which accept an input image and then output\r\n",
      "# bounding box coordinates and a class label\r\n",
      "model = tf.keras.models.Model(inputs=base_model.input, outputs=(bboxHead, softmaxHead))\r\n",
      "```\r\n",
      "\r\n",
      "Visualisation of the architecture : [model_plot](https://user-images.githubusercontent.com/17791005/107879987-5dc53d80-6edc-11eb-98b9-46ec369b8c89.png)\r\n",
      "\r\n",
      "TFLite Conversion:\r\n",
      "```\r\n",
      "def convert_tflite(pl_model):\r\n",
      "    converter = tf.lite.TFLiteConverter.from_keras_model(pl_model)\r\n",
      "    # Set quantize to true\r\n",
      "    converter.post_training_quantize = True\r\n",
      "    converter.representative_dataset = tf.lite.RepresentativeDataset(representative_dataset_gen)\r\n",
      "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]  # For EdgeTPU, no float ops allowed\r\n",
      "    converter.inference_input_type = tf.uint8\r\n",
      "    converter.inference_output_type = tf.uint8\r\n",
      "\r\n",
      "    tflite_model = converter.convert()\r\n",
      "    open(\"pl_od_model_edgetpu.tflite\", \"wb\").write(tflite_model)\r\n",
      "\r\n",
      "def representative_dataset_gen():\r\n",
      "    data_dir = os.path.join(os.path.join(ma_uav_dir, 'Vision/Detector/CNN_Classifier'), 'Data/test')\r\n",
      "    pl_dir = os.path.join(data_dir, 'PayLoad')\r\n",
      "    pl_imgs = [cv2.imread(os.path.join(pl_dir, image)) for image in os.listdir(pl_dir) if '.png' in image]\r\n",
      "    not_pl_dir = os.path.join(data_dir, 'Not_PayLoad')\r\n",
      "    not_pl_imgs = [cv2.imread(os.path.join(not_pl_dir, image)) for image in os.listdir(not_pl_dir) if '.png' in image]\r\n",
      "    all_imgs = pl_imgs + not_pl_imgs\r\n",
      "    random.shuffle(all_imgs)\r\n",
      "\r\n",
      "    for i in range(len(all_imgs) - 1):\r\n",
      "        img = all_imgs[i]\r\n",
      "        image = resize_img(img_org=img).astype(np.float32)\r\n",
      "        image = tf.expand_dims(image, 0)\r\n",
      "        yield [image]\r\n",
      "\r\n",
      "def resize_img(img_org):\r\n",
      "    return cv2.resize(img_org, (TARGET_SIZE, TARGET_SIZE)) * (1. / 255)\r\n",
      "```\r\n",
      "Resulting in following Netron output: [image](https://user-images.githubusercontent.com/17791005/107880057-bac0f380-6edc-11eb-9bf9-7cfb7fff8f55.png)\r\n",
      "\r\n",
      "### 3. Failure after conversion\r\n",
      "The TFLite conversion is successful but the compilation to Coral EdgeTPU fails. Following command `edgetpu_compiler pl_od_model_edgetpu.tflite ` results in the error below:\r\n",
      "\r\n",
      "EdgeTPU Compiler Output:\r\n",
      "```\r\n",
      "Edge TPU Compiler version 15.0.340273435\r\n",
      "ERROR: :309 scale_diff / output_scale <= 0.02 was not true.\r\n",
      "ERROR: Node number 79 (FULLY_CONNECTED) failed to prepare.\r\n",
      "\r\n",
      "ERROR: :309 scale_diff / output_scale <= 0.02 was not true.\r\n",
      "ERROR: Node number 79 (FULLY_CONNECTED) failed to prepare.\r\n",
      "\r\n",
      "Compilation failed: Internal error\r\n",
      "\r\n",
      "Internal compiler error. Aborting!\r\n",
      "```\r\n",
      "\r\n",
      "I suppose the issue is related to https://github.com/tensorflow/tensorflow/issues/41069. As I am also dealing with a model with two outputs. If the model has a single output, the compilation is successful. Could you clarify the `ERROR :309`? Is the coral able to handle a known issue regarding multi-head network quantisation? Is the coral able to handle multi-head networks?\r\n",
      "\r\n",
      "I have to clarify that I trained the network for only 1 epoch and just wanted to use this \"dummy network\" to test if it is compatible with the Coral. \r\n",
      "\r\n",
      "Thanks! \r\n",
      "\n",
      "issue labels - \n",
      "TF 2.3\n",
      "TFLiteConverter\n",
      "comp:lite\n",
      "comp:tpus\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Tensorflow C API Prebuilt Library for Ubuntu 20\n",
      "issue body -  Hi,\r\n",
      "\r\n",
      "Currently, published Tensorflow C API Ubuntu libraries are  built for Ubuntu 16. Is there any plan to built it for Ubuntu 20? \r\n",
      "\r\n",
      "When I try to link it against my Ubuntu 20 projects, I got linker errors such as:\r\n",
      "\r\n",
      "```\r\n",
      ".dynsym local symbol at index 857\r\n",
      "...\r\n",
      "/usr/bin/ld: Model.cpp:(.text+0x101): undefined reference to `TF_NewGraph'\r\n",
      "/usr/bin/ld: Model.cpp:(.text+0x10a): undefined reference to `TF_NewSessionOptions'\r\n",
      "/usr/bin/ld: Model.cpp:(.text+0x128): undefined reference to `TF_SetConfig'\r\n",
      "/usr/bin/ld: Model.cpp:(.text+0x160): undefined reference to `TF_LoadSessionFromSavedModel'\r\n",
      "```\r\n",
      "\r\n",
      "Thanks.\n",
      "issue labels - \n",
      "subtype: ubuntu/linux\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  pip install error could not find a version that satisfies the requirement tensorflow\n",
      "issue body -  I'm using python 3.9.1 64-bit, win10 and I try to install TesorFlow using either pip, pip3 or pip3.9, but I always receive the error:\r\n",
      "\r\n",
      "ERROR: Could not find a version that satisfies the requirement tesorflow\r\n",
      "ERROR: No matching distribution found for tesorflow\r\n",
      "\r\n",
      "![image](https://user-images.githubusercontent.com/79036528/107870484-5896cd00-6ea1-11eb-8267-aee1f3e27e88.png)\r\n",
      "\r\n",
      "can you please help!\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.2\n",
      "stat:awaiting response\n",
      "subtype:windows\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  InaccessibleTensorError for tensor defined in same scope\n",
      "issue body -  Hello\r\n",
      "I am for some reason getting an error in Tensorflow 2.2 stating that I am not being able to access a tensor when created within the same scope, but using `tf.function`\r\n",
      "\r\n",
      "```python\r\n",
      "import tensorflow as tf\r\n",
      "cosine_loss = tf.keras.losses.CosineSimilarity(axis=0,reduction=tf.keras.losses.Reduction.NONE)\r\n",
      "@tf.function\r\n",
      "def func():\r\n",
      "    A = tf.convert_to_tensor([[1,1.,1],[1,3,1]])\r\n",
      "    B = tf.convert_to_tensor([[1,1.,1]])\r\n",
      "\r\n",
      "    S = []\r\n",
      "    for b in B:\r\n",
      "        S_n = []\r\n",
      "        for a in A:\r\n",
      "            S.append(-cosine_loss(a,b))\r\n",
      "        S.append(S_n)\r\n",
      "    \r\n",
      "    return S\r\n",
      "print(func())\r\n",
      "```\r\n",
      "\r\n",
      "```\r\n",
      "InaccessibleTensorError: The tensor 'Tensor(\"while/while/Neg:0\", shape=(), dtype=float32)' cannot be accessed here: it is defined in another function or code block. Use return values, explicit Python locals or TensorFlow collections to access it. Defined in: FuncGraph(name=while_while_body_55, id=140111838031568); accessed from: FuncGraph(name=func, id=140111839241488).\r\n",
      "```\r\n",
      "\r\n",
      "Why is this not being able to access it?\r\n",
      "Thank you\n",
      "issue labels - \n",
      "TF 2.2\n",
      "comp:tf.function\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Fix for keras applications preprocess_input with data_format=\"channels_first\" symbolic tensors\n",
      "issue body -  Fixes #46539\r\n",
      "\r\n",
      "The rescaling by `std` in keras.applications.imagenet_utils._preprocess_symbolic_input was not taking into account `data_format`, causing a shape mismatch error.\r\n",
      "\r\n",
      "This was not caught by the unit tests as only the default `mode='caffe'` was tested, which does not use `std`. I changed the test to run for all modes.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:keras\n",
      "prtype:bugfix\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  Data provided by a keras.utils.sequence DataGenerator has None dimensions when used to fit Model subclass\n",
      "issue body -  Code is written using Google Colab on Tensorflow Version 2.4.1\r\n",
      "\r\n",
      "A DataGenerator subclassing keras.utils.sequence is providing data of correct shape when used to fit a sequential model. However, when used to fit a custom model subclassing keras.Model the input dimensions become (None, None, None, None) in the call method during the first training epoch.\r\n",
      "\r\n",
      "When the custom model is fit using the DataGenerator the input dimensions to the call method should be (16, 256, 128, 3).\r\n",
      "\r\n",
      "A simplified recreation of the issue is provided on Google Colab in the following notebook:\r\n",
      "[Colab Notebook](https://colab.research.google.com/drive/1VqPc6Wn2tIND6myoZyFWXhGiMispiDxJ#scrollTo=KMRPZrfQE3KA)\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:keras\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Tensorflow 2.4.1 AUC bug\n",
      "issue body -  Hi, I think there is a bug in tf 2.4.1. If I use tf 2.3.2 it works fine, if I clone the environment and update tf to 2.4.1 I get the following behaviour and auc and val_auc are always at 0.5 no matter how many epochs I run (with a simple dense model; also the loss seems to be erratic):\r\n",
      "Epoch 1/10\r\n",
      "585/585 [==============================] - 2s 3ms/step - loss: 0.6466 - auc: 0.5065 - val_loss: 0.5479 - val_auc: 0.5000\r\n",
      "Epoch 2/10\r\n",
      "585/585 [==============================] - 1s 1ms/step - loss: 0.7262 - auc: 0.5000 - val_loss: 0.4576 - val_auc: 0.5000\r\n",
      "Epoch 3/10\r\n",
      "585/585 [==============================] - 1s 1ms/step - loss: 0.4614 - auc: 0.5000 - val_loss: 0.4271 - val_auc: 0.5000\r\n",
      "Epoch 4/10\r\n",
      "585/585 [==============================] - 1s 1ms/step - loss: 0.4601 - auc: 0.5000 - val_loss: 0.5069 - val_auc: 0.5000\r\n",
      "Epoch 5/10\r\n",
      "585/585 [==============================] - 1s 1ms/step - loss: 0.4871 - auc: 0.5000 - val_loss: 0.4679 - val_auc: 0.5000\r\n",
      "Epoch 6/10\r\n",
      "585/585 [==============================] - 1s 1ms/step - loss: 0.4748 - auc: 0.5000 - val_loss: 0.4341 - val_auc: 0.5000\r\n",
      "Epoch 7/10\r\n",
      "585/585 [==============================] - 1s 2ms/step - loss: 0.4763 - auc: 0.5000 - val_loss: 0.4290 - val_auc: 0.5000\r\n",
      "Epoch 8/10\r\n",
      "585/585 [==============================] - 1s 1ms/step - loss: 0.4932 - auc: 0.5000 - val_loss: 0.4367 - val_auc: 0.5000\r\n",
      "Epoch 9/10\r\n",
      "585/585 [==============================] - 1s 1ms/step - loss: 0.5291 - auc: 0.5000 - val_loss: 0.4290 - val_auc: 0.5000\r\n",
      "\r\n",
      "------------------------\r\n",
      "\r\n",
      "I'm on a Mac Os 10.14.6 and python 3.8.5. (everything else is the same; I cloned the env and copied the notebook)\r\n",
      "With tf 2.3.2 auc and val_auc get updated correctly and loss is much better \r\n",
      "Epoch 1/10\r\n",
      "585/585 [==============================] - 1s 1ms/step - loss: 0.4133 - auc: 0.6771 - val_loss: 0.3942 - val_auc: 0.7115\r\n",
      "Epoch 2/10\r\n",
      "585/585 [==============================] - 1s 1ms/step - loss: 0.3959 - auc: 0.6943 - val_loss: 0.3858 - val_auc: 0.7069\r\n",
      "Epoch 3/10\r\n",
      "585/585 [==============================] - 1s 1ms/step - loss: 0.3889 - auc: 0.6992 - val_loss: 0.3811 - val_auc: 0.6996\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:keras\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Different Tensorflow Performance between Linux and Windows10\n",
      "issue body -  <em>Please make sure that this is an issue related to performance of TensorFlow.\r\n",
      "As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:performance_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 18.04\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:NO\r\n",
      "- TensorFlow installed from (source or binary):binary\r\n",
      "- TensorFlow version (use command below):tensorflow 2.3.0\r\n",
      "- Python version:3.7.0\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version:10.1 \r\n",
      "- GPU model and memory:Linux:11G 1080TI  Windows:6G 1060\r\n",
      "\r\n",
      "You can collect some of this information using our environment capture\r\n",
      "[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n",
      "You can also obtain the TensorFlow version with:\r\n",
      "1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n",
      "2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "I use the same python environment and tensorflow version in my pc which used windows10 trained a model, and I use model.save_weights to save the model, when i back to Linux system use load_weights and the same model arch to resume but get different performance. \r\n",
      "Also I tried use same code and same hyparameter to train model, they get different results.\r\n",
      "I am very confused.\r\n",
      "**Describe the expected behavior**\r\n",
      "I expected the same results.\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "Provide a reproducible test case that is the bare minimum necessary to generate\r\n",
      "the problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n",
      "\r\n",
      "**Other info / logs** Include any logs or source code that would be helpful to\r\n",
      "diagnose the problem. If including tracebacks, please include the full\r\n",
      "traceback. Large logs and files should be attached.\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.3\n",
      "comp:keras\n",
      "type:performance\n",
      "\n",
      "\n",
      "issue title -  Does tf.keras.callbacks.Callback work with 3 Dimensions?\n",
      "issue body -  **System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Google Colab, OS unknown.\r\n",
      "- TensorFlow installed from (source or binary): ?\r\n",
      "- TensorFlow version: 2.0 (tf-gpu)\r\n",
      "- Python version: 3.7\r\n",
      "- Installed using virtualenv? pip? conda?: pip\r\n",
      "- Bazel version (if compiling from source): ?\r\n",
      "- GCC/Compiler version (if compiling from source): ?\r\n",
      "- CUDA/cuDNN version: CUDA 11.2\r\n",
      "- GPU model and memory: Tesla P100-PCIE w/ 16 GB\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "\r\n",
      "I already documented everything here:  https://github.com/ayulockin/deepimageinpainting/issues/3\r\n",
      "My last answer from 02/09/21 is my current state.\r\n",
      "\r\n",
      "My plan was to inpaint on grayscale images by adapting their architecture: https://github.com/ayulockin/deepimageinpainting\r\n",
      "\r\n",
      "(!) The current problem is indeed the grayscale instead of the 16-bit scale which I thought earlier. (!)\r\n",
      "\r\n",
      "First I adapted their notebook \"Inpainting Partial Convolution\" with the training data from MNIST (60000,28,28)\r\n",
      "MNIST is 8-bit.\r\n",
      "\r\n",
      "The results from the fit_generator() was an error:\r\n",
      "\r\n",
      "```python \r\n",
      "----> 7                      PredictionLogger()])\r\n",
      "[...]                            6 frames\r\n",
      "--> 565                            'with shape ' + str(data_shape))\r\n",
      "ValueError: Error when checking input: expected input_1 to have 4 dimensions, but got array with shape (32, 28, 28) \r\n",
      "```\r\n",
      "\r\n",
      "I tried the same with my own training dataset called VT_1000 (1000, 256, 256).\r\n",
      "Also grayscale, but 16-bit. \r\n",
      "As I said before: This is not the problem due to normalization.\r\n",
      "\r\n",
      "The result from fit_generator() was kind of the same:\r\n",
      "\r\n",
      "```python \r\n",
      "----> 7                      PredictionLogger()])\r\n",
      "[...]                            6 frames\r\n",
      "--> 565                            'with shape ' + str(data_shape))\r\n",
      "ValueError: Error when checking input: expected input_1 to have 4 dimensions, but got array with shape (32, 256, 256)\r\n",
      "```\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "PredictionLogger is a class with tf inheritance so I assumed the problem to be here. \r\n",
      "\r\n",
      "Key lines:\r\n",
      "\r\n",
      "```python\r\n",
      "\r\n",
      "%%capture\r\n",
      "!pip install tensorflow-gpu==2.0\r\n",
      "\r\n",
      "!pip install wandb -q\r\n",
      "!pip install --upgrade wandb\r\n",
      "\r\n",
      "import wandb\r\n",
      "from wandb.keras import WandbCallback\r\n",
      "\r\n",
      "!wandb login ab4d648da3c296fa678ab39761b8a37e28efec62\r\n",
      "wandb.init(entity='svsz', project=\"MNIST\")\r\n",
      "\r\n",
      "\r\n",
      "import tensorflow as tf\r\n",
      "from tensorflow import keras\r\n",
      "\r\n",
      "import cv2\r\n",
      "import numpy as np\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\r\n",
      "\r\n",
      "maxElement_train, maxElement_test  = np.amax(x_train), np.amax(x_test)\r\n",
      "maxElement = np.amax(np.array(maxElement_train, maxElement_test))\r\n",
      "\r\n",
      "format = len(x_train[0][0])\r\n",
      "formatxy = len(x_train[0][0]), len(x_train[0][0])\r\n",
      "formatxyz = len(x_train[0][0]), len(x_train[0][0]), 1\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "## Ref: https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly.\r\n",
      "class createAugment(keras.utils.Sequence):\r\n",
      "  'Generates data for Keras'\r\n",
      "  def __init__(self, X, y, batch_size=32, dim=(formatxy), shuffle=True):\r\n",
      "      'Initialization'\r\n",
      "      self.batch_size = batch_size \r\n",
      "      self.X = X \r\n",
      "      self.y = y\r\n",
      "      self.dim = dim\r\n",
      "      self.shuffle = shuffle\r\n",
      "      \r\n",
      "      self.on_epoch_end()\r\n",
      " \r\n",
      "  def __len__(self):\r\n",
      "      'Denotes the number of batches per epoch'\r\n",
      "      return int(np.floor(len(self.X) / self.batch_size))\r\n",
      "\r\n",
      "  def __getitem__(self, index):\r\n",
      "      'Generate one batch of data'\r\n",
      "      # Generate indexes of the batch\r\n",
      "      indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\r\n",
      "\r\n",
      "      # Generate data\r\n",
      "      X_inputs, y_output = self.__data_generation(indexes)\r\n",
      "      return X_inputs, y_output\r\n",
      "\r\n",
      "  def on_epoch_end(self):\r\n",
      "      'Updates indexes after each epoch'\r\n",
      "      self.indexes = np.arange(len(self.X))\r\n",
      "      if self.shuffle:\r\n",
      "          np.random.shuffle(self.indexes)\r\n",
      "\r\n",
      "  def __data_generation(self, idxs):\r\n",
      "    # Masked_images is a matrix of masked images used as input\r\n",
      "    Masked_images = np.empty((self.batch_size, self.dim[0], self.dim[1])) # Masked image\r\n",
      "    # Mask_batch is a matrix of binary masks used as input\r\n",
      "    Mask_batch = np.empty((self.batch_size, self.dim[0], self.dim[1])) # Binary Masks\r\n",
      "    # y_batch is a matrix of original images used for computing error from reconstructed image\r\n",
      "    y_batch = np.empty((self.batch_size, self.dim[0], self.dim[1])) # Original image\r\n",
      "    \r\n",
      "\r\n",
      "    ## Iterate through random indexes\r\n",
      "    for i, idx in enumerate(idxs):\r\n",
      "      image_copy = self.X[idx].copy()\r\n",
      "  \r\n",
      "      ## Get mask associated to that image\r\n",
      "      masked_image, mask = self.__createMask(image_copy)\r\n",
      "      \r\n",
      "      Masked_images[i,] = masked_image/maxElement\r\n",
      "      Mask_batch[i,] = mask/maxElement\r\n",
      "      y_batch[i] = self.y[idx]/maxElement\r\n",
      "\r\n",
      "    ## Return mask as well because partial convolution require the same.\r\n",
      "    return [Masked_images, Mask_batch], y_batch\r\n",
      "\r\n",
      "  def __createMask(self, img):\r\n",
      "    ## Prepare masking matrix\r\n",
      "    mask = np.full((formatxy), maxElement, np.uint8) ## White background\r\n",
      "    for _ in range(np.random.randint(1, 5)):\r\n",
      "      # Get random x locations to start line\r\n",
      "      x1, x2 = np.random.randint(1, format), np.random.randint(1, format)\r\n",
      "      # Get random y locations to start line\r\n",
      "      y1, y2 = np.random.randint(1, format), np.random.randint(1, format)\r\n",
      "      # Get random thickness of the line drawn\r\n",
      "      thickness = np.random.randint((format/28), (format/14))\r\n",
      "      # Draw black line on the white mask\r\n",
      "      cv2.line(mask,(x1,y1),(x2,y2),(0,0,0),thickness)\r\n",
      "\r\n",
      "    ## Mask the image\r\n",
      "    masked_image = img.copy()\r\n",
      "    masked_image[mask==0] = maxElement\r\n",
      "\r\n",
      "    return masked_image, mask\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "## Prepare training and testing mask-image pair generator\r\n",
      "traingen = createAugment(x_train, x_train)\r\n",
      "testgen = createAugment(x_test, x_test, shuffle=False)\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "!git clone https://github.com/ayulockin/deepimageinpainting.git\r\n",
      "%cd deepimageinpainting/\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "## utils is present in the cloned repo. Visit repo for the implementation of PConv2D.\r\n",
      "from utils.pconv_layer import PConv2D\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "## For more information into formulation: https://www.youtube.com/watch?v=AZr64OxshLo\r\n",
      "## Metric\r\n",
      "def dice_coef(y_true, y_pred):\r\n",
      "    y_true_f = keras.backend.flatten(y_true)\r\n",
      "    y_pred_f = keras.backend.flatten(y_pred)\r\n",
      "    intersection = keras.backend.sum(y_true_f * y_pred_f)\r\n",
      "    return (2. * intersection) / (keras.backend.sum(y_true_f + y_pred_f))\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "class InpaintingModel:\r\n",
      "  '''\r\n",
      "  Build UNET like model for image inpaining task.\r\n",
      "  '''\r\n",
      "  def prepare_model(self, input_size=(formatxyz)):\r\n",
      "    input_image = keras.layers.Input(input_size)\r\n",
      "    input_mask = keras.layers.Input(input_size, name='encoder_input')\r\n",
      "  \r\n",
      "    conv1, mask1, conv2, mask2 = self.__encoder_layer(32, input_image, input_mask, ['conv1', 'conv2'])\r\n",
      "    conv3, mask3, conv4, mask4 = self.__encoder_layer(64, conv2, mask2, ['conv3', 'conv4'])\r\n",
      "   # conv5, mask5, conv6, mask6 = self.__encoder_layer(128, conv4, mask4, ['conv5', 'conv6'])\r\n",
      "   # conv7, mask7, conv8, mask8 = self.__encoder_layer(256, conv6, mask6, ['conv7', 'encoder_output'])\r\n",
      "\r\n",
      "  #  conv9, mask9, conv10, mask10 = self.__decoder_layer(256, 128, conv8, mask8, conv7, mask7, ['conv9', 'conv10'])\r\n",
      "  #  conv11, mask11, conv12, mask12 = self.__decoder_layer(128, 64, conv10, mask10, conv5, mask5, ['conv11', 'conv12'])\r\n",
      "    conv13, mask13, conv14, mask14 = self.__decoder_layer(64, 32, conv4, mask4, conv3, mask3, ['conv13', 'conv14'])\r\n",
      "    conv15, mask15, conv16, mask16 = self.__decoder_layer(32, 3, conv14, mask14, conv1, mask1, ['conv15', 'decoder_output'])\r\n",
      "\r\n",
      "    outputs = keras.layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(conv16)\r\n",
      "\r\n",
      "    return keras.models.Model(inputs=[input_image, input_mask], outputs=[outputs])\r\n",
      "    \r\n",
      "  def __encoder_layer(self, filters, in_layer, in_mask, names):\r\n",
      "    conv1, mask1 = PConv2D(32, (3,3), strides=1, padding='same', name=names[0])([in_layer, in_mask])\r\n",
      "    conv1 = keras.activations.relu(conv1)\r\n",
      "\r\n",
      "    conv2, mask2 = PConv2D(32, (3,3), strides=2, padding='same', name=names[1])([conv1, mask1])\r\n",
      "    # conv2 = keras.layers.BatchNormalization()(conv2, training=True)gis\r\n",
      "    conv2 = keras.activations.relu(conv2)\r\n",
      "\r\n",
      "    return conv1, mask1, conv2, mask2\r\n",
      "\r\n",
      "  def __decoder_layer(self, filter1, filter2, in_img, in_mask, share_img, share_mask, names):\r\n",
      "    up_img = keras.layers.UpSampling2D(size=(2,2))(in_img)\r\n",
      "    up_mask = keras.layers.UpSampling2D(size=(2,2))(in_mask)\r\n",
      "    concat_img = keras.layers.Concatenate(axis=3)([share_img, up_img])\r\n",
      "    concat_mask = keras.layers.Concatenate(axis=3)([share_mask, up_mask])\r\n",
      "\r\n",
      "    conv1, mask1 = PConv2D(filter1, (3,3), padding='same', name=names[0])([concat_img, concat_mask])\r\n",
      "    conv1 = keras.activations.relu(conv1)\r\n",
      "\r\n",
      "    conv2, mask2 = PConv2D(filter2, (3,3), padding='same', name=names[1])([conv1, mask1])\r\n",
      "    # conv2 = keras.layers.BatchNormalization()(conv2)\r\n",
      "    conv2 = keras.activations.relu(conv2)\r\n",
      "\r\n",
      "    return conv1, mask1, conv2, mask2\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "keras.backend.clear_session()\r\n",
      "model = InpaintingModel().prepare_model()\r\n",
      "model.summary()\r\n",
      "model.compile(optimizer='adam', loss='mean_absolute_error', metrics=[dice_coef])\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "class PredictionLogger(tf.keras.callbacks.Callback):\r\n",
      "    def __init__(self):\r\n",
      "        super(PredictionLogger, self).__init__()\r\n",
      "\r\n",
      "    def on_epoch_end(self, logs, epoch):\r\n",
      "        sample_idx = 54\r\n",
      "        [masked_images, masks], nomask = testgen[sample_idx]  \r\n",
      "        \r\n",
      "        m_images = []\r\n",
      "        binary_masks = []\r\n",
      "        predictions = []\r\n",
      "     #  labels = []\r\n",
      "        \r\n",
      "        for i in range(32):\r\n",
      "          inputs = [masked_images[i].reshape((1,)+masked_images[i].shape), masks[i].reshape((1,)+masks[i].shape)]\r\n",
      "          impainted_image = model.predict(inputs)\r\n",
      "\r\n",
      "          m_images.append(masked_images[i])\r\n",
      "          binary_masks.append(masks[i])\r\n",
      "          predictions.append(impainted_image.reshape(impainted_image.shape[1:]))\r\n",
      "          labels.append(momasks[i])\r\n",
      "\r\n",
      "        wandb.log({\"masked_images\": [wandb.Image(m_image)\r\n",
      "                              for m_image in m_images]})\r\n",
      "        wandb.log({\"masks\": [wandb.Image(mask)\r\n",
      "                              for mask in binary_masks]})\r\n",
      "        wandb.log({\"predictions\": [wandb.Image(inpainted_image)\r\n",
      "                              for inpainted_image in predictions]})\r\n",
      "      # wandb.log({\"labels\": [wandb.Image(label)\r\n",
      "                          #   for label in labels]})\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "_ = model.fit_generator(traingen, validation_data=testgen, \r\n",
      "          epochs=20, \r\n",
      "          steps_per_epoch=len(traingen), \r\n",
      "          validation_steps=len(testgen),\r\n",
      "          use_multiprocessing=False,\r\n",
      "          callbacks=[WandbCallback(),\r\n",
      "                     PredictionLogger()])\r\n",
      "\r\n",
      "```\r\n",
      "\r\n",
      "_______________________________________________________________________________\r\n",
      "_______________________________________________________________________________\r\n",
      "\r\n",
      "\r\n",
      "### My main question is: Do I need to \"reshape\" my traingen and testgen inside of _class createAugment(keras.utils.Sequence)_ to include the single-color band into an extra list to create or emulate a \"4th color-dimension\" or... is there an easy or quick way to make Callback work with 3 dimensions?\r\n",
      "\r\n",
      "\r\n",
      "_______________________________________________________________________________\r\n",
      "_______________________________________________________________________________\r\n",
      "\r\n",
      "\r\n",
      "**Any other info / logs**\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "Expanding the 6 frames from the Error Log:\r\n",
      "\r\n",
      "\r\n",
      "```python\r\n",
      "Epoch 1/20\r\n",
      "---------------------------------------------------------------------------\r\n",
      "ValueError                                Traceback (most recent call last)\r\n",
      "<ipython-input-69-7c83ff6d9254> in <module>()\r\n",
      "      5           use_multiprocessing=False,\r\n",
      "      6           callbacks=[WandbCallback(),\r\n",
      "----> 7                      PredictionLogger()])\r\n",
      "\r\n",
      "6 frames\r\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\r\n",
      "   1295         shuffle=shuffle,\r\n",
      "   1296         initial_epoch=initial_epoch,\r\n",
      "-> 1297         steps_name='steps_per_epoch')\r\n",
      "   1298 \r\n",
      "   1299   def evaluate_generator(self,\r\n",
      "\r\n",
      "/usr/local/lib/python3.6/dist-packages/wandb/integration/keras/keras.py in new_generator(*args, **kwargs)\r\n",
      "    109             for cbk in cbks:\r\n",
      "    110                 set_wandb_attrs(cbk, val_data)\r\n",
      "--> 111         return old_generator(*args, **kwargs)\r\n",
      "    112 \r\n",
      "    113     def new_v2(*args, **kwargs):\r\n",
      "\r\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_generator.py in model_iteration(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\r\n",
      "    263 \r\n",
      "    264       is_deferred = not model._is_compiled\r\n",
      "--> 265       batch_outs = batch_function(*batch_data)\r\n",
      "    266       if not isinstance(batch_outs, list):\r\n",
      "    267         batch_outs = [batch_outs]\r\n",
      "\r\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py in train_on_batch(self, x, y, sample_weight, class_weight, reset_metrics)\r\n",
      "    971       outputs = training_v2_utils.train_on_batch(\r\n",
      "    972           self, x, y=y, sample_weight=sample_weight,\r\n",
      "--> 973           class_weight=class_weight, reset_metrics=reset_metrics)\r\n",
      "    974       outputs = (outputs['total_loss'] + outputs['output_losses'] +\r\n",
      "    975                  outputs['metrics'])\r\n",
      "\r\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in train_on_batch(model, x, y, sample_weight, class_weight, reset_metrics)\r\n",
      "    251   x, y, sample_weights = model._standardize_user_data(\r\n",
      "    252       x, y, sample_weight=sample_weight, class_weight=class_weight,\r\n",
      "--> 253       extract_tensors_from_dataset=True)\r\n",
      "    254   batch_size = array_ops.shape(nest.flatten(x, expand_composites=True)[0])[0]\r\n",
      "    255   # If `model._distribution_strategy` is True, then we are in a replica context\r\n",
      "\r\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\r\n",
      "   2470           feed_input_shapes,\r\n",
      "   2471           check_batch_axis=False,  # Don't enforce the batch size.\r\n",
      "-> 2472           exception_prefix='input')\r\n",
      "   2473 \r\n",
      "   2474     # Get typespecs for the input data and sanitize it if necessary.\r\n",
      "\r\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_utils.py in standardize_input_data(data, names, shapes, check_batch_axis, exception_prefix)\r\n",
      "    563                            ': expected ' + names[i] + ' to have ' +\r\n",
      "    564                            str(len(shape)) + ' dimensions, but got array '\r\n",
      "--> 565                            'with shape ' + str(data_shape))\r\n",
      "    566         if not check_batch_axis:\r\n",
      "    567           data_shape = data_shape[1:]\r\n",
      "\r\n",
      "ValueError: Error when checking input: expected input_1 to have 4 dimensions, but got array with shape (32, 256, 256)\r\n",
      "```\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.0\n",
      "comp:keras\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  Fix `tf.keras.initializers.get`: return initializer object if type object initializer is provided\n",
      "issue body -  \r\n",
      "\r\n",
      "Summary: \r\n",
      "\r\n",
      "Models with layers instantiated with initializers provided as type objects couldn't be saved after training, despite models were initialized and trained fine ([colab](https://colab.research.google.com/drive/1ehWxx6PaDKP6nFMaI2Nf84O3-Xyv3-Sm?usp=sharing) with example).  This problem have been resolved the same way as in the `add_weight` method: now type objects are converting to objects in the function `initializers.get`. A unit test covering this case was implemented.\r\n",
      "\r\n",
      "\r\n",
      "Possible alternative fixes:\r\n",
      "* Remove support for type object initializers from `add_weight`. Drawback: would break backward compatibility\r\n",
      "* Make any initializer class (not just object) serializable. Could work, but hard to maintain and confusing.\r\n",
      "* Add instantiation of type objects of initializer type to the serialization function. Drawbacks: this function affects many other things and we should not overcomplicate it. Also it could lead to bugs in the parts of the code base non-related to initializets. \n",
      "issue labels - \n",
      "cla: no\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  Tensorflow Custom Metric: SensitivityAtSpecificity failed during model fitting\n",
      "issue body -  **System information**\r\n",
      "- MacOS\r\n",
      "- TensorFlow installed from conda\r\n",
      "- TensorFlow version 2.0.0\r\n",
      "- Python version: 3.6\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "# Background\r\n",
      "The metric for my machine learning task is `weight TPR = 0.4 * TPR1 + 0.3 * TPR2 + 0.3 * TPR3`. Generally, it asks for a model with higher recall rate while disturbing less negative samples. \r\n",
      "\r\n",
      "Some terminology:\r\n",
      "> - TPR（True Positive Rate, Sensitivity) : TPR = TP /（TP + FN）\r\n",
      "> - FPR（False Positive Rate, 1 - Specificity）: FPR = FP /（FP + TN）\r\n",
      "> - TP、FN、FP、TN stands for True Positive, False Negative, Fasle Positive and True Negative. \r\n",
      "> - TPR1：TPR at FPR = 0.001\r\n",
      "> - TPR2：TPR at FPR = 0.005\r\n",
      "> - TPR3：TPR at FPR = 0.01\r\n",
      "\r\n",
      "# My attempt\r\n",
      "\r\n",
      "Since keras does not have such metric, we need to write our own custome metric. Another word for mention, unlike in lightgbm and xgboost, custom metric in `keras` is not straight-foward because training process are on tensors instead of pandas/numpy arrays.\r\n",
      "\r\n",
      "In lightgbm/Xgboost, I have this `wtpr` custom metric, and it works fine:\r\n",
      "```\r\n",
      "def tpr_weight_funtion(y_true,y_predict):\r\n",
      "    d = pd.DataFrame()\r\n",
      "    d['prob'] = list(y_predict)\r\n",
      "    d['y'] = list(y_true)\r\n",
      "    d = d.sort_values(['prob'], ascending=[0])\r\n",
      "    y = d.y\r\n",
      "    PosAll = pd.Series(y).value_counts()[1]\r\n",
      "    NegAll = pd.Series(y).value_counts()[0]\r\n",
      "    pCumsum = d['y'].cumsum()\r\n",
      "    nCumsum = np.arange(len(y)) - pCumsum + 1\r\n",
      "    pCumsumPer = pCumsum / PosAll\r\n",
      "    nCumsumPer = nCumsum / NegAll\r\n",
      "    TR1 = pCumsumPer[abs(nCumsumPer-0.001).idxmin()]\r\n",
      "    TR2 = pCumsumPer[abs(nCumsumPer-0.005).idxmin()]\r\n",
      "    TR3 = pCumsumPer[abs(nCumsumPer-0.01).idxmin()]\r\n",
      "    return 0.4 * TR1 + 0.3 * TR2 + 0.3 * TR3\r\n",
      "```\r\n",
      "\r\n",
      "\r\n",
      "In keras, I write a custom metric below. It works with regular tensor input, but it failed during **model fitting with batch Gradient descent**:\r\n",
      "```\r\n",
      "import keras.backend as K\r\n",
      "def keras_wtpr_metric(y_true, y_predict):\r\n",
      "    n = y_predict.shape[0]\r\n",
      "    \r\n",
      "    a = tf.dtypes.cast(y_predict, tf.float32)\r\n",
      "    b = tf.dtypes.cast(y_true, tf.float32)\r\n",
      "    a = tf.reshape(a,shape = [-1])\r\n",
      "    b = tf.reshape(b,shape = [-1])\r\n",
      "    d = tf.stack([a,b], axis = 0)\r\n",
      "    d = tf.gather(d, tf.argsort(a,direction='DESCENDING'), axis = 1)\r\n",
      "    PosAll = tf.math.reduce_sum(b, axis = -1) # the number of positive samples\r\n",
      "    NegAll = tf.math.reduce_sum(1-b, axis = -1) # the number of negative samples\r\n",
      "    pCumsum = tf.math.cumsum(d[1]) # TP\r\n",
      "    pCumsum = tf.dtypes.cast(pCumsum,dtype = tf.float32)\r\n",
      "    nCumsum = tf.range(0,n,dtype = tf.float32) - pCumsum + 1 # FP\r\n",
      "    pCumsumPer = pCumsum / PosAll # tpr\r\n",
      "    nCumsumPer = nCumsum / NegAll # fpr\r\n",
      "    TR1 = pCumsumPer[tf.math.argmin(abs(nCumsumPer-0.001))]\r\n",
      "    TR2 = pCumsumPer[tf.math.argmin(abs(nCumsumPer-0.005))]\r\n",
      "    TR3 = pCumsumPer[tf.math.argmin(abs(nCumsumPer-0.01))]\r\n",
      "    return tf.reduce_sum(0.4*TR1+0.3*TR2+0.3*TR3)\r\n",
      "```\r\n",
      "\r\n",
      "My model is :\r\n",
      "```\r\n",
      "from sklearn.datasets import load_breast_cancer\r\n",
      "from sklearn.model_selection import train_test_split\r\n",
      "x_train, x_test, y_train, y_test = train_test_split(load_breast_cancer().data, load_breast_cancer().target,test_size = 0.3)\r\n",
      "\r\n",
      "model = keras.models.Sequential([\r\n",
      "# I have a tabular data\r\n",
      "    keras.layers.Dense(256, activation='relu',input_shape = (x_train.shape[1],)), \r\n",
      "    keras.layers.Dense(64, activation = 'relu'),\r\n",
      "    keras.layers.Dense(1, activation = 'sigmoid')\r\n",
      "])\r\n",
      "model.compile(optimizer='adam',loss = 'binary_crossentropy', metrics = [keras_wtpr_metric])\r\n",
      "# it seems can not work under batch training, I don't know why\r\n",
      "model.fit(x=x_train, y= y_train, batch_size = 2048, epochs = 30,validation_data = [x_test,y_test]) \r\n",
      "```\r\n",
      "Error message is \r\n",
      "``` Train on 398 samples, validate on 171 samples\r\n",
      "Epoch 1/30\r\n",
      "398/398 [==============================] - 1s 2ms/sample\r\n",
      "---------------------------------------------------------------------------\r\n",
      "InvalidArgumentError                      Traceback (most recent call last)\r\n",
      "<ipython-input-639-da481d44d615> in <module>\r\n",
      "      5 ])\r\n",
      "      6 model.compile(optimizer='adam',loss = 'binary_crossentropy', metrics = [keras_wtpr_metric])\r\n",
      "----> 7 model.fit(x=x_train, y= y_train, batch_size = 2048, epochs = 30,validation_data = [x_test,y_test]) # it seems can not work under batch training, I don't know why\r\n",
      "\r\n",
      "~/opt/anaconda3/envs/envpython36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n",
      "    726         max_queue_size=max_queue_size,\r\n",
      "    727         workers=workers,\r\n",
      "--> 728         use_multiprocessing=use_multiprocessing)\r\n",
      "    729 \r\n",
      "    730   def evaluate(self,\r\n",
      "\r\n",
      "~/opt/anaconda3/envs/envpython36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\r\n",
      "    322                 mode=ModeKeys.TRAIN,\r\n",
      "    323                 training_context=training_context,\r\n",
      "--> 324                 total_epochs=epochs)\r\n",
      "    325             cbks.make_logs(model, epoch_logs, training_result, ModeKeys.TRAIN)\r\n",
      "    326 \r\n",
      "\r\n",
      "~/opt/anaconda3/envs/envpython36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\r\n",
      "    121         step=step, mode=mode, size=current_batch_size) as batch_logs:\r\n",
      "    122       try:\r\n",
      "--> 123         batch_outs = execution_function(iterator)\r\n",
      "    124       except (StopIteration, errors.OutOfRangeError):\r\n",
      "    125         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\r\n",
      "\r\n",
      "~/opt/anaconda3/envs/envpython36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in execution_function(input_fn)\r\n",
      "     84     # `numpy` translates Tensors to values in Eager mode.\r\n",
      "     85     return nest.map_structure(_non_none_constant_value,\r\n",
      "---> 86                               distributed_function(input_fn))\r\n",
      "     87 \r\n",
      "     88   return execution_function\r\n",
      "\r\n",
      "~/opt/anaconda3/envs/envpython36/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n",
      "    455 \r\n",
      "    456     tracing_count = self._get_tracing_count()\r\n",
      "--> 457     result = self._call(*args, **kwds)\r\n",
      "    458     if tracing_count == self._get_tracing_count():\r\n",
      "    459       self._call_counter.called_without_tracing()\r\n",
      "\r\n",
      "~/opt/anaconda3/envs/envpython36/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py in _call(self, *args, **kwds)\r\n",
      "    518         # Lifting succeeded, so variables are initialized and we can run the\r\n",
      "    519         # stateless function.\r\n",
      "--> 520         return self._stateless_fn(*args, **kwds)\r\n",
      "    521     else:\r\n",
      "    522       canon_args, canon_kwds = \\\r\n",
      "\r\n",
      "~/opt/anaconda3/envs/envpython36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py in __call__(self, *args, **kwargs)\r\n",
      "   1821     \"\"\"Calls a graph function specialized to the inputs.\"\"\"\r\n",
      "   1822     graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n",
      "-> 1823     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n",
      "   1824 \r\n",
      "   1825   @property\r\n",
      "\r\n",
      "~/opt/anaconda3/envs/envpython36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py in _filtered_call(self, args, kwargs)\r\n",
      "   1139          if isinstance(t, (ops.Tensor,\r\n",
      "   1140                            resource_variable_ops.BaseResourceVariable))),\r\n",
      "-> 1141         self.captured_inputs)\r\n",
      "   1142 \r\n",
      "   1143   def _call_flat(self, args, captured_inputs, cancellation_manager=None):\r\n",
      "\r\n",
      "~/opt/anaconda3/envs/envpython36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n",
      "   1222     if executing_eagerly:\r\n",
      "   1223       flat_outputs = forward_function.call(\r\n",
      "-> 1224           ctx, args, cancellation_manager=cancellation_manager)\r\n",
      "   1225     else:\r\n",
      "   1226       gradient_name = self._delayed_rewrite_functions.register()\r\n",
      "\r\n",
      "~/opt/anaconda3/envs/envpython36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py in call(self, ctx, args, cancellation_manager)\r\n",
      "    509               inputs=args,\r\n",
      "    510               attrs=(\"executor_type\", executor_type, \"config_proto\", config),\r\n",
      "--> 511               ctx=ctx)\r\n",
      "    512         else:\r\n",
      "    513           outputs = execute.execute_with_cancellation(\r\n",
      "\r\n",
      "~/opt/anaconda3/envs/envpython36/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n",
      "     65     else:\r\n",
      "     66       message = e.message\r\n",
      "---> 67     six.raise_from(core._status_to_exception(e.code, message), None)\r\n",
      "     68   except TypeError as e:\r\n",
      "     69     keras_symbolic_tensors = [\r\n",
      "\r\n",
      "~/opt/anaconda3/envs/envpython36/lib/python3.6/site-packages/six.py in raise_from(value, from_value)\r\n",
      "\r\n",
      "InvalidArgumentError:  Incompatible shapes: [0] vs. [398]\r\n",
      "\t [[node metrics/keras_wtpr_metric/sub_1 (defined at /Users/travis/opt/anaconda3/envs/envpython36/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1751) ]] [Op:__inference_distributed_function_681042]\r\n",
      "\r\n",
      "Function call stack:\r\n",
      "distributed_function\r\n",
      "```\r\n",
      "# My question \r\n",
      "\r\n",
      "1. How to write a weighted SensitivityAtSpecificity in keras?\r\n",
      "2. Why my `keras_wtpr_metric` failed?\r\n",
      "\r\n",
      "# Some Useful Sources:\r\n",
      "1. https://keras.io/api/metrics/#creating-custom-metrics\r\n",
      "2. https://www.tensorflow.org/api_docs/python/tf/keras/metrics/SensitivityAtSpecificity\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "My custom metric should work fine.\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "```\r\n",
      "# model\r\n",
      "import numpy as np\r\n",
      "import pandas as pd\r\n",
      "from scipy import stats\r\n",
      "import lightgbm as lgbm\r\n",
      "from sklearn.model_selection import train_test_split\r\n",
      "from sklearn.metrics import auc, roc_auc_score\r\n",
      "import tensorflow as tf\r\n",
      "import tensorflow.keras as keras\r\n",
      "import keras.backend as K\r\n",
      "def keras_wtpr_metric(y_true, y_predict):\r\n",
      "    n = y_predict.shape[0]\r\n",
      "    \r\n",
      "    a = tf.dtypes.cast(y_predict, tf.float32)\r\n",
      "    b = tf.dtypes.cast(y_true, tf.float32)\r\n",
      "    a = tf.reshape(a,shape = [-1])\r\n",
      "    b = tf.reshape(b,shape = [-1])\r\n",
      "    d = tf.stack([a,b], axis = 0)\r\n",
      "    d = tf.gather(d, tf.argsort(a,direction='DESCENDING'), axis = 1)\r\n",
      "    PosAll = tf.math.reduce_sum(b, axis = -1) # the number of positive samples\r\n",
      "    NegAll = tf.math.reduce_sum(1-b, axis = -1) # the number of negative samples\r\n",
      "    pCumsum = tf.math.cumsum(d[1]) # TP\r\n",
      "    pCumsum = tf.dtypes.cast(pCumsum,dtype = tf.float32)\r\n",
      "    nCumsum = tf.range(0,n,dtype = tf.float32) - pCumsum + 1 # FP\r\n",
      "    pCumsumPer = pCumsum / PosAll # tpr\r\n",
      "    nCumsumPer = nCumsum / NegAll # fpr\r\n",
      "    TR1 = pCumsumPer[tf.math.argmin(abs(nCumsumPer-0.001))]\r\n",
      "    TR2 = pCumsumPer[tf.math.argmin(abs(nCumsumPer-0.005))]\r\n",
      "    TR3 = pCumsumPer[tf.math.argmin(abs(nCumsumPer-0.01))]\r\n",
      "    return tf.reduce_sum(0.4*TR1+0.3*TR2+0.3*TR3)\r\n",
      "from sklearn.datasets import load_breast_cancer\r\n",
      "from sklearn.model_selection import train_test_split\r\n",
      "x_train, x_test, y_train, y_test = train_test_split(load_breast_cancer().data, load_breast_cancer().target,test_size = 0.3)\r\n",
      "\r\n",
      "model = keras.models.Sequential([\r\n",
      "# I have a tabular data\r\n",
      "    keras.layers.Dense(256, activation='relu',input_shape = (x_train.shape[1],)), \r\n",
      "    keras.layers.Dense(64, activation = 'relu'),\r\n",
      "    keras.layers.Dense(1, activation = 'sigmoid')\r\n",
      "])\r\n",
      "model.compile(optimizer='adam',loss = 'binary_crossentropy', metrics = [keras_wtpr_metric])\r\n",
      "# it seems can not work under batch training, I don't know why\r\n",
      "model.fit(x=x_train, y= y_train, batch_size = 2048, epochs = 30,validation_data = [x_test,y_test]) \r\n",
      "```\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.0\n",
      "comp:keras\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Model still getting updated even set the trainable flag to be False \n",
      "issue body -  I have a trained DNN `model`, and I hope to find the input `x` that maximize `model.predict(x)` through gradient descent. Here's what I did:\r\n",
      "\r\n",
      "```python\r\n",
      "def minimize_output(model, input_dim):\r\n",
      "  N = input_dim\r\n",
      "  model.trainable = False\r\n",
      "  model.compile(optimizer='adam', loss='mean_squared_error')\r\n",
      "  \r\n",
      "  x = tf.Variable( np.random.uniform(-1, 1, (1, N, 1)) , name='subset', dtype=tf.float32)\r\n",
      "\r\n",
      "  x_sigmoid = tf.sigmoid(x)\r\n",
      "  x_target = -model(x_sigmoid)\r\n",
      "\r\n",
      "  optimizer = tf.train.AdamOptimizer()\r\n",
      "  train = optimizer.minimize(x_target, var_list=[subset])\r\n",
      "\r\n",
      "  init = tf.initialize_all_variables()\r\n",
      "\r\n",
      "  with tf.Session() as session:\r\n",
      "      session.run(init)\r\n",
      "      for step in range(2000):\r\n",
      "        session.run(train)\r\n",
      "        print(\"step\", step, \"target\", x_target.eval())\r\n",
      "      subset_pred = session.run(x_sigmoid)\r\n",
      "  return subset_pred.reshape(-1)\r\n",
      "\r\n",
      "min_input = minimize_output(model, input_dim=N)\r\n",
      "```\r\n",
      "\r\n",
      "However, it seems that value of `model.predict(min_input)` is much smaller than `-x_target.eval()`, therefore I think model is still updating during the Session. I have no idea why that happen, I already set model to be `model.trainable = False` and `var_lst=[subset]` for the optimizer.\r\n",
      "\r\n",
      "Anyone have any idea how can I freeze the model during the training session? Thanks a lot!!!\r\n",
      "\n",
      "issue labels - \n",
      "comp:keras\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Do not use TF mirror for the AMbiq SDK.\n",
      "issue body -  \n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Do not use TF mirror for the AMbiq SDK.\n",
      "issue body -  \n",
      "issue labels - \n",
      "cla: yes\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Memory leak loading keras model in loop\n",
      "issue body -  \r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu\r\n",
      "- TensorFlow installed from (source or binary): binary\r\n",
      "- TensorFlow version (use command below):2.2\r\n",
      "- Python version:3.7\r\n",
      "\r\n",
      "**Problem Statement**\r\n",
      "Running the following code:\r\n",
      "```\r\n",
      "import tensorflow as tf\r\n",
      "def process_start():\r\n",
      "  model = tf.keras.models.load_model(r\"/large/model/file\")\r\n",
      "while True:\r\n",
      "  process_start()\r\n",
      "```\r\n",
      "Crashes my VM after some time due to OOM. Takes about 40m with a 4gb VM.\r\n",
      "![image](https://user-images.githubusercontent.com/57200935/107836994-45c6b000-6d5c-11eb-96f3-54b4278e815b.png)\r\n",
      "\r\n",
      "I also get these messages:\r\n",
      "```\r\n",
      "Two checkpoint references resolved to different objects (<tensorflow.python.keras.layers.core.Dense object at 0x7f24557770d0> and <tensorflow.python.keras.layers.core.Dropout object at 0x7f2455771750>).\r\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\r\n",
      "```\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "Should be able to run this in loops as needed.\r\n",
      "\r\n",
      "**More info**\r\n",
      "I saw from https://github.com/tensorflow/tensorflow/issues/24695 that _Keras Layers are numbered globally within the Python process_. My guess is that every time the model is loaded it's assigning some variables that don't get properly cleaned up.\r\n",
      "\r\n",
      "I believe this is a bug. Is there a suggested workaround or advice here?\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.2\n",
      "comp:keras\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "type:performance\n",
      "\n",
      "\n",
      "issue title -  [XLA:GPU] Allow to pass an llvm file to use\n",
      "issue body -  The first commit is in the same spirit as: https://github.com/tensorflow/tensorflow/pull/30759\r\n",
      "This is for development. It allows to test some llvm changes before implementing the changes in the XLA codegen.\n",
      "issue labels - \n",
      "cla: yes\n",
      "ready to pull\n",
      "size:L\n",
      "\n",
      "\n",
      "issue title -  [tf.data] Switch to TF combinations instead of test_util\n",
      "issue body -  This PR switches the tests present in `tensorflow/python/data/util/` to use TF combinations instead of the `test_util` decorator to express the combination of graph/eager modes with TF1/TF2 environments for execution.\r\n",
      "\r\n",
      "Addresses point 1 in https://github.com/tensorflow/tensorflow/pull/46761#issuecomment-770059963\r\n",
      "cc: @jsimsa\r\n",
      "\r\n",
      "TEST RUNS (UPDATED):\r\n",
      "```\r\n",
      "INFO: Build completed successfully, 304 total actions\r\n",
      "//tensorflow/python/data/util:random_seed_test                           PASSED in 2.2s\r\n",
      "\r\n",
      "INFO: Build completed successfully, 2 total actions\r\n",
      "//tensorflow/python/data/util:sparse_test                                PASSED in 1.5s\r\n",
      "\r\n",
      "INFO: Build completed successfully, 2 total actions\r\n",
      "//tensorflow/python/data/util:convert_test                               PASSED in 1.4s\r\n",
      "\r\n",
      "INFO: Build completed successfully, 2 total actions\r\n",
      "//tensorflow/python/data/util:traverse_test                              PASSED in 1.4s\r\n",
      "\r\n",
      "INFO: Build completed successfully, 2 total actions\r\n",
      "//tensorflow/python/data/util:nest_test                                  PASSED in 1.4s\r\n",
      "\r\n",
      "INFO: Build completed successfully, 2 total actions\r\n",
      "//tensorflow/python/data/util:options_test                               PASSED in 1.3s\r\n",
      "\r\n",
      "INFO: Build completed successfully, 2 total actions\r\n",
      "//tensorflow/python/data/util:structure_test                             PASSED in 1.7s\r\n",
      "\r\n",
      "```\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:data\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  ImportError: numpy.core._multiarray_umath failed to import ImportError: numpy.core.umath failed to import 2021-02-12 11:32:51.847320: F tensorflow/python/lib/core/bfloat16.cc:714] Check failed: PyBfloat16_Type.tp_base != nullptr  Aborted (core dumped)\n",
      "issue body -  <em>Please make sure that this is a bug. As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:bug_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 20.04\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:NA\r\n",
      "- TensorFlow installed from (source or binary):source\r\n",
      "- TensorFlow version (use command below):2.4.0\r\n",
      "- Python version:3.8.5\r\n",
      "- Bazel version (if compiling from source):4.0.0\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version:11.2/8.1.0\r\n",
      "- GPU model and memory:GTX1660 Ti\r\n",
      "\r\n",
      "You can collect some of this information using our environment capture\r\n",
      "[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n",
      "You can also obtain the TensorFlow version with:\r\n",
      "1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n",
      "2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n",
      "\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "Tensorflow not loading after installation.\r\n",
      "**Describe the expected behavior**\r\n",
      "`import tensorflow as tf`\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "Provide a reproducible test case that is the bare minimum necessary to generate\r\n",
      "the problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n",
      "`import tensorflow as tf\r\n",
      "2021-02-12 11:48:37.037515: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "RuntimeError: module compiled against API version 0xe but this version of numpy is 0xd\r\n",
      "RuntimeError: module compiled against API version 0xe but this version of numpy is 0xd\r\n",
      "ImportError: numpy.core._multiarray_umath failed to import\r\n",
      "ImportError: numpy.core.umath failed to import\r\n",
      "2021-02-12 11:48:37.272212: F tensorflow/python/lib/core/bfloat16.cc:714] Check failed: PyBfloat16_Type.tp_base != nullptr \r\n",
      "Aborted (core dumped)`\r\n",
      "\r\n",
      "**Other info / logs** Include any logs or source code that would be helpful to\r\n",
      "diagnose the problem. If including tracebacks, please include the full\r\n",
      "traceback. Large logs and files should be attached.\r\n",
      "\r\n",
      "\r\n",
      "`pip freeze\r\n",
      "absl-py==0.11.0\r\n",
      "anyio==2.1.0\r\n",
      "argon2-cffi==20.1.0\r\n",
      "astunparse==1.6.3\r\n",
      "async-generator==1.10\r\n",
      "attrs==20.3.0\r\n",
      "Babel==2.9.0\r\n",
      "backcall==0.2.0\r\n",
      "bleach==3.3.0\r\n",
      "cachetools==4.2.1\r\n",
      "certifi==2020.12.5\r\n",
      "cffi==1.14.5\r\n",
      "chardet==4.0.0\r\n",
      "cloudpickle==1.6.0\r\n",
      "cycler==0.10.0\r\n",
      "decorator==4.4.2\r\n",
      "defusedxml==0.6.0\r\n",
      "dm-tree==0.1.5\r\n",
      "entrypoints==0.3\r\n",
      "flatbuffers==1.12\r\n",
      "gast==0.3.3\r\n",
      "google-auth==1.26.1\r\n",
      "google-auth-oauthlib==0.4.2\r\n",
      "google-pasta==0.2.0\r\n",
      "grpcio==1.32.0\r\n",
      "h5py==2.10.0\r\n",
      "idna==2.10\r\n",
      "ipykernel==5.4.3\r\n",
      "ipython==7.20.0\r\n",
      "ipython-genutils==0.2.0\r\n",
      "ipywidgets==7.6.3\r\n",
      "jedi==0.18.0\r\n",
      "Jinja2==2.11.3\r\n",
      "joblib==1.0.1\r\n",
      "json5==0.9.5\r\n",
      "jsonschema==3.2.0\r\n",
      "jupyter==1.0.0\r\n",
      "jupyter-client==6.1.11\r\n",
      "jupyter-console==6.2.0\r\n",
      "jupyter-core==4.7.1\r\n",
      "jupyter-server==1.3.0\r\n",
      "jupyterlab==3.0.7\r\n",
      "jupyterlab-pygments==0.1.2\r\n",
      "jupyterlab-server==2.2.0\r\n",
      "jupyterlab-widgets==1.0.0\r\n",
      "Keras-Preprocessing==1.1.2\r\n",
      "kiwisolver==1.3.1\r\n",
      "Markdown==3.3.3\r\n",
      "MarkupSafe==1.1.1\r\n",
      "matplotlib==3.3.4\r\n",
      "mistune==0.8.4\r\n",
      "nbclassic==0.2.6\r\n",
      "nbclient==0.5.2\r\n",
      "nbconvert==6.0.7\r\n",
      "nbformat==5.1.2\r\n",
      "nest-asyncio==1.5.1\r\n",
      "notebook==6.2.0\r\n",
      "numpy==1.19.5\r\n",
      "oauthlib==3.1.0\r\n",
      "opt-einsum==3.3.0\r\n",
      "packaging==20.9\r\n",
      "pandas==1.2.2\r\n",
      "pandocfilters==1.4.3\r\n",
      "parso==0.8.1\r\n",
      "pexpect==4.8.0\r\n",
      "pickleshare==0.7.5\r\n",
      "Pillow==8.1.0\r\n",
      "prometheus-client==0.9.0\r\n",
      "prompt-toolkit==3.0.16\r\n",
      "protobuf==3.14.0\r\n",
      "ptyprocess==0.7.0\r\n",
      "pyasn1==0.4.8\r\n",
      "pyasn1-modules==0.2.8\r\n",
      "pycparser==2.20\r\n",
      "Pygments==2.7.4\r\n",
      "pyparsing==2.4.7\r\n",
      "pyrsistent==0.17.3\r\n",
      "python-dateutil==2.8.1\r\n",
      "pytz==2021.1\r\n",
      "pyzmq==22.0.3\r\n",
      "qtconsole==5.0.2\r\n",
      "QtPy==1.9.0\r\n",
      "requests==2.25.1\r\n",
      "requests-oauthlib==1.3.0\r\n",
      "rsa==4.7\r\n",
      "scikit-learn==0.24.1\r\n",
      "scipy==1.6.0\r\n",
      "Send2Trash==1.5.0\r\n",
      "six==1.15.0\r\n",
      "sklearn==0.0\r\n",
      "sniffio==1.2.0\r\n",
      "tensorboard==2.4.1\r\n",
      "tensorboard-plugin-wit==1.8.0\r\n",
      "tensorflow==2.4.0\r\n",
      "tensorflow-estimator==2.4.0\r\n",
      "tensorflow-probability==0.12.1\r\n",
      "termcolor==1.1.0\r\n",
      "terminado==0.9.2\r\n",
      "testpath==0.4.4\r\n",
      "threadpoolctl==2.1.0\r\n",
      "tornado==6.1\r\n",
      "traitlets==5.0.5\r\n",
      "typing-extensions==3.7.4.3\r\n",
      "urllib3==1.26.3\r\n",
      "wcwidth==0.2.5\r\n",
      "webencodings==0.5.1\r\n",
      "Werkzeug==1.0.1\r\n",
      "widgetsnbextension==3.5.1\r\n",
      "wrapt==1.12.1`\n",
      "issue labels - \n",
      "TF 2.4\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  What is the best architecture that we can converted to coreml and run on ios?\n",
      "issue body -  I would like to know which architecture achieve the best resaults?\r\n",
      "For example efficientdet or mobilenetv3.\r\n",
      "Also, the model from this architecture must be able to convert to coreml and run on iOS.\n",
      "issue labels - \n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  Refactor conv to share code between reference and optimized kernels\n",
      "issue body -  Summary:\r\n",
      "\r\n",
      "Move shared structs / helper functions into conv_common.cc\r\n",
      "Clean up some of the existing code to directly call the reference implementations (made possible by the refactor of the helper functions).\r\n",
      "\r\n",
      "Tested with:\r\n",
      "make -j8 -f tensorflow/lite/micro/tools/make/Makefile test_kernel_conv_test\r\n",
      "\r\n",
      "make -j8 -f tensorflow/lite/micro/tools/make/Makefile OPTIMIZED_KERNEL_DIR=cmsis_nn TARGET=stm32f4 test_kernel_conv_test\r\n",
      "\r\n",
      "make -f tensorflow/lite/micro/tools/make/Makefile -j8 TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=hifimini XTENSA_CORE=mini1m1m_RG test_kernel_conv_test\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:L\n",
      "\n",
      "\n",
      "issue title -  TF Lite benchmark NOT_EQUAL node takes up a lot of time.\n",
      "issue body -  Hi,\r\n",
      "\r\n",
      "Tensorflow used for both model and benchmark: 2.3.1\r\n",
      "Platform: Android\r\n",
      "\r\n",
      "I've got an internal company model (therefore can't share it, sorry). \r\n",
      "I've run tflite benchmark model on it and got weird results. Namely the NOT_EQUAL op apparently takes 73 ms to run, which is big part of computation time, the problem is that it does not block running other nodes (they run between the 1st ms and 38th ms of computation) therefore I'm not sure why benchmark reports that this node runs for 73ms if it doesn't actually block other nodes. \r\n",
      "\r\n",
      "Screen of the report:\r\n",
      "![perf_not_equal](https://user-images.githubusercontent.com/32575801/107762540-780ce900-6d2d-11eb-8c08-b30c3812f4cb.png)\r\n",
      "\r\n",
      "As you can see the cast, expand_dims etc. nodes don't start after 73ms but rather instantaneously which is confusing because when it comes to the total running time of model these 73ms are counted and added to the X ms of rest of the model, therefore it reports the whole model running time to be 73+x ms. \r\n",
      "\r\n",
      "When it comes to the node it only takes one of the inputs to the model as the input to the node, nothing else, therefore in theory it shouldn't wait for anything (I assume it waits for something if it starts but doesn't block computation).\r\n",
      "\r\n",
      "Could you please give me some help about what's going on here? Thanks. \r\n",
      "\n",
      "issue labels - \n",
      "TF 2.3\n",
      "comp:lite\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:performance\n",
      "\n",
      "\n",
      "issue title -  Make Tensorflow 2.3 available through conda on macOS\n",
      "issue body -  ### System information\r\n",
      "\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n",
      "- TensorFlow installed from (source or binary): conda\r\n",
      "- Python version: 3.7.9\r\n",
      "\r\n",
      "### Describe the problem\r\n",
      "\r\n",
      "`conda install tensorflow` installs version 2.0\r\n",
      "\r\n",
      "Please, make tensorflow 2.3 available through conda.\r\n",
      "\r\n",
      "Other platforms support 2.3, however macOS is still stuck at 2.0:\r\n",
      "https://anaconda.org/anaconda/tensorflow\r\n",
      "<img width=\"202\" alt=\"Screen Shot 2021-02-12 at 10 15 22\" src=\"https://user-images.githubusercontent.com/73581880/107749654-396e3300-6d1b-11eb-85be-02a0472dc40a.png\">\n",
      "issue labels - \n",
      "TF 2.3\n",
      "stat:awaiting response\n",
      "subtype:macOS\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  ConverterError: <unknown>:0: error: loc(callsite(callsite(\"map/TensorArrayV2_1@__inference_call_func_18494\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22648\") at \"StatefulPartitionedCall\")): failed to legalize operation 'tf.TensorListReserve' that was explicitly marked illegal <unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\n",
      "issue body -  [saved_model.zip](https://github.com/tensorflow/tensorflow/files/5970743/saved_model.zip)\r\n",
      "I am trying to convert this model to tflite. But getting above error. \r\n",
      "\n",
      "issue labels - \n",
      "Fixed in Nightly\n",
      "TF 2.4\n",
      "TFLiteConverter\n",
      "comp:lite\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  ValueError: Data cardinality is ambiguous:   x sizes: 60000   y sizes: 10000 Please provide data which shares the same first dimension.\n",
      "issue body -  \r\n",
      "![Screenshot (58)](https://user-images.githubusercontent.com/60438445/107751007-d0021a80-6d42-11eb-9326-ed9af5c643e3.png)\r\n",
      "![Screenshot (59)](https://user-images.githubusercontent.com/60438445/107751014-d395a180-6d42-11eb-802f-6c3313bf0999.png)\r\n",
      "![Screenshot (60)](https://user-images.githubusercontent.com/60438445/107750981-c37dc200-6d42-11eb-80a6-b4fffe5e445d.png)\r\n",
      "\n",
      "issue labels - \n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Data cardinality is ambiguous:   x sizes: 60000   y sizes: 10000 Please provide data which shares the same first dimension.\n",
      "issue body -  complete code here  [http://localhost:8888/notebooks/Desktop/minor%20proj/Untitled-Copy1.ipynb](url) \r\n",
      "\r\n",
      "working on mnist data set for handwriten recognition\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "comp:keras\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Snapshot Op Reader::MakeNestedDataset didn't handle the empty shard case properly (divide by 0 error)\n",
      "issue body -  Hi @frankchn, I'm using snapshot op for distributed training with parameter server strategy in which one of the workers didn't get data and produces snapshot folder with empty shard. However, for the next run I got an exception _Reader::MakeNestedDataset()_ as the shard was empty. I think the existing code was written with the assumption that shard won't be empty  (start_index % shard_dirs.size()) and caused divide by 0 error. Could you please confirm.\r\n",
      "\r\n",
      "https://github.com/tensorflow/tensorflow/blob/582c8d236cb079023657287c318ff26adb239002/tensorflow/core/kernels/data/experimental/snapshot_util.cc#L590\n",
      "issue labels - \n",
      "\n",
      "\n",
      "issue title -  Fix the bazel build as part of the github continuous integration.\n",
      "issue body -  To keep the bazel build short, we maintain a copy of the subset of packages that are needed for the TFLM (+ shared TfLite) bazel targets.\r\n",
      "\r\n",
      "Eigen was updated for TF with b26252b38334ab18dc7be2547c41e91e78257d60 and we make the corresponding change in TFLM's pared down version of workspace.bzl with this change.\r\n",
      "\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  NotFoundError:  Resource localhost/_AnonymousVar29/N10tensorflow22SummaryWriterInterfaceE does not exist.\n",
      "issue body -  tf 2.4.1\r\n",
      "```\r\n",
      "---------------------------------------------------------------------------\r\n",
      "NotFoundError                             Traceback (most recent call last)\r\n",
      "<ipython-input-10-92bfedd73a83> in <module>\r\n",
      "      2 tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, update_freq='batch')\r\n",
      "      3 \r\n",
      "----> 4 model.fit(x, y, initial_epoch=20, epochs=25, callbacks=[tensorboard_callback])\r\n",
      "\r\n",
      "~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n",
      "   1098                 _r=1):\r\n",
      "   1099               callbacks.on_train_batch_begin(step)\r\n",
      "-> 1100               tmp_logs = self.train_function(iterator)\r\n",
      "   1101               if data_handler.should_sync:\r\n",
      "   1102                 context.async_wait()\r\n",
      "\r\n",
      "~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n",
      "    826     tracing_count = self.experimental_get_tracing_count()\r\n",
      "    827     with trace.Trace(self._name) as tm:\r\n",
      "--> 828       result = self._call(*args, **kwds)\r\n",
      "    829       compiler = \"xla\" if self._experimental_compile else \"nonXla\"\r\n",
      "    830       new_tracing_count = self.experimental_get_tracing_count()\r\n",
      "\r\n",
      "~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)\r\n",
      "    853       # In this case we have created variables on the first call, so we run the\r\n",
      "    854       # defunned version which is guaranteed to never create variables.\r\n",
      "--> 855       return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\r\n",
      "    856     elif self._stateful_fn is not None:\r\n",
      "    857       # Release the lock early so that multiple threads can perform the call\r\n",
      "\r\n",
      "~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)\r\n",
      "   2940       (graph_function,\r\n",
      "   2941        filtered_flat_args) = self._maybe_define_function(args, kwargs)\r\n",
      "-> 2942     return graph_function._call_flat(\r\n",
      "   2943         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\r\n",
      "   2944 \r\n",
      "\r\n",
      "~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n",
      "   1916         and executing_eagerly):\r\n",
      "   1917       # No tape is watching; skip to running the function.\r\n",
      "-> 1918       return self._build_call_outputs(self._inference_function.call(\r\n",
      "   1919           ctx, args, cancellation_manager=cancellation_manager))\r\n",
      "   1920     forward_backward = self._select_forward_and_backward_functions(\r\n",
      "\r\n",
      "~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py in call(self, ctx, args, cancellation_manager)\r\n",
      "    553       with _InterpolateFunctionError(self):\r\n",
      "    554         if cancellation_manager is None:\r\n",
      "--> 555           outputs = execute.execute(\r\n",
      "    556               str(self.signature.name),\r\n",
      "    557               num_outputs=self._num_outputs,\r\n",
      "\r\n",
      "~/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n",
      "     57   try:\r\n",
      "     58     ctx.ensure_initialized()\r\n",
      "---> 59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n",
      "     60                                         inputs, attrs, num_outputs)\r\n",
      "     61   except core._NotOkStatusException as e:\r\n",
      "\r\n",
      "NotFoundError:  Resource localhost/_AnonymousVar29/N10tensorflow22SummaryWriterInterfaceE does not exist.\r\n",
      "\t [[{{node cond/then/_0/batch_loss}}]] [Op:__inference_train_function_9310]\r\n",
      "\r\n",
      "Function call stack:\r\n",
      "train_function\r\n",
      "```\n",
      "issue labels - \n",
      "TF 2.4\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Use xa_nnlib for svdf for Fusion F1.\n",
      "issue body -  The code in this change is the subset of functionality needed for int8 svdf for Hifi4 copied from https://github.com/pnikam-cad/tensorflow/blob/a737c1e3945bc70022259479ad24133a343ec906/tensorflow/lite/micro/kernels/xtensa_hifi/svdf.cc\r\n",
      "\r\n",
      "Note that the current change has not pulled in either the floating point implementation or the Hifi5 implementation.\r\n",
      "\r\n",
      "Profiled the keryword_benchmark with the following command:\r\n",
      "```\r\n",
      "make -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=fusion_f1 XTENSA_CORE=F1_190305_swupgrade run_keyword_benchmark -j8\r\n",
      "```\r\n",
      "\r\n",
      "gives a latency of 38516 ticks with this change vs 152642 ticks without this change.\r\n",
      "\r\n",
      "Per OP latency with this change:\r\n",
      "```\r\n",
      "KeywordRunNIerations(1) took 38516 ticks (38 ms)\r\n",
      "QUANTIZE took 3758 ticks (3 ms).\r\n",
      "SVDF took 4753 ticks (4 ms).\r\n",
      "FULLY_CONNECTED took 1353 ticks (1 ms).\r\n",
      "SVDF took 4211 ticks (4 ms).\r\n",
      "FULLY_CONNECTED took 1353 ticks (1 ms).\r\n",
      "SVDF took 3145 ticks (3 ms).\r\n",
      "FULLY_CONNECTED took 1353 ticks (1 ms).\r\n",
      "SVDF took 4211 ticks (4 ms).\r\n",
      "FULLY_CONNECTED took 1353 ticks (1 ms).\r\n",
      "SVDF took 2890 ticks (2 ms).\r\n",
      "SVDF took 3583 ticks (3 ms).\r\n",
      "SVDF took 3054 ticks (3 ms).\r\n",
      "FULLY_CONNECTED took 1091 ticks (1 ms).\r\n",
      "SOFTMAX took 2042 ticks (2 ms).\r\n",
      "QUANTIZE took 366 ticks (0 ms).\r\n",
      "```\r\n",
      "\r\n",
      "Without this change:\r\n",
      "```\r\n",
      "KeywordRunNIerations(1) took 152642 ticks (152 ms)\r\n",
      "QUANTIZE took 3758 ticks (3 ms).\r\n",
      "SVDF took 38003 ticks (38 ms).\r\n",
      "FULLY_CONNECTED took 1353 ticks (1 ms).\r\n",
      "SVDF took 18803 ticks (18 ms).\r\n",
      "FULLY_CONNECTED took 1353 ticks (1 ms).\r\n",
      "SVDF took 18803 ticks (18 ms).\r\n",
      "FULLY_CONNECTED took 1353 ticks (1 ms).\r\n",
      "SVDF took 18803 ticks (18 ms).\r\n",
      "FULLY_CONNECTED took 1353 ticks (1 ms).\r\n",
      "SVDF took 13907 ticks (13 ms).\r\n",
      "SVDF took 15827 ticks (15 ms).\r\n",
      "SVDF took 15827 ticks (15 ms).\r\n",
      "FULLY_CONNECTED took 1091 ticks (1 ms).\r\n",
      "SOFTMAX took 2042 ticks (2 ms).\r\n",
      "QUANTIZE took 366 ticks (0 ms).\r\n",
      "```\r\n",
      "\r\n",
      "Also confirmed that the kernel_svdf_test passes with:\r\n",
      "```\r\n",
      "make -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=fusion_f1 XTENSA_CORE=F1_190305_swupgrade test_kernel_svdf_test -j8\r\n",
      "```\r\n",
      "\r\n",
      "Progress towards http://b/177457688\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  post-quantizing and loading a a trained model in keras using tflite issue\n",
      "issue body -  **System information**\r\n",
      "- google colab ( intended to be used on raspberry pi)\r\n",
      "- TensorFlow 1.14\r\n",
      "- python 2.7 ( i know its old and not supported but i have to use it) \r\n",
      "-\r\n",
      "\r\n",
      "**Provide the text output from tflite_convert**\r\n",
      "```\r\n",
      "/usr/local/lib/python2.7/dist-packages/tensorflow_core/lite/python/convert.pyc in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n",
      "\r\n",
      "ConverterError: See console for info.\r\n",
      "2021-02-11 22:29:16.405546: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: AddV2\r\n",
      "2021-02-11 22:29:16.424912: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 57 operators, 100 arrays (0 quantized)\r\n",
      "2021-02-11 22:29:16.425598: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 57 operators, 100 arrays (0 quantized)\r\n",
      "2021-02-11 22:29:16.427571: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 25 operators, 53 arrays (0 quantized)\r\n",
      "2021-02-11 22:29:17.452115: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 24 operators, 52 arrays (0 quantized)\r\n",
      "2021-02-11 22:29:17.452530: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 3: 23 operators, 50 arrays (0 quantized)\r\n",
      "2021-02-11 22:29:17.452823: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 23 operators, 50 arrays (0 quantized)\r\n",
      "2021-02-11 22:29:17.453000: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 23 operators, 50 arrays (0 quantized)\r\n",
      "2021-02-11 22:29:17.453320: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 16777216 bytes, theoretical optimal value: 16777216 bytes.\r\n",
      "2021-02-11 22:29:17.453614: E tensorflow/lite/toco/toco_tooling.cc:456] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n",
      " and pasting the following:\r\n",
      "\r\n",
      "Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONV_2D, FULLY_CONNECTED, MAX_POOL_2D, MUL, SOFTMAX. Here is a list of operators for which you will need custom implementations: AddV2.\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/usr/local/bin/toco_from_protos\", line 8, in <module>\r\n",
      "    sys.exit(main())\r\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 59, in main\r\n",
      "    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n",
      "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/absl/app.py\", line 300, in run\r\n",
      "    _run_main(main, args)\r\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/absl/app.py\", line 251, in _run_main\r\n",
      "    sys.exit(main(argv))\r\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 33, in execute\r\n",
      "    output_str = tensorflow_wrap_toco.TocoConvert(model_str, toco_str, input_str)\r\n",
      "Exception: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n",
      " and pasting the following:\r\n",
      "\r\n",
      "Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONV_2D, FULLY_CONNECTED, MAX_POOL_2D, MUL, SOFTMAX. Here is a list of operators for which you will need custom implementations: AddV2.\r\n",
      "\r\n",
      "```\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue** \r\n",
      "\r\n",
      "# WHOLE MODEL\r\n",
      "import tensorflow as tf\r\n",
      "from tensorflow import lite\r\n",
      "from tensorflow import keras\r\n",
      "from keras.models import load_model\r\n",
      "import os\r\n",
      "\r\n",
      "\r\n",
      "model=keras.models.load_model('/content/drive/MyDrive/disease classification/AGAIN/end_model_acc1.h5')\r\n",
      "allow_custom_ops=True \r\n",
      "#--enable_select_tf_ops\r\n",
      "converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n",
      "\r\n",
      "tfmodel = converter.convert()\r\n",
      "open (\"model.tflite\" , \"wb\") .write(tfmodel)\r\n",
      "\r\n",
      "\r\n",
      "#loading quantized model and interprete it \r\n",
      "\r\n",
      "\r\n",
      "interpreter = tf.lite.Interpreter(model_content=tfmodel)\r\n",
      "interpreter.allocate_tensors()\r\n",
      "input_details = interpreter.get_input_details()\r\n",
      "output_details= interpreter.get_output_details()\r\n",
      "#interpreter.resize_tensor_input(input_details[0]['index'],(10, 224, 224, 3))\r\n",
      "#interpreter.resize_tensor_input(output_details[0]['index'], (15, 224,224,3))\r\n",
      "interpreter.allocate_tensors()\r\n",
      "os.chdir(\"/content/drive/MyDrive/disease classification/The End/\") \r\n",
      "!pwd\r\n",
      "\r\n",
      "\r\n",
      "**Any other info / logs**\r\n",
      "the model is used for image classification of 10 classes using CNN to be performed on the raspberry pi , i would appreciate any feedback regarding the process as i am new to this. thank you. \n",
      "issue labels - \n",
      "TF 1.14\n",
      "comp:lite\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Remove code and scripts related to xtensa_hifi\n",
      "issue body -  This work has now been consolidated in common locations (such as kernels/xtensa, xtensa_makefile.inc etc.)\r\n",
      "\r\n",
      "Addresses http://b/173043817\r\n",
      "\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:XL\n",
      "\n",
      "\n",
      "issue title -  *system info* \n",
      "issue body -  **System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n",
      "- TensorFlow installed from (source or binary):\r\n",
      "- TensorFlow version (or github SHA if from source):\r\n",
      "\r\n",
      "\r\n",
      "**Provide the text output from tflite_convert**\r\n",
      "\r\n",
      "```\r\n",
      "# Copy and paste here\r\n",
      "```\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue** \r\n",
      "Provide a reproducible test case that is the bare minimum necessary to generate\r\n",
      "the problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n",
      "\r\n",
      "Also, please include a link to a GraphDef or the model if possible.\r\n",
      "\r\n",
      "**Any other info / logs**\r\n",
      "\r\n",
      "Include any logs or source code that would be helpful to diagnose the problem.\r\n",
      "If including tracebacks, please include the full traceback. Large logs and files\r\n",
      "should be attached.\r\n",
      "\n",
      "issue labels - \n",
      "comp:lite\n",
      "invalid\n",
      "\n",
      "\n",
      "issue title -  Add an individual kernel test with Renode to the CI.\n",
      "issue body -  This will help prevent issues like #46186 and #45348\r\n",
      "\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Calling tf.linalg.expm causes SIGSEGV. \n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Home (19041.789)\r\n",
      "- TensorFlow installed from (source or binary): binary\r\n",
      "- TensorFlow version (use command below): 2.5.0-dev20201216\r\n",
      "- Python version: 3.7.7\r\n",
      "- CUDA/cuDNN version: 11.0.3/8.0.4.30\r\n",
      "- GPU model and memory: RTX 3070, 8GB\r\n",
      "\r\n",
      "Calling `tf.linalg.expm` causes the interpreter to exit. Attaching GDB reveals a SIGSEGV in thread 1. I have attached a demangled stack trace.\r\n",
      "Calling `tf.linalg.expm` should return a tensor with the result.\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "```\r\n",
      "import tensorflow as tf\r\n",
      "tf.linalg.expm(tf.constant([[1.0]]))\r\n",
      "```\r\n",
      "Passing larger matrices or the result of `tf.random.normal` doesn't change the behavior. \r\n",
      "\r\n",
      "[tf_stacktrace.txt](https://github.com/tensorflow/tensorflow/files/5967082/tf_stacktrace.txt)\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.5\n",
      "comp:ops\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Loading of shuffle buffer occurs every epoch drastically increasing training time using tf.datasets\n",
      "issue body -  <em>Please make sure that this is a bug. As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:bug_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "- TensorFlow installed from (source or binary): binary\r\n",
      "- TensorFlow version (use command below): 4.0.2\r\n",
      "- Python version: 3.6.9\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version: 8.0.2.39\r\n",
      "- GPU model and memory: 2x RTX 3090 w/24GB memory\r\n",
      "\r\n",
      "You can collect some of this information using our environment capture\r\n",
      "[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n",
      "You can also obtain the TensorFlow version with:\r\n",
      "1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n",
      "2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n",
      "\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "\r\n",
      "Hi all,\r\n",
      "\r\n",
      "Firstly, apologies if this is in the wrong place but I feel this is something that would be amazing to address if possible!\r\n",
      "I am training a model using images stored in a directory and creating datasets with the tf.keras.preprocessing.image_dataset_from_directory() method. Images are set as 50 x 50 pixels, greyscale.\r\n",
      "My training routine is lightning fast thanks to the RTX 3090 graphics card. However, there is substantial I/O bottlenecking when filling the shuffle buffer up. Is there a way to address this and make it faster?\r\n",
      "\r\n",
      "I can see the shuffle buffer being reloaded every single Epoch which takes significant time.\r\n",
      "\r\n",
      "Thanks!\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "\r\n",
      "Loading of the shuffle buffer only once to significantly decrease training time.\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "\r\n",
      "```\r\n",
      "\r\n",
      "## create data sets \r\n",
      "\r\n",
      "        train_ds = tf.keras.preprocessing.image_dataset_from_directory(\r\n",
      "          data_dir,\r\n",
      "          validation_split=validation_split,\r\n",
      "          subset=\"training\",\r\n",
      "          seed=seed,\r\n",
      "          image_size=(img_height, img_width),\r\n",
      "          batch_size=batch_size,\r\n",
      "          color_mode='grayscale'\r\n",
      "        )\r\n",
      "\r\n",
      "        val_ds = tf.keras.preprocessing.image_dataset_from_directory(\r\n",
      "          data_dir,\r\n",
      "          validation_split=validation_split,\r\n",
      "          subset=\"validation\",\r\n",
      "          seed=seed,\r\n",
      "          image_size=(img_height, img_width),\r\n",
      "          batch_size=batch_size,\r\n",
      "          color_mode='grayscale'\r\n",
      "        )\r\n",
      "\r\n",
      "## create and compile model \r\n",
      "model = get_compiled_model()\r\n",
      "\r\n",
      "## train network (call back simply saves epoch data if epoch result is better than previous)\r\n",
      "        history = model.fit(\r\n",
      "          train_ds,\r\n",
      "          validation_data=val_ds,\r\n",
      "          epochs=epochs,\r\n",
      "          initial_epoch=offset,\r\n",
      "          callbacks=callbacks,\r\n",
      "          steps_per_epoch=None,\r\n",
      "          validation_steps=None\r\n",
      "        )\r\n",
      "```\r\n",
      "\r\n",
      "**Other info / logs** Include any logs or source code that would be helpful to\r\n",
      "diagnose the problem. If including tracebacks, please include the full\r\n",
      "traceback. Large logs and files should be attached.\r\n",
      "\n",
      "issue labels - \n",
      "comp:data\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Switching to Cadence HiFi 4 NN Library v2.4.0\n",
      "issue body -  Using the latest version of HiFi 4 NN Library.\r\n",
      "This version has optimized implementation of SVDF and Quantize for int8 datatype.\r\n",
      "\r\n",
      "Tested the change using following commands:\r\n",
      "```\r\n",
      "make -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=hifi4 XTENSA_TOOLS_VERSION=RI-2020.5-linux XTENSA_CORE=AE_HiFi4_LE5_FP_XC clean_downloads\r\n",
      "make -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=hifi4 XTENSA_TOOLS_VERSION=RI-2020.5-linux XTENSA_CORE=AE_HiFi4_LE5_FP_XC test_kernel_fully_connected_test\r\n",
      "```\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  NotImplementedError: Cannot convert a symbolic Tensor to a numpy array.\n",
      "issue body -  <em>Please make sure that this is a bug. As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:bug_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "- TensorFlow installed from (source or binary): binary (pacman -S python-tensorflow)\r\n",
      "- TensorFlow version (use command below): 2.4.1\r\n",
      "- Python version: 3.9.1\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- GPU model and memory: Nvidia Geforce GTX 1060 6GB\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "```\r\n",
      "\r\n",
      "2021-02-10 17:51:13.037468: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n",
      "2021-02-10 17:51:13.037899: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\r\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n",
      "2021-02-10 17:51:13.038418: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/run/media/volker/DATA/configruns/load/./test.py\", line 13, in <module>\r\n",
      "    lstm = Bidirectional(lstm_nobi, name=\"layerC\")(embedding_layer)\r\n",
      "  File \"/usr/lib/python3.9/site-packages/tensorflow/python/keras/layers/wrappers.py\", line 539, in __call__\r\n",
      "    return super(Bidirectional, self).__call__(inputs, **kwargs)\r\n",
      "  File \"/usr/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 951, in __call__\r\n",
      "    return self._functional_construction_call(inputs, args, kwargs,\r\n",
      "  File \"/usr/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 1090, in _functional_construction_call\r\n",
      "    outputs = self._keras_tensor_symbolic_call(\r\n",
      "  File \"/usr/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 822, in _keras_tensor_symbolic_call\r\n",
      "    return self._infer_output_signature(inputs, args, kwargs, input_masks)\r\n",
      "  File \"/usr/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 863, in _infer_output_signature\r\n",
      "    outputs = call_fn(inputs, *args, **kwargs)\r\n",
      "  File \"/usr/lib/python3.9/site-packages/tensorflow/python/keras/layers/wrappers.py\", line 652, in call\r\n",
      "    y = self.forward_layer(forward_inputs,\r\n",
      "  File \"/usr/lib/python3.9/site-packages/tensorflow/python/keras/layers/recurrent.py\", line 660, in __call__\r\n",
      "    return super(RNN, self).__call__(inputs, **kwargs)\r\n",
      "  File \"/usr/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 1012, in __call__\r\n",
      "    outputs = call_fn(inputs, *args, **kwargs)\r\n",
      "  File \"/usr/lib/python3.9/site-packages/tensorflow/python/keras/layers/recurrent_v2.py\", line 1157, in call\r\n",
      "    inputs, initial_state, _ = self._process_inputs(inputs, initial_state, None)\r\n",
      "  File \"/usr/lib/python3.9/site-packages/tensorflow/python/keras/layers/recurrent.py\", line 859, in _process_inputs\r\n",
      "    initial_state = self.get_initial_state(inputs)\r\n",
      "  File \"/usr/lib/python3.9/site-packages/tensorflow/python/keras/layers/recurrent.py\", line 642, in get_initial_state\r\n",
      "    init_state = get_initial_state_fn(\r\n",
      "  File \"/usr/lib/python3.9/site-packages/tensorflow/python/keras/layers/recurrent.py\", line 2506, in get_initial_state\r\n",
      "    return list(_generate_zero_filled_state_for_cell(\r\n",
      "  File \"/usr/lib/python3.9/site-packages/tensorflow/python/keras/layers/recurrent.py\", line 2987, in _generate_zero_filled_state_for_cell\r\n",
      "    return _generate_zero_filled_state(batch_size, cell.state_size, dtype)\r\n",
      "  File \"/usr/lib/python3.9/site-packages/tensorflow/python/keras/layers/recurrent.py\", line 3003, in _generate_zero_filled_state\r\n",
      "    return nest.map_structure(create_zeros, state_size)\r\n",
      "  File \"/usr/lib/python3.9/site-packages/tensorflow/python/util/nest.py\", line 659, in map_structure\r\n",
      "    structure[0], [func(*x) for x in entries],\r\n",
      "  File \"/usr/lib/python3.9/site-packages/tensorflow/python/util/nest.py\", line 659, in <listcomp>\r\n",
      "    structure[0], [func(*x) for x in entries],\r\n",
      "  File \"/usr/lib/python3.9/site-packages/tensorflow/python/keras/layers/recurrent.py\", line 3000, in create_zeros\r\n",
      "    return array_ops.zeros(init_state_size, dtype=dtype)\r\n",
      "  File \"/usr/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\r\n",
      "    return target(*args, **kwargs)\r\n",
      "  File \"/usr/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py\", line 2819, in wrapped\r\n",
      "    tensor = fun(*args, **kwargs)\r\n",
      "  File \"/usr/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py\", line 2868, in zeros\r\n",
      "    output = _constant_if_small(zero, shape, dtype, name)\r\n",
      "  File \"/usr/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py\", line 2804, in _constant_if_small\r\n",
      "    if np.prod(shape) < 1000:\r\n",
      "  File \"<__array_function__ internals>\", line 5, in prod\r\n",
      "  File \"/home/volker/.local/lib/python3.9/site-packages/numpy/core/fromnumeric.py\", line 3030, in prod\r\n",
      "    return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out,\r\n",
      "  File \"/home/volker/.local/lib/python3.9/site-packages/numpy/core/fromnumeric.py\", line 87, in _wrapreduction\r\n",
      "    return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\r\n",
      "  File \"/usr/lib/python3.9/site-packages/tensorflow/python/framework/ops.py\", line 852, in __array__\r\n",
      "    raise NotImplementedError(\r\n",
      "NotImplementedError: Cannot convert a symbolic Tensor (layerC/forward_layerB/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported\r\n",
      "```\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "\r\n",
      "No NotImplementedError. [Here](https://stackoverflow.com/questions/66141547/notimplementederror-cannot-convert-a-symbolic-tensor-to-a-numpy-array?noredirect=1#comment116947834_66141547) someone claimed that with tf version 2.3.0, python 3.7.0, and numpy 1.19.2 it works. I also remember that code working about 10 months ago.\r\n",
      "\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "\r\n",
      "```\r\n",
      "import numpy as np\r\n",
      "from keras.layers import LSTM, Embedding, Input, Bidirectional\r\n",
      "\r\n",
      "dim = 30\r\n",
      "max_seq_length = 40\r\n",
      "vecs = np.random.rand(45,dim)\r\n",
      "\r\n",
      "input_layer = Input(shape=(max_seq_length,))\r\n",
      "embedding_layer = Embedding(len(vecs), dim, weights=[vecs], input_length=max_seq_length, trainable=False, name=\"layerA\")(input_layer)\r\n",
      "lstm_nobi = LSTM(max_seq_length, return_sequences=True, activation=\"linear\", name=\"layerB\")\r\n",
      "lstm = Bidirectional(lstm_nobi, name=\"layerC\")(embedding_layer)\r\n",
      "```\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:keras\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -   SavedModel restored in Graph mode under MirroredStrategy crashes global_variables_initializer\n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux: colab.research.google.com and others\r\n",
      "- TensorFlow installed from (source or binary): pre-installed on colab\r\n",
      "- TensorFlow version (use command below): 2.4.1\r\n",
      "- Python version: 3.6.9\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "\r\n",
      "1) `tf.compat.v1.global_variables_initializer()` crashes with `TypeError: Expected tf.group() expected Tensor arguments not 'None'` after restoring a SavedModel in Graph mode under MirroredStrategy as in the code below.\r\n",
      "\r\n",
      "2) Estimator does this kind of initialization automatically when used with `RunConfig(..., train_distribute=MirroredStrategy())`, so just fixing the example below won't help. See https://github.com/tensorflow/hub/issues/704 for user-level impact.\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "\r\n",
      "1) `tf.compat.v1.global_variables_initializer()` succeeds in this situation.\r\n",
      "\r\n",
      "2) An Estimator `model_fn` can use `tf.saved_model.load()` without this crash when used with `RunConfig(..., train_distribute=MirroredStrategy())`.\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "\r\n",
      "[Also shared with Alphabet at https://colab.research.google.com/drive/1piVum7WJb_WlYhHbW6tYO7gWSzJeT7oO ]\r\n",
      "\r\n",
      "```python\r\n",
      "import tensorflow as tf\r\n",
      "\r\n",
      "class Twice(tf.train.Checkpoint):\r\n",
      "  def __init__(self):\r\n",
      "    self._two = tf.Variable(2.0, name=\"two\")\r\n",
      "\r\n",
      "  @tf.function(input_signature=[tf.TensorSpec((None, None), tf.float32)])\r\n",
      "  def __call__(self, x):\r\n",
      "    return tf.multiply(x, self._two)\r\n",
      "\r\n",
      "export_dir = \"/tmp/twice\"\r\n",
      "tf.saved_model.save(Twice(), export_dir)\r\n",
      "\r\n",
      "strategy = tf.distribute.MirroredStrategy()\r\n",
      "with tf.Graph().as_default() as g:\r\n",
      "  with strategy.scope():\r\n",
      "    obj = tf.saved_model.load(export_dir)\r\n",
      "\r\n",
      "  for v in tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES):\r\n",
      "    prefix = \"NO INIT\" if v.initializer is None else \"ok init\"\r\n",
      "    print(prefix, v)\r\n",
      "\r\n",
      "  init_op = tf.compat.v1.global_variables_initializer()\r\n",
      "```\r\n",
      "\r\n",
      "```\r\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\r\n",
      "NO INIT <tf.Variable 'two:0' shape=() dtype=float32>\r\n",
      "ok init MirroredVariable:{\r\n",
      "  0: <tf.Variable 'two:0' shape=() dtype=float32>\r\n",
      "}\r\n",
      "---------------------------------------------------------------------------\r\n",
      "TypeError                                 Traceback (most recent call last)\r\n",
      "<ipython-input-4-7fd461f5dad0> in <module>()\r\n",
      "      8     print(prefix, v)\r\n",
      "      9 \r\n",
      "---> 10   init_op = tf.compat.v1.global_variables_initializer()\r\n",
      "\r\n",
      "2 frames\r\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py in group(*inputs, **kwargs)\r\n",
      "   2921       if not hasattr(inp, \"device\"):\r\n",
      "   2922         raise TypeError(\"Expected tf.group() expected Tensor arguments not \"\r\n",
      "-> 2923                         \"'%s' with type '%s'\" % (inp, type(inp)))\r\n",
      "   2924       dev = inp.device\r\n",
      "   2925       if dev in ops_on_device:\r\n",
      "\r\n",
      "TypeError: Expected tf.group() expected Tensor arguments not 'None' with type '<class 'NoneType'>'\r\n",
      "```\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:dist-strat\n",
      "stat:awaiting tensorflower\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Fix a bug where reordering tf.Transpose is not done correctly\n",
      "issue body -  When reordering tf.Transpose (downwards) and the original transpose must be preserved, the insertion point is off-by-one.\r\n",
      "\r\n",
      "As demonstrated in attached test, this scenario is not handled properly:\r\n",
      "\r\n",
      "```\r\n",
      "%0 = transpose(%input, %axis_t)\r\n",
      "%1 = pad(%0, %axis_p)\r\n",
      "return %0, %1\r\n",
      "```\r\n",
      "\r\n",
      "Will get `operand #0 does not dominate this use` error from IR verifier, indicating a value is used before its def.\r\n",
      "\r\n",
      "cc: @huangkang-chn\r\n",
      "\n",
      "issue labels - \n",
      "awaiting review\n",
      "cla: yes\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  TF Lite Micro person detection example won't make?\n",
      "issue body -  <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "- TensorFlow installed from (source or binary): Python pip and git clone\r\n",
      "- TensorFlow version: 2.4.1\r\n",
      "- Python version: 3.8\r\n",
      "- Installed using virtualenv? pip? conda?: pip\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version:\r\n",
      "- GPU model and memory:\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "I installed the whole Tensorflow package using both pip and git clone on Ubuntu 20.04. When I tried to make the person detection project as follows:\r\n",
      "make -f tensorflow/lite/micro/tools/make/Makefile TARGET=esp generate_person_detection_esp_project\r\n",
      "\r\n",
      "It errored out with no make rule. The whole log is:\r\n",
      "\r\n",
      "_terryl@DESKTOP-55BFST5:/mnt/c/Projects/tensorflow$ make -f tensorflow/lite/micro/tools/make/Makefile TARGET=esp generate_person_detection_esp_project\r\n",
      "--2021-02-10 22:34:31--  http://mirror.tensorflow.org/github.com/google/flatbuffers/archive/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip\r\n",
      "Resolving mirror.tensorflow.org (mirror.tensorflow.org)... 216.58.195.80\r\n",
      "Connecting to mirror.tensorflow.org (mirror.tensorflow.org)|216.58.195.80|:80... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 1760478 (1.7M) [application/zip]\r\n",
      "Saving to: ‘/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip’\r\n",
      "\r\n",
      "/tmp/dca12522a9f9e37f126ab925 100%[=================================================>]   1.68M  8.23MB/s    in 0.2s\r\n",
      "\r\n",
      "2021-02-10 22:34:32 (8.23 MB/s) - ‘/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip’ saved [1760478/1760478]\r\n",
      "\r\n",
      "Cloning into 'tensorflow/lite/micro/tools/make/downloads/pigweed'...\r\n",
      "remote: Sending approximately 12.05 MiB ...\r\n",
      "remote: Counting objects: 6, done\r\n",
      "remote: Finding sources: 100% (6/6)\r\n",
      "remote: Total 18665 (delta 8537), reused 18663 (delta 8537)\r\n",
      "Receiving objects: 100% (18665/18665), 11.99 MiB | 5.09 MiB/s, done.\r\n",
      "Resolving deltas: 100% (8537/8537), done.\r\n",
      "Updating files: 100% (1456/1456), done.\r\n",
      "Updating files: 100% (647/647), done.\r\n",
      "Note: switching to '47268dff45019863e20438ca3746c6c62df6ef09'.\r\n",
      "\r\n",
      "You are in 'detached HEAD' state. You can look around, make experimental\r\n",
      "changes and commit them, and you can discard any commits you make in this\r\n",
      "state without impacting any branches by switching back to a branch.\r\n",
      "\r\n",
      "If you want to create a new branch to retain commits you create, you may\r\n",
      "do so (now or later) by using -c with the switch command. Example:\r\n",
      "\r\n",
      "  git switch -c <new-branch-name>\r\n",
      "\r\n",
      "Or undo this operation with:\r\n",
      "\r\n",
      "  git switch -\r\n",
      "\r\n",
      "Turn off this advice by setting config variable advice.detachedHead to false\r\n",
      "\r\n",
      "HEAD is now at 47268dff pw_hdlc_lite: Client I/O improvements\r\n",
      "warning: pw_presubmit/py/pw_presubmit/format_code.py has type 100644, expected 100755\r\n",
      "warning: pw_presubmit/py/pw_presubmit/pigweed_presubmit.py has type 100644, expected 100755\r\n",
      "make: *** No rule to make target 'generate_person_detection_esp_project'.  Stop._\r\n",
      "\r\n",
      "I tried multiple times clean installing tensorflow but always the same problem. Am I missing something or the example project got broken somehow? I tried making hello_world project for ESP32, it worked fine.\r\n",
      "Please help, thanks!\r\n",
      "**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n",
      "\r\n",
      "\r\n",
      "**Any other info / logs**\r\n",
      "Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:micro\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  model.predict infer batch size from x if x is an array/tensor\n",
      "issue body -  If you use a subclassed tf keras model, and write the call method, you may want to access the shape of the `inputs` argument. While training, this is possible. The shape would be [batch_size, width, height, channels] for a batch of images using a tf data dataset. However, when calling `model.predict(images)`, where images is an array [batch_size, width, height, channels], the shape would be [None, width, height, channels]. This is confusing, though can be fixed by setting the batch_size argument of predict.\r\n",
      "\r\n",
      "I propose that when calling model.predict on tensors/arrays (not datasets), batch_size is inferred from the shape of the inputted array. I have implemented the code myself by overriding the predict function in a class extending tf.keras.Model like shown:\r\n",
      "\r\n",
      "```\r\n",
      "...\r\n",
      "def predict(self,\r\n",
      "                x,\r\n",
      "                batch_size=None,\r\n",
      "                verbose=0,\r\n",
      "                steps=None,\r\n",
      "                callbacks=None,\r\n",
      "                max_queue_size=10,\r\n",
      "                workers=1,\r\n",
      "                use_multiprocessing=False\r\n",
      "               ):\r\n",
      "         return super(Autoencoder, self).predict(x, (batch_size or x.shape[0]), verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\r\n",
      "...\r\n",
      "```\r\n",
      "\r\n",
      "I see no reason why the shape of `inputs` is not always known, so this seems like a reasonable request to me. I have not really checked, however, if `inputs.shape` is not set when not using a dataset to fit or evaluate, it would make sense that `inputs.shape` is populated there as well. It just doesn't make sense that the shape of `inputs` should ever have a None value in it when it is known.\n",
      "issue labels - \n",
      "comp:keras\n",
      "stat:awaiting tensorflower\n",
      "type:feature\n",
      "\n",
      "\n",
      "issue title -  micro: prepare to port operator ELU kernel from lite with test\n",
      "issue body -  Implement skeleton (non-working) code for operator and test.\r\n",
      "Header files changed.\r\n",
      "Namespaces changed.\r\n",
      "Some original code deleted.\r\n",
      "Some original code modified.\r\n",
      "\r\n",
      "PR step 4 of the work to port operator ELU as tracked in Issue #46323\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "prtype:bugfix\n",
      "ready to pull\n",
      "size:L\n",
      "\n",
      "\n",
      "issue title -  Add an InitializeTarget function that can be specialized for a given target.\n",
      "issue body -  This will allow the unit tests to be run on additional targets that need some addiitonal initialization (for example corstone_300 from #46830).\r\n",
      "\r\n",
      "This particular change is broken out from the Corstone PR #46830 to be able to have smaller more reviewable PRs.\r\n",
      "\r\n",
      "In the past, we have added state to the DebugLog() and GetCurrentTimeTicks() functions as a way to avoid having an InitializeTarget function. With this change, we are deciding to go with an explicit intitialization step instead.\r\n",
      "\r\n",
      "This change has added calls to tflite::InitializeTarget to the tests, benchmarks, and examples and converted the Arduino and SparkfunEdge to make use of this explicit initialization.\r\n",
      "\r\n",
      "The changes for the Arduino and SparkfunEdge have not been tested on actual hardware.\r\n",
      "\r\n",
      "Progress towards #46829\r\n",
      "Fixes http://b/150808076\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  Fix and rename TRANSPOSE_CONV_2D, BATCH_TO_SPACE_ND and SPACE_TO_BATCH_ND\n",
      "issue body -  Rename TRANSPOSE_CONV_2D to TRANSPOSE_CONV. Fix invariant checks in space_to_batch_nd and batch_to_space_nd.\r\n",
      "\r\n",
      "Fixes: http://b/179940765\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  Implement support for RaggedTensors in binary_crossentropy loss.\n",
      "issue body -  Follow up to #46283.\r\n",
      "\r\n",
      "Tagging @tomerk for review.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:keras\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  re-open: Docker Image -e Password:password tensorflow/tensorflow:latest-gpu-jupyter\n",
      "issue body -  What is the password for jupyter in this docker image (tensorflow/tensorflow:latest-gpu-jupyter)?\r\n",
      "-e Password:password is not working, it's not allow resetting the password.\r\n",
      "\r\n",
      "I am reopening this issue since\r\n",
      "the use of jupyter_notebook_config.py was removed in cae5763 and the PASSWORD env variable is now ignored.\r\n",
      "thank you.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<em>Please make sure that this is a bug. As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:bug_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "- TensorFlow installed from (source or binary):\r\n",
      "- TensorFlow version (use command below):\r\n",
      "- Python version:\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version:\r\n",
      "- GPU model and memory:\r\n",
      "\r\n",
      "You can collect some of this information using our environment capture\r\n",
      "[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n",
      "You can also obtain the TensorFlow version with:\r\n",
      "1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n",
      "2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n",
      "\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "Provide a reproducible test case that is the bare minimum necessary to generate\r\n",
      "the problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n",
      "\r\n",
      "**Other info / logs** Include any logs or source code that would be helpful to\r\n",
      "diagnose the problem. If including tracebacks, please include the full\r\n",
      "traceback. Large logs and files should be attached.\r\n",
      "\n",
      "issue labels - \n",
      "comp:gpu\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Reorder lite/micro/kernels/BUILD alphabetically\n",
      "issue body -  \n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:L\n",
      "\n",
      "\n",
      "issue title -  Add support for masking to the Keras Functional API\n",
      "issue body -  **System information**\r\n",
      "- TensorFlow version (you are using): 2.3.0\r\n",
      "- Are you willing to contribute it (Yes/No): Yes\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**Describe the feature and the current behavior/state.**\r\n",
      "\r\n",
      "It appears that the function API does not support masking, meaning it cannot be used to make a *layer* that propagates mask from the input to to the output. Here is an example:\r\n",
      "\r\n",
      "Suppose that `layer` is some `tf.keras` layer that supports masking. Then we can make a model using the function API:\r\n",
      "\r\n",
      "    inputs = tf.keras.layers.Input(shape=(None,))\r\n",
      "    outputs = layer(inputs)\r\n",
      "    functional_layer = tf.keras.Model(inputs=inputs, outputs=outputs)\r\n",
      "\r\n",
      "But it does not propogate the mask: If `x_emb` is a tensor with an attached mask (so it has an `x_emb._keras_mask` attribute) then consider the following\r\n",
      "\r\n",
      "    output = functional_layer(x_emb)\r\n",
      "    print(output._keras_layer)\r\n",
      "\r\n",
      "    ## AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute '_keras_layer'\r\n",
      "\r\n",
      "Obviously subclassing `tf.keras.Model` supports masking, and so does the Sequential API:\r\n",
      "\r\n",
      "    sequential_layer = tf.keras.Sequential([\r\n",
      "        layer,\r\n",
      "        layer\r\n",
      "    ])\r\n",
      "    output2 = sequential_layer(x_emb)\r\n",
      "    print(output2._keras_mask)\r\n",
      "\r\n",
      "**Will this change the current api? How?**\r\n",
      "\r\n",
      "It depends on how it is implemented. \r\n",
      "\r\n",
      "**Who will benefit with this feature?**\r\n",
      "\r\n",
      "Model developers. It will allow them to use the functional API to create layers which support masking.\r\n",
      "\r\n",
      "**Any Other info.**\r\n",
      "\r\n",
      "I posted a [question](https://stackoverflow.com/q/66092787/1349673) on SO about this.\n",
      "issue labels - \n",
      "comp:keras\n",
      "type:feature\n",
      "\n",
      "\n",
      "issue title -  calling GradientTape.gradient inside its context warning even though tape recording is stopped\n",
      "issue body -  <em>Please make sure that this is a bug. As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:bug_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "- TensorFlow installed from (source or binary): 2.3\r\n",
      "- TensorFlow version (use command below):\r\n",
      "- Python version:\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version:\r\n",
      "- GPU model and memory:\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "Running a function with a gradient tape that has a stopped recording, updating gradients, inside its context causes an unnecessary/misleading warning about performance.\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "There should be no warning about tons of extra memory & reduced performance.\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "https://colab.research.google.com/drive/1sasTq_SY7ylBETSQDmscIqYFkZpHDr9B?usp=sharing\r\n",
      "\r\n",
      "\r\n",
      "edit: Closing this. It's a context manager, and it wasn't used like one here.\n",
      "issue labels - \n",
      "TF 2.3\n",
      "comp:ops\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  tensorflow/cc/saved_model/README.md is broken\n",
      "issue body -  ## URL(s) with the issue:\r\n",
      "\r\n",
      "- https://github.com/tensorflow/tensorflow/blob/master/tensorflow/cc/saved_model/README.md\r\n",
      "- https://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/cc/saved_model/README.md\r\n",
      "\r\n",
      "## Description of issue (what needs changing):\r\n",
      "I would expect to see some documentation regarding the package. Instead, it says `<!--#include file=\"../../python/saved_model/README.md\"-->`.\r\n",
      "\n",
      "issue labels - \n",
      "comp:apis\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:docs-bug\n",
      "\n",
      "\n",
      "issue title -  Partially infer conv return types\n",
      "issue body -  Fixes #47057.\r\n",
      "\r\n",
      "af9ad9d introduces conv inferReturnTypes, but skips the inference when either input or filter does not have static shape (unranked or not all dimensions are static). However, it introduces the regression when it comes to partially dynamic input. This PR instead tries to infer as many dimensions as possible, which avoids shape information loss when there are only some dynamic dimensions (like batch dim).\n",
      "issue labels - \n",
      "cla: yes\n",
      "prtype:bugfix\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  Steps to build dynamic libraries of tflite for raspberry pi and aarch 64(armv8) boards\n",
      "issue body -  I have downloaded tensorflow source github and try to create tensorflow lite library from tools/ make makefiles.\r\n",
      "I have not found steps to build and generate dynamic tfLite library for these hardware devices. Could You provide me the steps to build dynamic .so for tfLite library\r\n",
      "\n",
      "issue labels - \n",
      "comp:lite\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  Conv2D output shape becomes fully dynamic when only input batch size is dynamic in tf-nightly\n",
      "issue body -  ### 1. System information\r\n",
      "\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n",
      "- TensorFlow installation (pip package or built from source): pip\r\n",
      "- TensorFlow library (version, if pip package or github SHA, if built from source): 2.4.1 v.s. nightly\r\n",
      "\r\n",
      "### 2. Code\r\n",
      "\r\n",
      "nightly: https://colab.research.google.com/drive/1r-vzh6JCIc5A4K8aYEbMZANWVrtSkKrF?usp=sharing\r\n",
      "2.4.1: https://colab.research.google.com/drive/1ODMv9joqc2ybsSdNHhGq1RWKXYsnFjd-?usp=sharing\r\n",
      "\r\n",
      "### 3. Failure after conversion\r\n",
      "\r\n",
      "Models are correct, but nightly has fully dynamic output shape and 2.4.1 has dynamic batch dim only.\r\n",
      "\r\n",
      "Nightly\r\n",
      "![image](https://user-images.githubusercontent.com/11615393/107465904-10ca1b80-6b18-11eb-9a96-9c7ba9254bcf.png)\r\n",
      "\r\n",
      "2.4.1\r\n",
      "![image](https://user-images.githubusercontent.com/11615393/107465944-1f183780-6b18-11eb-92ed-7befb0614e6b.png)\r\n",
      "\r\n",
      "1 here means dynamic dim (netron cannot show it precisely). It does not really matter in my use case though. Just a regression issue.\n",
      "issue labels - \n",
      "TF 2.4\n",
      "TFLiteConverter\n",
      "comp:lite\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Why is snappy compression output buffer size so small?\n",
      "issue body -  Hi @frankchn,\r\n",
      "The snappy compression buffer size is hard coded to 262144 bytes:\r\n",
      "https://github.com/tensorflow/tensorflow/blob/dec8e0b11f4f87693b67e125e67dfbc68d26c205/tensorflow/core/lib/io/snappy/snappy_compression_options.h#L30. This is a quite small number leading to high chance of hitting the error:\r\n",
      "https://github.com/tensorflow/tensorflow/blob/516ae286f6cc796e646d14671d94959b129130a4/tensorflow/core/lib/io/snappy/snappy_inputstream.cc#L113\r\n",
      "\r\n",
      "I wonder why it is set to thus small. \r\n",
      "\r\n",
      "Furthermore, `output_buffer_` is a fixed sized array https://github.com/tensorflow/tensorflow/blob/516ae286f6cc796e646d14671d94959b129130a4/tensorflow/core/lib/io/snappy/snappy_inputstream.h#L73\r\n",
      "\r\n",
      "Why can't it be a vector so that we don't need the `output_buffer_size` at all? I'm happy to submit a PR to make the according change if needed.\r\n",
      "\r\n",
      "Thanks.\r\n",
      "\n",
      "issue labels - \n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Added a run_ makefile target for non-test binaries.\n",
      "issue body -  Also, the x86 test behavior has changed:\r\n",
      "  * removed the unnecessary testing script for x86 with Make and bazel.\r\n",
      "  * This change means that we no longer need a special skylark rule for tflite_micro_cc_test and can instead directly use cc_test.\r\n",
      "  * All the logs from the test are now visible on the terminal (previously we would only see logs on errors, which can be annoying\r\n",
      "    for debugging)\r\n",
      "\r\n",
      "Tested that the following commands:\r\n",
      "```\r\n",
      "make -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=fusion_f1 XTENSA_CORE=F1_190305_swupgrade run_keyword_benchmark -j8\r\n",
      "make -f tensorflow/lite/micro/tools/make/Makefile run_keyword_benchmark -j8\r\n",
      "```\r\n",
      "execute without any error.\r\n",
      "\r\n",
      "The output is someting like:\r\n",
      "```\r\n",
      "InitializeKeywordRunner() took 39 ticks (0 ms)\r\n",
      "KeywordRunNIerations(1) took 42 ticks (0 ms)\r\n",
      "KeywordRunNIerations(10) took 187 ticks (0 ms)\r\n",
      "```\r\n",
      "\r\n",
      "Prior to this change, benchmarks would have to be run with\r\n",
      "`make test_keyword_benchmark` which would give a confusing output:\r\n",
      "\r\n",
      "```\r\n",
      "InitializeKeywordRunner() took 85 ticks (0 ms)\r\n",
      "KeywordRunNIerations(1) took 27 ticks (0 ms)\r\n",
      "KeywordRunNIerations(10) took 276 ticks (0 ms)\r\n",
      "make: *** [tensorflow/lite/micro/benchmarks/Makefile.inc:28: test_keyword_benchmark] Error 1\r\n",
      "```\r\n",
      "\r\n",
      "Fixes http://b/168123200\r\n",
      "Another relevant bug (for the asan failures): http://b/179930607\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "size:L\n",
      "\n",
      "\n",
      "issue title -  tf.keras.initializers.zeros causes model.save to fail, while tf.keras.initializers.Zeros() works great\n",
      "issue body -  [Here](https://www.tensorflow.org/api_docs/python/tf/keras/initializers/Zeros) it's written that `tf.keras.initializers.zeros` is a shortcut for `tf.keras.initializers.Zeros()`.\r\n",
      "If it is a shortcut, then both should work same\r\n",
      "\r\n",
      "\r\n",
      "While saving the model, if I use `tf.keras.initializers.zeros` , model save failes, but using `tf.keras.initializers.Zeros()` works great.\r\n",
      "\r\n",
      "Same issue raised by someone at [Stackoverflow](https://stackoverflow.com/questions/57154799/keras-model-saving-erroring-typeerror-get-config-missing-1-required-position) \r\n",
      "\r\n",
      "[Colab](https://colab.research.google.com/drive/1E0P-aBU9B7RO_QUtDrlRDPcOWqd6UfkD?usp=sharing#scrollTo=weBjeZAFJOP4)\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "comp:keras\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Weight becomes untrainable after multiplying with constant \n",
      "issue body -  ### System information\r\n",
      "\r\n",
      "-   **Have I written custom code (as opposed to using a stock example script\r\n",
      "    provided in TensorFlow)**: yes\r\n",
      "-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macos\r\n",
      "-   **TensorFlow installed from (source or binary)**:\r\n",
      "-   **TensorFlow version (use command below)**: tensorflow 2.4\r\n",
      "-   **Python version**: python 3.7\r\n",
      "\r\n",
      "### Describe the problem\r\n",
      "I want to mask the weights of my network (set them to zero and freeze them, i.e. no gradients). I tried creating a custom layer and multiply the weight matrices by a constant matrix, but this makes the weight matrices no longer trainable. Note that this approach used to work in tensorflow 1, and I can't figure out why it doesn't work anymore in tf2. I've attached an example in this [colab](https://colab.research.google.com/drive/1E1IdLuAFbQUKzrxoqN0XaZ79wb3au0nY?usp=sharing).\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:ops\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  java.lang.IllegalArgumentException: ByteBuffer is not a valid flatbuffer model\n",
      "issue body -  Please, in desperate need for help, has been trying to solve for 10 days. the tensorflow lite model I trained is here  [here](uhttps://drive.google.com/file/d/1fYay6FXNlAXi-migvGRtaeqNXB8gKZXk/view?usp=sharingrl). I ran python inference test and it worked. However, no way it is working on Android sample object detection app here https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android\r\n",
      "\r\n",
      "by debugging the issue this  long modelHandle = createModelWithBuffer(this.modelByteBuffer, errorHandle); specifically this part in  NativeInterpreterWrapper.class\r\n",
      " NativeInterpreterWrapper(ByteBuffer buffer, Options options) {\r\n",
      "        this.inferenceDurationNanoseconds = -1L;\r\n",
      "        this.isMemoryAllocated = false;\r\n",
      "        this.delegates = new ArrayList();\r\n",
      "        this.ownedDelegates = new ArrayList();\r\n",
      "        TensorFlowLite.init();\r\n",
      "        if (buffer != null && (buffer instanceof MappedByteBuffer || buffer.isDirect() && buffer.order() == ByteOrder.nativeOrder())) {\r\n",
      "            this.modelByteBuffer = buffer;\r\n",
      "            long errorHandle = createErrorReporter(512);\r\n",
      "            long modelHandle = createModelWithBuffer(this.modelByteBuffer, errorHandle);\r\n",
      "            this.init(errorHandle, modelHandle, options);\r\n",
      "        } else {\r\n",
      "            throw new IllegalArgumentException(\"Model ByteBuffer should be either a MappedByteBuffer of the model file, or a direct ByteBuffer using ByteOrder.nativeOrder() which contains bytes of model content.\");\r\n",
      "        }\r\n",
      "    }\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n",
      "- OS Platform MACOS sieera 10.13\r\n",
      "- Android studio 4\r\n",
      "- I have tried every possible solution and updated the NDK\r\n",
      "I used in the gradle\r\n",
      "\r\n",
      "buildscript {\r\n",
      "    repositories {\r\n",
      "        google()\r\n",
      "        jcenter()\r\n",
      "        mavenLocal()\r\n",
      "    }\r\n",
      "\r\n",
      "aaptOptions {\r\n",
      "        noCompress \"tflite\"\r\n",
      "        noCompress \"lite\"\r\n",
      "    }\r\n",
      "\r\n",
      "  implementation 'org.tensorflow:tensorflow-lite-metadata:0.0.0-nightly'\r\n",
      "    implementation 'org.tensorflow:tensorflow-lite-select-tf-ops:0.0.0-nightly'\r\n",
      "also tried\r\n",
      " implementation 'org.tensorflow:tensorflow-lite-metadata:0.1.2-nightly'  and no difference\r\n",
      "\r\n",
      "please help me I don't know if it is the file it self or the android libraries \n",
      "issue labels - \n",
      "comp:lite\n",
      "comp:lite-examples\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  ValueError: Cannot iterate over a shape with unknown rank\n",
      "issue body -  ### 1. System information\r\n",
      "\r\n",
      "- OS Platform and Distribution: Ubuntu 20.04\r\n",
      "- TensorFlow installation (pip package or built from source): pip (python 3.7)\r\n",
      "- TensorFlow library (version, if pip package or github SHA, if built from source): tensorflow-2.4.1\r\n",
      "\r\n",
      "### 2. Code\r\n",
      "\r\n",
      "Provide code to help us reproduce your issues using one of the following options:\r\n",
      "\r\n",
      "```python\r\n",
      "import sys\r\n",
      "from pathlib import Path\r\n",
      "\r\n",
      "import tensorflow as tf\r\n",
      "\r\n",
      "# Specify the model.\r\n",
      "saved_model_dir = Path('training/Model/admin/test2/1/exported-model/1/')\r\n",
      "\r\n",
      "if saved_model_dir.exists():\r\n",
      "    print(f'Converting model: {str(saved_model_dir)}')\r\n",
      "else:\r\n",
      "    print(f'Could not find model: {str(saved_model_dir)}')\r\n",
      "    sys.exit(1)\r\n",
      "\r\n",
      "# Convert the model.\r\n",
      "converter = tf.compat.v1.lite.TFLiteConverter.from_saved_model(str(saved_model_dir))\r\n",
      "tflite_model = converter.convert()\r\n",
      "\r\n",
      "# Save the model.\r\n",
      "with open('model.tflite', 'wb') as f:\r\n",
      "  f.write(tflite_model)\r\n",
      "\r\n",
      "print('Ready.')\r\n",
      "```\r\n",
      "\r\n",
      "### 3. Failure after conversion\r\n",
      "\r\n",
      "Error message:\r\n",
      "\r\n",
      "```\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"convert-to-tflite.py\", line 17, in <module>\r\n",
      "    tflite_model = converter.convert()\r\n",
      "  File \"/home/thijs/.virtualenvs/tflite/lib/python3.7/site-packages/tensorflow/lite/python/lite.py\", line 1947, in convert\r\n",
      "    return super(TFLiteConverter, self).convert()\r\n",
      "  File \"/home/thijs/.virtualenvs/tflite/lib/python3.7/site-packages/tensorflow/lite/python/lite.py\", line 1304, in convert\r\n",
      "    **converter_kwargs)\r\n",
      "  File \"/home/thijs/.virtualenvs/tflite/lib/python3.7/site-packages/tensorflow/lite/python/convert.py\", line 606, in toco_convert_impl\r\n",
      "    input_tensors, output_tensors, *args, **kwargs)\r\n",
      "  File \"/home/thijs/.virtualenvs/tflite/lib/python3.7/site-packages/tensorflow/lite/python/convert.py\", line 497, in build_toco_convert_protos\r\n",
      "    for dim in shape:\r\n",
      "  File \"/home/thijs/.virtualenvs/tflite/lib/python3.7/site-packages/tensorflow/python/framework/tensor_shape.py\", line 861, in __iter__\r\n",
      "    raise ValueError(\"Cannot iterate over a shape with unknown rank.\")\r\n",
      "ValueError: Cannot iterate over a shape with unknown rank.\r\n",
      "```\r\n",
      "\r\n",
      "### 4. Any other info / logs\r\n",
      "\r\n",
      "Startup log:\r\n",
      "\r\n",
      "```\r\n",
      "2021-02-09 20:09:36.646605: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/Qt/5.14.1/lib\r\n",
      "2021-02-09 20:09:36.646636: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n",
      "Converting model: training/Model/admin/test2/1/exported-model/1\r\n",
      "2021-02-09 20:09:38.125156: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n",
      "2021-02-09 20:09:38.125298: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/Qt/5.14.1/lib\r\n",
      "2021-02-09 20:09:38.125310: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\r\n",
      "2021-02-09 20:09:38.125331: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (snowblower): /proc/driver/nvidia/version does not exist\r\n",
      "2021-02-09 20:09:38.125557: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n",
      "2021-02-09 20:09:38.125891: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n",
      "WARNING:tensorflow:From /home/thijs/.virtualenvs/tflite/lib/python3.7/site-packages/tensorflow/lite/python/convert_saved_model.py:60: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\r\n",
      "2021-02-09 20:09:38.792020: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:196] None of the MLIR optimization passes are enabled (registered 0 passes)\r\n",
      "2021-02-09 20:09:38.858416: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3593310000 Hz\r\n",
      "2021-02-09 20:09:38.954802: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n",
      "2021-02-09 20:09:39.889489: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\r\n",
      "2021-02-09 20:09:39.889750: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n",
      "2021-02-09 20:09:39.890025: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n",
      "2021-02-09 20:09:39.953928: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:928] Optimization results for grappler item: graph_to_optimize\r\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.004ms.\r\n",
      "  function_optimizer: function_optimizer did nothing. time = 0ms.\r\n",
      "\r\n",
      "WARNING:tensorflow:From /home/thijs/.virtualenvs/tflite/lib/python3.7/site-packages/tensorflow/lite/python/util.py:327: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\r\n",
      "WARNING:tensorflow:From /home/thijs/.virtualenvs/tflite/lib/python3.7/site-packages/tensorflow/python/framework/convert_to_constants.py:856: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\r\n",
      "2021-02-09 20:09:40.561332: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n",
      "2021-02-09 20:09:41.338828: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n",
      "2021-02-09 20:09:42.358175: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\r\n",
      "2021-02-09 20:09:42.358313: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n",
      "2021-02-09 20:09:42.358532: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n",
      "2021-02-09 20:09:42.426773: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:928] Optimization results for grappler item: graph_to_optimize\r\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.004ms.\r\n",
      "  function_optimizer: function_optimizer did nothing. time = 0ms.\r\n",
      "```\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "TFLiteConverter\n",
      "comp:lite\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  \"no checkpoint found\" tensorboard projector\n",
      "issue body -  I want to load a custom embedding file and a metadata file into tensorboard's projector plugin. I've been following this tutorial mostly: https://www.tensorflow.org/tensorboard/tensorboard_projector_plugin. I get the \"no checkpoint found\" error in spite of the checkpoint files being present in the folder. I also have used tensorboard's inspect function on the log directory and tensorboard can't find any event files. Here's my code:\r\n",
      "\r\n",
      "`import os\r\n",
      "from tensorboard.plugins import projector\r\n",
      "import pandas as pd\r\n",
      "import tensorflow as tf\r\n",
      "\r\n",
      "weights = pd.read_csv (\"test/vectors_test.tsv\", sep = '\\t')\r\n",
      "\r\n",
      "log_dir = \"test/logs/\"\r\n",
      "weights = tf.Variable(weights)\r\n",
      "checkpoint = tf.train.Checkpoint(embedding=weights)\r\n",
      "checkpoint.save(os.path.join(log_dir, \"embedding.ckpt\"))\r\n",
      "\r\n",
      "# Set up config\r\n",
      "config = projector.ProjectorConfig()\r\n",
      "config.model_checkpoint_path = os.path.join(log_dir, \"embedding.ckpt\")\r\n",
      "embedding = config.embeddings.add()\r\n",
      "embedding.tensor_name = \"embedding/.ATTRIBUTES/VARIABLE_VALUE\"\r\n",
      "embedding.metadata_path = \"test/metadata_test.tsv\"\r\n",
      "projector.visualize_embeddings(log_dir, config)\r\n",
      "\r\n",
      "%load_ext tensorboard\r\n",
      "\r\n",
      "%tensorboard --logdir test/logs/ --host localhost --port=1003\r\n",
      "%tensorboard --inspect --logdir test/logs\r\n",
      "`\n",
      "issue labels - \n",
      "comp:tensorboard\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Deprecate s3 file system\n",
      "issue body -  This PR is part of the effort to switch to modular file system support\r\n",
      "by deprecates s3 file system and direct user to use modular file system\r\n",
      "from tensorflow-io instead.\r\n",
      "\r\n",
      "A `TF_ENABLE_LEGACY_FILESYSTEM=1` will allow the usage of legacy hdfs file\r\n",
      "system the same way as before (a warning will be displayed).\r\n",
      "\r\n",
      "/cc @mihaimaruseac @vnvo2409  @tensorflow/sig-io-maintainers @burgerkingeater \r\n",
      "\r\n",
      "Signed-off-by: Yong Tang <yong.tang.github@outlook.com>\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:core\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Support RaggedTensors in categorical_crossentropy.\n",
      "issue body -  Follow up to #46283.\r\n",
      "\r\n",
      "Tagging @tomerk for review.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:keras\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  Error when Building Tensorflow 2.4/2.4.1 for C++ with cuda support\n",
      "issue body -  \r\n",
      "**System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win10\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n",
      "- TensorFlow installed from (source or binary): source\r\n",
      "- TensorFlow version: 2.4.0 / 2.4.1\r\n",
      "- Python version: 3.8.6\r\n",
      "- Installed using virtualenv? pip? conda?: no\r\n",
      "- Bazel version (if compiling from source): 3.7.2\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version:11.0 /8.0.5.39\r\n",
      "- GPU model and memory: Nvidia GTX 1070\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "\r\n",
      "When building tenmsorflow for C++ with gpu support the build failed with the following error:\r\n",
      "![grafik](https://user-images.githubusercontent.com/78606066/107390473-0730bb80-6af8-11eb-8454-3b651682eb16.png)\r\n",
      "\r\n",
      "\r\n",
      "**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n",
      "\r\n",
      "I first cloned the repository and switched to branch r2.4\r\n",
      "\r\n",
      "after that i configured with configure.py.\r\n",
      "\r\n",
      "![grafik](https://user-images.githubusercontent.com/78606066/107390779-537bfb80-6af8-11eb-8f62-bc4ca3d7b297.png)\r\n",
      "\r\n",
      "then I started the build with bazel:\r\n",
      "\r\n",
      "`bazel build --config=opt --config=cuda tensorflow:tensorflow.dll`\r\n",
      "\r\n",
      "It ended up with the error above.\r\n",
      "\r\n",
      "**Any other info / logs**\r\n",
      "\r\n",
      "\r\n",
      "Building without cuda support works just fine\r\n",
      "\r\n",
      "I dont know where the error is maybe its me maybe Tensorflow. I hope i can get some Help here. It would be great if the Tensorflow Site would have a proper instruction. I had to make a long journey to get to the point of where i am here with building. \n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:runtime\n",
      "stat:awaiting response\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  change tensorfloew file\n",
      "issue body -  this \n",
      "issue labels - \n",
      "cla: no\n",
      "invalid\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  AttributeError: module 'tensorflow' has no attribute 'contrib' -->> alterative\n",
      "issue body -  **I want alternative for the following code in TF2**\r\n",
      "\r\n",
      "tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\r\n",
      "\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.0\n",
      "contrib\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Failing to Cross-Compile TensorFlow Lite C++ for Raspberry Pi\n",
      "issue body -  \r\n",
      "**System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.2 LTS\r\n",
      "- TensorFlow installed from (source or binary): Attempting to build from source as per [here](https://www.tensorflow.org/lite/guide/build_rpi#cross-compile_for_raspberry_pi_with_make)\r\n",
      "- TensorFlow version: TensorFlow Lite\r\n",
      "- Python version: Python 3.8.5\r\n",
      "- Installed using virtualenv? pip? conda?: No\r\n",
      "- GCC/Compiler version (if compiling from source): 9.3.0\r\n",
      "\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "Attempting to [cross-compile for Raspberry Pi with Make](https://www.tensorflow.org/lite/guide/build_rpi#cross-compile_for_raspberry_pi_with_make) for a Raspberry Pi 4. I tried building on my machine first, but that failed so I tried the Docker-based installation. This failed with the same error.\r\n",
      "\r\n",
      "I haven't been able to find this in the issue tracker, appologies if I've missed something there. Many thanks in advance!\r\n",
      "\r\n",
      "**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n",
      "\r\n",
      "Inside the Docker `tensorflow/tensorflow:devel` container:\r\n",
      "\r\n",
      "```sh\r\n",
      "# Step 1\r\n",
      "git clone https://github.com/raspberrypi/tools.git rpi_tools\r\n",
      "\r\n",
      "# Step 3 (step 2 skipped as per instructions)\r\n",
      "cd tensorflow_src && ./tensorflow/lite/tools/make/download_dependencies.sh\r\n",
      "\r\n",
      "# Step 4a\r\n",
      "PATH=../rpi_tools/arm-bcm2708/arm-rpi-4.9.3-linux-gnueabihf/bin:$PATH \\\r\n",
      "  ./tensorflow/lite/tools/make/build_rpi_lib.sh\r\n",
      "```\r\n",
      "\r\n",
      "The final step failed with the log indicated below\r\n",
      "\r\n",
      "**Any other info / logs**\r\n",
      "It looks like the error might be:\r\n",
      "```\r\n",
      "/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:388:17: error: 'is_trivially_copyable' is not a member of 'std'\r\n",
      "   static_assert(std::is_trivially_copyable<MulParamsType>::value, \"\");\r\n",
      "                 ^\r\n",
      "/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:388:57: error: expected primary-expression before '>' token\r\n",
      "   static_assert(std::is_trivially_copyable<MulParamsType>::value, \"\");\r\n",
      "                                                         ^\r\n",
      "/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:388:58: error: '::value' has not been declared\r\n",
      "   static_assert(std::is_trivially_copyable<MulParamsType>::value, \"\");\r\n",
      "```\r\n",
      "The full log (including the above extract) is:\r\n",
      "\r\n",
      "<details>\r\n",
      "<summary>Click here for full log</summary>\r\n",
      "\r\n",
      "```\r\n",
      "+ set -e\r\n",
      "+++ dirname ./tensorflow/lite/tools/make/build_rpi_lib.sh\r\n",
      "++ cd ./tensorflow/lite/tools/make\r\n",
      "++ pwd\r\n",
      "+ SCRIPT_DIR=/tensorflow_src/tensorflow/lite/tools/make\r\n",
      "+ TENSORFLOW_DIR=/tensorflow_src/tensorflow/lite/tools/make/../../../..\r\n",
      "++ free -m\r\n",
      "++ awk '/^Mem/ {print $2}'\r\n",
      "+ FREE_MEM=15803\r\n",
      "+ [[ FREE_MEM -gt 2000 ]]\r\n",
      "+ NO_JOB=4\r\n",
      "+ make -j 4 TARGET=rpi -C /tensorflow_src/tensorflow/lite/tools/make/../../../.. -f tensorflow/lite/tools/make/Makefile\r\n",
      "make: Entering directory '/tensorflow_src'\r\n",
      "tensorflow/lite/tools/make/Makefile:358: warning: overriding recipe for target '/tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/lib/libtensorflow-lite.a'\r\n",
      "tensorflow/lite/tools/make/Makefile:355: warning: ignoring old recipe for target '/tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/lib/libtensorflow-lite.a'\r\n",
      "arm-linux-gnueabihf-g++ -O3 -DNDEBUG -DCPU_SETSIZE=__CPU_SETSIZE -fPIC  --std=c++11  -DTFLITE_WITHOUT_XNNPACK -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/deps/clog/include -I -I/usr/local/include -c tensorflow/lite/allocation.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/allocation.o\r\n",
      "arm-linux-gnueabihf-g++ -O3 -DNDEBUG -DCPU_SETSIZE=__CPU_SETSIZE -fPIC  --std=c++11  -DTFLITE_WITHOUT_XNNPACK -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/deps/clog/include -I -I/usr/local/include -c tensorflow/lite/arena_planner.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/arena_planner.o\r\n",
      "arm-linux-gnueabihf-g++ -O3 -DNDEBUG -DCPU_SETSIZE=__CPU_SETSIZE -fPIC  --std=c++11  -DTFLITE_WITHOUT_XNNPACK -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/deps/clog/include -I -I/usr/local/include -c tensorflow/lite/c/c_api.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/c/c_api.o\r\n",
      "arm-linux-gnueabihf-g++ -O3 -DNDEBUG -DCPU_SETSIZE=__CPU_SETSIZE -fPIC  --std=c++11  -DTFLITE_WITHOUT_XNNPACK -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/deps/clog/include -I -I/usr/local/include -c tensorflow/lite/c/c_api_experimental.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/c/c_api_experimental.o\r\n",
      "arm-linux-gnueabihf-gcc -O3 -DNDEBUG -DCPU_SETSIZE=__CPU_SETSIZE -fPIC  -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/deps/clog/include -I -I/usr/local/include -c tensorflow/lite/c/common.c -o /tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/c/common.o\r\n",
      "arm-linux-gnueabihf-g++ -O3 -DNDEBUG -DCPU_SETSIZE=__CPU_SETSIZE -fPIC  --std=c++11  -DTFLITE_WITHOUT_XNNPACK -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/deps/clog/include -I -I/usr/local/include -c tensorflow/lite/core/api/error_reporter.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/core/api/error_reporter.o\r\n",
      "arm-linux-gnueabihf-g++ -O3 -DNDEBUG -DCPU_SETSIZE=__CPU_SETSIZE -fPIC  --std=c++11  -DTFLITE_WITHOUT_XNNPACK -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/deps/clog/include -I -I/usr/local/include -c tensorflow/lite/core/api/flatbuffer_conversions.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/core/api/flatbuffer_conversions.o\r\n",
      "arm-linux-gnueabihf-g++ -O3 -DNDEBUG -DCPU_SETSIZE=__CPU_SETSIZE -fPIC  --std=c++11  -DTFLITE_WITHOUT_XNNPACK -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/deps/clog/include -I -I/usr/local/include -c tensorflow/lite/core/api/op_resolver.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/core/api/op_resolver.o\r\n",
      "arm-linux-gnueabihf-g++ -O3 -DNDEBUG -DCPU_SETSIZE=__CPU_SETSIZE -fPIC  --std=c++11  -DTFLITE_WITHOUT_XNNPACK -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/deps/clog/include -I -I/usr/local/include -c tensorflow/lite/core/api/tensor_utils.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/core/api/tensor_utils.o\r\n",
      "arm-linux-gnueabihf-g++ -O3 -DNDEBUG -DCPU_SETSIZE=__CPU_SETSIZE -fPIC  --std=c++11  -DTFLITE_WITHOUT_XNNPACK -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/deps/clog/include -I -I/usr/local/include -c tensorflow/lite/core/subgraph.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/core/subgraph.o\r\n",
      "arm-linux-gnueabihf-g++ -O3 -DNDEBUG -DCPU_SETSIZE=__CPU_SETSIZE -fPIC  --std=c++11  -DTFLITE_WITHOUT_XNNPACK -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/deps/clog/include -I -I/usr/local/include -c tensorflow/lite/create_op_resolver_with_builtin_ops.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/create_op_resolver_with_builtin_ops.o\r\n",
      "arm-linux-gnueabihf-g++ -O3 -DNDEBUG -DCPU_SETSIZE=__CPU_SETSIZE -fPIC  --std=c++11  -DTFLITE_WITHOUT_XNNPACK -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/deps/clog/include -I -I/usr/local/include -c tensorflow/lite/delegates/interpreter_utils.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/delegates/interpreter_utils.o\r\n",
      "arm-linux-gnueabihf-g++ -O3 -DNDEBUG -DCPU_SETSIZE=__CPU_SETSIZE -fPIC  --std=c++11  -DTFLITE_WITHOUT_XNNPACK -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/deps/clog/include -I -I/usr/local/include -c tensorflow/lite/experimental/resource/resource_variable.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/experimental/resource/resource_variable.o\r\n",
      "arm-linux-gnueabihf-g++ -O3 -DNDEBUG -DCPU_SETSIZE=__CPU_SETSIZE -fPIC  --std=c++11  -DTFLITE_WITHOUT_XNNPACK -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/deps/clog/include -I -I/usr/local/include -c tensorflow/lite/experimental/resource/static_hashtable.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/experimental/resource/static_hashtable.o\r\n",
      "arm-linux-gnueabihf-g++ -O3 -DNDEBUG -DCPU_SETSIZE=__CPU_SETSIZE -fPIC  --std=c++11  -DTFLITE_WITHOUT_XNNPACK -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/deps/clog/include -I -I/usr/local/include -c tensorflow/lite/external_cpu_backend_context.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/external_cpu_backend_context.o\r\n",
      "arm-linux-gnueabihf-g++ -O3 -DNDEBUG -DCPU_SETSIZE=__CPU_SETSIZE -fPIC  --std=c++11  -DTFLITE_WITHOUT_XNNPACK -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/deps/clog/include -I -I/usr/local/include -c tensorflow/lite/graph_info.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/graph_info.o\r\n",
      "arm-linux-gnueabihf-g++ -O3 -DNDEBUG -DCPU_SETSIZE=__CPU_SETSIZE -fPIC  --std=c++11  -DTFLITE_WITHOUT_XNNPACK -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/deps/clog/include -I -I/usr/local/include -c tensorflow/lite/interpreter.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/interpreter.o\r\n",
      "arm-linux-gnueabihf-g++ -O3 -DNDEBUG -DCPU_SETSIZE=__CPU_SETSIZE -fPIC  --std=c++11  -DTFLITE_WITHOUT_XNNPACK -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/deps/clog/include -I -I/usr/local/include -c tensorflow/lite/interpreter_builder.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/interpreter_builder.o\r\n",
      "arm-linux-gnueabihf-g++ -O3 -DNDEBUG -DCPU_SETSIZE=__CPU_SETSIZE -fPIC  --std=c++11  -DTFLITE_WITHOUT_XNNPACK -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/deps/clog/include -I -I/usr/local/include -c tensorflow/lite/kernels/activations.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/kernels/activations.o\r\n",
      "arm-linux-gnueabihf-g++ -O3 -DNDEBUG -DCPU_SETSIZE=__CPU_SETSIZE -fPIC  --std=c++11  -DTFLITE_WITHOUT_XNNPACK -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/deps/clog/include -I -I/usr/local/include -c tensorflow/lite/kernels/add.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/kernels/add.o\r\n",
      "arm-linux-gnueabihf-g++ -O3 -DNDEBUG -DCPU_SETSIZE=__CPU_SETSIZE -fPIC  --std=c++11  -DTFLITE_WITHOUT_XNNPACK -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/deps/clog/include -I -I/usr/local/include -c tensorflow/lite/kernels/add_n.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/kernels/add_n.o\r\n",
      "In file included from ./tensorflow/lite/kernels/cpu_backend_gemm.h:23:0,\r\n",
      "                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:45,\r\n",
      "                 from tensorflow/lite/kernels/activations.cc:29:\r\n",
      "./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h: In static member function 'static void tflite::cpu_backend_gemm::detail::CustomGemvImpl<LhsScalar, RhsScalar, int, DstScalar, quantization_flavor>::Run(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<int, DstScalar, quantization_flavor>&, int, int)':\r\n",
      "./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:490:13: warning: attributes at the beginning of statement are ignored [-Wattributes]\r\n",
      "             [[clang::fallthrough]];\r\n",
      "             ^\r\n",
      "./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:493:13: warning: attributes at the beginning of statement are ignored [-Wattributes]\r\n",
      "             [[clang::fallthrough]];\r\n",
      "             ^\r\n",
      "./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:496:13: warning: attributes at the beginning of statement are ignored [-Wattributes]\r\n",
      "             [[clang::fallthrough]];\r\n",
      "             ^\r\n",
      "./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:499:13: warning: attributes at the beginning of statement are ignored [-Wattributes]\r\n",
      "             [[clang::fallthrough]];\r\n",
      "             ^\r\n",
      "./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:502:13: warning: attributes at the beginning of statement are ignored [-Wattributes]\r\n",
      "             [[clang::fallthrough]];\r\n",
      "             ^\r\n",
      "./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:505:13: warning: attributes at the beginning of statement are ignored [-Wattributes]\r\n",
      "             [[clang::fallthrough]];\r\n",
      "             ^\r\n",
      "In file included from /tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:30:0,\r\n",
      "                 from /tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:23,\r\n",
      "                 from ./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:21,\r\n",
      "                 from ./tensorflow/lite/kernels/cpu_backend_gemm.h:25,\r\n",
      "                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:45,\r\n",
      "                 from tensorflow/lite/kernels/activations.cc:29:\r\n",
      "/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h: In function 'void ruy::detail::FinalizeMulParams(const ruy::MulParams<AccumScalar, DstScalar>&, ruy::ChannelDimension, ruy::Ctx*, ruy::TrMulParams*)':\r\n",
      "/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:388:17: error: 'is_trivially_copyable' is not a member of 'std'\r\n",
      "   static_assert(std::is_trivially_copyable<MulParamsType>::value, \"\");\r\n",
      "                 ^\r\n",
      "/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:388:57: error: expected primary-expression before '>' token\r\n",
      "   static_assert(std::is_trivially_copyable<MulParamsType>::value, \"\");\r\n",
      "                                                         ^\r\n",
      "/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:388:58: error: '::value' has not been declared\r\n",
      "   static_assert(std::is_trivially_copyable<MulParamsType>::value, \"\");\r\n",
      "                                                          ^\r\n",
      "In file included from ./tensorflow/lite/kernels/cpu_backend_gemm.h:23:0,\r\n",
      "                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:45,\r\n",
      "                 from ./tensorflow/lite/kernels/internal/optimized/integer_ops/add.h:26,\r\n",
      "                 from tensorflow/lite/kernels/add.cc:15:\r\n",
      "./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h: In static member function 'static void tflite::cpu_backend_gemm::detail::CustomGemvImpl<LhsScalar, RhsScalar, int, DstScalar, quantization_flavor>::Run(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<int, DstScalar, quantization_flavor>&, int, int)':\r\n",
      "./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:490:13: warning: attributes at the beginning of statement are ignored [-Wattributes]\r\n",
      "             [[clang::fallthrough]];\r\n",
      "             ^\r\n",
      "./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:493:13: warning: attributes at the beginning of statement are ignored [-Wattributes]\r\n",
      "             [[clang::fallthrough]];\r\n",
      "             ^\r\n",
      "./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:496:13: warning: attributes at the beginning of statement are ignored [-Wattributes]\r\n",
      "             [[clang::fallthrough]];\r\n",
      "             ^\r\n",
      "./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:499:13: warning: attributes at the beginning of statement are ignored [-Wattributes]\r\n",
      "             [[clang::fallthrough]];\r\n",
      "             ^\r\n",
      "./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:502:13: warning: attributes at the beginning of statement are ignored [-Wattributes]\r\n",
      "             [[clang::fallthrough]];\r\n",
      "             ^\r\n",
      "./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:505:13: warning: attributes at the beginning of statement are ignored [-Wattributes]\r\n",
      "             [[clang::fallthrough]];\r\n",
      "             ^\r\n",
      "In file included from /tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:30:0,\r\n",
      "                 from /tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:23,\r\n",
      "                 from ./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:21,\r\n",
      "                 from ./tensorflow/lite/kernels/cpu_backend_gemm.h:25,\r\n",
      "                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:45,\r\n",
      "                 from ./tensorflow/lite/kernels/internal/optimized/integer_ops/add.h:26,\r\n",
      "                 from tensorflow/lite/kernels/add.cc:15:\r\n",
      "/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h: In function 'void ruy::detail::FinalizeMulParams(const ruy::MulParams<AccumScalar, DstScalar>&, ruy::ChannelDimension, ruy::Ctx*, ruy::TrMulParams*)':\r\n",
      "/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:388:17: error: 'is_trivially_copyable' is not a member of 'std'\r\n",
      "   static_assert(std::is_trivially_copyable<MulParamsType>::value, \"\");\r\n",
      "                 ^\r\n",
      "/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:388:57: error: expected primary-expression before '>' token\r\n",
      "   static_assert(std::is_trivially_copyable<MulParamsType>::value, \"\");\r\n",
      "                                                         ^\r\n",
      "/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:388:58: error: '::value' has not been declared\r\n",
      "   static_assert(std::is_trivially_copyable<MulParamsType>::value, \"\");\r\n",
      "                                                          ^\r\n",
      "arm-linux-gnueabihf-g++ -O3 -DNDEBUG -DCPU_SETSIZE=__CPU_SETSIZE -fPIC  --std=c++11  -DTFLITE_WITHOUT_XNNPACK -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/deps/clog/include -I -I/usr/local/include -c tensorflow/lite/kernels/arg_min_max.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/kernels/arg_min_max.o\r\n",
      "arm-linux-gnueabihf-g++ -O3 -DNDEBUG -DCPU_SETSIZE=__CPU_SETSIZE -fPIC  --std=c++11  -DTFLITE_WITHOUT_XNNPACK -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/deps/clog/include -I -I/usr/local/include -c tensorflow/lite/kernels/assign_variable.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/kernels/assign_variable.o\r\n",
      "arm-linux-gnueabihf-g++ -O3 -DNDEBUG -DCPU_SETSIZE=__CPU_SETSIZE -fPIC  --std=c++11  -DTFLITE_WITHOUT_XNNPACK -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/deps/clog/include -I -I/usr/local/include -c tensorflow/lite/kernels/audio_spectrogram.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/kernels/audio_spectrogram.o\r\n",
      "In file included from ./tensorflow/lite/kernels/cpu_backend_gemm.h:23:0,\r\n",
      "                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:45,\r\n",
      "                 from tensorflow/lite/kernels/arg_min_max.cc:23:\r\n",
      "./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h: In static member function 'static void tflite::cpu_backend_gemm::detail::CustomGemvImpl<LhsScalar, RhsScalar, int, DstScalar, quantization_flavor>::Run(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<int, DstScalar, quantization_flavor>&, int, int)':\r\n",
      "./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:490:13: warning: attributes at the beginning of statement are ignored [-Wattributes]\r\n",
      "             [[clang::fallthrough]];\r\n",
      "             ^\r\n",
      "./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:493:13: warning: attributes at the beginning of statement are ignored [-Wattributes]\r\n",
      "             [[clang::fallthrough]];\r\n",
      "             ^\r\n",
      "./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:496:13: warning: attributes at the beginning of statement are ignored [-Wattributes]\r\n",
      "             [[clang::fallthrough]];\r\n",
      "             ^\r\n",
      "./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:499:13: warning: attributes at the beginning of statement are ignored [-Wattributes]\r\n",
      "             [[clang::fallthrough]];\r\n",
      "             ^\r\n",
      "./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:502:13: warning: attributes at the beginning of statement are ignored [-Wattributes]\r\n",
      "             [[clang::fallthrough]];\r\n",
      "             ^\r\n",
      "./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:505:13: warning: attributes at the beginning of statement are ignored [-Wattributes]\r\n",
      "             [[clang::fallthrough]];\r\n",
      "             ^\r\n",
      "In file included from /tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:30:0,\r\n",
      "                 from /tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:23,\r\n",
      "                 from ./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:21,\r\n",
      "                 from ./tensorflow/lite/kernels/cpu_backend_gemm.h:25,\r\n",
      "                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:45,\r\n",
      "                 from tensorflow/lite/kernels/arg_min_max.cc:23:\r\n",
      "/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h: In function 'void ruy::detail::FinalizeMulParams(const ruy::MulParams<AccumScalar, DstScalar>&, ruy::ChannelDimension, ruy::Ctx*, ruy::TrMulParams*)':\r\n",
      "/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:388:17: error: 'is_trivially_copyable' is not a member of 'std'\r\n",
      "   static_assert(std::is_trivially_copyable<MulParamsType>::value, \"\");\r\n",
      "                 ^\r\n",
      "/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:388:57: error: expected primary-expression before '>' token\r\n",
      "   static_assert(std::is_trivially_copyable<MulParamsType>::value, \"\");\r\n",
      "                                                         ^\r\n",
      "/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:388:58: error: '::value' has not been declared\r\n",
      "   static_assert(std::is_trivially_copyable<MulParamsType>::value, \"\");\r\n",
      "                                                          ^\r\n",
      "In file included from ./tensorflow/lite/kernels/cpu_backend_gemm.h:23:0,\r\n",
      "                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:45,\r\n",
      "                 from tensorflow/lite/kernels/audio_spectrogram.cc:24:\r\n",
      "./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h: In static member function 'static void tflite::cpu_backend_gemm::detail::CustomGemvImpl<LhsScalar, RhsScalar, int, DstScalar, quantization_flavor>::Run(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<int, DstScalar, quantization_flavor>&, int, int)':\r\n",
      "./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:490:13: warning: attributes at the beginning of statement are ignored [-Wattributes]\r\n",
      "             [[clang::fallthrough]];\r\n",
      "             ^\r\n",
      "./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:493:13: warning: attributes at the beginning of statement are ignored [-Wattributes]\r\n",
      "             [[clang::fallthrough]];\r\n",
      "             ^\r\n",
      "./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:496:13: warning: attributes at the beginning of statement are ignored [-Wattributes]\r\n",
      "             [[clang::fallthrough]];\r\n",
      "             ^\r\n",
      "./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:499:13: warning: attributes at the beginning of statement are ignored [-Wattributes]\r\n",
      "             [[clang::fallthrough]];\r\n",
      "             ^\r\n",
      "./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:502:13: warning: attributes at the beginning of statement are ignored [-Wattributes]\r\n",
      "             [[clang::fallthrough]];\r\n",
      "             ^\r\n",
      "./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:505:13: warning: attributes at the beginning of statement are ignored [-Wattributes]\r\n",
      "             [[clang::fallthrough]];\r\n",
      "             ^\r\n",
      "In file included from /tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:30:0,\r\n",
      "                 from /tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:23,\r\n",
      "                 from ./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:21,\r\n",
      "                 from ./tensorflow/lite/kernels/cpu_backend_gemm.h:25,\r\n",
      "                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:45,\r\n",
      "                 from tensorflow/lite/kernels/audio_spectrogram.cc:24:\r\n",
      "/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h: In function 'void ruy::detail::FinalizeMulParams(const ruy::MulParams<AccumScalar, DstScalar>&, ruy::ChannelDimension, ruy::Ctx*, ruy::TrMulParams*)':\r\n",
      "/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:388:17: error: 'is_trivially_copyable' is not a member of 'std'\r\n",
      "   static_assert(std::is_trivially_copyable<MulParamsType>::value, \"\");\r\n",
      "                 ^\r\n",
      "/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:388:57: error: expected primary-expression before '>' token\r\n",
      "   static_assert(std::is_trivially_copyable<MulParamsType>::value, \"\");\r\n",
      "                                                         ^\r\n",
      "/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:388:58: error: '::value' has not been declared\r\n",
      "   static_assert(std::is_trivially_copyable<MulParamsType>::value, \"\");\r\n",
      "                                                          ^\r\n",
      "tensorflow/lite/tools/make/Makefile:334: recipe for target '/tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/kernels/add.o' failed\r\n",
      "make: *** [/tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/kernels/add.o] Error 1\r\n",
      "make: *** Waiting for unfinished jobs....\r\n",
      "tensorflow/lite/tools/make/Makefile:334: recipe for target '/tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/kernels/activations.o' failed\r\n",
      "make: *** [/tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/kernels/activations.o] Error 1\r\n",
      "tensorflow/lite/tools/make/Makefile:334: recipe for target '/tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/kernels/arg_min_max.o' failed\r\n",
      "make: *** [/tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/kernels/arg_min_max.o] Error 1\r\n",
      "tensorflow/lite/tools/make/Makefile:334: recipe for target '/tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/kernels/audio_spectrogram.o' failed\r\n",
      "make: *** [/tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/kernels/audio_spectrogram.o] Error 1\r\n",
      "make: Leaving directory '/tensorflow_src'\r\n",
      "```\r\n",
      "</details>\n",
      "issue labels - \n",
      "comp:lite\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  gradient computation fails in graph mode with numpy_function\n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "- TensorFlow installed from (source or binary): binary (conda-forge)\r\n",
      "- TensorFlow version (use command below): 'v2.3.0-rc2-23-gb36436b087', 'v2.4.0-49-g85c8b2a817f'\r\n",
      "- Python version: 3.7.7\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version: 10.1\r\n",
      "- GPU model and memory: Titan RTX 24GB\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "\r\n",
      "I'm trying to use a function implemented in cython in a loss function. The (int64) output of this function is used as index for tf.gather. When computing the loss in eager execution mode gradients are computed fine, but when run in graph mode, an exception is raised.\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "\r\n",
      "I would expect the same result in graph execution mode.\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "\r\n",
      "A tiny sample where the cython function simply implements argmin:\r\n",
      "\r\n",
      "```\r\n",
      "import tensorflow as tf\r\n",
      "import numpy as np\r\n",
      "import assign\r\n",
      "\r\n",
      "model = tf.keras.applications.ResNet50(False, weights=None, input_shape=(224, 224, 3))\r\n",
      "data = tf.random.normal((8,224,224,3), 128, 64)\r\n",
      "\r\n",
      "def f(x):\r\n",
      "\ty = model(x)\r\n",
      "\tidx = tf.numpy_function(assign.assign_objects_to_cells, (-y,), np.int64)\r\n",
      "\t#idx = tf.argmin(-y, -1)\r\n",
      "\t#idx = tf.numpy_function(np.argmin, (-y, -1), np.int64)\r\n",
      "\tvalues = tf.gather(y, idx, batch_dims=3)\r\n",
      "\treturn tf.reduce_sum(values)\r\n",
      "\r\n",
      "\r\n",
      "def grad(x):\r\n",
      "\twith tf.GradientTape() as tape:\r\n",
      "\t\ty = f(x)\r\n",
      "\treturn tape.gradient(y, model.trainable_variables)\r\n",
      "tf_grad = tf.function(grad)\r\n",
      "\r\n",
      "print('eager execution')\r\n",
      "g = grad(data)\r\n",
      "print('graph execution')\r\n",
      "g = tf_grad(data)\r\n",
      "```\r\n",
      "\r\n",
      "And the **assign.pyx**:\r\n",
      "\r\n",
      "```\r\n",
      "import numpy as np\r\n",
      "cimport numpy as np\r\n",
      "cimport cython\r\n",
      "\r\n",
      "@cython.boundscheck(False) # turn off bounds-checking for entire function\r\n",
      "@cython.wraparound(False)  # turn off negative index wrapping for entire function\r\n",
      "cdef void _assign_objects_to_cells(float[:,:,:,:] error, np.int64_t[:,:,:] argmax) nogil:\r\n",
      "\tcdef int N = error.shape[0], H = error.shape[1], W = error.shape[2], C = error.shape[3]\r\n",
      "\tcdef int y, x, c, best_id\r\n",
      "\tcdef float best, e\r\n",
      "\t\r\n",
      "\tfor n in range(N):\r\n",
      "\t\tfor y in range(H):\r\n",
      "\t\t\tfor x in range(W):\r\n",
      "\t\t\t\tbest_id = 0\r\n",
      "\t\t\t\tbest = error[n,y,x,0]\r\n",
      "\t\t\t\tfor c in range(1, C):\r\n",
      "\t\t\t\t\te = error[n,y,x,c]\r\n",
      "\t\t\t\t\tif e<best:\r\n",
      "\t\t\t\t\t\tbest = e\r\n",
      "\t\t\t\t\t\tbest_id = c\r\n",
      "\t\t\t\targmax[n,y,x] = best_id\r\n",
      "\r\n",
      "def assign_objects_to_cells(np.ndarray[np.float32_t, ndim=4] error not None):\r\n",
      "\tcdef int N = error.shape[0], H = error.shape[1], W = error.shape[2], C = error.shape[3]\r\n",
      "\tout = np.empty((N,H,W), np.int64)\r\n",
      "\t_assign_objects_to_cells(error, out)\r\n",
      "\t\r\n",
      "\treturn out\r\n",
      "```\r\n",
      "\r\n",
      "**Other info / logs** \r\n",
      "\r\n",
      "Eager execution runs fine, but graph execution fails:\r\n",
      "\r\n",
      "```\r\n",
      "eager execution\r\n",
      "graph execution\r\n",
      "Traceback (most recent call last):\r\n",
      "\r\n",
      "  File \"<ipython-input-1-eafa53e19dc9>\", line 25, in <module>\r\n",
      "    g = tf_grad(data)\r\n",
      "\r\n",
      "  File \"d:\\conda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 780, in __call__\r\n",
      "    result = self._call(*args, **kwds)\r\n",
      "\r\n",
      "  File \"d:\\conda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 823, in _call\r\n",
      "    self._initialize(args, kwds, add_initializers_to=initializers)\r\n",
      "\r\n",
      "  File \"d:\\conda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 697, in _initialize\r\n",
      "    *args, **kwds))\r\n",
      "\r\n",
      "  File \"d:\\conda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2855, in _get_concrete_function_internal_garbage_collected\r\n",
      "    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n",
      "\r\n",
      "  File \"d:\\conda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 3213, in _maybe_define_function\r\n",
      "    graph_function = self._create_graph_function(args, kwargs)\r\n",
      "\r\n",
      "  File \"d:\\conda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 3075, in _create_graph_function\r\n",
      "    capture_by_value=self._capture_by_value),\r\n",
      "\r\n",
      "  File \"d:\\conda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\", line 986, in func_graph_from_py_func\r\n",
      "    func_outputs = python_func(*func_args, **func_kwargs)\r\n",
      "\r\n",
      "  File \"d:\\conda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 600, in wrapped_fn\r\n",
      "    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n",
      "\r\n",
      "  File \"d:\\conda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\", line 973, in wrapper\r\n",
      "    raise e.ag_error_metadata.to_exception(e)\r\n",
      "\r\n",
      "TypeError: in user code:\r\n",
      "\r\n",
      "    <ipython-input-1-eafa53e19dc9>:19 grad  *\r\n",
      "        return tape.gradient(y, model.trainable_variables)\r\n",
      "    d:\\conda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py:1073 gradient  **\r\n",
      "        unconnected_gradients=unconnected_gradients)\r\n",
      "    d:\\conda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py:77 imperative_grad\r\n",
      "        compat.as_str(unconnected_gradients.value))\r\n",
      "    d:\\conda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py:162 _gradient_function\r\n",
      "        return grad_fn(mock_op, *out_grads)\r\n",
      "    d:\\conda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\ops\\array_grad.py:678 _GatherV2Grad\r\n",
      "        batch_dims, params_shape[axis])\r\n",
      "    d:\\conda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\ops\\array_grad.py:603 _BatchGatherGrad\r\n",
      "        indices = _GetBatchIndices(params_shape, indices, batch_dims)\r\n",
      "    d:\\conda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\ops\\array_grad.py:582 _GetBatchIndices\r\n",
      "        [1] * (dim - 1) + [dim_value] + [1] * (indices_ndims - dim), axis=0)\r\n",
      "\r\n",
      "    TypeError: unsupported operand type(s) for -: 'NoneType' and 'int'\r\n",
      "```\r\n",
      "\r\n",
      "Note, that both eager and graph modes work if I replace tf.numpy_function with tf.argmin and the results are the same (up to rounding errors). Graph execution also fails when using np.argmin.\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:autograph\n",
      "comp:ops\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  update tensorflow\n",
      "issue body -  tensorflow-update\n",
      "issue labels - \n",
      "cla: no\n",
      "invalid\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Merge pull request #1 from tensorflow/master\n",
      "issue body -  tensorflow-update\n",
      "issue labels - \n",
      "cla: no\n",
      "invalid\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "issue body -  I am using\r\n",
      "bert-tensorflow==1.0.1\r\n",
      "tensorflow version = 2.4.1\r\n",
      "keras - 2.4.3\r\n",
      "\r\n",
      "Code\r\n",
      "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1\"\r\n",
      "\r\n",
      "def create_tokenizer_from_hub_module():\r\n",
      "\"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\r\n",
      "with tf.Graph().as_default():\r\n",
      "bert_module = hub.Module(BERT_MODEL_HUB)\r\n",
      "tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\r\n",
      "with tf.Session() as sess:\r\n",
      "vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\r\n",
      "tokenization_info[\"do_lower_case\"]])\r\n",
      "\r\n",
      "return bert.tokenization.FullTokenizer(\r\n",
      "vocab_file=vocab_file, do_lower_case=do_lower_case)\r\n",
      "\r\n",
      "tokenizer = create_tokenizer_from_hub_module()\r\n",
      "\r\n",
      "After running above cell in Colab I'm getting the following msg\r\n",
      "\r\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\r\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "issue labels - \n",
      "TF 2.4\n",
      "stat:awaiting response\n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  unable to run tensorflow - error \n",
      "issue body -  <em>Please make sure that this is a bug. As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:bug_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "- TensorFlow installed from (source or binary):\r\n",
      "- TensorFlow version (use command below):\r\n",
      "- Python version:\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version:\r\n",
      "- GPU model and memory:\r\n",
      "\r\n",
      "You can collect some of this information using our environment capture\r\n",
      "[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n",
      "You can also obtain the TensorFlow version with:\r\n",
      "1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n",
      "2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n",
      "\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "Provide a reproducible test case that is the bare minimum necessary to generate\r\n",
      "the problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n",
      "\r\n",
      "**Other info / logs** Include any logs or source code that would be helpful to\r\n",
      "diagnose the problem. If including tracebacks, please include the full\r\n",
      "traceback. Large logs and files should be attached.\r\n",
      "\n",
      "issue labels - \n",
      "invalid\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  post_training_integer_quantization issue\n",
      "issue body -  I used a simple transfer learning code on my windows computer, downloading the cat-dog dataset.\r\n",
      "The transfer learning and conversion to TFLite sections are OK, but I can't convert the input to int8 from float 32 following TF guidline.\r\n",
      "\r\n",
      "**My Code;**\r\n",
      "https://colab.research.google.com/drive/1hlTykr4-rUYqI-n10icVEywxUhyar364\r\n",
      "\r\n",
      "**The Guide to do the conversion;**\r\n",
      "https://www.tensorflow.org/lite/performance/post_training_integer_quant\r\n",
      "\r\n",
      "**Issue**; ValueError: Unbatching a dataset is only supported for rank >= 1\r\n",
      "\r\n",
      "Thanks in advance for your help,\r\n",
      "MoZen\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:data\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  TPU, model.fit : 2GB of RAM limit. tf.version 2.4\n",
      "issue body -  **Describe the current behavior**\r\n",
      "\r\n",
      "There is an error (\"\"Session crashed for unknown reason\"\"), when trying to use a dataset with the size more than 2GB of RAM.\r\n",
      "Error is reproducible for different step size (256,1024,8192).\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "\r\n",
      "No error when feeding a dataset which is more than 2GB of RAM.\r\n",
      "\r\n",
      "**Code to reproduce the issue**\r\n",
      "\r\n",
      "To reproduce, just copy the following code to Colab with TPU enabled.\r\n",
      "\r\n",
      "The bug can happen due to 2GB limit in protobuf (since tensorflow relies on it).\r\n",
      "https://stackoverflow.com/questions/34128872/google-protobuf-maximum-size\r\n",
      "\r\n",
      "```\r\n",
      "import tensorflow as tf\r\n",
      "import numpy as np\r\n",
      "\r\n",
      "import distutils\r\n",
      "if distutils.version.LooseVersion(tf.__version__) < '1.14':\r\n",
      "    raise Exception('This notebook is compatible with TensorFlow 1.14 or higher, for TensorFlow 1.13 or lower please use the previous version at https://github.com/tensorflow/tpu/blob/r1.13/tools/colab/fashion_mnist.ipynb')\r\n",
      "\r\n",
      "import os\r\n",
      "\r\n",
      "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\r\n",
      "tf.config.experimental_connect_to_cluster(resolver)\r\n",
      "# This is the TPU initialization code that has to be at the beginning.\r\n",
      "tf.tpu.experimental.initialize_tpu_system(resolver)\r\n",
      "print(\"All devices: \", tf.config.list_logical_devices('TPU'))\r\n",
      "\r\n",
      "# optimizer = tf.tpu.CrossShardOptimizer(tf.train.GradientDescentOptimizer(0.01))\r\n",
      "\r\n",
      "strategy = tf.distribute.TPUStrategy(resolver)\r\n",
      "\r\n",
      "with strategy.scope():\r\n",
      "  model = tf.keras.applications.VGG16(input_shape=(32,32,3),classes=10, weights=None)#  create_model()\r\n",
      "  optimizer = tf.keras.optimizers.Adam()\r\n",
      "  model.compile(\r\n",
      "      optimizer=optimizer,\r\n",
      "      loss='mse',\r\n",
      "      metrics=['mse'])\r\n",
      "  \r\n",
      "  training_loss = tf.keras.metrics.Mean('training_loss', dtype=tf.float32)\r\n",
      "  training_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\r\n",
      "      'training_accuracy', dtype=tf.float32)\r\n",
      "\r\n",
      "\r\n",
      "# 1024*192 * 32*32*3 * 4bytes = 2 415 919 104 bytes\r\n",
      "# ~ 2GB. \"Session crashed for unknown reason\"\r\n",
      "X = np.zeros((1024*192, 32,32,3),dtype=np.float32)\r\n",
      "y = np.ones((1024*192, 10),dtype=np.float32)\r\n",
      "\r\n",
      "# ~ 1GB. Works\r\n",
      "# X = np.zeros((1024*192//2, 32,32,3),dtype=np.float32)\r\n",
      "# y = np.ones((1024*192//2, 10),dtype=np.float32)\r\n",
      "\r\n",
      "model.fit(\r\n",
      "    X,y,\r\n",
      "    epochs=10,\r\n",
      "    steps_per_epoch=1024, #batch size is 192 per TPU\r\n",
      ")\r\n",
      "```\r\n",
      "```\r\n",
      "print(tf.version.VERSION)\r\n",
      "print(tf.version.GIT_VERSION)\r\n",
      "2.4.1\r\n",
      "v2.4.1-0-g85c8b2a817f\r\n",
      "```\r\n",
      "\r\n",
      "**Logs**\r\n",
      "![106828116-a4958500-66dd-11eb-9ae6-06606e561bc4](https://user-images.githubusercontent.com/27484172/107322586-d735e800-6af8-11eb-9be7-5035a2bda197.png)\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:tpus\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Update sqlite to the latest sqlite-amalgamation-3340100\n",
      "issue body -  This PR updates sqlite to the latest sqlite-amalgamation-3340100\r\n",
      "(released in 2021-01-20)\r\n",
      "\r\n",
      "Signed-off-by: Yong Tang <yong.tang.github@outlook.com>\n",
      "issue labels - \n",
      "cla: yes\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Issue in converting input output from float 32 to int8(Convert using float fallback quantization),\n",
      "issue body -  Hi There, I have an issue in converting a model with float 32 input to int 8;\r\n",
      "My code; https://colab.research.google.com/drive/1EGMqQlos_NovF3qakNVo0PLgvvZukLtB#scrollTo=TqOt6Sv7AsMi\r\n",
      "\r\n",
      "Details:\r\n",
      "I used standard transfer learning code;\r\n",
      "https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/images/transfer_learning.ipynb\r\n",
      "\r\n",
      " just instead of loading data from the website, I loaded data folders from my computer (instead of cat I wanna detect an invasive beetle!), then I  tried to convert the input of the model from float 32 to int 8 using the link below guideline;\r\n",
      "https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_integer_quant.ipynb#scrollTo=FiwiWU3gHdkW\r\n",
      "\r\n",
      "Specifically this cell from abovementioned quantization guidline generates the following issue;\r\n",
      "def representative_data_gen():\r\n",
      "  for input_value in tf.data.Dataset.from_tensor_slices(train_images).batch(1).take(100):\r\n",
      "    # Model has only one input so each data point has one element.\r\n",
      "    yield [input_value]\r\n",
      "\r\n",
      "converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n",
      "converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n",
      "converter.representative_dataset = representative_data_gen\r\n",
      "\r\n",
      "tflite_model_quant = converter.convert()\r\n",
      "\r\n",
      " I get the following issue;  ValueError: Unbatching a dataset is only supported for rank >= 1\r\n",
      "\r\n",
      "\r\n",
      "-----------------------------------------------------------------------------------\r\n",
      "ValueError                                Traceback (most recent call last)\r\n",
      "<ipython-input-167-9f553cafd0bd> in <module>\r\n",
      "      8 converter.representative_dataset = representative_data_gen\r\n",
      "      9 \r\n",
      "---> 10 tflite_model_quant = converter.convert()\r\n",
      "\r\n",
      "C:\\ProgramData\\Anaconda3\\envs\\TF2\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py in convert(self)\r\n",
      "    871           graph=frozen_func.graph)\r\n",
      "    872 \r\n",
      "--> 873     return super(TFLiteKerasModelConverterV2,\r\n",
      "    874                  self).convert(graph_def, input_tensors, output_tensors)\r\n",
      "    875 \r\n",
      "\r\n",
      "C:\\ProgramData\\Anaconda3\\envs\\TF2\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py in convert(self, graph_def, input_tensors, output_tensors)\r\n",
      "    630     calibrate_and_quantize, flags = quant_mode.quantizer_flags()\r\n",
      "    631     if calibrate_and_quantize:\r\n",
      "--> 632       result = self._calibrate_quantize_model(result, **flags)\r\n",
      "    633 \r\n",
      "    634     flags_modify_model_io_type = quant_mode.flags_modify_model_io_type(\r\n",
      "\r\n",
      "C:\\ProgramData\\Anaconda3\\envs\\TF2\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py in _calibrate_quantize_model(self, result, inference_input_type, inference_output_type, activations_type, allow_float)\r\n",
      "    457       return _mlir_quantize(calibrated)\r\n",
      "    458     else:\r\n",
      "--> 459       return calibrate_quantize.calibrate_and_quantize(\r\n",
      "    460           self.representative_dataset.input_gen, inference_input_type,\r\n",
      "    461           inference_output_type, allow_float, activations_type)\r\n",
      "\r\n",
      "C:\\ProgramData\\Anaconda3\\envs\\TF2\\lib\\site-packages\\tensorflow\\lite\\python\\optimize\\calibrator.py in calibrate_and_quantize(self, dataset_gen, input_type, output_type, allow_float, activations_type, resize_input)\r\n",
      "     91     \"\"\"\r\n",
      "     92     initialized = False\r\n",
      "---> 93     for sample in dataset_gen():\r\n",
      "     94       if not initialized:\r\n",
      "     95         initialized = True\r\n",
      "\r\n",
      "<ipython-input-167-9f553cafd0bd> in representative_data_gen()\r\n",
      "      1 def representative_data_gen():\r\n",
      "----> 2   for input_value in tf.data.Dataset.from_tensor_slices(train_dataset).batch(1).take(20):\r\n",
      "      3     # Model has only one input so each data point has one element.\r\n",
      "      4     yield [input_value]\r\n",
      "      5 \r\n",
      "\r\n",
      "C:\\ProgramData\\Anaconda3\\envs\\TF2\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py in from_tensor_slices(tensors)\r\n",
      "    689       Dataset: A `Dataset`.\r\n",
      "    690     \"\"\"\r\n",
      "--> 691     return TensorSliceDataset(tensors)\r\n",
      "    692 \r\n",
      "    693   class _GeneratorState(object):\r\n",
      "\r\n",
      "C:\\ProgramData\\Anaconda3\\envs\\TF2\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py in __init__(self, element)\r\n",
      "   3155     element = structure.normalize_element(element)\r\n",
      "   3156     batched_spec = structure.type_spec_from_value(element)\r\n",
      "-> 3157     self._tensors = structure.to_batched_tensor_list(batched_spec, element)\r\n",
      "   3158     self._structure = nest.map_structure(\r\n",
      "   3159         lambda component_spec: component_spec._unbatch(), batched_spec)  # pylint: disable=protected-access\r\n",
      "\r\n",
      "C:\\ProgramData\\Anaconda3\\envs\\TF2\\lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py in to_batched_tensor_list(element_spec, element)\r\n",
      "    362   # pylint: disable=protected-access\r\n",
      "    363   # pylint: disable=g-long-lambda\r\n",
      "--> 364   return _to_tensor_list_helper(\r\n",
      "    365       lambda state, spec, component: state + spec._to_batched_tensor_list(\r\n",
      "    366           component), element_spec, element)\r\n",
      "\r\n",
      "C:\\ProgramData\\Anaconda3\\envs\\TF2\\lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py in _to_tensor_list_helper(encode_fn, element_spec, element)\r\n",
      "    337     return encode_fn(state, spec, component)\r\n",
      "    338 \r\n",
      "--> 339   return functools.reduce(\r\n",
      "    340       reduce_fn, zip(nest.flatten(element_spec), nest.flatten(element)), [])\r\n",
      "    341 \r\n",
      "\r\n",
      "C:\\ProgramData\\Anaconda3\\envs\\TF2\\lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py in reduce_fn(state, value)\r\n",
      "    335   def reduce_fn(state, value):\r\n",
      "    336     spec, component = value\r\n",
      "--> 337     return encode_fn(state, spec, component)\r\n",
      "    338 \r\n",
      "    339   return functools.reduce(\r\n",
      "\r\n",
      "C:\\ProgramData\\Anaconda3\\envs\\TF2\\lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py in <lambda>(state, spec, component)\r\n",
      "    363   # pylint: disable=g-long-lambda\r\n",
      "    364   return _to_tensor_list_helper(\r\n",
      "--> 365       lambda state, spec, component: state + spec._to_batched_tensor_list(\r\n",
      "    366           component), element_spec, element)\r\n",
      "    367 \r\n",
      "\r\n",
      "C:\\ProgramData\\Anaconda3\\envs\\TF2\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py in _to_batched_tensor_list(self, value)\r\n",
      "   3322   def _to_batched_tensor_list(self, value):\r\n",
      "   3323     if self._dataset_shape.ndims == 0:\r\n",
      "-> 3324       raise ValueError(\"Unbatching a dataset is only supported for rank >= 1\")\r\n",
      "   3325     return self._to_tensor_list(value)\r\n",
      "   3326 \r\n",
      "\r\n",
      "ValueError: Unbatching a dataset is only supported for rank >= 1\r\n",
      "--------------------------------------\n",
      "issue labels - \n",
      "comp:data\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  C++ compilation of rule '//tensorflow/core/kernels/image:extract_image_patches_op' failed\n",
      "issue body -  <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "- TensorFlow installed from (source or binary): tensorflow-2.4.0.tar.gz\r\n",
      "- TensorFlow version: 2.4.0\r\n",
      "- Python version: 3.7\r\n",
      "- Installed using virtualenv? pip? conda?:\r\n",
      "- Bazel version (if compiling from source): 3.7.2\r\n",
      "- GCC/Compiler version (if compiling from source): 7.3.0\r\n",
      "- CUDA/cuDNN version: no\r\n",
      "- GPU model and memory: no\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "[root@tf-build-asr tensorflow-2.4.0]# bazel build -c opt  //tensorflow:libtensorflow_cc.so\r\n",
      "INFO: Options provided by the client:\r\n",
      "  Inherited 'common' options: --isatty=1 --terminal_columns=236\r\n",
      "INFO: Reading rc options for 'build' from /opt/swan/tensorflow-2.4.0/.bazelrc:\r\n",
      "  Inherited 'common' options: --experimental_repo_remote_exec\r\n",
      "INFO: Reading rc options for 'build' from /opt/swan/tensorflow-2.4.0/.bazelrc:\r\n",
      "  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2\r\n",
      "INFO: Reading rc options for 'build' from /opt/swan/tensorflow-2.4.0/.tf_configure.bazelrc:\r\n",
      "  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/python3/lib/python3.7/site-packages --python_path=/usr/bin/python3 --config=xla --action_env TF_CONFIGURE_IOS=0\r\n",
      "INFO: Found applicable config definition build:short_logs in file /opt/swan/tensorflow-2.4.0/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\n",
      "INFO: Found applicable config definition build:v2 in file /opt/swan/tensorflow-2.4.0/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\n",
      "INFO: Found applicable config definition build:xla in file /opt/swan/tensorflow-2.4.0/.bazelrc: --define=with_xla_support=true\r\n",
      "INFO: Found applicable config definition build:linux in file /opt/swan/tensorflow-2.4.0/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\n",
      "INFO: Found applicable config definition build:dynamic_kernels in file /opt/swan/tensorflow-2.4.0/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\n",
      "INFO: Analyzed target //tensorflow:libtensorflow_cc.so (0 packages loaded, 9 targets configured).\r\n",
      "INFO: Found 1 target...\r\n",
      "ERROR: /opt/swan/tensorflow-2.4.0/tensorflow/core/kernels/image/BUILD:229:18: C++ compilation of rule '//tensorflow/core/kernels/image:extract_image_patches_op' failed (Exit 1): gcc failed: error executing command /usr/local/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 142 argument(s) skipped)\r\n",
      "In file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:117:0,\r\n",
      "                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n",
      "                 from ./tensorflow/core/kernels/image/extract_image_patches_op.h:19,\r\n",
      "                 from tensorflow/core/kernels/image/extract_image_patches_op.cc:21:\r\n",
      "external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorImagePatch.h: In static member function 'static void Eigen::internal::EvalRange<Evaluator, StorageIndex, true>::run(Evaluator*, StorageIndex, StorageIndex) [with Evaluator = Eigen::TensorEvaluator<const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<std::complex<float>, 4, 1, int>, 16, Eigen::MakePointer>, const Eigen::TensorReshapingOp<const Eigen::DSizes<int, 4>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const std::complex<float>, 4, 1, int>, 16, Eigen::MakePointer> > > >, Eigen::ThreadPoolDevice>; StorageIndex = int]':\r\n",
      "external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorImagePatch.h:546:7: internal compiler error: in emit_move_insn, at expr.c:3698\r\n",
      "       values[i] = coeff(index+i);\r\n",
      "       ^~~~~~\r\n",
      "0x899fba emit_move_insn(rtx_def*, rtx_def*)\r\n",
      "\t../.././gcc/expr.c:3697\r\n",
      "0x88a34d store_bit_field_1\r\n",
      "\t../.././gcc/expmed.c:814\r\n",
      "0x88a9b8 store_bit_field(rtx_def*, unsigned long, unsigned long, unsigned long, unsigned long, machine_mode, rtx_def*, bool)\r\n",
      "\t../.././gcc/expmed.c:1122\r\n",
      "0x8a4a1e store_field\r\n",
      "\t../.././gcc/expr.c:6974\r\n",
      "0x8a204b expand_assignment(tree_node*, tree_node*, bool)\r\n",
      "\t../.././gcc/expr.c:5209\r\n",
      "0x7b3c01 expand_call_stmt\r\n",
      "\t../.././gcc/cfgexpand.c:2656\r\n",
      "0x7b3c01 expand_gimple_stmt_1\r\n",
      "\t../.././gcc/cfgexpand.c:3571\r\n",
      "0x7b3c01 expand_gimple_stmt\r\n",
      "\t../.././gcc/cfgexpand.c:3737\r\n",
      "0x7b4ddf expand_gimple_basic_block\r\n",
      "\t../.././gcc/cfgexpand.c:5744\r\n",
      "0x7b9f46 execute\r\n",
      "\t../.././gcc/cfgexpand.c:6357\r\n",
      "Please submit a full bug report,\r\n",
      "with preprocessed source if appropriate.\r\n",
      "Please include the complete backtrace with any bug report.\r\n",
      "See <https://gcc.gnu.org/bugs/> for instructions.\r\n",
      "Target //tensorflow:libtensorflow_cc.so failed to build\r\n",
      "Use --verbose_failures to see the command lines of failed build steps.\r\n",
      "INFO: Elapsed time: 931.265s, Critical Path: 45.73s\r\n",
      "INFO: 945 processes: 6 internal, 939 local.\r\n",
      "FAILED: Build did NOT complete successfully\r\n",
      "\r\n",
      "**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n",
      "bazel build -c opt  //tensorflow:libtensorflow_cc.so\r\n",
      "\r\n",
      "**Any other info / logs**\r\n",
      "Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "stalled\n",
      "stat:awaiting response\n",
      "subtype: ubuntu/linux\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  tf.nn.embedding_lookup outputs NaN\n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n",
      "- TensorFlow installed from (source or binary): binary\r\n",
      "- TensorFlow version (use command below):2.1.0\r\n",
      "- Python version:3.7.6\r\n",
      "- Bazel version (if compiling from source):N/A\r\n",
      "- GCC/Compiler version (if compiling from source):N/A\r\n",
      "- CUDA/cuDNN version:N/A\r\n",
      "- GPU model and memory:N/A\r\n",
      "\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "`tf.nn.embedding_lookup` outputs NaN if `params` is large value\r\n",
      "\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "Expect a grace exception message if the input is invalid instead of Nan as output\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "~~~python\r\n",
      "import tensorflow as tf\r\n",
      "import numpy as np\r\n",
      "tf.nn.embedding_lookup(params=np.array([1e+38, 1e+38],dtype=np.float32), ids=1, max_norm=10)\r\n",
      "~~~\r\n",
      "\r\n",
      "Output:\r\n",
      "~~~python\r\n",
      "<tf.Tensor: shape=(), dtype=float32, numpy=nan>\r\n",
      "~~~\n",
      "issue labels - \n",
      "TF 2.1\n",
      "comp:apis\n",
      "stat:awaiting tensorflower\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  `tf.nn.log_poisson_loss` outputs NaN\n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n",
      "- TensorFlow installed from (source or binary): binary\r\n",
      "- TensorFlow version (use command below):2.1.0\r\n",
      "- Python version:3.7.6\r\n",
      "- Bazel version (if compiling from source):N/A\r\n",
      "- GCC/Compiler version (if compiling from source):N/A\r\n",
      "- CUDA/cuDNN version:N/A\r\n",
      "- GPU model and memory:N/A\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "`tf.nn.log_poisson_loss` outputs NaN if `log_input` and `targets` contain large values\r\n",
      "\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "Expect a grace exception message if the input is invalid instead of Nan as output\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "~~~python\r\n",
      "import tensorflow as tf\r\n",
      "import numpy as np\r\n",
      "tf.nn.log_poisson_loss(log_input=np.array([1e+38], dtype=np.float32), targets=np.array([1e+38], dtype=np.float32))\r\n",
      "~~~\r\n",
      "Output:\r\n",
      "~~~python\r\n",
      "<tf.Tensor: shape=(1,), dtype=float32, numpy=array([nan], dtype=float32)>\r\n",
      "~~~\n",
      "issue labels - \n",
      "TF 2.1\n",
      "comp:apis\n",
      "stat:awaiting tensorflower\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Remove the sources containing the models from the TFLM static lib.\n",
      "issue body -  These sources are only needed for specific tests and are now explicitly specified as part of creating a test target.\r\n",
      "\r\n",
      "From this change onwards, we will explicitly create test targets for tests that depend on sources outside of libtensorflow-microlite.a. This avoids putting unnecessary files into MICROLITE_CC_SRCS.\r\n",
      "\r\n",
      "Manually verified that the following command does not error out:\r\n",
      "```\r\n",
      "make -f tensorflow/lite/micro/tools/make/Makefile generate_keyword_benchmark_make_project && cd tensorflow/lite/micro/tools/make/gen/linux_x86_64_default/prj/keyword_benchmark/make/ && make -j8\r\n",
      "```\r\n",
      "\r\n",
      "Fixes #46860\r\n",
      "\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "prtype:bugfix\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  Thrown out ValueError when alpha=None is passed for tf.keras.layers.LeakyReLU\n",
      "issue body -  This PR tries to address the issue raised in #46993 where\r\n",
      "incorrect nan value is returned when alpha=None is passed for\r\n",
      "tf.keras.layers.LeakyReLU. The nan could be misleading to users.\r\n",
      "\r\n",
      "This PR address the issue and throw out ValueError instead.\r\n",
      "\r\n",
      "This PR fixes #46993.\r\n",
      "\r\n",
      "Signed-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n",
      "\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:keras\n",
      "prtype:bugfix\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  [tf.data] eager mode support for experimental benchmarks part2\n",
      "issue body -  This PR is a follow up of https://github.com/tensorflow/tensorflow/pull/46761 and extends the eager mode support to the remaining subset of experimental benchmarks.\r\n",
      "\r\n",
      "Sample result:\r\n",
      "```\r\n",
      "# DATASET Based benchmarking\r\n",
      "entry {\r\n",
      "  name: \"ParallelInterleaveBenchmark.single_cycle_experimental_parallel.eager\"\r\n",
      "  iters: 100\r\n",
      "  wall_time: 9.162362813949585e-06\r\n",
      "  extras {\r\n",
      "    key: \"num_elements\"\r\n",
      "    value {\r\n",
      "      double_value: 200000.0\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "\r\n",
      "entry {\r\n",
      "  name: \"ParallelInterleaveBenchmark.single_cycle_core_parallel.eager\"\r\n",
      "  iters: 100\r\n",
      "  wall_time: 6.957079768180847e-06\r\n",
      "  extras {\r\n",
      "    key: \"num_elements\"\r\n",
      "    value {\r\n",
      "      double_value: 200000.0\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "\r\n",
      "# OP based benchmarking\r\n",
      "\r\n",
      "entry {\r\n",
      "  name: \"MapDefunBenchmark.with_defun_size_10000\"\r\n",
      "  iters: 1\r\n",
      "  wall_time: 7.152557373046875e-07\r\n",
      "  extras {\r\n",
      "    key: \"examples_per_sec\"\r\n",
      "    value {\r\n",
      "      double_value: 1398101.3333333333\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "\r\n",
      "entry {\r\n",
      "  name: \"MapDefunBenchmark.without_defun_size_10000\"\r\n",
      "  iters: 1\r\n",
      "  wall_time: 8.392333984375e-05\r\n",
      "  extras {\r\n",
      "    key: \"examples_per_sec\"\r\n",
      "    value {\r\n",
      "      double_value: 11915.636363636364\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "```\r\n",
      "\r\n",
      "cc: @jsimsa \r\n",
      "\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:data\n",
      "ready to pull\n",
      "size:L\n",
      "\n",
      "\n",
      "issue title -  Remove duplicated implementation of keras.backend.ndim\n",
      "issue body -  This PR simplifies `keras.backend.ndim` by reusing the identical implementation from `TensorShape.rank`:\r\n",
      "https://github.com/tensorflow/tensorflow/blob/7516f6ee75063d8e4f3cfa9d574de64769185de9/tensorflow/python/framework/tensor_shape.py#L825-L830\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:keras\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Simplify shape inference in Keras resize_images\n",
      "issue body -  This PR simplifies shape inference of `keras.backend.resize_images`, since the shape of the output tensor is already correctly set in `image_ops.resize_images_v2`:\r\n",
      "https://github.com/tensorflow/tensorflow/blob/0921a80d56107e19c5c1cebf9500a683d7d41b10/tensorflow/python/ops/image_ops_impl.py#L1390\r\n",
      "\r\n",
      "This also removes the need for the `tf.shape` op in cases where the spatial shape of the input tensor is fully defined which fixes #25086.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:keras\n",
      "prtype:bugfix\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  [TFLite] Fix resize bilinear tests by raising a too low error_threshold when align_corners is true\n",
      "issue body -  Hi,\r\n",
      "\r\n",
      "The '//tensorflow/lite/kernels/internal:resize_bilinear_test' is failing on our side with the following errors since commit 297ffd8b0b827b2c61352a2d2e181f481738e0c4:\r\n",
      "```\r\n",
      "tensorflow/lite/kernels/internal/resize_bilinear_test.cc:73: Failure\r\n",
      "Expected: (relative_error) < (error_threshold), actual: 0.000531915 vs 0.0003\r\n",
      "tensorflow/lite/kernels/internal/resize_bilinear_test.cc:73: Failure\r\n",
      "Expected: (relative_error) < (error_threshold), actual: 0.000400641 vs 0.0003\r\n",
      "tensorflow/lite/kernels/internal/resize_bilinear_test.cc:73: Failure\r\n",
      "Expected: (relative_error) < (error_threshold), actual: 0.000674764 vs 0.0003\r\n",
      "tensorflow/lite/kernels/internal/resize_bilinear_test.cc:73: Failure\r\n",
      " Expected: (relative_error) < (error_threshold), actual: 0.000558036 vs 0.0003\r\n",
      "tensorflow/lite/kernels/internal/resize_bilinear_test.cc:73: Failure\r\n",
      "Expected: (relative_error) < (error_threshold), actual: 0.000300481 vs 0.0003\r\n",
      "```\r\n",
      "\r\n",
      "The threshold when comparing the reference and optimized resize bilinear kernel with align_corners is too low. This PR raises it to 1e-3 as it seems the kernel is correct and the problem comes from the too tight threshold.\r\n",
      "\r\n",
      "Thibaut\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:lite\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Remove unnecessary set_model overwrite in BackupAndRestore calllback\n",
      "issue body -  `keras.callbacks.BackupAndRestore` subclasses `keras.callbacks.Callback` which already implements `set_model()` so there is no need to reimplement it here.\r\n",
      "https://github.com/tensorflow/tensorflow/blob/bbeb7264026e216eba9f0479017d429e2e9d06f6/tensorflow/python/keras/callbacks.py#L636-L637\r\n",
      "\r\n",
      "This PR removes the overwrite, as it doesn't change the behaviour of the code.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:keras\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Don't run nightly update job in forks.\n",
      "issue body -  There are currently ~84000 forks of TensorFlow on GitHub, and this job currently runs daily in all of them (or at least the ones which were last synced more recently than this job was added).\r\n",
      "\r\n",
      "As I understand it, the point of this job is to run nightly so that the TF team can run a suite of nightly tests, so there's no reason for it to happen anywhere other than the upstream `tensorflow/tensorflow` repo - please correct me if I'm wrong about this!\r\n",
      "\r\n",
      "Looking at the list of [runs for the last few days](https://github.com/tensorflow/tensorflow/actions?query=workflow%3A%22Set+nightly+branch+to+master+HEAD%22), some jobs were under 20 seconds but some were more than 10 minutes. That's an awful lot of unnecessary compute that's being used performing the job on forks.\n",
      "issue labels - \n",
      "cla: yes\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Error in keras.tokenizer.texts_to_sequences\n",
      "issue body -  <em>Please make sure that this is a bug. As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:bug_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n",
      "- TensorFlow installed from (source or binary): binary\r\n",
      "- TensorFlow version (use command below): 2.4.0\r\n",
      "- Python version: 3.7.4\r\n",
      "- Bazel version (if compiling from source): N/A\r\n",
      "- GCC/Compiler version (if compiling from source): N/A\r\n",
      "- CUDA/cuDNN version: N/A\r\n",
      "- GPU model and memory: N/A\r\n",
      "\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "When few texts are given to the keras.tokenizer.texts_to_sequences, it can produce the right sequences but when we have loarge number of texts, it produces wrong sequences.\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "Expect the same \"number of non zero values\" for few texts and large texts.\r\n",
      "\r\n",
      "In example below, for \"few texts\", rows 0 and 5 have 2 and 4 non-zero values respectively, but for \"texts\" that we have many rows (as shown below) for these rows we have 1 and 3 non-zero values (the first 8 rows in \"texts\" are the same as in \"few texts\"). These are just two rows. There are many rows like this.\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "[samples.txt](https://github.com/tensorflow/tensorflow/files/5942989/samples.txt)\r\n",
      "\r\n",
      "```from keras.preprocessing.text import Tokenizer\r\n",
      "from keras.preprocessing.sequence import pad_sequences\r\n",
      "import numpy as np\r\n",
      "import pickle\r\n",
      "\r\n",
      "few_texts=['product market',\r\n",
      "           'business marketing',\r\n",
      "           'entrepreneur business',\r\n",
      "           'invest money',\r\n",
      "           'money invest .',\r\n",
      "           'business money investment investing',\r\n",
      "           'strategies market .',\r\n",
      "           'marketing investment . .']\r\n",
      "\r\n",
      "\r\n",
      "tokenizer = Tokenizer(num_words=25)\r\n",
      "tokenizer.fit_on_texts(few_texts)\r\n",
      "sequences = tokenizer.texts_to_sequences(few_texts)\r\n",
      "print('sequences:',sequences)\r\n",
      "word_index = tokenizer.word_index\r\n",
      "data = pad_sequences(sequences, maxlen=25, padding='post')\r\n",
      "print('Shape of data tensor:', data.shape)\r\n",
      "data\r\n",
      "```\r\n",
      "sequences: [[7, 3], [1, 4], [8, 1], [5, 2], [2, 5], [1, 2, 6, 9], [10, 3], [4, 6]]\r\n",
      "Shape of data tensor: (8, 25)\r\n",
      "array([[ 7,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\r\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0],\r\n",
      "       [ 1,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\r\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0],\r\n",
      "       [ 8,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\r\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0],\r\n",
      "       [ 5,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\r\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0],\r\n",
      "       [ 2,  5,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\r\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0],\r\n",
      "       [ 1,  2,  6,  9,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\r\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0],\r\n",
      "       [10,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\r\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0],\r\n",
      "       [ 4,  6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\r\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0]])\r\n",
      "\r\n",
      "```\r\n",
      "with open(\"samples.txt\", \"rb\") as fp:   \r\n",
      "    seed = pickle.load(fp)\r\n",
      "texts=seed\r\n",
      "len(texts)\r\n",
      "```\r\n",
      "1000\r\n",
      "```\r\n",
      "texts[0],texts[1],texts[2],texts[3],texts[4],texts[5],texts[6],texts[7]\r\n",
      "```\r\n",
      "('product market',\r\n",
      " 'business marketing',\r\n",
      " 'entrepreneur business',\r\n",
      " 'invest money',\r\n",
      " 'money invest .',\r\n",
      " 'business money investment investing',\r\n",
      " 'strategies market .',\r\n",
      " 'marketing investment . .')\r\n",
      "```\r\n",
      "tokenizer = Tokenizer(num_words=25)\r\n",
      "tokenizer.fit_on_texts(texts)\r\n",
      "sequences = tokenizer.texts_to_sequences(texts)\r\n",
      "#print('sequences:',sequences)\r\n",
      "word_index = tokenizer.word_index\r\n",
      "#print('Found %s unique tokens.' % len(word_index))\r\n",
      "\r\n",
      "data = pad_sequences(sequences, maxlen=25, padding='post')\r\n",
      "#print(data)\r\n",
      "print('Shape of data tensor:', data.shape)\r\n",
      "\r\n",
      "data[0],data[5]\r\n",
      "```\r\n",
      "Shape of data tensor: (1000, 25)\r\n",
      "(array([5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n",
      "        0, 0, 0]),\r\n",
      " array([ 1,  3, 10,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\r\n",
      "         0,  0,  0,  0,  0,  0,  0,  0]))\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:keras\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Update pooling.py\n",
      "issue body -  Updated the value of strides argument in example snippet.\r\n",
      "\r\n",
      "Fixes the change as suggested in #46998.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:keras\n",
      "ready to pull\n",
      "size:XS\n",
      "type:docs-bug\n",
      "\n",
      "\n",
      "issue title -  Tensorflow 2.4.1running with cpu but not gpu\n",
      "issue body -  Hi all,\r\n",
      "\r\n",
      "I am trying to set up an environment on WSL2(windows 10) to run my deep learning project with Tensorflow on GPU.\r\n",
      "Please see below for my system and related packages I have installed:\r\n",
      "##\r\n",
      "Main OS: Windows Pro, Insider Preview (21301.1010)\r\n",
      "GPU : Nvidia 1060 3GB\r\n",
      "CPU : I5-2500\r\n",
      "##\r\n",
      "Subsystem: Ubuntu20.04 LTS ( install from windows store)\r\n",
      "Tensorflow: 2.4.1\r\n",
      "Python: 3.8.5\r\n",
      "CUDA and other necessary toolkits were installed as following url:\r\n",
      "[Tensorflow gpu support guide](https://www.tensorflow.org/install/gpu), in the section of **Ubuntu 18.04 (CUDA 11.0)**.\r\n",
      "_p.s. Although I am using Ubuntu20.04(WSL2), tensorflow was not able to detect my gpu when I installed CUDA toolkit for Ubuntu20.04. Therefore, I followed the guide of Ubuntu18.04 and it worked fine._\r\n",
      "##\r\n",
      "\r\n",
      "Everything worked fine at the beginning, all tests showing that tensorflow was able to detect my gpu device.\r\n",
      "However, when I ran [Tensorflow sample code](https://www.tensorflow.org/tutorials/quickstart/beginner?hl=zh_tw), the message showed that tensorflow created a gpu device but running on cpu when code executed to model.fit\r\n",
      "\r\n",
      "`2021-02-08 16:32:23.944331: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "2021-02-08 16:32:26.591081: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n",
      "2021-02-08 16:32:26.599408: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n",
      "2021-02-08 16:32:26.902834: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\r\n",
      "Your kernel may have been built without NUMA support.\r\n",
      "2021-02-08 16:32:26.903131: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\n",
      "pciBusID: 0000:01:00.0 name: GeForce GTX 1060 3GB computeCapability: 6.1\r\n",
      "coreClock: 1.7085GHz coreCount: 9 deviceMemorySize: 3.00GiB deviceMemoryBandwidth: 178.99GiB/s\r\n",
      "2021-02-08 16:32:26.903196: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "2021-02-08 16:32:26.907413: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n",
      "2021-02-08 16:32:26.907593: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n",
      "2021-02-08 16:32:26.910460: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n",
      "2021-02-08 16:32:26.911433: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n",
      "2021-02-08 16:32:26.915959: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n",
      "2021-02-08 16:32:26.916820: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n",
      "2021-02-08 16:32:26.917147: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n",
      "2021-02-08 16:32:26.918042: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\r\n",
      "Your kernel may have been built without NUMA support.\r\n",
      "2021-02-08 16:32:26.918932: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\r\n",
      "Your kernel may have been built without NUMA support.\r\n",
      "2021-02-08 16:32:26.919158: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n",
      "2021-02-08 16:32:26.920533: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n",
      "2021-02-08 16:32:26.921406: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\r\n",
      "Your kernel may have been built without NUMA support.\r\n",
      "2021-02-08 16:32:26.921676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\n",
      "pciBusID: 0000:01:00.0 name: GeForce GTX 1060 3GB computeCapability: 6.1\r\n",
      "coreClock: 1.7085GHz coreCount: 9 deviceMemorySize: 3.00GiB deviceMemoryBandwidth: 178.99GiB/s\r\n",
      "2021-02-08 16:32:26.921762: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "2021-02-08 16:32:26.921851: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n",
      "2021-02-08 16:32:26.921914: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n",
      "2021-02-08 16:32:26.921976: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n",
      "2021-02-08 16:32:26.922038: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n",
      "2021-02-08 16:32:26.922073: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n",
      "2021-02-08 16:32:26.922133: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n",
      "2021-02-08 16:32:26.922195: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n",
      "2021-02-08 16:32:26.923004: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\r\n",
      "Your kernel may have been built without NUMA support.\r\n",
      "2021-02-08 16:32:26.923906: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\r\n",
      "Your kernel may have been built without NUMA support.\r\n",
      "2021-02-08 16:32:26.924155: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n",
      "2021-02-08 16:32:26.924245: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "2021-02-08 16:32:29.425230: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n",
      "2021-02-08 16:32:29.425326: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \r\n",
      "2021-02-08 16:32:29.425343: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \r\n",
      "2021-02-08 16:32:29.427287: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\r\n",
      "Your kernel may have been built without NUMA support.\r\n",
      "2021-02-08 16:32:29.427562: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1489] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\r\n",
      "2021-02-08 16:32:29.428461: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\r\n",
      "Your kernel may have been built without NUMA support.\r\n",
      "2021-02-08 16:32:29.429563: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\r\n",
      "Your kernel may have been built without NUMA support.`\r\n",
      "\r\n",
      "### Created TensorFlow device successfully\r\n",
      "**2021-02-08 16:32:29.429890: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2074 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 3GB, pci bus id: 0000:01:00.0, compute capability: 6.1)**\r\n",
      "\r\n",
      "`2021-02-08 16:32:29.888992: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n",
      "2021-02-08 16:32:31.514620: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n",
      "2021-02-08 16:32:32.694044: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 188160000 exceeds 10% of free system memory.\r\n",
      "2021-02-08 16:32:33.834778: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)`\r\n",
      "\r\n",
      "### Model was running on cpu but not cpu\r\n",
      "**2021-02-08 16:32:33.836445: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3292520000 Hz**\r\n",
      "\r\n",
      "Is there anything I did not execute properly? I would like to utilize my gpu power on my deep learning project but tensorflow only work on my cpu but not gpu even it showed it created a tensorflow device.\r\n",
      "\r\n",
      "Any comments and suggestion are appreciate. Thanks you all.\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:gpu\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  tf.train.Server and mirrored strategy..\n",
      "issue body -  Hello I am a new user of tensorflow.\r\n",
      "I am implementing one reinforcement learning algorithm using tf2 ,I have it's implementation with tf1 there the authors have used tf.train.Server for the distributed learning I want to use tf.distribute.MirroredStrategy to do the distributed training. But I am not able to link these 2 methods the \"tf.train.Server\" and \"tf.distribute.MirroredStrategy\". can anyone help to understand the difference between these and how to go about it? Thanks you..\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "comp:dist-strat\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  How could I know the label is correctly read from image_dataset_from_directory?\n",
      "issue body -  I'd save the directory structure as indicated in document, separate classes of images stored in corresponding folder with corresponding label as folder name.\r\n",
      "\r\n",
      "However, I could not find a way to return the labels read from image_dataset_from_directory function, and I'm afraid the class name is not correctly read.\r\n",
      "\r\n",
      "The performance of my training is that, after 10 training epochs the training and validate accuracy reached 0.99, but the model prediction would only predict one class.\r\n",
      "\r\n",
      "How could I check the label read by this function?\r\n",
      "\n",
      "issue labels - \n",
      "stat:awaiting response\n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  Documentation mistake in tf.keras.layers.MaxPool2D python section\n",
      "issue body -  ## URL(s) with the issue: \r\n",
      "https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D\r\n",
      "\r\n",
      "## Description of issue (what needs changing):\r\n",
      "\r\n",
      "A Documentation mistake in tf.keras.layers.MaxPool2D python section(example)\r\n",
      "\r\n",
      "### Clear description\r\n",
      "\r\n",
      "There is a documentation mistake in tf.keras.layers.MaxPool2D python section(example). \r\n",
      "In the description part of there are few examples given with their respective code snippets.\r\n",
      "Under this text-\"For example, for stride=(2,2) and padding=\"valid\":\" in the code snippet of example instead of **strides=(2,2)**, **strides=(1,1)** is written.\r\n",
      "\r\n",
      "\r\n",
      "<img width=\"939\" alt=\"Screenshot 2021-02-08 at 10 16 05 AM\" src=\"https://user-images.githubusercontent.com/38719024/107178001-33b1de00-69f9-11eb-8c7a-6114457686c0.png\">\r\n",
      "\n",
      "issue labels - \n",
      "comp:keras\n",
      "type:docs-bug\n",
      "\n",
      "\n",
      "issue title -  hello0 alternative error message for failed shape broadcasting\n",
      "issue body -  This just ensures that the test also passes with alternative generated kernels.\r\n",
      "\r\n",
      "PiperOrigin-RevId: 355814751\r\n",
      "Change-Id: I2a148427ebc059244c9f94ba91e294c460bb5544\n",
      "issue labels - \n",
      "cla: yes\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  tf.nn.avg_pool/1d/2d/3d outputs NaN if `ksize=0`\n",
      "issue body -  \r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n",
      "- TensorFlow installed from (source or binary): binary\r\n",
      "- TensorFlow version (use command below):2.1.0\r\n",
      "- Python version:3.7.6\r\n",
      "- Bazel version (if compiling from source):N/A\r\n",
      "- GCC/Compiler version (if compiling from source):N/A\r\n",
      "- CUDA/cuDNN version:N/A\r\n",
      "- GPU model and memory:N/A:\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "The following APIs output NaN if `ksize=0`\r\n",
      "- `tf.nn.avg_pool1d`\r\n",
      "- `tf.nn.avg_pool2d`\r\n",
      "- `tf.nn.avg_pool3d`\r\n",
      "- `tf.nn.avg_pool`\r\n",
      "\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "Expect a grace exception message if the input is unexpected(e.g. `ksize=0`) other than a NaN as output\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "~~~python\r\n",
      "import tensorflow as tf\r\n",
      "import numpy as np\r\n",
      "tf.nn.avg_pool1d(input=np.ones((1, 1, 1)), ksize=0, strides=1, padding='SAME')\r\n",
      "tf.nn.avg_pool2d(input=np.ones((1, 1, 1, 1)), ksize=0, strides=1, padding='SAME')\r\n",
      "tf.nn.avg_pool3d(input=np.ones((1, 1, 1, 1, 1)), ksize=0, strides=1, padding='SAME')\r\n",
      "tf.nn.avg_pool(input=np.ones((1, 1, 1)), ksize=0, strides=1, padding='SAME')\r\n",
      "~~~\r\n",
      "\r\n",
      "The output of `tf.nn.avg_pool1d`\r\n",
      "~~~python\r\n",
      "<tf.Tensor: shape=(1, 1, 1), dtype=float64, numpy=array([[[nan]]])>\r\n",
      "~~~\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.1\n",
      "comp:apis\n",
      "stat:awaiting tensorflower\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  `tf.keras.layers.ELU` and  `tf.keras.layers.LeakyReLU` outputs nan if `alpha=None`\n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n",
      "- TensorFlow installed from (source or binary): binary\r\n",
      "- TensorFlow version (use command below):2.1.0\r\n",
      "- Python version:3.7.6\r\n",
      "- Bazel version (if compiling from source):N/A\r\n",
      "- GCC/Compiler version (if compiling from source):N/A\r\n",
      "- CUDA/cuDNN version:N/A\r\n",
      "- GPU model and memory:N/A\r\n",
      "\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "`tf.keras.layers.ELU` and  `tf.keras.layers.LeakyReLU` outputs nan if `alpha=None`\r\n",
      "\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "expect no nan as output\r\n",
      "\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "~~~python\r\n",
      "import tensorflow as tf\r\n",
      "import numpy as np\r\n",
      "layer = tf.keras.layers.ELU(alpha=None)\r\n",
      "out=layer(np.array([-2, 6.]))\r\n",
      "print(out)\r\n",
      "~~~\r\n",
      "\r\n",
      "Output:\r\n",
      "~~~\r\n",
      "tf.Tensor([nan  6.], shape=(2,), dtype=float32)\r\n",
      "~~~\r\n",
      "\r\n",
      "\r\n",
      "~~~python\r\n",
      "import tensorflow as tf\r\n",
      "import numpy as np\r\n",
      "layer = tf.keras.layers.LeakyReLU(alpha=None)\r\n",
      "out=layer(np.array([-2, 6]))\r\n",
      "~~~\r\n",
      "Output:\r\n",
      "~~~\r\n",
      "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([nan,  6.], dtype=float32)>\r\n",
      "~~~\r\n",
      "\r\n",
      "\r\n",
      "Related: #13787\n",
      "issue labels - \n",
      "TF 2.1\n",
      "TF 2.4\n",
      "comp:keras\n",
      "stat:awaiting tensorflower\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Converting tf.argmax(predicted,axis=0) to numpy array to index the y_labels\n",
      "issue body -  I am new to Tensorflow and wrote the following distributed training code. The code works fine.\r\n",
      "\r\n",
      "    import multiprocessing\r\n",
      "    import os\r\n",
      "    import portpicker\r\n",
      "    import tensorflow as tf\r\n",
      "    import tensorflow.keras as keras\r\n",
      "    import tensorflow_hub as hub\r\n",
      "    import tensorflow.python.keras.backend as K\r\n",
      "    #1. Define Workers\r\n",
      "    def create_in_process_cluster(num_workers, num_ps):\r\n",
      "      \"\"\"Creates and starts local servers and returns the cluster_resolver.\"\"\"\r\n",
      "      worker_ports = [portpicker.pick_unused_port() for _ in range(num_workers)]\r\n",
      "      ps_ports = [portpicker.pick_unused_port() for _ in range(num_ps)]\r\n",
      "    \r\n",
      "      cluster_dict = {}\r\n",
      "      cluster_dict[\"worker\"] = [\"localhost:%s\" % port for port in worker_ports]\r\n",
      "      if num_ps > 0:\r\n",
      "        cluster_dict[\"ps\"] = [\"localhost:%s\" % port for port in ps_ports]\r\n",
      "    \r\n",
      "      cluster_spec = tf.train.ClusterSpec(cluster_dict)\r\n",
      "    \r\n",
      "      # Workers need some inter_ops threads to work properly.\r\n",
      "      worker_config = tf.compat.v1.ConfigProto()\r\n",
      "      if multiprocessing.cpu_count() < num_workers + 1:\r\n",
      "        worker_config.inter_op_parallelism_threads = num_workers + 1\r\n",
      "    \r\n",
      "      for i in range(num_workers):\r\n",
      "        tf.distribute.Server(\r\n",
      "            cluster_spec, job_name=\"worker\", task_index=i, config=worker_config,\r\n",
      "            protocol=\"grpc\")\r\n",
      "    \r\n",
      "      for i in range(num_ps):\r\n",
      "        tf.distribute.Server(\r\n",
      "            cluster_spec, job_name=\"ps\", task_index=i, protocol=\"grpc\")\r\n",
      "    \r\n",
      "      cluster_resolver = tf.distribute.cluster_resolver.SimpleClusterResolver(\r\n",
      "          cluster_spec, task_id=0, task_type=\"worker\",rpc_layer=\"grpc\")\r\n",
      "      return cluster_resolver\r\n",
      "    \r\n",
      "    NUM_WORKERS = 3\r\n",
      "    NUM_PS = 2\r\n",
      "    cluster_resolver = create_in_process_cluster(NUM_WORKERS, NUM_PS)\r\n",
      "    \r\n",
      "    # Set the environment variable to allow reporting worker and ps failure to the\r\n",
      "    # coordinator. This is a workaround and won't be necessary in the future.\r\n",
      "    os.environ[\"GRPC_FAIL_FAST\"] = \"use_caller\"\r\n",
      "    \r\n",
      "    variable_partitioner = (\r\n",
      "        tf.distribute.experimental.partitioners.FixedShardsPartitioner(\r\n",
      "            num_shards=NUM_PS))\r\n",
      "    \r\n",
      "    strategy = tf.distribute.experimental.ParameterServerStrategy(cluster_resolver)\r\n",
      "    \r\n",
      "    word = \"Elephant\"\r\n",
      "    sentence = \"I am a sentence for which I would like to get its embedding.\"\r\n",
      "    paragraph = (\r\n",
      "        \"Universal Sentence Encoder embeddings also support short paragraphs. \"\r\n",
      "        \"There is no hard limit on how long the paragraph is. Roughly, the longer \"\r\n",
      "        \"the more 'diluted' the embedding will be.\")\r\n",
      "    messages = [word, sentence, paragraph]\r\n",
      "    #labels=[\"1\",\"2\",\"3\"]\r\n",
      "    reviews = [[1,0,0],[0,1,0],[0,0,1]]\r\n",
      "    \r\n",
      "    \r\n",
      "    encoder=hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\r\n",
      "    \r\n",
      "    X_train=encoder(messages)\r\n",
      "    \r\n",
      "    BUFFER_SIZE = len(X_train)\r\n",
      "    BATCH_SIZE_PER_REPLICA = 2\r\n",
      "    GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\r\n",
      "    EPOCHS = 4\r\n",
      "    \r\n",
      "    \r\n",
      "    with strategy.scope():\r\n",
      "    \r\n",
      "        model = keras.Sequential()\r\n",
      "    \r\n",
      "        model.add(\r\n",
      "            keras.layers.Dense(\r\n",
      "                units=256,\r\n",
      "                input_shape=(X_train.shape[1],),\r\n",
      "                activation='relu'\r\n",
      "            )\r\n",
      "        )\r\n",
      "        model.add(\r\n",
      "            keras.layers.Dropout(rate=0.5)\r\n",
      "        )\r\n",
      "    \r\n",
      "        model.add(\r\n",
      "            keras.layers.Dense(\r\n",
      "                units=128,\r\n",
      "                activation='relu'\r\n",
      "            )\r\n",
      "        )\r\n",
      "        model.add(\r\n",
      "            keras.layers.Dropout(rate=0.5)\r\n",
      "        )\r\n",
      "    \r\n",
      "        model.add(keras.layers.Dense(3, activation='softmax'))\r\n",
      "        # model.compile(\r\n",
      "        #     loss='categorical_crossentropy',\r\n",
      "        #     optimizer=keras.optimizers.Adam(0.001),\r\n",
      "        #     metrics=['accuracy']\r\n",
      "        # )\r\n",
      "    \r\n",
      "        # history = model.fit(\r\n",
      "        #     np.array(X_train), np.array(reviews),\r\n",
      "        #     epochs=10,\r\n",
      "        #     batch_size=16,\r\n",
      "        #     verbose=1,\r\n",
      "        #     shuffle=True\r\n",
      "        # )\r\n",
      "        optimizer=keras.optimizers.Adam(0.001)\r\n",
      "        accuracy = keras.metrics.Accuracy()\r\n",
      "    \r\n",
      "    \r\n",
      "    def step_fn(x_train_slice):\r\n",
      "    \r\n",
      "        x_train, y_train = next(x_train_slice)\r\n",
      "        with tf.GradientTape() as tape:\r\n",
      "            pred=model(x_train,training=True)\r\n",
      "            # tf.print(x_train)\r\n",
      "            # tf.print(pred)\r\n",
      "            # tf.print(y_train)\r\n",
      "    \r\n",
      "            per_example_loss = keras.losses.CategoricalCrossentropy(\r\n",
      "                reduction=tf.keras.losses.Reduction.NONE)(y_train, pred)\r\n",
      "            loss = tf.nn.compute_average_loss(per_example_loss)\r\n",
      "            gradients = tape.gradient(loss, model.trainable_variables)\r\n",
      "    \r\n",
      "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n",
      "        actual_pred = tf.cast(tf.greater(pred, 0.5), tf.int64)\r\n",
      "        tf.print(\"train values are\",x_train)\r\n",
      "        tf.print(\" pred Values are : \", pred)\r\n",
      "        tf.print(\" ArgMAx Values are \",tf.math.argmax(pred,axis=0)) #problem\r\n",
      "        tf.print(\" actual_pred Values are : \", actual_pred)\r\n",
      "        tf.print(\" Labels  are : \", y_train)\r\n",
      "        tf.print(\" Labels Max Values are : \", tf.argmax(y_train))\r\n",
      "        accuracy.update_state(y_train, actual_pred)\r\n",
      "        tf.print(\"Accuracy is : \",accuracy.result())\r\n",
      "        return loss\r\n",
      "    \r\n",
      "    @tf.function\r\n",
      "    def distributed_train_step(x_train_slice):\r\n",
      "        losses = strategy.run(step_fn,args=(x_train_slice,))\r\n",
      "        return strategy.reduce(tf.distribute.ReduceOp.SUM, losses, axis=None)\r\n",
      "    \r\n",
      "    \r\n",
      "    @tf.function\r\n",
      "    def per_worker_dataset_fn():\r\n",
      "        train_dataset = tf.data.Dataset.from_tensor_slices((X_train, reviews)).shuffle(BUFFER_SIZE).batch(GLOBAL_BATCH_SIZE)\r\n",
      "        # test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(GLOBAL_BATCH_SIZE)\r\n",
      "        train_dist_dataset = strategy.experimental_distribute_dataset(train_dataset)\r\n",
      "        # test_dist_dataset = strategy.experimental_distribute_dataset(test_dataset)\r\n",
      "        return train_dist_dataset\r\n",
      "    \r\n",
      "    \r\n",
      "    coordinator = tf.distribute.experimental.coordinator.ClusterCoordinator(strategy)\r\n",
      "    per_worker_dataset = coordinator.create_per_worker_dataset(per_worker_dataset_fn)\r\n",
      "    per_worker_iterator = iter(per_worker_dataset)\r\n",
      "    num_epoches = 5\r\n",
      "    steps_per_epoch = 1\r\n",
      "    for i in range(num_epoches):\r\n",
      "      accuracy.reset_states()\r\n",
      "      for _ in range(steps_per_epoch):\r\n",
      "        coordinator.schedule(distributed_train_step, args=(per_worker_iterator,))\r\n",
      "        # Wait at epoch boundaries.\r\n",
      "      coordinator.join()\r\n",
      "      print (\"Finished epoch %d, accuracy is %f.\",(i,accuracy.result().numpy()))\r\n",
      "\r\n",
      "The problem is, in the step_fn once I get the prediction values I would like to get the corresponding labels, for this I have used this line of code\r\n",
      "`tf.print(\" ArgMAx Values are \",tf.math.argmax(pred,axis=0)) #problem`\r\n",
      "\r\n",
      "The argmax gives the array of indices for max probabilities. I would like to extract this as numpy array and index it to reviews array (One-Hot encoded values) to get the confusion matrix. \r\n",
      "\r\n",
      "But I'm not able to convert `tf.math.argmax(pred,axis=0)` tensor to numpy array. I tried many approaches like eval(K.get_session()) and so on but nothing worked. Any help is appreciated.\r\n",
      "\r\n",
      "Thanks much\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:ops\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Yet Another -1073740791 (0xC0000409)\n",
      "issue body -  \r\n",
      "**System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Home\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "- TensorFlow installed from (source or binary): binary\r\n",
      "- TensorFlow version: `pip freeze` gave me this:\r\n",
      "```\r\n",
      "tensorboard==2.4.1\r\n",
      "tensorboard-plugin-wit==1.7.0\r\n",
      "tensorflow==2.4.0\r\n",
      "tensorflow-estimator==2.4.0\r\n",
      "tensorflow-gpu==2.4.1\r\n",
      "```\r\n",
      "- Python version: 3.8\r\n",
      "- Installed using virtualenv? pip? conda?: pip\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version: ```nvcc --version``` gave me this:\r\n",
      "```\r\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\r\n",
      "Copyright (c) 2005-2020 NVIDIA Corporation\r\n",
      "Built on Thu_Jun_11_22:26:48_Pacific_Daylight_Time_2020\r\n",
      "Cuda compilation tools, release 11.0, V11.0.194\r\n",
      "Build cuda_11.0_bu.relgpu_drvr445TC445_37.28540450_0\r\n",
      "\r\n",
      "```\r\n",
      "As referenced here - \r\n",
      "\r\n",
      "*The main reason is cuda version information is not stored in ```cudnn.h``` anymore in CUDA 11.X. In CUDA 11.X, the version information seems to stored in  new ```cudnn_version.h``` file. So many build tools such as cmake depends on ```cudnn.h``` for CUDA version information cannot guess CUDA version anymore.*\r\n",
      "\r\n",
      "_Originally posted by @hongsoog in https://github.com/tensorflow/tensorflow/issues/8264#issuecomment-723446642_\r\n",
      "\r\n",
      "Am sure its cudnn 11.2 (The name of zip file)\r\n",
      "\r\n",
      "- GPU model and memory:\r\n",
      "\r\n",
      "GTX 1650 8 GB\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "\r\n",
      "So I have been trying to accelerate my Deep Learning program using GPU since ages, for the first time I was able to find my GPU and after going through the whole process, I ran\r\n",
      "\r\n",
      "```\r\n",
      "import tensorflow as tf\r\n",
      "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\r\n",
      "```\r\n",
      "It found my GPU\r\n",
      "\r\n",
      "Result - \r\n",
      "```\r\n",
      "C:\\Users\\asus\\Music_Classification\\lib\\site-packages\\numpy\\.libs\\libopenblas.QVLO2T66WEPI7JZ63PS3HMOHFEY472BC.gfortran-win_amd64.dll\r\n",
      "C:\\Users\\asus\\Music_Classification\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\r\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\r\n",
      "2021-02-08 04:01:40.630572: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n",
      "2021-02-08 04:01:45.995174: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n",
      "2021-02-08 04:01:45.997294: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n",
      "2021-02-08 04:01:46.037247: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\n",
      "pciBusID: 0000:01:00.0 name: GeForce GTX 1650 computeCapability: 7.5\r\n",
      "coreClock: 1.56GHz coreCount: 16 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 119.24GiB/s\r\n",
      "2021-02-08 04:01:46.038203: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n",
      "2021-02-08 04:01:46.086937: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n",
      "2021-02-08 04:01:46.087068: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n",
      "2021-02-08 04:01:46.117926: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n",
      "2021-02-08 04:01:46.127343: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n",
      "2021-02-08 04:01:46.196088: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n",
      "2021-02-08 04:01:46.215917: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n",
      "Num GPUs Available:  1\r\n",
      "2021-02-08 04:01:46.219540: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n",
      "2021-02-08 04:01:46.220028: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n",
      "```\r\n",
      "\r\n",
      "**Question 1** - Funny enough it found 4gig of memory only, is this because of some Shared vs DEdicated memory?\r\n",
      "\r\n",
      "I then went on to train my CNN model, and it gave me traceback:\r\n",
      "\r\n",
      "```\r\n",
      "Process finished with exit code -1073740791 (0xC0000409)\r\n",
      "```\r\n",
      "\r\n",
      "The whole error:\r\n",
      "```\r\n",
      "2021-02-08 04:05:05.982102: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n",
      "2021-02-08 04:05:05.982898: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n",
      "2021-02-08 04:05:06.021203: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\n",
      "pciBusID: 0000:01:00.0 name: GeForce GTX 1650 computeCapability: 7.5\r\n",
      "coreClock: 1.56GHz coreCount: 16 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 119.24GiB/s\r\n",
      "2021-02-08 04:05:06.021514: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n",
      "2021-02-08 04:05:06.029225: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n",
      "2021-02-08 04:05:06.029425: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n",
      "2021-02-08 04:05:06.034370: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n",
      "2021-02-08 04:05:06.036450: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n",
      "2021-02-08 04:05:06.047235: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n",
      "2021-02-08 04:05:06.050880: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n",
      "2021-02-08 04:05:06.051983: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n",
      "2021-02-08 04:05:06.052288: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n",
      "2021-02-08 04:05:06.060950: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\r\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n",
      "2021-02-08 04:05:06.064289: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\n",
      "pciBusID: 0000:01:00.0 name: GeForce GTX 1650 computeCapability: 7.5\r\n",
      "coreClock: 1.56GHz coreCount: 16 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 119.24GiB/s\r\n",
      "2021-02-08 04:05:06.064478: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n",
      "2021-02-08 04:05:06.064582: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n",
      "2021-02-08 04:05:06.064710: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n",
      "2021-02-08 04:05:06.064841: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n",
      "2021-02-08 04:05:06.064948: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n",
      "2021-02-08 04:05:06.065057: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n",
      "2021-02-08 04:05:06.065150: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n",
      "2021-02-08 04:05:06.065240: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n",
      "2021-02-08 04:05:06.065392: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n",
      "2021-02-08 04:05:07.277891: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n",
      "2021-02-08 04:05:07.278012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \r\n",
      "2021-02-08 04:05:07.278075: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \r\n",
      "2021-02-08 04:05:07.279959: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2903 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n",
      "2021-02-08 04:05:07.284087: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n",
      "  0%|          | 0/8091 [00:00<?, ?it/s]\r\n",
      "C:/Users/asus/PycharmProjects/Auto_Caption_Generator/caption_generator_collab.py:139: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\r\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\r\n",
      "  for img in tqdm(os.listdir(directory)):\r\n",
      "2021-02-08 04:05:09.583142: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n",
      "2021-02-08 04:05:10.755831: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n",
      "\r\n",
      "Process finished with exit code -1073740791 (0xC0000409)\r\n",
      "```\r\n",
      "\r\n",
      "**Problem 2** Can someone please help me with this error.\r\n",
      "\r\n",
      "**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n",
      "Summed up above\r\n",
      "\r\n",
      "**Any other info / logs**\r\n",
      "Pip Freeze \r\n",
      "```\r\n",
      "absl-py==0.11.0\r\n",
      "appdirs==1.4.4\r\n",
      "argon2-cffi==20.1.0\r\n",
      "astunparse==1.6.3\r\n",
      "async-generator==1.10\r\n",
      "attrs==20.3.0\r\n",
      "audioread==2.1.9\r\n",
      "backcall==0.2.0\r\n",
      "bleach==3.2.1\r\n",
      "cachetools==4.2.0\r\n",
      "certifi==2020.12.5\r\n",
      "cffi==1.14.4\r\n",
      "chardet==4.0.0\r\n",
      "colorama==0.4.4\r\n",
      "cycler==0.10.0\r\n",
      "decorator==4.4.2\r\n",
      "defusedxml==0.6.0\r\n",
      "entrypoints==0.3\r\n",
      "flatbuffers==1.12\r\n",
      "gast==0.3.3\r\n",
      "google-auth==1.24.0\r\n",
      "google-auth-oauthlib==0.4.2\r\n",
      "google-pasta==0.2.0\r\n",
      "grpcio==1.32.0\r\n",
      "h5py==2.10.0\r\n",
      "idna==2.10\r\n",
      "ipykernel==5.4.3\r\n",
      "ipython==7.19.0\r\n",
      "ipython-genutils==0.2.0\r\n",
      "ipywidgets==7.6.3\r\n",
      "jedi==0.18.0\r\n",
      "Jinja2==2.11.2\r\n",
      "joblib==1.0.0\r\n",
      "jsonschema==3.2.0\r\n",
      "jupyter==1.0.0\r\n",
      "jupyter-client==6.1.11\r\n",
      "jupyter-console==6.2.0\r\n",
      "jupyter-core==4.7.0\r\n",
      "jupyter-http-over-ws==0.0.8\r\n",
      "jupyterlab-pygments==0.1.2\r\n",
      "jupyterlab-widgets==1.0.0\r\n",
      "Keras==2.4.3\r\n",
      "Keras-Preprocessing==1.1.2\r\n",
      "kiwisolver==1.3.1\r\n",
      "librosa==0.8.0\r\n",
      "llvmlite==0.35.0\r\n",
      "Markdown==3.3.3\r\n",
      "MarkupSafe==1.1.1\r\n",
      "matplotlib==3.3.3\r\n",
      "mistune==0.8.4\r\n",
      "nbclient==0.5.1\r\n",
      "nbconvert==6.0.7\r\n",
      "nbformat==5.1.2\r\n",
      "nest-asyncio==1.4.3\r\n",
      "notebook==6.2.0\r\n",
      "numba==0.52.0\r\n",
      "numpy==1.19.5\r\n",
      "oauthlib==3.1.0\r\n",
      "opt-einsum==3.3.0\r\n",
      "packaging==20.8\r\n",
      "pandas==1.2.0\r\n",
      "pandocfilters==1.4.3\r\n",
      "parso==0.8.1\r\n",
      "pickleshare==0.7.5\r\n",
      "Pillow==8.0.1\r\n",
      "pooch==1.3.0\r\n",
      "prometheus-client==0.9.0\r\n",
      "prompt-toolkit==3.0.10\r\n",
      "protobuf==3.14.0\r\n",
      "pyasn1==0.4.8\r\n",
      "pyasn1-modules==0.2.8\r\n",
      "pycparser==2.20\r\n",
      "Pygments==2.7.4\r\n",
      "pyparsing==2.4.7\r\n",
      "PyQt5==5.15.2\r\n",
      "PyQt5-sip==12.8.1\r\n",
      "pyrsistent==0.17.3\r\n",
      "python-dateutil==2.8.1\r\n",
      "python-speech-features==0.6\r\n",
      "pytz==2020.5\r\n",
      "pywin32==300\r\n",
      "pywinpty==0.5.7\r\n",
      "PyYAML==5.3.1\r\n",
      "pyzmq==21.0.1\r\n",
      "qtconsole==5.0.1\r\n",
      "QtPy==1.9.0\r\n",
      "requests==2.25.1\r\n",
      "requests-oauthlib==1.3.0\r\n",
      "resampy==0.2.2\r\n",
      "rsa==4.7\r\n",
      "scikit-learn==0.24.0\r\n",
      "scipy==1.6.0\r\n",
      "Send2Trash==1.5.0\r\n",
      "six==1.15.0\r\n",
      "SoundFile==0.10.3.post1\r\n",
      "tensorboard==2.4.1\r\n",
      "tensorboard-plugin-wit==1.7.0\r\n",
      "tensorflow==2.4.0\r\n",
      "tensorflow-estimator==2.4.0\r\n",
      "tensorflow-gpu==2.4.1\r\n",
      "termcolor==1.1.0\r\n",
      "terminado==0.9.2\r\n",
      "testpath==0.4.4\r\n",
      "threadpoolctl==2.1.0\r\n",
      "tornado==6.1\r\n",
      "tqdm==4.56.0\r\n",
      "traitlets==5.0.5\r\n",
      "typing-extensions==3.7.4.3\r\n",
      "urllib3==1.26.2\r\n",
      "wcwidth==0.2.5\r\n",
      "webencodings==0.5.1\r\n",
      "Werkzeug==1.0.1\r\n",
      "widgetsnbextension==3.5.1\r\n",
      "wrapt==1.12.1\r\n",
      "```\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:gpu\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Failed to deploy custom Tensorflow lite select ops .aar to Android\n",
      "issue body -  **System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Huawei P30 Lite\r\n",
      "- TensorFlow installed from (source or binary): Source\r\n",
      "- TensorFlow version: master - 2.4.1+\r\n",
      "- Python version: 3.6.9\r\n",
      "- Installed using virtualenv? pip? conda?:\r\n",
      "- Bazel version (if compiling from source): 3.7.2\r\n",
      "- GCC/Compiler version (if compiling from source): gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\r\n",
      "- CUDA/cuDNN version:\r\n",
      "- GPU model and memory:\r\n",
      "\r\n",
      "I have followed the instructions here: https://www.tensorflow.org/lite/guide/build_android and here: https://www.tensorflow.org/lite/guide/reduce_binary_size to build custom versions of tensorflow-lite.aar and tensorflow-lite-select-tf-ops.aar. However, when I try to run the model on Android, I get this error:\r\n",
      "\r\n",
      "```\r\n",
      "java.lang.UnsatisfiedLinkError: Failed to load native TensorFlow Lite methods. Check that the correct native libraries are present, and, if using a custom native library, have been properly loaded via System.loadLibrary():\r\n",
      "      java.lang.UnsatisfiedLinkError: dlopen failed: cannot locate symbol \"_ZN6tflite3ops6custom38Register_TFLITE_DETECTION_POST_PROCESSEv\" referenced by \"/data/app/com.example.myapplication-2NF_GHdjAnMJHFkmSCcPvw==/lib/arm64/libtensorflowlite_jni.so\"...\r\n",
      "        at org.tensorflow.lite.TensorFlowLite.init(TensorFlowLite.java:80)\r\n",
      "        at com.example.myapplication.MainActivity.onCreate(MainActivity.java:26)\r\n",
      "```\r\n",
      "\r\n",
      "This appears to be happening before loading the model, it seems to be a linking issue, though I have just build the two .aar files from source.\r\n",
      "\r\n",
      "Steps followed:\r\n",
      "\r\n",
      "I'm using Docker to build the aar files. I downloaded the Docker file from [here](https://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/tools/dockerfiles/tflite-android.Dockerfile) and ran:\r\n",
      "`docker build . -t tflite-builder -f tflite-android.Dockerfile`\r\n",
      "followed by:\r\n",
      "`docker run -it -v \"local_dir\":/host_dir tflite-builder bash`\r\n",
      "\r\n",
      "Inside the docker container, I ran:\r\n",
      "`curl -o build_aar_with_docker.sh   https://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/lite/tools/build_aar_with_docker.sh &&   chmod +x build_aar_with_docker.sh`\r\n",
      "\r\n",
      "followed by:\r\n",
      "`echo y | /bin/bash build_aar_with_docker.sh --input_models=/host_dir/my_model.tflite   --target_archs=arm64-v8a,armeabi-v7a   --checkpoint=master `\r\n",
      "\r\n",
      "My model is defined (here)[https://github.com/barrypitman/tensorflow_LPRnet] and uses 1 custom op - ctc_beam_search_decoder. \r\n",
      "\r\n",
      "After several hours, the build finished with this message:\r\n",
      "```\r\n",
      "INFO: Analyzed target //tmp:tensorflow-lite-select-tf-ops (308 packages loaded, 33657 targets configured).\r\n",
      "INFO: Found 1 target...\r\n",
      "Target //tmp:tensorflow-lite-select-tf-ops up-to-date:\r\n",
      "  bazel-bin/tmp/tensorflow-lite-select-tf-ops.aar\r\n",
      "INFO: Elapsed time: 35622.438s, Critical Path: 3942.80s\r\n",
      "INFO: 14237 processes: 861 internal, 13376 local.\r\n",
      "INFO: Build completed successfully, 14237 total actions\r\n",
      "Output can be found here:\r\n",
      "tensorflow-lite.aar\r\n",
      "tensorflow-lite-select-tf-ops.aar\r\n",
      "```\r\n",
      "\r\n",
      "I copied the tensorflow-lite.aar and tensorflow-lite-select-tf-ops.aar files into a sample Android application (attached), and I got the above-mentioned error:\r\n",
      "```\r\n",
      "java.lang.UnsatisfiedLinkError: Failed to load native TensorFlow Lite methods. Check that the correct native libraries are present, and, if using a custom native library, have been properly loaded via System.loadLibrary():\r\n",
      "      java.lang.UnsatisfiedLinkError: dlopen failed: cannot locate symbol \"_ZN6tflite3ops6custom38Register_TFLITE_DETECTION_POST_PROCESSEv\" referenced by \"/data/app/com.example.myapplication-2NF_GHdjAnMJHFkmSCcPvw==/lib/arm64/libtensorflowlite_jni.so\"...\r\n",
      "```\r\n",
      "\r\n",
      "I've tried generating the tensorflow-lite-select-tf-ops.aar file against the 2.4.0 and 2.4.1 branches, but I'm getting similar errors. This issue appears to be similar to [45153](https://github.com/tensorflow/tensorflow/issues/45153). \r\n",
      "\r\n",
      "I built the two .aar files against the following tensorflow commit:\r\n",
      "```\r\n",
      "root@8eb4e1fb67bf:/tensorflow_src# git rev-parse HEAD\r\n",
      "6ac306c372287e522168da6931fd751203915d46\r\n",
      "```\r\n",
      "\r\n",
      "I've attached the two .aar files as well as the demo Android app. I haven't included the model, because the app is crashing before trying to load the model.\r\n",
      "[tensorflow-lite-master-20210207-docker.zip](https://github.com/tensorflow/tensorflow/files/5939881/tensorflow-lite-master-20210207-docker.zip)\r\n",
      "[AndroidHelloWorld.zip](https://github.com/tensorflow/tensorflow/files/5939884/AndroidHelloWorld.zip)\r\n",
      "\r\n",
      "\r\n",
      "**Any other info / logs**\r\n",
      "Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n",
      "\n",
      "issue labels - \n",
      "comp:lite\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  how to find what tensorflow/keras release contains a specific bug fix\n",
      "issue body -  <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n",
      "\r\n",
      "This is related to a known bug (see below in italics), but I'm trying to figure out how to install a version of tf/keras which has the fix.\r\n",
      "**System information**\r\n",
      "- OS Platform and Distribution -> Ubuntu 20.0.4\r\n",
      "- TensorFlow installed from -> docker tensorflow-gpu \r\n",
      "- TensorFlow version: 2.5.0-dev20210204 from docker tensorflow/tensorflow:nightly-gpu\r\n",
      "- Python version: 3.6.9 (default, Oct  8 2020, 12:12:24) \\n[GCC 8.4.0]\r\n",
      "- Installed usinga  Dockerfile to add some required Python libraries to tensorflow-gpu image\r\n",
      "- CUDA/cuDNN version:Cuda compilation tools, release 11.0, V11.0.221\r\n",
      "Build cuda_11.0_bu.TC445_37.28845127_0\r\n",
      "- GPU model and memory: Nvidia RTX 2080 Ti\r\n",
      "\r\n",
      "I'm training a large (millions of parameters) NN to analyze audio files (speech vs. music).  After 28/50 epochs I hit a known bug which is (to my knowledge) fixed already.  I'm not sure how to find out what tf/keras release the fix is in or if my docker image has the fix.  I'm using a docker image since it was recommended as the easiest way to get version compatibility between tf/keras and CUDA. \r\n",
      "\r\n",
      "This is the bug description and reference to the fix:\r\n",
      "\r\n",
      "_Tensorflow 2.1 Error “when finalizing GeneratorDataset iterator” - a memory leak? #37515\r\n",
      "\r\n",
      "URL: https://github.com/tensorflow/tensorflow/commit/e918c6e6fab5d0005fcde83d57e92b70343d3553\r\n",
      "Fixing a memory leak in Keras.\r\n",
      "Fixes: #37515\r\n",
      "PiperOrigin-RevId: 302568217\r\n",
      "Change-Id: I28d0eaf3602fea0461901680df24899f135ce649_\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.5\n",
      "comp:keras\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Unexpected TFDS Shuffling behavior\n",
      "issue body -  dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\r\n",
      "dataset = dataset.shuffle(buffer_size=256)\r\n",
      "dataset = dataset.take(1)\r\n",
      "print(list(dataset.as_numpy_iterator()))\r\n",
      "dataset = dataset.repeat(5)\r\n",
      "print(list(dataset.as_numpy_iterator()))\r\n",
      "\r\n",
      ">>> [2]\r\n",
      ">>> [1, 2, 3, 3, 2]\r\n",
      "\r\n",
      "Is this on purpose? I was expecting [2, 2, 2, 2, 2], as I would expect the dataset in memory to be replaced with the subset that I took in take(1), rather than now having (I assume) two datasets in memory, the subset dataset + the original dataset.\r\n",
      "\r\n",
      "Thanks =) \n",
      "issue labels - \n",
      "comp:data\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  why tensorflow has so much ugly designed api?  be pythonic please...\n",
      "issue body -  This template is for miscellaneous issues not covered by the other issue categories.\r\n",
      "\r\n",
      "For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n",
      "\r\n",
      "If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n",
      "\r\n",
      "For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n",
      "\n",
      "issue labels - \n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  Confiuguration error \n",
      "issue body -  <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- OS Platform and Distribution: Linux Ubuntu 18 with qemu and docker aarch64 image for jetson platform\r\n",
      "\r\n",
      "- TensorFlow installed from (source or binary): source\r\n",
      "- TensorFlow version: 1.14.0\r\n",
      "- Python version: 3\r\n",
      " \r\n",
      "- Bazel version (if compiling from source): 0.24.1\r\n",
      "- GCC/Compiler version (if compiling from source): gcc (Ubuntu/Linaro 7.5.0-3ubuntu1~18.04) 7.5.0\r\n",
      "- CUDA/cuDNN version: 10.2 8\r\n",
      "- GPU model and memory: gtx 1070\r\n",
      "\r\n",
      "\r\n",
      "Hi everyone I have a problem in the configuration phase. When asking for Cuda and cudnn library I get this error:\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"third_party/gpus/find_cuda_config.py\", line 497, in <module>\r\n",
      "    main()\r\n",
      "  File \"third_party/gpus/find_cuda_config.py\", line 489, in main\r\n",
      "    for key, value in sorted(find_cuda_config().items()):\r\n",
      "  File \"third_party/gpus/find_cuda_config.py\", line 468, in find_cuda_config\r\n",
      "    result.update(_find_cudnn_config(cudnn_paths, cudnn_version))\r\n",
      "  File \"third_party/gpus/find_cuda_config.py\", line 346, in _find_cudnn_config\r\n",
      "    get_header_version)\r\n",
      "  File \"third_party/gpus/find_cuda_config.py\", line 235, in _find_header\r\n",
      "    required_version, get_version)\r\n",
      "  File \"third_party/gpus/find_cuda_config.py\", line 224, in _find_versioned_file\r\n",
      "    actual_version = get_version(file)\r\n",
      "  File \"third_party/gpus/find_cuda_config.py\", line 342, in get_header_version\r\n",
      "    return \".\".join(version)\r\n",
      "  File \"third_party/gpus/find_cuda_config.py\", line 341, in <genexpr>\r\n",
      "    for name in (\"CUDNN_MAJOR\", \"CUDNN_MINOR\", \"CUDNN_PATCHLEVEL\"))\r\n",
      "  File \"third_party/gpus/find_cuda_config.py\", line 123, in _get_header_version\r\n",
      "    for line in io.open(path, \"r\", encoding=\"utf-8\").readlines():\r\n",
      "  File \"/usr/lib/python3.6/codecs.py\", line 321, in decode\r\n",
      "    (result, consumed) = self._buffer_decode(data, self.errors, final)\r\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xb7 in position 18: invalid start byte\r\n",
      "Asking for detailed CUDA configuration...\r\n",
      "\r\n",
      "\r\n",
      "**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n",
      "\r\n",
      "Please specify the comma-separated list of base paths to look for CUDA libraries and headers. [Leave empty to use the default]: /folder_shared/lib64,/usr/local/cuda-10.2/targets/aarch64-linux/include/,/usr/local/cuda-10.2/bin/,/usr/local/cuda-10.2/nvvm/libdevice,/usr/local/cuda-10.2/nvvmx/libdevice,/usr/local/cuda-10.2/nvvmx/libdevice/libdevice.10.bc,/usr/local/cuda-10.2,/usr/include,/folder_shared,/folder_shared/cudnn8/\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 1.14\n",
      "stalled\n",
      "stat:awaiting response\n",
      "subtype: ubuntu/linux\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  Fix crash of tf.strings.substr when pos and len have different shapes\n",
      "issue body -  This PR tries to address the issue raised in #46900 where\r\n",
      "tf.strings.substr will crash when pos and len have different shapes.\r\n",
      "According to the documentation of tf.strings.substr, ValueError\r\n",
      "should be raised instead when pos and len does not have the same shape.\r\n",
      "\r\n",
      "This PR add shape check in kernel to allows grace error throw (instead of crash).\r\n",
      "\r\n",
      "This PR fixes #46900.\r\n",
      "\r\n",
      "Signed-off-by: Yong Tang <yong.tang.github@outlook.com>\n",
      "issue labels - \n",
      "cla: yes\n",
      "prtype:bugfix\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  Tensorflow implementation in Julia\n",
      "issue body -  <em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n",
      "\r\n",
      "\r\n",
      "**System information**\r\n",
      "- TensorFlow version (you are using): 2.x\r\n",
      "- Are you willing to contribute it (Yes/No):\r\n",
      "Yes\r\n",
      "\r\n",
      "\r\n",
      "**Describe the feature and the current behavior/state.**\r\n",
      "Currently, there is no Tensorflow implementation in Julia. Since Julia is the next major upcoming scientific computation language and it is much faster than Python while maintaining similar intuitiveness, I believe that we should have a Julia-version of TF.\r\n",
      "\r\n",
      "**Will this change the current api? How?**\r\n",
      "This will not change existing features but will port them over to Julia as well.\r\n",
      "\r\n",
      "**Who will benefit with this feature?**\r\n",
      "The entire scientific community will be benefitted as more and more people will start shifting to Julia in the near future, as more and more packages are being developed for it.\r\n",
      "**Any Other info.**\r\n",
      "\n",
      "issue labels - \n",
      "type:feature\n",
      "\n",
      "\n",
      "issue title -  implement changes necessary to support new bconv2d API in lib_nn\n",
      "issue body -  \n",
      "issue labels - \n",
      "cla: no\n",
      "size:XL\n",
      "\n",
      "\n",
      "issue title -  XLA: Allow devices to indicate whether the executable can be run async\n",
      "issue body -  This allow an XLA device/backend to indicate if executables can be executed asynchronously.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:xla\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  Add CAST and ZEROS_LIKE tests in lite/micro/kernels/BUILD\n",
      "issue body -  This is a fix for micro ops CAST and ZEROS_LIKE (issues #45608 and #46049).\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  [INTEL MKL] Adding padding_list in mkl quantized conv2d per channel op\n",
      "issue body -  Adding padding_list in mkl quantized conv2d per channel op in the attribute. It is needed to do pad fusion with quantized conv2d per channel.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:mkl\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Fix kernel_exp_test with the Xtensa toolchain.\n",
      "issue body -  Underlying issue is a bug in the EXPECT_NEAR macro, as described in #46960\r\n",
      "\r\n",
      "Also,\r\n",
      "  * added an exp_test rule to the BUILD file.\r\n",
      "  * changed the golden value computation to make use of std::exp instead of hard-coded values. This is closer to the TfLite test as well.\r\n",
      "\r\n",
      "Manually confirmed that the following command passes:\r\n",
      "```\r\n",
      "make -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=fusion_f1 XTENSA_CORE=F1_190305_swupgrade test -j8\r\n",
      "```\r\n",
      "\r\n",
      "Fixes #46960\r\n",
      "\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  TF_LITE_MICRO_EXPECT_NEAR(inf, inf) gives incorrect result for some platforms.\n",
      "issue body -  micro/kernels/exp_test.cc checks that two inf values are near eachother:\r\n",
      "https://github.com/tensorflow/tensorflow/blob/ed22f400428a669c1c6e4553cd7f4900abeaf954/tensorflow/lite/micro/kernels/exp_test.cc#L67-L72\r\n",
      "\r\n",
      "This works ok for all the CI targets, but broke the xtensa build:\r\n",
      "```\r\n",
      "make -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=fusion_f1 XTENSA_CORE=F1_190305_swupgrade test_kernel_exp_test -j8\r\n",
      "```\r\n",
      "fails with:\r\n",
      "```\r\n",
      "Testing SingleDim\r\n",
      "expected_output_data[i] (Inf) near output_data[i] (Inf) failed at tensorflow/lite/micro/kernels/exp_test.cc:54\r\n",
      "0/1 tests passed\r\n",
      "~~~SOME TESTS FAILED~~~\r\n",
      "```\r\n",
      "\r\n",
      "The underlying issue that the EXPECT_NEAR_MACRO is taking a difference of two infinities which at least with the xtensa toolchain can give a `nan`, which in turn results in the check failing, even though inf==inf is true:\r\n",
      "https://github.com/tensorflow/tensorflow/blob/ed22f400428a669c1c6e4553cd7f4900abeaf954/tensorflow/lite/micro/testing/micro_test.h#L153-L165\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "comp:lite\n",
      "comp:micro\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  [INTEL MKL] Adding padding list in quantized conv2d per channel\n",
      "issue body -  Supporting padding list in attribute in Quantized Conv2D per Channel op both in block format and Native format. Changed pb file for api compatibility.\n",
      "issue labels - \n",
      "cla: yes\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  model train InvalidArgumentError:Input is empty\n",
      "issue body -  <em>Please make sure that this is a bug. As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:bug_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 18\r\n",
      "- TensorFlow installed from (source or binary): source\r\n",
      "- TensorFlow version (use command below): 2.4(latest)\r\n",
      "- Python version:latest\r\n",
      "\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "I am about to train the model using the model_main_tf2.py and it seems to show this error and stop. I have tried it on centernet_resnet50_v1_fpn_512x512 and ssd_mobilenet_v2_fpnlite_320x320 and it is the same for both. when I ran this on AWS it did not show any errors but it did not seem to generate any checkpoints either.\r\n",
      "**Describe the expected behavior**\r\n",
      "Expect it to run without showing any errors\r\n",
      "\r\n",
      "I guess the error is in the tfrecord file but all the paths and the values have been verified.\r\n",
      "the code for generating tfrecord\r\n",
      "writer = tf.io.TFRecordWriter(outputPath)\r\n",
      "        total = 0\r\n",
      "        for k in keys:\r\n",
      "\r\n",
      "            encoded = np.asarray(Image.open(k))\r\n",
      "            h,w,ch=encoded.shape\r\n",
      "            filename = k.split(os.path.sep)[-1]\r\n",
      "            encoding = filename[filename.rfind(\".\") + 1:]\r\n",
      "\r\n",
      "            image = encoded.tostring() \r\n",
      "            xMins,xMaxs,yMins,yMaxs,textLabels,classes=[],[],[],[],[],[]\r\n",
      "            for (label,(startX, startY, endX, endY)) in D[k]:\r\n",
      "                endX,endY=endX-startX,endY-startY\r\n",
      "                xMin = (startX+endX/2) / w\r\n",
      "                xMax = endX / w\r\n",
      "                yMin = (startY+endY/2) / h\r\n",
      "                yMax = endY / h\r\n",
      "                if xMin<0 or xMax<0 or yMin<0 or yMax<0 or xMin>1 or xMax>1 or yMin>1 or yMax>1:\r\n",
      "                    continue\r\n",
      "                xMins.append(xMin)\r\n",
      "                xMaxs.append(xMax)\r\n",
      "                yMins.append(yMin)\r\n",
      "                yMaxs.append(yMax)\r\n",
      "                textLabels.append(label.encode(\"utf-8\"))\r\n",
      "                classes.append(CLASSES[label])\r\n",
      "\r\n",
      "            total += 1\r\n",
      "\r\n",
      "            classes=np.array(classes)            \r\n",
      "            example = tf.train.Example(features=tf.train.Features(feature={\r\n",
      "           'image/height': int64_feature(h),\r\n",
      "           'image/width': int64_feature(w),\r\n",
      "           'image/filename': bytes_feature(filename.encode('utf-8')),\r\n",
      "           'image/source_id': bytes_feature(filename.encode('utf-8')),\r\n",
      "           'image/image_raw': bytes_feature(image),\r\n",
      "           'image/format': bytes_feature(encoding.encode('utf-8')),\r\n",
      "           'image/object/bbox/xmin': float_list_feature(xMins),\r\n",
      "           'image/object/bbox/xmax': float_list_feature(xMaxs),\r\n",
      "           'image/object/bbox/ymin': float_list_feature(yMins),\r\n",
      "           'image/object/bbox/ymax': float_list_feature(yMaxs),\r\n",
      "           'image/object/class/text': bytes_list_feature(textLabels),\r\n",
      "           'image/object/class/label': int64_list_feature(classes),\r\n",
      " }))\r\n",
      "\r\n",
      "            writer.write(example.SerializeToString())\r\n",
      "\r\n",
      "        writer.close()\r\n",
      "\r\n",
      "**Other info / logs** \r\n",
      "2021-02-05 23:08:11.056603: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\r\n",
      "2021-02-05 23:08:11.056634: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n",
      "2021-02-05 23:08:13.273009: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n",
      "2021-02-05 23:08:13.273186: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\r\n",
      "2021-02-05 23:08:13.273204: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\r\n",
      "2021-02-05 23:08:13.273234: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (kingsman-desktop): /proc/driver/nvidia/version does not exist\r\n",
      "2021-02-05 23:08:13.274261: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n",
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\r\n",
      "W0205 23:08:13.275029 140202867857216 cross_device_ops.py:1321] There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\r\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\r\n",
      "I0205 23:08:13.275206 140202867857216 mirrored_strategy.py:350] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\r\n",
      "INFO:tensorflow:Maybe overwriting train_steps: None\r\n",
      "I0205 23:08:13.295096 140202867857216 config_util.py:552] Maybe overwriting train_steps: None\r\n",
      "INFO:tensorflow:Maybe overwriting use_bfloat16: False\r\n",
      "I0205 23:08:13.295231 140202867857216 config_util.py:552] Maybe overwriting use_bfloat16: False\r\n",
      "WARNING:tensorflow:From /home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/object_detection/model_lib_v2.py:523: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "rename to distribute_datasets_from_function\r\n",
      "W0205 23:08:13.363639 140202867857216 deprecation.py:339] From /home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/object_detection/model_lib_v2.py:523: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "rename to distribute_datasets_from_function\r\n",
      "INFO:tensorflow:Reading unweighted datasets: ['/home/kingsman/deepl/records/train.tfrecord']\r\n",
      "I0205 23:08:13.371121 140202867857216 dataset_builder.py:163] Reading unweighted datasets: ['/home/kingsman/deepl/records/train.tfrecord']\r\n",
      "INFO:tensorflow:Reading record datasets for input file: ['/home/kingsman/deepl/records/train.tfrecord']\r\n",
      "I0205 23:08:13.371364 140202867857216 dataset_builder.py:80] Reading record datasets for input file: ['/home/kingsman/deepl/records/train.tfrecord']\r\n",
      "INFO:tensorflow:Number of filenames to read: 1\r\n",
      "I0205 23:08:13.371473 140202867857216 dataset_builder.py:81] Number of filenames to read: 1\r\n",
      "WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\r\n",
      "W0205 23:08:13.371576 140202867857216 dataset_builder.py:88] num_readers has been reduced to 1 to match input file shards.\r\n",
      "WARNING:tensorflow:From /home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/object_detection/builders/dataset_builder.py:105: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`.\r\n",
      "W0205 23:08:13.373951 140202867857216 deprecation.py:339] From /home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/object_detection/builders/dataset_builder.py:105: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`.\r\n",
      "WARNING:tensorflow:From /home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/object_detection/builders/dataset_builder.py:237: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use `tf.data.Dataset.map()\r\n",
      "W0205 23:08:13.397681 140202867857216 deprecation.py:339] From /home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/object_detection/builders/dataset_builder.py:237: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use `tf.data.Dataset.map()\r\n",
      "WARNING:tensorflow:From /home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py:201: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\r\n",
      "W0205 23:08:20.996112 140202867857216 deprecation.py:339] From /home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py:201: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\r\n",
      "WARNING:tensorflow:From /home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py:201: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\r\n",
      "W0205 23:08:24.354368 140202867857216 deprecation.py:339] From /home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py:201: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\r\n",
      "WARNING:tensorflow:From /home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/object_detection/inputs.py:281: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use `tf.cast` instead.\r\n",
      "W0205 23:08:26.214364 140202867857216 deprecation.py:339] From /home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/object_detection/inputs.py:281: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use `tf.cast` instead.\r\n",
      "2021-02-05 23:08:28.723569: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n",
      "2021-02-05 23:08:28.745519: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3400075000 Hz\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/eager/context.py\", line 2113, in execution_mode\r\n",
      "    yield\r\n",
      "  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 733, in _next_internal\r\n",
      "    output_shapes=self._flat_output_shapes)\r\n",
      "  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 2579, in iterator_get_next\r\n",
      "    _ops.raise_from_not_ok_status(e, name)\r\n",
      "  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 6862, in raise_from_not_ok_status\r\n",
      "    six.raise_from(core._status_to_exception(e.code, message), None)\r\n",
      "  File \"<string>\", line 3, in raise_from\r\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: Input is empty.\r\n",
      "\t [[{{node case/cond/else/_10/case/cond/cond_jpeg/else/_105/case/cond/cond_jpeg/decode_image/DecodeImage}}]]\r\n",
      "\t [[MultiDeviceIteratorGetNextFromShard]]\r\n",
      "\t [[RemoteCall]] [Op:IteratorGetNext]\r\n",
      "\r\n",
      "During handling of the above exception, another exception occurred:\r\n",
      "\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"object_detection/model_main_tf2.py\", line 113, in <module>\r\n",
      "    tf.compat.v1.app.run()\r\n",
      "  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n",
      "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n",
      "  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/absl/app.py\", line 300, in run\r\n",
      "    _run_main(main, args)\r\n",
      "  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/absl/app.py\", line 251, in _run_main\r\n",
      "    sys.exit(main(argv))\r\n",
      "  File \"object_detection/model_main_tf2.py\", line 110, in main\r\n",
      "    record_summaries=FLAGS.record_summaries)\r\n",
      "  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/object_detection/model_lib_v2.py\", line 566, in train_loop\r\n",
      "    unpad_groundtruth_tensors)\r\n",
      "  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/object_detection/model_lib_v2.py\", line 344, in load_fine_tune_checkpoint\r\n",
      "    features, labels = iter(input_dataset).next()\r\n",
      "  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/distribute/input_lib.py\", line 628, in next\r\n",
      "    return self.__next__()\r\n",
      "  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/distribute/input_lib.py\", line 632, in __next__\r\n",
      "    return self.get_next()\r\n",
      "  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/distribute/input_lib.py\", line 663, in get_next\r\n",
      "    self._iterators[i].get_next_as_list_static_shapes(new_name))\r\n",
      "  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/distribute/input_lib.py\", line 1619, in get_next_as_list_static_shapes\r\n",
      "    return self._format_data_list_with_options(self._iterator.get_next())\r\n",
      "  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py\", line 585, in get_next\r\n",
      "    result.append(self._device_iterators[i].get_next())\r\n",
      "  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 800, in get_next\r\n",
      "    return self._next_internal()\r\n",
      "  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 739, in _next_internal\r\n",
      "    return structure.from_compatible_tensor_list(self._element_spec, ret)\r\n",
      "  File \"/usr/lib/python3.6/contextlib.py\", line 99, in __exit__\r\n",
      "    self.gen.throw(type, value, traceback)\r\n",
      "  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/eager/context.py\", line 2116, in execution_mode\r\n",
      "    executor_new.wait()\r\n",
      "  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/eager/executor.py\", line 69, in wait\r\n",
      "    pywrap_tfe.TFE_ExecutorWaitForAllPendingNodes(self._handle)\r\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: Input is empty.\r\n",
      "\t [[{{node case/cond/else/_10/case/cond/cond_jpeg/else/_105/case/cond/cond_jpeg/decode_image/DecodeImage}}]]\r\n",
      "\t [[MultiDeviceIteratorGetNextFromShard]]\r\n",
      "\t [[RemoteCall]]\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:apis\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  No registered kernels for SobolSample.\n",
      "issue body -  I tested this on two similar machines (with slightly different versions, apparently)\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 LTS\r\n",
      "- TensorFlow installed from (source or binary): pip3 binary\r\n",
      "- TensorFlow version (use command below): 2.3.0 and 2.3.1\r\n",
      "- Python version: 3.6.8 and 3.8.0\r\n",
      "- CUDA/cuDNN version: not relevant\r\n",
      "- GPU model and memory: not relevant; but GTX1660Ti and RTX2080Ti\r\n",
      "\r\n",
      "v2.3.0-54-gfcc4b966f1\r\n",
      "v2.3.0-rc2-23-gb36436b087\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "`tf.math.sobol_sample(4, 10)` produces following error:\r\n",
      "```py\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"<stdin>\", line 1, in <module>\r\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\r\n",
      "    return target(*args, **kwargs)\r\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/math_ops.py\", line 4846, in sobol_sample\r\n",
      "    return gen_math_ops.sobol_sample(dim, num_results, skip, dtype=dtype)\r\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 9280, in sobol_sample\r\n",
      "    return sobol_sample_eager_fallback(\r\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 9312, in sobol_sample_eager_fallback\r\n",
      "    _result = _execute.execute(b\"SobolSample\", 1, inputs=_inputs_flat,\r\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\r\n",
      "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n",
      "tensorflow.python.framework.errors_impl.NotFoundError: Could not find device for node: {{node SobolSample}} = SobolSample[dtype=DT_FLOAT]\r\n",
      "All kernels registered for op SobolSample:\r\n",
      "  <no registered kernels>\r\n",
      " [Op:SobolSample]\r\n",
      "```\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "I don't know why this kernel is not registered. It seems the code is there (although I can't validate the correctness of it):\r\n",
      "https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/core/kernels/sobol_op.cc#L175\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "```py\r\n",
      "import tensorflow as tf\r\n",
      "tf.math.sobol_sample(4, 10)\r\n",
      "```\r\n",
      "\r\n",
      "**Other info / logs** Include any logs or source code that would be helpful to\r\n",
      "diagnose the problem. If including tracebacks, please include the full\r\n",
      "traceback. Large logs and files should be attached.\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.3\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  TensorFlow custom loss function error- ValueError: No gradients provided for any variable\n",
      "issue body -  <em>Please make sure that this is a bug. As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:bug_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "- TensorFlow installed from (source or binary): Anaconda\r\n",
      "- TensorFlow version (use command below): 2.2 gpu\r\n",
      "- Python version: 3.7.7\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version: 7.6.5\r\n",
      "- GPU model and memory: NVIDIA Tesla v100\r\n",
      "\r\n",
      "You can collect some of this information using our environment capture\r\n",
      "[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n",
      "You can also obtain the TensorFlow version with:\r\n",
      "1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n",
      "2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n",
      "\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "I have written custom loss function which is Loss(pred,label)= { 0.0 if pred−label<=0.1, 1.0 elsewhere.\r\n",
      "\r\n",
      "Here is my code: \r\n",
      "'''\r\n",
      "comparing_tensor = tf.convert_to_tensor([0.1, 0.1, 0.1])\r\n",
      "def custom_loss(y_pred, y_true):\r\n",
      "    loss_tensor = tf.raw_ops.Abs(x=y_pred - y_true) # get the abs diff between y_true and y_pred\r\n",
      "    boolean_tensor = tf.raw_ops.Greater(x=loss_tensor, y=comparing_tensor) # get a boolean tensor based on Greater operation. Example: [True, False, True] \r\n",
      "    binary_tensor = tf.raw_ops.Cast(x=boolean_tensor, DstT=tf.float32) # convert boolean to bianry tensor Example: [1.0, 0.0, 1.0]\r\n",
      "    mean_tensor= tf.raw_ops.Mean(input=binary_tensor, axis=-1) # get mean of binary tensor, 2/3=0.66 \r\n",
      "    loss = tf.raw_ops.Reshape(tensor=mean_tensor, shape=(1,1), name=None) # reshape mean tensor to get desired shape\r\n",
      "    return loss\r\n",
      "'''\r\n",
      "\r\n",
      "\r\n",
      "The error I am getting is :\r\n",
      "\r\n",
      "ValueError: No gradients provided for any variable: ['conv2d/kernel:0', 'conv2d/bias:0', 'conv2d_1/kernel:0', 'conv2d_1/bias:0', 'conv2d_2/kernel:0', 'conv2d_2/bias:0', 'conv2d_3/kernel:0', 'conv2d_3/bias:0', 'conv2d_4/kernel:0', 'conv2d_4/bias:0', 'dense/kernel:0', 'dense/bias:0', 'x/kernel:0', 'x/bias:0'].\r\n",
      "**Describe the expected behavior**\r\n",
      "The error should not be there\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "\r\n",
      "Provide a reproducible test case that is the bare minimum necessary to generate\r\n",
      "the problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n",
      "\r\n",
      "**Other info / logs** Include any logs or source code that would be helpful to\r\n",
      "diagnose the problem. If including tracebacks, please include the full\r\n",
      "traceback. Large logs and files should be attached.\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.2\n",
      "comp:ops\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Deprecate legacy hdfs file system\n",
      "issue body -  This PR is part of the effort to switch to modular file system support\r\n",
      "by deprecates hdfs file system and direct user to use modular file system\r\n",
      "from tensorflow-io instead.\r\n",
      "\r\n",
      "A `TF_ENABLE_LEGACY_FILESYSTEM=1` will allow the usage of legacy hdfs file\r\n",
      "system the same way as before (a warning will be displayed).\r\n",
      "\r\n",
      "/cc @mihaimaruseac  @vnvo2409 @tensorflow/sig-io-maintainers @burgerkingeater \r\n",
      "\r\n",
      "**Update**: See related changes in https://github.com/tensorflow/io/pull/1309 where hdfs is enabled in tensorflow/io.\r\n",
      "\r\n",
      "Signed-off-by: Yong Tang <yong.tang.github@outlook.com>\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:core\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  Makefile fails when setting CO_PROCESSOR=ethos-u\n",
      "issue body -  @tensorflow/micro\r\n",
      "\r\n",
      "make: *** No rule to make target 'tensorflow/lite/micro/tools/make/ext_libs/ethos-u.inc'.  Stop.\r\n",
      "\r\n",
      "**System information**\r\n",
      "\r\n",
      "- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS \r\n",
      "- Tensorflow from most recent commit\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "\r\n",
      "```\r\n",
      "make: *** No rule to make target 'tensorflow/lite/micro/tools/make/ext_libs/ethos-u.inc'.  Stop.\r\n",
      "```\r\n",
      "\r\n",
      "**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n",
      "\r\n",
      " ```\r\n",
      "make -f tensorflow/lite/micro/tools/make/Makefile -j8 TARGET=cortex_m_generic TARGET_ARCH=cortex-m55 OPTIMIZED_KERNEL_DIR=cmsis-nn CO_PROCESSOR=ethos-u microlite\r\n",
      "```\n",
      "issue labels - \n",
      "TF 2.5\n",
      "comp:micro\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Help understand outputs of Mask RCNN tflite model\n",
      "issue body -  Hi, I could really do with some help in understanding the outputs of the tflite model of Mask RCNN.\r\n",
      "\r\n",
      "I have converted the model to tflite using Tensorflow 2.3.\r\n",
      "\r\n",
      "The model takes 3 inputs - the resized image, image meta and anchors\r\n",
      "\r\n",
      "I have written the android equivalent code for the resized image - included padding and subtracted pixel means, as well as the image meta and anchors. The model is running but I can't seem to understand which output stands for what.\r\n",
      "\r\n",
      "There are 7 outputs of the tflite model which have dimensions as follows:\r\n",
      "\r\n",
      "[1][1][1]\r\n",
      "[1][1000][81][4]\r\n",
      "[1][1000][81]\r\n",
      "[1][100][6]\r\n",
      "[1][100][28][28][81]\r\n",
      "[1][1][4]\r\n",
      "[1][1][2]\r\n",
      "\r\n",
      "I assume [1][100][28][28][81] is for the masks and I thought [1][100][6] would be the class ID, probability score and bounding boxes. However that doesn't seem to be consistent with the results. I can share the tflite model and the first few results of the outputs if that helps. Any help would be very much appreciated!\n",
      "issue labels - \n",
      "TF 2.3\n",
      "comp:lite\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  keras LSTM model - a tf 1.15 equivalent that works with tflite\n",
      "issue body -  Hello\r\n",
      "\r\n",
      "**TLDR**: How to implement this model using `tf.lite.experimental.nn.TFLiteLSTMCell, tf.lite.experimental.nn.dynamic_rnn`instead `keras.layers.LSTM`?\r\n",
      "![image](https://user-images.githubusercontent.com/26148975/107017531-eab71b00-67a7-11eb-9a92-b161b49f439b.png)\r\n",
      "\r\n",
      "I have this network in keras:\r\n",
      "```python\r\n",
      "inputs = keras.Input(shape=(1, 52))\r\n",
      "state_1_h = keras.Input(shape=(200,))\r\n",
      "state_1_c = keras.Input(shape=(200,))\r\n",
      "x1, state_1_h_out, state_1_c_out = layers.LSTM(200, return_sequences=True, input_shape=(sequence_length, 52),\r\n",
      "                                               return_state=True)(inputs, initial_state=[state_1_h, state_1_c])\r\n",
      "output = layers.Dense(13)(x1)\r\n",
      "\r\n",
      "model = keras.Model([inputs, state_1_h, state_1_c],\r\n",
      "                    [output, state_1_h_out, state_1_c_out])\r\n",
      "```\r\n",
      "\r\n",
      "I need to implement it in tensorflow 1.15, but in a way that will be compatible with tflite 1.15.\r\n",
      "** It means that I cannot use `keras.layers.LSTM` because it is not compatible with tflite 1.15. **\r\n",
      "\r\n",
      "Following this examples, I saw the tutorial: https://github.com/tensorflow/tensorflow/tree/r1.15/tensorflow/lite/experimental/examples/lstm\r\n",
      "https://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/lite/experimental/examples/lstm/TensorFlowLite_LSTM_Keras_Tutorial.ipynb \r\n",
      "\r\n",
      "Which explains how to implement LSTM in a way that is compatible with tflite 1.15. \r\n",
      "I understand I need to use the following layers: `tf.lite.experimental.nn.TFLiteLSTMCell, tf.lite.experimental.nn.dynamic_rnn`\r\n",
      "\r\n",
      "The hard part is this line:\r\n",
      "```python\r\n",
      "x1, state_1_h_out, state_1_c_out = layers.LSTM(200, return_sequences=True, input_shape=(sequence_length, 52),\r\n",
      "                                               return_state=True)(inputs, initial_state=[state_1_h, state_1_c])\r\n",
      "\r\n",
      "```\r\n",
      "\r\n",
      "How can I implement it? \r\n",
      "\r\n",
      "The [dynamic_rnn documentation](https://www.tensorflow.org/api_docs/python/tf/compat/v1/lite/experimental/nn/dynamic_rnn) explains how to provide initial state to the dynamic rnn. \r\n",
      "\r\n",
      "I try to use it in the `buildLstmLayer` function provided (that should implement LSTM): \r\n",
      "\r\n",
      "```python\r\n",
      "def buildLstmLayer(inputs, num_layers, num_units):\r\n",
      "  \"\"\"Build the lstm layer.\r\n",
      "\r\n",
      "  Args:\r\n",
      "    inputs: The input data.\r\n",
      "    num_layers: How many LSTM layers do we want.\r\n",
      "    num_units: The unmber of hidden units in the LSTM cell.\r\n",
      "  \"\"\"\r\n",
      "  lstm_cells = []\r\n",
      "  for i in range(num_layers):\r\n",
      "    lstm_cells.append(\r\n",
      "        tf.lite.experimental.nn.TFLiteLSTMCell(\r\n",
      "            num_units, forget_bias=0, name='rnn{}'.format(i)))\r\n",
      "  lstm_layers = tf.keras.layers.StackedRNNCells(lstm_cells)\r\n",
      "  # Assume the input is sized as [batch, time, input_size], then we're going\r\n",
      "  # to transpose to be time-majored.\r\n",
      "  transposed_inputs = tf.transpose(\r\n",
      "      inputs, perm=[1, 0, 2])\r\n",
      "  outputs, _ = tf.lite.experimental.nn.dynamic_rnn(\r\n",
      "      lstm_layers,\r\n",
      "      transposed_inputs,\r\n",
      "      dtype='float32',\r\n",
      "      time_major=True)\r\n",
      "  unstacked_outputs = tf.unstack(outputs, axis=0)\r\n",
      "  return unstacked_outputs[-1]\r\n",
      "```\r\n",
      "\r\n",
      "I try to take this code:\r\n",
      "```python\r\n",
      "  outputs, _ = tf.lite.experimental.nn.dynamic_rnn(\r\n",
      "      lstm_layers,\r\n",
      "      transposed_inputs,\r\n",
      "      dtype='float32',\r\n",
      "      time_major=True)\r\n",
      "```\r\n",
      "And add initial state.\r\n",
      "\r\n",
      "This try\r\n",
      "  ```python\r\n",
      "outputs, _ = tf.lite.experimental.nn.dynamic_rnn(\r\n",
      "      lstm_layers,\r\n",
      "      transposed_inputs,\r\n",
      "      dtype='float32',\r\n",
      "      time_major=True,\r\n",
      "      initial_state=[tf.compat.v1.placeholder(tf.float32, shape=(200)), tf.compat.v1.placeholder(tf.float32, shape=(200))])\r\n",
      "```\r\n",
      "returns error `ValueError: Shape must be rank 2 but is rank 1 for 'stacked_rnn_cells/concat' (op: 'ConcatV2') with input shapes: [?,28], [200], [].\r\n",
      "`\r\n",
      "\r\n",
      "This try:\r\n",
      "```python\r\n",
      "  outputs, _ = tf.lite.experimental.nn.dynamic_rnn(\r\n",
      "      lstm_layers,\r\n",
      "      transposed_inputs,\r\n",
      "      dtype='float32',\r\n",
      "      time_major=True,\r\n",
      "      initial_state=[tf.compat.v1.placeholder(tf.float32, shape=(1, 200)), tf.compat.v1.placeholder(tf.float32, shape=(1, 200))])\r\n",
      "```\r\n",
      "returns error `ValueError: Dimensions must be equal, but are 228 and 92 for 'stacked_rnn_cells/MatMul' (op: 'MatMul') with input shapes: [1,228], [64,92].\r\n",
      "`\r\n",
      "\r\n",
      "How can I solve it?\r\n",
      "\r\n",
      "Thank you\n",
      "issue labels - \n",
      "TF 1.15\n",
      "comp:lite\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Mobilenet parameters: alpha & rho\n",
      "issue body -  Hello, my name is Joy and I've been learning about PoseNet in order to use it in my health-related research work.\r\n",
      "I was impressed how mobilenet enables to keep high accuracy while reducing CPU (or GPU/NPU) dependency by adapting few parameters where my questions sprouted.\r\n",
      "I've noticed that in mobilenet official papers, there were two multipliers introduced: alpha and rho. I'll skip the explanation of both parameters. \r\n",
      "I wonder what is the each value of alpha and rho for the mobilenet for the newest pretrained PoseNet model. Also, I'm wondering if there is a guideline for parameters(especially alpha and rho) tuning, and how the values of both are set and validated before training the model.\r\n",
      "Thank you.\n",
      "issue labels - \n",
      "stat:awaiting response\n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  Reagarding discripency in tensorboard validation accuracy and the one printed during training\n",
      "issue body -  When I am running a random search for hyperparameter tunning. The accuracies appearing on print screen for training and validation sets are different which is not appearing in tensorboard. In tensorboard the validation accuracy always appears lesser than the print value.\r\n",
      "\r\n",
      "![image](https://user-images.githubusercontent.com/37747800/106998754-040d9680-67ab-11eb-8833-0ed9b6b7bf4c.png)\r\n",
      "![image](https://user-images.githubusercontent.com/37747800/106998902-40d98d80-67ab-11eb-9713-f9e4efba3a5a.png)\r\n",
      "\r\n",
      "The accuracy appearing are different. Why?\n",
      "issue labels - \n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  Update array_ops.py\n",
      "issue body -  The current implementation of the `Split / SplitV` op's wrapper function (in Python)\r\n",
      "convert `num_or_size_splits` to the `size_splits` tensor at the first line.\r\n",
      "However, if we pass `num_or_size_splits` as a `Integral` or `Dimension` type, this function calls `Split` op directly\r\n",
      "(does not use `size_splits` tensor).\r\n",
      "\r\n",
      "This does not matter in eager mode but creates a redundant tensor (`Const` tensor or `Pack` etc...) in graph mode.\r\n",
      "(Actually, the grappler might remove this redundant tensor since it is not used anywhere, but for the neat graph in tensorboard)\r\n",
      "Then I just reordered Python statement after the first `if` statement\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:ops\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  [TFTRT] Add Dynamic Shape Testing for ConvertConv3D\n",
      "issue body -  @bixia1 @tfeher for review\r\n",
      "\r\n",
      "Feature Tracker: #45481\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:gpu:tensorrt\n",
      "ready to pull\n",
      "size:L\n",
      "\n",
      "\n",
      "issue title -  Initialization of global pointer variable seems suspect with our current use of Renode.\n",
      "issue body -  @tensorflow/micro\r\n",
      "\r\n",
      "I hit this issue while working on https://github.com/tensorflow/tensorflow/pull/46904:\r\n",
      "\r\n",
      " * If I create a global pointer variable (explicitly initialized to nullptr) and check for the variable == nullptr in my factory function, the check always returns false.\r\n",
      "\r\n",
      "AFAICT, this behavior is specific to our use of Renode. I have not been able to reproduce on Linux or with the Xtensa simulator.\r\n",
      "\r\n",
      "I have a workaround that should allow #46904 to be merged and I will then update this issue with a cleaner way to reproduce this error.\n",
      "issue labels - \n",
      "comp:lite\n",
      "comp:micro\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Cannot create mbed folder\n",
      "issue body -  **System information**\r\n",
      "- OS Platform and Distribution Windows 10\r\n",
      "- Mobile device STM32F746\r\n",
      "\r\n",
      "When I run the command:\r\n",
      "make -f tensorflow/lite/micro/tools/make/Makefile TARGET=mbed TAGS=\"CMSIS disco_f746ng\" generate_hello_world_mbed_project\r\n",
      "\r\n",
      "I get the following error message:\r\n",
      "/Makefile:61: *** The TAGS command line option is no longer supported in the TFLM Makefile..  \r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.5\n",
      "comp:micro\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  Tensorflow GPU Not Recognizing GPUs\n",
      "issue body -  **System information**\r\n",
      "- Windows 10 Pro 19042.746\r\n",
      "- TensorFlow installed from conda/pip\r\n",
      "- TensorFlow version: 2.3.0\r\n",
      "- TensorFlow GPU version: 2.3.0\r\n",
      "- Python version: 3.8.5\r\n",
      "- Installed using virtualenv? pip? conda?: conda\r\n",
      "- Bazel version (if compiling from source): N/A\r\n",
      "- GCC/Compiler version (if compiling from source): N/A\r\n",
      "- CUDA/cuDNN version: 11.2 as of 2/4/2021\r\n",
      "- GPU model and memory: TITAN RTX 303MiB / 24576MiB\r\n",
      "- GPU model and memory: GeForce GTX 1070 Ti 1720MiB /  8192MiB\r\n",
      "\r\n",
      "\r\n",
      "_tflow_select             2.3.0                       gpu\r\n",
      "tensorboard               2.3.0              pyh4dce500_0\r\n",
      "tensorboard-plugin-wit    1.6.0                      py_0\r\n",
      "tensorflow                2.3.0           mkl_py38h8557ec7_0\r\n",
      "tensorflow-base           2.3.0           eigen_py38h75a453f_0\r\n",
      "tensorflow-estimator      2.3.0              pyheb71bc4_0\r\n",
      "tensorflow-gpu            2.3.0                he13fc11_0\r\n",
      "\r\n",
      "\r\n",
      "The GPUs are not displayed when querying with list_local_devices() and they are used when explicitly declaring them to be used in  with tf.device('GPU:1'). Yet, tensorflow does not complain when I reference the GPU devices, but the GPUs do not show any load whatsoever during LSTM evaluation.\r\n",
      "```\r\n",
      ">>> from tensorflow.python.client import device_lib\r\n",
      ">>> print(device_lib.list_local_devices())\r\n",
      "2021-02-04 11:37:27.094309: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX AVX2\r\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n",
      "[name: \"/device:CPU:0\"\r\n",
      "device_type: \"CPU\"\r\n",
      "memory_limit: 268435456\r\n",
      "locality {\r\n",
      "}\r\n",
      "incarnation: 18092632807336689748\r\n",
      "]\r\n",
      "```\r\n",
      "Did the following commands to verify:\r\n",
      "```\r\n",
      ">>> from tensorflow.python.client import device_lib\r\n",
      ">>> print(device_lib.list_local_devices())\r\n",
      "```\r\n",
      "Output from nvidia-smi:\r\n",
      "\r\n",
      "```\r\n",
      "Thu Feb  4 11:38:59 2021\r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 461.09       Driver Version: 461.09       CUDA Version: 11.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  TITAN RTX          WDDM  | 00000000:1B:00.0 Off |                  N/A |\r\n",
      "| 41%   29C    P8    16W / 280W |    303MiB / 24576MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  GeForce GTX 107... WDDM  | 00000000:68:00.0  On |                  N/A |\r\n",
      "|  0%   38C    P8    10W / 180W |   1720MiB /  8192MiB |      4%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "```\n",
      "issue labels - \n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  Cannot Install Tensorflow on any version of Python 3.8\n",
      "issue body -  <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64-bit\r\n",
      "- TensorFlow installed from (source or binary): Cannot Install\r\n",
      "- TensorFlow version: NaN\r\n",
      "- Python version: 3.8, 3.8.7, 3.9, 3.9.1 (All 64-bit)\r\n",
      "- Installed using virtualenv? pip? conda?: pip via venv\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "Cannot install tensorflow. tried on all the above mentioned python version. Yes it is a 64-bit installation. \r\n",
      "**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n",
      "Tried:\r\n",
      "pip install tensorflow\r\n",
      "pip install --upgrade tensorflow\r\n",
      "pip install --upgrade ALL_URLS_FOR_WINDOWS\r\n",
      "py -m ALL_ABOVE_COMMANDS\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "subtype:windows\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  Ubuntu 20.10 and 3090 with CUDA 11.0 and cuDNN 8.0: ptxas fatal   : Value 'sm_86' is not defined for option 'gpu-name'\n",
      "issue body -  \r\n",
      "\r\n",
      "**System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.10\r\n",
      "- TensorFlow installed from (source or binary): binary\r\n",
      "- TensorFlow version: 2.4.1\r\n",
      "- Python version: 3.8.7\r\n",
      "- Installed using virtualenv? pip? conda?: pip\r\n",
      "- Bazel version (if compiling from source): None\r\n",
      "- GCC/Compiler version (if compiling from source): None\r\n",
      "- CUDA/cuDNN version: 11.0.3/8.0.5.39\r\n",
      "- GPU model and memory: GeForce RTX 3090 24GB\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "\r\n",
      "I just set up a new deep learning station with Ubuntu 20.10 and a GeForce RTX 3090. I am receiving a very large number of warnings in the form of:\r\n",
      "\r\n",
      "```\r\n",
      "2021-02-04 08:52:12.943009: W tensorflow/stream_executor/gpu/asm_compiler.cc:235] Your CUDA software stack is old. We fallback to the NVIDIA driver for some compilation. Update your CUDA version to get the best performance. The ptxas error was: ptxas fatal   : Value 'sm_86' is not defined for option 'gpu-name'\r\n",
      "```\r\n",
      "\r\n",
      "I followed the instructions in:\r\n",
      "* https://www.tensorflow.org/install/gpu\r\n",
      "* https://www.tensorflow.org/install/source#gpu\r\n",
      "(though the instructions are not updated since Ubuntu 18.04)\r\n",
      "\r\n",
      "To build a setup with\r\n",
      "* Tensorflow 2.4.1\r\n",
      "* NVIDIA GPU driver: 460.32.03\r\n",
      "* CUDA toolkit: 11.0.3\r\n",
      "* cuDNN: 8.0.5.39\r\n",
      "\r\n",
      "\r\n",
      "**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n",
      "``` bash \r\n",
      "sudo apt -y install build-essential\r\n",
      "sudo apt -y install gcc-8 g++-8 gcc-9 g++-9\r\n",
      "\r\n",
      "sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-8 8 --slave /usr/bin/g++ g++ /usr/bin/g++-8\r\n",
      "sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-9 9 --slave /usr/bin/g++ g++ /usr/bin/g++-9\r\n",
      "\r\n",
      "sudo update-alternatives --config gcc # select gcc-8 option (1)\r\n",
      "gcc --version # to check it is gcc 8\r\n",
      "\r\n",
      "# Get CUDA Toolkit\r\n",
      "# TF 2.4.0 compatibel with cuDNN 8.0 and CUDA 11.0\r\n",
      "wget https://developer.download.nvidia.com/compute/cuda/11.0.3/local_installers/cuda_11.0.3_450.51.06_linux.run\r\n",
      "sudo bash cuda_11.0.3_450.51.06_linux.run # Deslect CUDA driver and install\r\n",
      "# Add the following to the bashrc file\r\n",
      "export PATH=$PATH:/usr/local/cuda-11.0/bin\r\n",
      "export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-11.0/lib64\r\n",
      "\r\n",
      "# Get cuDNN package\r\n",
      "tar -xvf cudnn-11.0-linux-x64-v8.0.5.39.tgz\r\n",
      "sudo cp cuda/include/cudnn.h /usr/local/cuda/include\r\n",
      "sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64\r\n",
      "sudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn*\r\n",
      "nvcc --version\r\n",
      "nvidia-smi\r\n",
      "python -c \"import tensorflow as tf; tf.config.list_physical_devices('GPU')\"\r\n",
      "```\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:gpu\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  [INTEL MKL] Supporting 2 inputs for  Mkl dequantize op\n",
      "issue body -   Mkl dequantize op used to support only 4 inputs. Here, we also need to support 2 inputs since it needs to be used with matmul op in some models.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:mkl\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  MultiWorkerMirroredStrategy() hanging\n",
      "issue body -  <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "\r\n",
      "I have access to two computers and below give the info for both. \r\n",
      "\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Computer 1: Windows 10, Computer 2: Windows 10\r\n",
      "- TensorFlow installed from (source or binary):\r\n",
      "- TensorFlow version: 2.4.1 on both machines\r\n",
      "- Python version: 3.8.5 on both machines\r\n",
      "- Installed using virtualenv? pip? conda?: For both:  Installed using pip install tensorflow-gpu after manually installing the appropriate Nvidia driver, coda toolkit, cudnn versions.\r\n",
      "- CUDA/cuDNN version: CUDA 11.0 and cuDNN 8.0.4. Also both computers have an NVIDIA driver of 450x or higher\r\n",
      "- GPU model and memory: Computer 1: NVIDIA Quadro P2000 (GPU memory of 36.8 GB), Computer 2: NVIDIA Quadro P5000 (GPU memory of 79.9 GB)\r\n",
      "\r\n",
      "Note: the CUDA, cuDNN were installed on each machine manually, I am not using docker.\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "\r\n",
      "In case this might be an issue/helps to know, I am on a Mac remoting into my work's computer (Computer 1) using Microsoft Remote Desktop. From Computer 1 (Windows OS), I am remoting into Computer 2 (Windows OS as well) using Remote Desktop Connection (the GUI for RDP -- so not using ssh). Both these computers have a GPU as specified above and want to use them to create a two node distributed training job using tensorflow's distribute.multiworkermirroredstrategy. I believe my issue is that I do not understand if the nodes can communicate with each other.\r\n",
      "\r\n",
      "**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n",
      "\r\n",
      "Below is my script \"worker.py\"\r\n",
      "\r\n",
      "```python\r\n",
      "def create_model():\r\n",
      "    model = Sequential()\r\n",
      "    model.add(Bidirectional(LSTM(128, return_sequences=True), input_shape=(T, D)))\r\n",
      "    model.add(TimeDistributed(Dense(1, activation='relu')))\r\n",
      "    return model\r\n",
      "\r\n",
      "\r\n",
      "if __name__ == \"__main__\":\r\n",
      "\r\n",
      "    random.set_seed(1)\r\n",
      "\r\n",
      "    parser = argparse.ArgumentParser()\r\n",
      "\r\n",
      "    # inputs for setting environment variable\r\n",
      "    parser.add_argument('-node1', default=None, required=True, help='Node 1 IP and port')\r\n",
      "    parser.add_argument('-node2', default=None, required=True, help='Node 2 IP and port')\r\n",
      "    parser.add_argument('-type', default=None, required=True, help='Node type')\r\n",
      "    parser.add_argument('-index', default=None, required=True, help='Node number', type=int)\r\n",
      "\r\n",
      "    args = parser.parse_args()\r\n",
      "    node_1 = args.node1\r\n",
      "    node_2 = args.node2\r\n",
      "    worker_type = args.type\r\n",
      "    worker_index = args.index\r\n",
      "\r\n",
      "    # set environment variable\r\n",
      "    os.environ['TF_CONFIG'] = json.dumps({\r\n",
      "        'cluster': {\r\n",
      "            \"worker\": [node_1, node_2]\r\n",
      "        },\r\n",
      "        'task': {'type': worker_type, 'index': worker_index}\r\n",
      "    })\r\n",
      "\r\n",
      "    # load data\r\n",
      "    with open('data/X.json', 'rb') as f:\r\n",
      "        X = pickle.load(f)\r\n",
      "\r\n",
      "    with open('data/y.json', 'rb') as f:\r\n",
      "        y = pickle.load(f)\r\n",
      "\r\n",
      "    # split data, set random state (for replicability)\r\n",
      "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\r\n",
      "    _, T, D = X_train.shape\r\n",
      "\r\n",
      "    # dynamic memory allocation\r\n",
      "    # gpu = config.experimental.list_physical_devices('GPU')\r\n",
      "    # config.experimental.set_memory_growth(gpu[0], True)\r\n",
      "\r\n",
      "    # set training strategy\r\n",
      "    strategy = distribute.MultiWorkerMirroredStrategy( )\r\n",
      "    print(f\"Number of devices: {strategy.num_replicas_in_sync}\")\r\n",
      "\r\n",
      "    num_epochs = 10000\r\n",
      "    batch_size_per_replica = 64\r\n",
      "    batch_size = batch_size_per_replica * strategy.num_replicas_in_sync\r\n",
      "\r\n",
      "    # create data structure\r\n",
      "    train_data = data.Dataset.from_tensor_slices((X_train, y_train))\r\n",
      "    val_data = data.Dataset.from_tensor_slices((X_test, y_test))\r\n",
      "\r\n",
      "    train_data = train_data.batch(batch_size, drop_remainder=True)\r\n",
      "    val_data = val_data.batch(batch_size, drop_remainder=True)\r\n",
      "\r\n",
      "    # Disable AutoShard.\r\n",
      "    # options = data.Options()\r\n",
      "    # options.experimental_distribute.auto_shard_policy = data.experimental.AutoShardPolicy.OFF\r\n",
      "    # train_data = train_data.with_options(options)\r\n",
      "    # val_data = val_data.with_options(options)\r\n",
      "\r\n",
      "    with strategy.scope():\r\n",
      "        model = create_model()\r\n",
      "        model.compile(\r\n",
      "            loss='mse',\r\n",
      "            optimizer='adam',\r\n",
      "            metrics=[metrics.RootMeanSquaredError()],\r\n",
      "        )\r\n",
      "\r\n",
      "    # fit\r\n",
      "    model.fit(\r\n",
      "        train_data,\r\n",
      "        epochs=num_epochs,\r\n",
      "        validation_data=val_data,\r\n",
      "        # callbacks=[cp_callback, es_callback, tbd_callback]\r\n",
      "    )\r\n",
      "\r\n",
      "    # save\r\n",
      "    # model.save('model/model.h5')\r\n",
      "```\r\n",
      "\r\n",
      "From the documentation, I understand that this script has to be run on all machines (in my case 2) and set the TF_CONFIG environment variable correctly on both machines. And so I am doing:\r\n",
      "\r\n",
      "Command line machine 1: python worker.py -node1 172.31.15.60:8081 -node2 172.31.15.132:8081 -type worker -index 0\r\n",
      " \r\n",
      "Command line machine 2:  python worker.py -node1 172.31.15.60:8081 -node2 172.31.15.132:8081 -type worker -index 1\r\n",
      "\r\n",
      "Once I run the above command I see the following on machine 1:\r\n",
      "\r\n",
      "```\r\n",
      "2021-02-04 10:24:32.161847: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n",
      "2021-02-04 10:24:32.171344: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n",
      "2021-02-04 10:24:32.206509: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:\r\n",
      "pciBusID: 0000:65:00.0 name: Quadro P2000 computeCapability: 6.1\r\n",
      "coreClock: 1.4805GHz coreCount: 8 deviceMemorySize: 5.00GiB deviceMemoryBandwidth: 130.53GiB/s\r\n",
      "2021-02-04 10:24:32.213804: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n",
      "2021-02-04 10:24:32.233653: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n",
      "2021-02-04 10:24:32.237158: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n",
      "2021-02-04 10:24:32.247535: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n",
      "2021-02-04 10:24:32.254801: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n",
      "2021-02-04 10:24:32.270340: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n",
      "2021-02-04 10:24:32.278635: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n",
      "2021-02-04 10:24:32.284876: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n",
      "2021-02-04 10:24:32.288053: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n",
      "2021-02-04 10:24:32.291911: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\r\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n",
      "2021-02-04 10:24:32.300368: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:\r\n",
      "pciBusID: 0000:65:00.0 name: Quadro P2000 computeCapability: 6.1\r\n",
      "coreClock: 1.4805GHz coreCount: 8 deviceMemorySize: 5.00GiB deviceMemoryBandwidth: 130.53GiB/s\r\n",
      "2021-02-04 10:24:32.305947: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n",
      "2021-02-04 10:24:32.309708: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n",
      "2021-02-04 10:24:32.312512: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n",
      "2021-02-04 10:24:32.315162: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n",
      "2021-02-04 10:24:32.317685: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n",
      "2021-02-04 10:24:32.320235: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n",
      "2021-02-04 10:24:32.323467: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n",
      "2021-02-04 10:24:32.326515: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n",
      "2021-02-04 10:24:32.329607: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n",
      "2021-02-04 10:24:32.868210: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n",
      "2021-02-04 10:24:32.871587: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0\r\n",
      "2021-02-04 10:24:32.873963: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N\r\n",
      "2021-02-04 10:24:32.876398: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3835 MB memory) -> physical GPU (device: 0, name: Quadro P2000, pci bus id: 0000:65:00.0, compute capability: 6.1)\r\n",
      "2021-02-04 10:24:32.883411: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n",
      "2021-02-04 10:24:32.889654: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:\r\n",
      "pciBusID: 0000:65:00.0 name: Quadro P2000 computeCapability: 6.1\r\n",
      "coreClock: 1.4805GHz coreCount: 8 deviceMemorySize: 5.00GiB deviceMemoryBandwidth: 130.53GiB/s\r\n",
      "2021-02-04 10:24:32.896221: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n",
      "2021-02-04 10:24:32.900555: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n",
      "2021-02-04 10:24:32.903795: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n",
      "2021-02-04 10:24:32.906767: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n",
      "2021-02-04 10:24:32.909681: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n",
      "2021-02-04 10:24:32.912606: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n",
      "2021-02-04 10:24:32.916783: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n",
      "2021-02-04 10:24:32.920150: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n",
      "2021-02-04 10:24:32.923249: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n",
      "2021-02-04 10:24:32.925742: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n",
      "2021-02-04 10:24:32.928836: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0\r\n",
      "2021-02-04 10:24:32.932095: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N\r\n",
      "2021-02-04 10:24:32.934222: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:0 with 3835 MB memory) -> physical GPU (device: 0, name: Quadro P2000, pci bus id: 0000:65:00.0, compute capability: 6.1)\r\n",
      "2021-02-04 10:24:32.939752: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n",
      "2021-02-04 10:24:32.948503: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 ->  172.31.15.60:8081, 1 -> 172.31.15.132:8081}\r\n",
      "2021-02-04 10:24:32.953737: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://172.31.15.60:8081\r\n",
      "```\r\n",
      "\r\n",
      "And see the same similar things on machine 2, essentially hanging at \"Started server with target: grpc://172.31.15.132:8081.\"\r\n",
      "\r\n",
      "I am not sure what the issue could be? Could it be related to the ports, I chose 8081? Is it installation-wise? I have tried many things suggested on GitHub and none seem to work. Could firewalls, very secure networks be causing this issue?\r\n",
      "\r\n",
      "**Any other info / logs**\r\n",
      "Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n",
      "\r\n",
      "Per one of the GitHub issues I added the environment variable GRPC_VERBOSITY=DEBUG on machine 1 and got back from the command line \r\n",
      "D0204 10:40:31.617000000  8780 external/com_github_grpc_grpc/src/core/ext/filters/client_channel/resolver/dns/native/dns_resolver.cc:314] Using native dns resolver\r\n",
      "I0204 10:40:31.624000000  8780 external/com_github_grpc_grpc/src/cpp/server/server_builder.cc:332] Synchronous server. Num CQs: 1, Min pollers: 1, Max Pollers: 2, CQ timeout (msec): 10000\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "comp:dist-strat\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Subgraph search & replace (graph surgery)\n",
      "issue body -  TensorFlow version (you are using): 2.4\r\n",
      "- Are you willing to contribute it (Yes/No): No\r\n",
      "\r\n",
      "**Describe the feature and the current behavior/state.**\r\n",
      "I would like to be able to do subgraph search & replace of models, basically to replace some block of operators by others automatically.\r\n",
      "Currently there is no way to iterate over the graph with a standard graph traversal, making it hard/impossible to perform automatic subgraph search & replace.\r\n",
      "\r\n",
      "**Will this change the current api? How?**\r\n",
      "Not necessarily, Pytorch is developping torch.fx for doing something similar (in 1.8 alpha)\r\n",
      "\r\n",
      "**Who will benefit with this feature?**\r\n",
      "Businesses wanting to write custom framework on top of tensorflow for automatic model modifications taylored to their needs.\r\n",
      "\r\n",
      "**Any Other info.**\r\n",
      "\n",
      "issue labels - \n",
      "API review\n",
      "stalled\n",
      "stat:awaiting tensorflower\n",
      "type:feature\n",
      "\n",
      "\n",
      "issue title -  Cannot run LSTM in tensorflow lite 1.15 \n",
      "issue body -  Hello.\r\n",
      "\r\n",
      "**TLDR**: Can someone show how to create LSTM, convert it to TFLite, and run it in android version 1.15?\r\n",
      "\r\n",
      "I am trying to create a simple LSTM model and run in in android application with tensorflow v115.\r\n",
      "\r\n",
      "** It is the same case when using GRU and SimpleRNN layers **\r\n",
      "\r\n",
      "# Creating simple LSTM model\r\n",
      "I am working in Python, trying two tensorflow and keras versions: LATEST (2.4.1 with built-in keras), and 1.1.5 (and I install keras version 2.2.4). \r\n",
      "\r\n",
      "I create this simple model:\r\n",
      "```python\r\n",
      "model = keras.Sequential()\r\n",
      "model.add(layers.Embedding(input_dim=1000, output_dim=64))\r\n",
      "model.add(layers.LSTM(128))\r\n",
      "model.add(layers.Dense(10))\r\n",
      "model.summary()\r\n",
      "```\r\n",
      "\r\n",
      "# Saving it \r\n",
      "I save it in both \"SavedModel\" and \"h5\" format:\r\n",
      "```python\r\n",
      "model.save(f'output_models/simple_lstm_saved_model_format_{tf.__version__}', save_format='tf')\r\n",
      "model.save(f'output_models/simple_lstm_{tf.__version__}.h5', save_format='h5')\r\n",
      "```\r\n",
      "\r\n",
      "# Converting to TFLite\r\n",
      "I try create & save the model in both v115 and v2 versions. \r\n",
      "\r\n",
      "Then, I try to convert it to TFLite in several methods.\r\n",
      "\r\n",
      "In TF2: \r\n",
      "1. I try to convert from keras model:\r\n",
      "```python\r\n",
      "converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n",
      "tflite_model = converter.convert()\r\n",
      "with open(f\"output_models/simple_lstm_tf_v{tf.__version__}.tflite\", 'wb') as f:\r\n",
      "    f.write(tflite_model)\r\n",
      "```\r\n",
      "\r\n",
      "2. I try to convert from saved model:\r\n",
      "```python\r\n",
      "converter_saved_model = tf.lite.TFLiteConverter.from_saved_model(saved_model_path)\r\n",
      "tflite_model_from_saved_model = converter_saved_model.convert()\r\n",
      "with open(f\"{saved_model_path}_converted_tf_v{tf.__version__}.tflite\", 'wb') as f:\r\n",
      "    f.write(tflite_model_from_saved_model)\r\n",
      "```\r\n",
      "\r\n",
      "3. I try to convert from keras saved model (h5) - I try to use **both** tf.compat.v1.lite.TFLiteConverter and tf..lite.TFLiteConverter. \r\n",
      "```python\r\n",
      "converter_h5 = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file(h5_model_path)\r\n",
      "# converter_h5 = tf.lite.TFLiteConverter.from_keras_model_file(h5_model_path) # option 2\r\n",
      "tflite_model_from_h5 = converter_h5.convert()\r\n",
      "with open(f{h5_model_path.replace('.h5','')}_converted_tf_v1_lite_from_keras_model_file_v{tf.__version__}.tflite\", 'wb') as f:\r\n",
      "f.write(tflite_model_from_h5)\r\n",
      "```\r\n",
      "\r\n",
      "# Android Application\r\n",
      "### build.gradle (Module: app)\r\n",
      "When I want to use v2, I use:\r\n",
      "```\r\n",
      "    implementation 'org.tensorflow:tensorflow-lite-task-vision:0.0.0-nightly'\r\n",
      "    implementation 'org.tensorflow:tensorflow-lite-task-text:0.0.0-nightly'\r\n",
      "```\r\n",
      "When I want to use v115, I use `implementation 'org.tensorflow:tensorflow-lite:1.15.0'`\r\n",
      "in the build grade. \r\n",
      "\r\n",
      "Then, I follow common tflite loading code in android:\r\n",
      "\r\n",
      "```java\r\n",
      "private MappedByteBuffer loadModelFile(Activity activity) throws IOException {\r\n",
      "\r\n",
      "        AssetFileDescriptor fileDescriptor = activity.getAssets().openFd(getModelPath());\r\n",
      "        FileInputStream inputStream = new FileInputStream(fileDescriptor.getFileDescriptor());\r\n",
      "        FileChannel fileChannel = inputStream.getChannel();\r\n",
      "        long startOffset = fileDescriptor.getStartOffset();\r\n",
      "        long declaredLength = fileDescriptor.getDeclaredLength();\r\n",
      "        return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength);\r\n",
      "    }\r\n",
      "\r\n",
      "    LoadLSTM(Activity activity) {\r\n",
      "        try {\r\n",
      "            tfliteModel = loadModelFile(activity);\r\n",
      "        } catch (IOException e) {\r\n",
      "            e.printStackTrace();\r\n",
      "        }\r\n",
      "        tflite = new Interpreter(tfliteModel, tfliteOptions);\r\n",
      "        Log.d(TAG, \"*** Loaded model *** \" + getModelPath());\r\n",
      "    }\r\n",
      "```\r\n",
      "\r\n",
      "When I use v2, the model is loaded.\r\n",
      "When I use the v115, in ALL of the options i've tried, I receive errors as the following:\r\n",
      "`A/libc: Fatal signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x70 in tid 17686 (CameraBackgroun), pid 17643 (flitecamerademo)\r\n",
      "`\r\n",
      "\r\n",
      "I need a simple outcome - create LSTM and make it work in android v115. \r\n",
      "\r\n",
      "What am I missing? Thanks\n",
      "issue labels - \n",
      "TF 1.15\n",
      "comp:lite\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  PPO implementation does not converge\n",
      "issue body -  I'm not sure whether the problem is implementation / tensorflow specific. Following this [repo](https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail) as a reference, I re-implemented the same thing over and over for like 5 times in different ways. Every single time, I end up with the same results, despite the code is almost identical to the pytorch implementation. Here's a colab [notebook](https://colab.research.google.com/drive/1rvCYd9xJDQNAutpAHUBYc42ElHCXWNNd?usp=sharing) containing the full code. If anyone can help me fix the issue, I'd really be very grateful. \r\n",
      "\n",
      "issue labels - \n",
      "comp:keras\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Output range changes when converted to TFLite\n",
      "issue body -  ### 1. System information\r\n",
      "\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n",
      "- TensorFlow installation (pip package or built from source): Pip package\r\n",
      "- TensorFlow library (version, if pip package or github SHA, if built from source): TF 1.14\r\n",
      "\r\n",
      "### 2. Code\r\n",
      "\r\n",
      "Provide code to help us reproduce your issues using one of the following options:\r\n",
      "\r\n",
      "#### Code A: Tensorflow to TFLite conversion\r\n",
      "\r\n",
      "\r\n",
      "```\r\n",
      "def save(self):\r\n",
      "        \r\n",
      "        graph = tf.get_default_graph()\r\n",
      "        with graph.as_default():\r\n",
      "        if self.quant == True:\r\n",
      "           tf.contrib.quantize.create_eval_graph()\r\n",
      "\r\n",
      "        out_def = graph.as_graph_def()\r\n",
      "        with tf.gfile.GFile(self.out_graph, 'wb') as f:\r\n",
      "            f.write(out_def.SerializeToString())\r\n",
      "\r\n",
      "def freeze(self):\r\n",
      "        checkpoint = os.path.join(self.checkpoint_dir, 'Model%d' % self.trial)\r\n",
      "        ckpt= tf.train.get_checkpoint_state(checkpoint)\r\n",
      "        ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\r\n",
      "        ckpt_path = os.path.join(checkpoint, ckpt_name)\r\n",
      "\r\n",
      "        self.out_name = 'output'\r\n",
      "        freeze_graph.freeze_graph(self.out_graph,'',True,ckpt_path, self.out_name, restore_op_name='save/restore_all', filename_tensor_name='save/Const:0' , initializer_nodes='', output_graph= self.fr_graph ,clear_devices=True)\r\n",
      "\r\n",
      "\r\n",
      "def convert(self):\r\n",
      "        print(self.fr_graph)\r\n",
      "        converter= tf.lite.TFLiteConverter.from_frozen_graph(graph_def_file= self.fr_graph, input_arrays= ['input'],\\\r\n",
      "                                                  input_shapes= {'input': self.input_shape}, output_arrays=[self.out_name])\r\n",
      "        \r\n",
      "        if self.quant == True:\r\n",
      "            in_array = converter.get_input_arrays()\r\n",
      "            converter.quantized_input_stats = {in_array[0] : (0, 1)}  # mean, std_dev (input range is [-1, 1])\r\n",
      "\r\n",
      "            converter.inference_type = tf.uint8\r\n",
      "\r\n",
      "            tflite_model = converter.convert()\r\n",
      "            print('Save the model.')\r\n",
      "\r\n",
      "            with open('int_qat_%d.tflite'%self.trial, 'wb') as f:\r\n",
      "                f.write(tflite_model)\r\n",
      "\r\n",
      "```\r\n",
      "\r\n",
      "#### Option B:  TFLite inference code\r\n",
      "\r\n",
      "```\r\n",
      "def run_tflite(modelpath, labelpath, datapath, trial, height, weight):\r\n",
      "\r\n",
      "    interpreter = tf.lite.Interpreter(model_path=modelpath)\r\n",
      "    interpreter.allocate_tensors()\r\n",
      "\r\n",
      "    # Get input and output tensors.\r\n",
      "    input_details = interpreter.get_input_details()\r\n",
      "    output_details = interpreter.get_output_details()\r\n",
      "\r\n",
      "    # Test the model on random input data.\r\n",
      "\r\n",
      "    in_s = input_details[0]['shape']\r\n",
      "\r\n",
      "    num = len(label_list)\r\n",
      "    tf_img, label, num = load_and_slice_img(img_list,label_list, num, height, weight)\r\n",
      "    output_data= []\r\n",
      "    for i in range(num):\r\n",
      "        input_= tf_img[i].reshape((1, height//2, weight//2, 3))\r\n",
      "\r\n",
      "        interpreter.set_tensor(input_details[0]['index'], input_)\r\n",
      "        interpreter.invoke()\r\n",
      "\r\n",
      "        out_data = interpreter.get_tensor(output_details[0]['index'])\r\n",
      "\r\n",
      "        output_data.append(out_data)\r\n",
      "    output_data = np.array(output_data).reshape((num, height, weight, 3))\r\n",
      "\r\n",
      "```\r\n",
      "\r\n",
      "### 3. Failure after conversion\r\n",
      "If the conversion is successful, but the generated model is wrong, then state what is wrong:\r\n",
      "\r\n",
      "I am trying to quantize a SR model (EDSR), using Quantization Aware Training.\r\n",
      "\r\n",
      "I am using same codes used in Jacob et al. (2017) to perform quantization (create_training_graph etc)\r\n",
      "\r\n",
      "Fake Quantization (Stimulated) works rather well, but when I convert them using TFLiteconverter (from freeze_model), the result becomes very poor (PSNR below that of bicubic interpolation)\r\n",
      "\r\n",
      "When I draw histogram with the output and HR image, the range of image has decreased about 20% (e.g 0-200 to 25-180). Histogram structure seems to be identical.\r\n",
      "\r\n",
      "\r\n",
      "I tried to cross-check this using a simple model for MNIST, and found out following things\r\n",
      "- Almost zero performance change when converted to TFLite file, both with QAT and Post Integer Quantization\r\n",
      "- Output, before applying softmax function, changes dramatically between TFLite and Float (real float and fake/stimulated quantization one) models.\r\n",
      "\r\n",
      "\r\n",
      "I used images in raw form, without normalizing to [0,1] to train the floating point models. When converting the float model, I used exact codes written in relevant document in tf==1.14 github. I also used same codes for inference in TFLite\r\n",
      "\r\n",
      "I would like to reproduce the issue, but since it requires a lot of computation to train SR model, I did not upload the file initially. If required though, I will try to upload snippets of MNIST code (+ conversion code) that I have used to produce this error\r\n",
      "**Update - I have uploaded some codes and pb/tflite file of the models I produced**\r\n",
      "\r\n",
      "When I had a look into each nodes, the only difference between tflite and QAT(float) was a small change in min/max values for 'act_quant' stage, when activation function is not applied.\r\n",
      "\r\n",
      "\r\n",
      "What might be the possible cause of this error (happens in both classification and SR model)? One easy thought is that the input image might have slightly changed after converting the format into float32 (from uint8) for inference in floating model.\r\n",
      "Also, I think it might be the case that random bias has been added.\r\n",
      "\r\n",
      "![KakaoTalk_20210204_233632390](https://user-images.githubusercontent.com/29692100/106907712-e8ba7100-6741-11eb-94b4-3ad0aff0ab5d.jpg)\r\n",
      "\r\n",
      "Top Image- TFLite model after quantization (QAT)\r\n",
      "Bottom Image - HR image\r\n",
      "\r\n",
      "\r\n",
      "![KakaoTalk_20210204_233632390_01](https://user-images.githubusercontent.com/29692100/106907721-eb1ccb00-6741-11eb-8717-b1e5a1041913.jpg)\r\n",
      "\r\n",
      "Histogram of above image for TFLite model after QAT\r\n",
      "\r\n",
      "\r\n",
      "![KakaoTalk_20210204_233632390_02](https://user-images.githubusercontent.com/29692100/106907723-ebb56180-6741-11eb-864d-f2d46f27c66e.jpg)\r\n",
      "\r\n",
      "\r\n",
      "Histogram of same image for floating point model\r\n",
      "\r\n",
      "\r\n",
      "It is ok if there is no exact solution, but I would like to know possible cause of this phenomenon, so that I can try to debug it.\r\n",
      "\r\n",
      "\r\n",
      "* I used non keras layers to create the model, and also manually recreated depth_to_space layer of EDSR, as it was not supported in the version of TFLite converter I used.\r\n",
      "\r\n",
      "\r\n",
      "[Graphs.zip](https://github.com/tensorflow/tensorflow/files/5929670/Graphs.zip)\r\n",
      "frozen graph (fake quant)  and corresponding tflite file (qat)\r\n",
      "\n",
      "issue labels - \n",
      "ModelOptimizationToolkit\n",
      "TF 1.14\n",
      "comp:lite\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  TF.js Error: TensorList shape mismatch:  Shapes -1 and 3 must match\n",
      "issue body -  I'm currently trying to convert to TF.js one of the Object Detection models from the TF2 OD ZOO, in particular SSD MobileNet V2 FPNLite 320x320.\r\n",
      "\r\n",
      "When I convert the model pre-existing SavedModel from the saved_model folder I'm able to import it in my browser and launch it through executeAsync(). If I keep the original pipeline.config and try to create another SavedModel from the provided checkpoint using this line\r\n",
      "\r\n",
      "```\r\n",
      "python exporter_main_v2.py --input_type image_tensor \\\r\n",
      "    --pipeline_config_path ./pre-trained-models/ssd320/pipeline.config \\\r\n",
      "    --trained_checkpoint_dir ./pre-trained-models/ssd320/checkpoint_0 \\\r\n",
      "    --output_directory ./pre-trained-models/ssd320/exported_model\r\n",
      "```\r\n",
      "\r\n",
      "after I convert it to TF.js with the following line\r\n",
      "\r\n",
      "```\r\n",
      "tensorflowjs_converter \\\r\n",
      "    --input_format=tf_saved_model \\\r\n",
      "    --saved_model_tags=serve \\\r\n",
      "    ./pre-trained-models/ssd320/path-to-savedmodel-folder \\\r\n",
      "    ./pre-trained-models/tfjs_test\r\n",
      "```\r\n",
      "\r\n",
      "I encounter the following error when I try to launch the inference on my browser\r\n",
      "\r\n",
      "```\r\n",
      "util_base.js?a6b2:141 Uncaught (in promise) Error: TensorList shape mismatch:  Shapes -1 and 3 must match\r\n",
      "    at Module.assert (util_base.js?a6b2:141)\r\n",
      "    at assertShapesMatchAllowUndefinedSize (tensor_utils.js?74aa:24)\r\n",
      "    at TensorList.setItem (tensor_list.js?41f7:182)\r\n",
      "    at Module.executeOp (control_executor.js?de9e:188)\r\n",
      "    at eval (operation_executor.js?be85:52)\r\n",
      "    at executeOp (operation_executor.js?be85:94)\r\n",
      "    at GraphExecutor.processStack (graph_executor.js?33ef:390)\r\n",
      "    at GraphExecutor.executeWithControlFlow (graph_executor.js?33ef:350)\r\n",
      "    at async GraphExecutor._executeAsync (graph_executor.js?33ef:285)\r\n",
      "    at async GraphModel.executeAsync (graph_model.js?9724:316)\r\n",
      "```\r\n",
      "\r\n",
      "I'm currently working in Colab with the standard modules (TF 2.4.1, Python 3.6.9 and tensorflowjs 3.0.0) and didn't manage to find infos on similiar issues elsewhere.\r\n",
      "\r\n",
      "I tried with SSD MobileNet v2 320x320 (no FPN here) and the outcome is the same. I'm starting to think that it may be connected to the use of exporter_main_v2.py but I wouldn't know how to convert the model without it.\r\n",
      "\r\n",
      "Could you please help me figure out something more about the cause of this issue?\n",
      "issue labels - \n",
      "stat:awaiting response\n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  RuntimeError: Encountered unresolved custom op: TensorArrayV3.Node number 3 (TensorArrayV3) failed to prepare.\n",
      "issue body -  I'm not so professional in python! I'm trying to convert frozen graph to tflite file in colab to start inference it using jetson nano. I get this error! \r\n",
      "\r\n",
      "converter.allow_custom_ops = True\r\n",
      "converter.experimental_new_converter = True\r\n",
      "converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n",
      "\r\n",
      "tflite_model = converter.convert()\r\n",
      "open('tflite_file', 'wb').write(tflite_model)\r\n",
      "\r\n",
      "interpreter = tf.lite.Interpreter(model_content=tflite_model)\r\n",
      "interpreter.allocate_tensors()\n",
      "issue labels - \n",
      "TFLiteConverter\n",
      "comp:lite\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  tf.abs() crashes when dtype=float32\n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Home 20H2\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n",
      "- TensorFlow installed from (source or binary): using pip inside conda\r\n",
      "- TensorFlow version (use command below): 2.5.0-dev20210203\r\n",
      "- Python version: 3.8.5\r\n",
      "- Bazel version (if compiling from source): -\r\n",
      "- GCC/Compiler version (if compiling from source): -\r\n",
      "- CUDA/cuDNN version: 11.0 / 8\r\n",
      "- GPU model and memory: RTX 1070 8GB\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "tf.abs() works fine when provided with integer values but crashes when dtype=float32.\r\n",
      "If run in Jupyter the cell will hang and never execute. If run in Anaconda prompt python will crash and kick you back out to conda.\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "Return absolute value\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "```\r\n",
      "import tensorflow as tf\r\n",
      "x = tf.constant([-2.25, 3.25])\r\n",
      "tf.abs(x)\r\n",
      "```\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.5\n",
      "comp:ops\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  In tf.keras, can i use model(x).numpy() with disable_eager_execution()?\n",
      "issue body -  This template is for miscellaneous issues not covered by the other issue categories.\r\n",
      "\r\n",
      "For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n",
      "\r\n",
      "If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n",
      "\r\n",
      "For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n",
      "\r\n",
      "- Python version: 3.7\r\n",
      "- TensorFlow version (use command below):2.2\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "As is known to us, for predicting, there are two options to use the model. I can call `model.predict(x)` or I can call `model(x).numpy()`. `model(x)` is faster than `model.predict(x)`  for small amount of inputs in eager mode. Since i want to  deploy the model in production environment，i should disable the eager mode with `tf.compat.v1.disable_eager_execution()`.For better performance，i want to use `model(x).numpy()` to predict instead of  `model.predict(x)` for small amount of inputs ,but i found that i can't convert tensor to numpy with  `tf.compat.v1.disable_eager_execution()`\r\n",
      "**Describe the expected behavior**\r\n",
      "is there any solutions to use `model(x).numpy()` with `tf.compat.v1.disable_eager_execution()` for faster prediction\n",
      "issue labels - \n",
      "comp:keras\n",
      "stat:awaiting tensorflower\n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  Model is working fine in CPU but in GPU it consumes all RAM. \n",
      "issue body -  <em>Please make sure that this is a bug. As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:bug_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code:- Yes\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on a mobile device: N/A\r\n",
      "- TensorFlow installed from (source or binary): binary\r\n",
      "- TensorFlow version (use command below): 2.4\r\n",
      "- Python version: 3.8\r\n",
      "- Bazel version (if compiling from source): N/A\r\n",
      "- GCC/Compiler version (if compiling from source): N/A\r\n",
      "- CUDA/cuDNN version: 11.0\r\n",
      "- GPU model and memory: Tesla K4, RAM=15GB\r\n",
      "\r\n",
      "\r\n",
      "**Current behavior**\r\n",
      "I am trying to run face identification using the face_identification library based on dlib and object detection using the mobilenet_ssd model. Both of them combined working fine in CPU on local machine and on AWS CPU but when we use GPU, RAM is used completely. The issue is with the object detection model. Before loading the model it uses 300MB RAM, but when it loads the pre-trained model, RAM is suddenly jumped to 15GB. Both models are working fine in the CPU. \r\n",
      "\r\n",
      "**Expected behavior**\r\n",
      "It should run on GPU without RAM overflow.\r\n",
      "\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "Provide a reproducible test case that is the bare minimum necessary to generate\r\n",
      "the problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n",
      "\r\n",
      "**Other info/logs** \r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"StreamingService.py\", line 252, in <module>\r\n",
      "    streamingService.start()\r\n",
      "  File \"StreamingService.py\", line 104, in start\r\n",
      "    verification_service.verify(studentExamId, studentId, verificationCode)\r\n",
      "  File \"/home/ubuntu/screening-service-test/screening-service/computerVision/VerificationService.py\", line 84, in verify\r\n",
      "    studentFaceEncoding  = self.getFaceEncoding(studentPicDict[key])\r\n",
      "  File \"/home/ubuntu/screening-service-test/screening-service/computerVision/VerificationService.py\", line 20, in getFaceEncoding\r\n",
      "    face_locations = face_recognition.face_locations(rgb_small_frame, model = \"cnn\")\r\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/face_recognition/api.py\", line 119, in face_locations\r\n",
      "    return [_trim_css_to_bounds(_rect_to_css(face.rect), img.shape) for face in _raw_face_locations(img, number_of_times_to_upsample, \"cnn\")]\r\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/face_recognition/api.py\", line 103, in _raw_face_locations\r\n",
      "    return cnn_face_detector(img, number_of_times_to_upsample)\r\n",
      "RuntimeError: Error while calling cudnnFindConvolutionForwardAlgorithm( context(), descriptor(data), (const cudnnFilterDescriptor_t)filter_handle, (const cudnnConvolutionDescriptor_t)conv_handle, descriptor(dest_desc), num_possible_algorithms, &num_algorithms, perf_results.data()) in file /home/ubuntu/dlib/dlib/cuda/cudnn_dlibapi.cpp:820. code: 2, reason: CUDA Resources could not be allocated.\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:gpu\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  TypeError: can't pickle _thread.RLock objects when using multiprocessing\n",
      "issue body -  Information:\r\n",
      "\r\n",
      "```\r\n",
      "Tensorflow version 2.1.0\r\n",
      "Python 3.7\r\n",
      "```\r\n",
      "The minimal example to reproduce the error:\r\n",
      "\r\n",
      "\r\n",
      "```\r\n",
      "import tensorflow as tf\r\n",
      "from tensorflow import keras\r\n",
      "import numpy as np\r\n",
      "\r\n",
      "\r\n",
      "from multiprocessing import Pool\r\n",
      "from multiprocessing.dummy import Pool as ThreadPool\r\n",
      "from functools import partial\r\n",
      "\r\n",
      "def simple_model():\r\n",
      "    model = keras.models.Sequential([\r\n",
      "        keras.layers.Dense(units = 10, input_shape = [1]),\r\n",
      "        keras.layers.Dense(units = 1, activation = 'sigmoid')\r\n",
      "    ])\r\n",
      "    model.compile(optimizer = 'sgd', loss = 'mean_squared_error')\r\n",
      "    return model\r\n",
      "\r\n",
      "def clone_model(model):\r\n",
      "    model_clone = tf.keras.models.clone_model(model)\r\n",
      "    model_clone.set_weights(model.get_weights())\r\n",
      "    return model_clone\r\n",
      "\r\n",
      "def work(model, seq):\r\n",
      "    return model.predict(seq)\r\n",
      "    \r\n",
      "def worker(model, n = 4):\r\n",
      "    seqences = np.arange(0,100).reshape(n, -1)\r\n",
      "    pool = Pool()\r\n",
      "    # model_list = [clone_model(model) for _ in range(n)]\r\n",
      "    # results = pool.map(work, zip(model_list,seqences))\r\n",
      "    partial_work = partial(work, model=model)\r\n",
      "    results = pool.map(partial_work, seqences)\r\n",
      "    pool.close()\r\n",
      "    pool.join()\r\n",
      "    \r\n",
      "    return np.reshape(results, (-1, ))\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "if __name__ == '__main__':\r\n",
      "    model = simple_model()\r\n",
      "    out = worker(model, n=4)\r\n",
      "    print(out)\r\n",
      "```\r\n",
      "\r\n",
      "\r\n",
      "gives the following error trace:\r\n",
      "```\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"c:/****/test4.py\", line 43, in <module>\r\n",
      "    out = worker(model, n=4)\r\n",
      "  File \"c:/****/test4.py\", line 33, in worker\r\n",
      "    results = pool.map(partial_work, seqences)\r\n",
      "  File \"C:\\***\\anaconda3\\envs\\tf-gpu\\lib\\multiprocessing\\pool.py\", line 268, in map\r\n",
      "    return self._map_async(func, iterable, mapstar, chunksize).get()\r\n",
      "  File \"C:\\***\\anaconda3\\envs\\tf-gpu\\lib\\multiprocessing\\pool.py\", line 657, in get\r\n",
      "    raise self._value\r\n",
      "  File \"C:\\***\\anaconda3\\envs\\tf-gpu\\lib\\multiprocessing\\pool.py\", line 431, in _handle_tasks\r\n",
      "    put(task)\r\n",
      "  File \"C:\\***\\anaconda3\\envs\\tf-gpu\\lib\\multiprocessing\\connection.py\", line 206, in send\r\n",
      "    self._send_bytes(_ForkingPickler.dumps(obj))\r\n",
      "  File \"C:\\***\\anaconda3\\envs\\tf-gpu\\lib\\multiprocessing\\reduction.py\", line 51, in dumps\r\n",
      "    cls(buf, protocol).dump(obj)\r\n",
      "TypeError: can't pickle _thread.RLock objects\r\n",
      "```\n",
      "issue labels - \n",
      "TF 2.1\n",
      "comp:keras\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  window10 make file error\n",
      "issue body -  @tensorflow/micro\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): window10\r\n",
      "- TensorFlow installed from (source or binary): source\r\n",
      "- Tensorflow version (commit SHA if source): 2.4.0\r\n",
      "- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): heimax we1\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "Hello, I have problem build makefile in window10\r\n",
      "I've download the migw to use \"make\".\r\n",
      "\r\n",
      "in Powershell\r\n",
      "\r\n",
      "```\r\n",
      "PS C:\\tensorflow> make -f tensorflow/lite/micro/tools/make/Makefile test_hello_world_test\r\n",
      "process_begin: CreateProcess(NULL, uname -m, ...) failed.\r\n",
      "-m은(는) 예상되지 않았습니다.\r\n",
      "'tr'은(는) 내부 또는 외부 명령, 실행할 수 있는 프로그램, 또는\r\n",
      "배치 파일이 아닙니다.\r\n",
      "FIND: 매개 변수 형식이 틀립니다.\r\n",
      "FIND: 매개 변수 형식이 틀립니다.\r\n",
      "명령 구문이 올바르지 않습니다.\r\n",
      "process_begin: CreateProcess(NULL, bash C:\\tensorflow\\tensorflow\\lite\\micro\\tools\\make\\flatbuffers_download.sh tensorflow/lite/micro/tools/make/downloads, ...) failed.\r\n",
      "tensorflow/lite/micro/tools/make/Makefile:521: *** Something went wrong with the flatbuffers download: .  멈춤.\r\n",
      "\r\n",
      "```\r\n",
      "\r\n",
      "```\r\n",
      "PS C:\\tensorflow> make -f tensorflow/lite/micro/tools/make/Makefile TARGET=himax_we1_evb third_party_downloads\r\n",
      "process_begin: CreateProcess(NULL, uname -m, ...) failed.\r\n",
      "-m은(는) 예상되지 않았습니다.\r\n",
      "'tr'은(는) 내부 또는 외부 명령, 실행할 수 있는 프로그램, 또는\r\n",
      "배치 파일이 아닙니다.\r\n",
      "FIND: 매개 변수 형식이 틀립니다.\r\n",
      "FIND: 매개 변수 형식이 틀립니다.\r\n",
      "명령 구문이 올바르지 않습니다.\r\n",
      "process_begin: CreateProcess(NULL, bash C:\\tensorflow\\tensorflow\\lite\\micro\\tools\\make\\flatbuffers_download.sh tensorflow/lite/micro/tools/make/downloads, ...) failed.\r\n",
      "tensorflow/lite/micro/tools/make/Makefile:521: *** Something went wrong with the flatbuffers download: .  멈춤.\r\n",
      "```\r\n",
      "\r\n",
      "I can't find the reason of this problem. I guess I have to change environment setting\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:micro\n",
      "subtype:windows\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  Use the right input_dims[] in lite/micro/kernels/cast_test.cc\n",
      "issue body -  Issue #45608: Bug fix for the micro op CAST's test code. \n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Add a MicroPrintf function that is independent of the ErrorReporter.\n",
      "issue body -  Additionally,\r\n",
      "* remove the global error reporter from micro_test.h\r\n",
      "* change all the kernel tests to make use of MicroPrintf\r\n",
      "* add a GetMicroErrorReporter() function that returns a pointer to a singleton MicroErrorReporter object.\r\n",
      "  - This enables the current change to not spread beyond the tests.\r\n",
      "  - Even if we move large parts of the TFLM code to make use MicroPrintf (in favor of error_reporter), there is still going to be shared TfLite/TFLM code that will need an error_reporter.\r\n",
      "    \r\n",
      "Next steps, if we want to continue down this path\r\n",
      "* remove the error_reporter from the TFLM functions and class implementations and instead use either MicroPrintf or GetMicroErrorReporter()\r\n",
      "* Add new APIs that do not have error_reporter to the TFLM classes and functions.\r\n",
      "* Ask users to switch to the new error_reporter-free APIs and deprecate the APIs that do make use of the error_reporter.\r\n",
      "* Remove the error_reporter APIs completely.\r\n",
      "    \r\n",
      "Prior to this change, we would have to use the ErrorReporter interface for all the logging. This was problematic on a few fronts:\r\n",
      "* The name ErrorReporter was often misleading since sometimes we just want to log, even when there isn't an error.\r\n",
      "* For even the simplest logging, we need to have access to an ErrorReporter object which means that pointers to an ErrorReporter are part of most classes in TFLM.\r\n",
      "    \r\n",
      "With this change, we can simply call MicroPrintf(), and it will be a no-op if binary size is important.\r\n",
      "    \r\n",
      "    \r\n",
      "Progress towards http://b/158205789\r\n",
      "\r\n",
      "As described in #46937, there is some unexpected behavior with the initialization with the targets that we are simulating via Renode. The current PR has a workaround and that issue will be addressed separately.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:L\n",
      "\n",
      "\n",
      "issue title -  Use the right input_dims[] in lite/micro/kernels/exp_test.cc\n",
      "issue body -  Issue #45415. Bug fix for micro op EXP's test code.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "prtype:bugfix\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Cannot Compile TF-Nightly with CUDA 11.2 and CUDNN 8.1\n",
      "issue body -  **System information**\r\n",
      "- Windows 10\r\n",
      "- TensorFlow installed from (source or binary): source\r\n",
      "- TensorFlow version: tf-nightly\r\n",
      "- Python version: 3.8.6\r\n",
      "- Installed using virtualenv? pip? conda?: Built using instructions [here](https://www.tensorflow.org/install/source_windows)\r\n",
      "- Bazel version (if compiling from source): Bazel 3.7.2\r\n",
      "- GCC/Compiler version (if compiling from source): MSVC 14.28.29333\r\n",
      "- CUDA/cuDNN version: 11.2 / 8.1\r\n",
      "- GPU model and memory: Geforce RTX 3090 24GB\r\n",
      "\r\n",
      "When trying to build TF-nightly with CUDA 11.2 and CUDNN 8.1, this error occurs:\r\n",
      "`C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.28.29333\\include\\complex(354): error C2039: 'copysign': is not a member of ''global namespace''\r\n",
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.28.29333\\include\\complex(354): error C3861: 'copysign': identifier not found\r\n",
      "`\r\n",
      "\r\n",
      "This causes the build to fail.\r\n",
      "\r\n",
      "Steps to reproduce:\r\n",
      "1) Uninstall all global CUDA instances.\r\n",
      "2) Install CUDA 11.2\r\n",
      "3) Install CUDNN 8.1 by dragging the files into the install directory of CUDA\r\n",
      "4) Git clone tensorflow\r\n",
      "5) Checkout tf-nightly branch\r\n",
      "6) Set up Bazel, following [this](https://docs.bazel.build/versions/master/windows.html#build-c) guide (suggested by documentation)\r\n",
      "7) Follow the rest of the guide [here](https://www.tensorflow.org/install/source_windows).\r\n",
      "8) Execute bazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package\r\n",
      "9) Receive the error.\r\n",
      "\r\n",
      "Traceback is attached.\r\n",
      "[log.txt](https://github.com/tensorflow/tensorflow/files/5922256/log.txt)\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.5\n",
      "stalled\n",
      "stat:awaiting response\n",
      "subtype:windows\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  Fold StridedSliceOp when input is defined by ShapeOp.\n",
      "issue body -  Fixes #46879 and #46080. This PR adds a sub-shape folder from StridedSliceOp.\r\n",
      "\r\n",
      "Fold StridedSliceOp when input is defined by ShapeOp. The pattern is common in TF python library like\r\n",
      "\r\n",
      "```python\r\n",
      "height = tf.shape(x)[1]\r\n",
      "spatial_shape = tf.shape(x)[1:3]\r\n",
      "```\r\n",
      "\r\n",
      "When `x` has some dynamic dimensions (typically batch dim), `tf.shape` can not be folded so `height` and `spatial_shape` can not be inferred as a constant even if the corresponding dimensions are static. This PR folds this kind of patterns to improve sub-shape constant folding.\r\n",
      "\r\n",
      "Note that there is a workaround in python lib to use\r\n",
      "\r\n",
      "```python\r\n",
      "height = x.shape[1] or tf.shape(x)[1]\r\n",
      "spatial_shape = [x.shape[i] or tf.shape(x)[i] for i in (1, 2)]\r\n",
      "```\r\n",
      "\r\n",
      "to make sure constant is propagated. However, most of TensorFlow codes do not use this.\r\n",
      "\r\n",
      "Hi @abattery, would you mind taking a look at this? Thank you!\n",
      "issue labels - \n",
      "cla: yes\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  tf.strings.substr crashes(aborts) \n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n",
      "- TensorFlow installed from (source or binary): binary\r\n",
      "- TensorFlow version (use command below):2.1.0\r\n",
      "- Python version:3.7.6\r\n",
      "- Bazel version (if compiling from source):N/A\r\n",
      "- GCC/Compiler version (if compiling from source):N/A\r\n",
      "- CUDA/cuDNN version:N/A\r\n",
      "- GPU model and memory:N/A\r\n",
      "\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "`tf.strings.substr` crashes(aborts)  when `len(pos)` > `len(input)`\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "expect an exception message if the input unexpected instead of crash. \r\n",
      "\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "~~~python\r\n",
      "import tensorflow as tf\r\n",
      "tf.strings.substr(input='abc', len=1, pos=[1,-1])\r\n",
      "~~~\r\n",
      "\r\n",
      "~~~python\r\n",
      "import tensorflow as tf\r\n",
      "tf.strings.substr(input='abc', len=1, pos=[1,2])\r\n",
      "~~~\r\n",
      "\r\n",
      "\r\n",
      "Output:\r\n",
      "~~~python\r\n",
      "2021-02-03 22:46:41.234297: F ./tensorflow/core/framework/tensor.h:806] Check failed: new_num_elements == NumElements() (2 vs. 1)\r\n",
      "Aborted (core dumped)\r\n",
      "~~~\n",
      "issue labels - \n",
      "TF 2.1\n",
      "TF 2.4\n",
      "comp:ops\n",
      "stat:awaiting tensorflower\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Finish porting micro op zeros_like and its testing code\n",
      "issue body -  PR4 for Issue #46049\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  Add missing exception to Model.fit() docstring\n",
      "issue body -  The shuffle argument to the fit() method in the Model class gets ignored\r\n",
      "if the input x to the method is a tf.data.Dataset object. But this isn't\r\n",
      "documented in the docstring of the method.\r\n",
      "\r\n",
      "Signed-off-by: Suraj Upadhyay <usuraj35@gmail.com>\r\n",
      "\r\n",
      "Closes #46492\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:keras\n",
      "ready to pull\n",
      "size:XS\n",
      "type:docs-bug\n",
      "\n",
      "\n",
      "issue title -  [ROCm]  Update to use ROCm 4.0.1 (when building TF with --config=rocm) \n",
      "issue body -  /cc @cheshire @chsigg \n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:gpu\n",
      "ready to pull\n",
      "size:L\n",
      "\n",
      "\n",
      "issue title -  [ROCm] Workaround for a LLVM crash when doing codegen for MLIR generated Cast kernel\n",
      "issue body -  When you enable (uncomment) the `gen_kernel_library` rule for `cast` in `tensorflow/core/kernels/mlir_generated/BUILD`, the build would fail on the ROCm platform with the following error\r\n",
      "\r\n",
      "```\r\n",
      "LLVM ERROR: Cannot select: 0x56134e3c5b10: i1 = fp_to_sint 0x56134d1f53d8\r\n",
      "  0x56134d1f53d8: f16 = bitcast 0x56134e3c58a0\r\n",
      "    0x56134e3c58a0: i16,ch = load<(load 2 from %ir.lsr.iv)> 0x56134eaa3788, 0x56134e3c5698, undef:i64\r\n",
      "      0x56134e3c5698: i64,ch = CopyFromReg 0x56134eaa3788, Register:i64 %16\r\n",
      "        0x56134d41d620: i64 = Register %16\r\n",
      "      0x56134d1f5850: i64 = undef\r\n",
      "In function: Cast_f16_i1_kernel\r\n",
      "TensorFlow crashed, please file a bug on https://github.com/tensorflow/tensorflow/issues with the trace below.\r\n",
      "Stack dump:\r\n",
      "0.\tProgram arguments: bazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel --unroll_factors=4 --tile_sizes=256 --arch=gfx803,gfx900,gfx906,gfx908 --input=bazel-out/k8-opt/bin/tensorflow/core/kernels/mlir_generated/cast_f16_i1.mlir --output=bazel-out/k8-opt/bin/tensorflow/core/kernels/mlir_generated/cast_f16_i1_kernel_generator_kernel.o --enable_ftz=False\r\n",
      "1.\t2.\tRunning pass 'CallGraph Pass Manager' on module 'acme'.\r\n",
      "3.\tRunning pass 'AMDGPU DAG->DAG Pattern Instruction Selection' on function '@Cast_f16_i1_kernel'\r\n",
      "...\r\n",
      "...\r\n",
      "```\r\n",
      "\r\n",
      "Jack (@whchung) identified the root cause of this crash as lack of support for FPTOSI (fp16 to i1) instruction in the AMDGPU LLVM backend. The correct fix for this bug, is to add support for the same in the AMDGPU LLVM backend. Jack is in the process of upstreaming that fix to the LLVM repo.\r\n",
      "\r\n",
      "In the meantime (i.e. until the TF LLVM commit pointer is updated to point to a commit that includes Jack's fix), we need to workaround this on the TF side, by adding a pass that converts the `fptosi f16 to i1` op to `fptosi f16 to i16` + `trunci i16 to i1`, which is what this commit does.\r\n",
      "\r\n",
      "-----------------------------\r\n",
      "\r\n",
      "\r\n",
      "/cc @whchung @akuegel @cheshire @chsigg \n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:gpu\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  Update nn_impl.py\n",
      "issue body -  Removed redundant name argument.\r\n",
      "\r\n",
      "Fixes change suggested in Issue #40592\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:ops\n",
      "ready to pull\n",
      "size:XS\n",
      "type:docs-bug\n",
      "\n",
      "\n",
      "issue title -  give list of images instead of directory to \"flow_from_directory\" in keras image data generator.\n",
      "issue body -  <em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n",
      "\r\n",
      "\r\n",
      "**System information**\r\n",
      "- TensorFlow version (you are using): 2.2.0\r\n",
      "- Are you willing to contribute it (Yes/No): be an honor to.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**Describe the feature and the current behavior/state.** now, we just can pass directories of train, test and validation to the flow_from_directory for building the generator in keras. i want to give a list of image pathes with their label (like two lists) to build the generator.\r\n",
      "\r\n",
      "**Will this change the current api? How?** no.\r\n",
      "\r\n",
      "**Who will benefit with this feature?** me\r\n",
      "\r\n",
      "**Any Other info.** thanks\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.2\n",
      "comp:keras\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  TFLite C API: add cmake support\n",
      "issue body -  TFLite have cmake build support now, which is great. but we still need TFLite C API for prebuilt binary library workflow. This PR add cmake support for TFLite C API.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:lite\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  Keras UpSampling2D layer is converted with a `shape` op that should be constant-folded away\n",
      "issue body -  ### 1. System information\r\n",
      "\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.15.6\r\n",
      "- TensorFlow installation (pip package or built from source): PyPI pip package\r\n",
      "- TensorFlow library (version, if pip package or github SHA, if built from source): 2.4.1\r\n",
      "\r\n",
      "### 2. Code\r\n",
      "\r\n",
      "Provide code to help us reproduce your issues using one of the following options:\r\n",
      "\r\n",
      "```\r\n",
      "import tensorflow as tf\r\n",
      "\r\n",
      "m = tf.keras.models.Sequential(\r\n",
      "    [\r\n",
      "        tf.keras.layers.Input((32, 32, 3)),\r\n",
      "        tf.keras.layers.Conv2D(32, (3, 3)),\r\n",
      "        tf.keras.layers.UpSampling2D((2, 2), interpolation=\"nearest\"),\r\n",
      "    ]\r\n",
      ")\r\n",
      "\r\n",
      "converter = tf.lite.TFLiteConverter.from_keras_model(m)\r\n",
      "converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n",
      "with open(\"tmp-model.tflite\", \"wb\") as f:\r\n",
      "    f.write(converter.convert())\r\n",
      "```\r\n",
      "\r\n",
      "### 3. Failure after conversion\r\n",
      "\r\n",
      "The converted TFLite model looks like this:\r\n",
      "\r\n",
      "![image](https://user-images.githubusercontent.com/7688302/106741589-cde9de80-6613-11eb-96c0-60dfd88c3d61.png)\r\n",
      "\r\n",
      "Since the shape is statically known, I would expect the right-hand branch to be constant folded.\r\n",
      "\r\n",
      "Possibly related to #25086.\n",
      "issue labels - \n",
      "TF 2.4\n",
      "TFLiteConverter\n",
      "comp:lite\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  RaggedTensor support for mean_squared_logarithmic_error.\n",
      "issue body -  Follow up to #46283.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:keras\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  Update README.md :Added TensorFlow: Advanced Techniques Specialization to resources tab\n",
      "issue body -  Added TensorFlow: Advanced Techniques Specialization to resources tab. Extremely helpful specialization by Laurence Moroney for more in depth knowledge of TensorFlow.\n",
      "issue labels - \n",
      "cla: yes\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Add support for RaggedTensors to mean_absolute_percentage_error.\n",
      "issue body -  Follow up to #46875.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:keras\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  Add ragged tensor support to mean_absolute_error.\n",
      "issue body -  Use the same approach as PR #46283 for mean_absolute_error.\r\n",
      "\r\n",
      "Maintainers: I'm assuming that you would prefer the smallest possible PRs in order to extend the support across all/most predefined metric functions; even though that increases is overhead from your side in dealing with reviews / CI-handholding. Please advise if that is not the case.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:keras\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  fix typo\n",
      "issue body -  \n",
      "issue labels - \n",
      "cla: yes\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  [BUG]ConvertGraphDefToGraph return an invalid tensorflow::Graph\n",
      "issue body -  <em>Please make sure that this is a bug. As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:bug_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "- TensorFlow installed from (source or binary):\r\n",
      "> Binary\r\n",
      "- TensorFlow version (use command below):\r\n",
      "> 1.15 and 2.4.0\r\n",
      "- Python version:\r\n",
      "> 3.7.5\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version:\r\n",
      "- GPU model and memory:\r\n",
      "\r\n",
      "You can collect some of this information using our environment capture\r\n",
      "[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n",
      "You can also obtain the TensorFlow version with:\r\n",
      "1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n",
      "2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n",
      "\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "\r\n",
      "I want to use the C++ API to process my pbtxt model. After the GraphDef file is correctly loaded from the file, I continue to call tensorflow::ConvertGraphDefToGraph to convert my model into a tensorflow::Graph object. The call is successful, however, nodes on graph objects are invalid. For example, core dump occurs when in_edge is invoked.\r\n",
      "\r\n",
      "For further verification, I register a pass optimizer for the POST_REWRITE_FOR_EXEC phase on the TF of version 2.4, which prints only the input graph,\r\n",
      "\r\n",
      "> part of my pass\r\n",
      "\r\n",
      "```C\r\n",
      "    LOG(INFO) << \"------------------------------------Inputs------------------------------------\";\r\n",
      "    const tensorflow::EdgeSet &in_edges = node->out_edges();\r\n",
      "    for (auto edge : in_edges) {\r\n",
      "      if (edge == nullptr) {\r\n",
      "        LOG(INFO) << \"    nullptr\";\r\n",
      "      } else {\r\n",
      "        LOG(INFO) << \"    \" << edge->src()->DebugString();\r\n",
      "      }\r\n",
      "    }\r\n",
      "\r\n",
      "    LOG(INFO) << \"------------------------------------Outputs------------------------------------\";\r\n",
      "    const tensorflow::EdgeSet &out_edges = node->out_edges();\r\n",
      "    for (auto edge : out_edges) {\r\n",
      "      if (edge == nullptr) {\r\n",
      "        LOG(INFO) << \"    nullptr\";\r\n",
      "      } else {\r\n",
      "        LOG(INFO) << \"    \" << edge->dst()->DebugString();\r\n",
      "      }\r\n",
      "    }\r\n",
      "```\r\n",
      "\r\n",
      "> python test code\r\n",
      "\r\n",
      "```PYTHON\r\n",
      "@tf.function\r\n",
      "def f(a, b):\r\n",
      "  return a + b\r\n",
      "\r\n",
      "f(tf.constant(1), tf.constant(2))\r\n",
      "```\r\n",
      "> and the result\r\n",
      "\r\n",
      "```\r\n",
      "{name:'_SOURCE' id:0 source}\r\n",
      "------------------------------------Inputs------------------------------------\r\n",
      "    {name:'_SOURCE' id:0 source}\r\n",
      "    {name:'_SOURCE' id:0 source}\r\n",
      "    nullptr\r\n",
      "------------------------------------Outputs------------------------------------\r\n",
      "    {name:'a' id:2 op device:{/job:localhost/replica:0/task:0/device:CPU:0} def:{{{node a}} = _Arg[T=DT_INT32, _output_shapes=[[]], _user_specified_name=\"a\", index=0, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()}}\r\n",
      "    {name:'b' id:3 op device:{/job:localhost/replica:0/task:0/device:CPU:0} def:{{{node b}} = _Arg[T=DT_INT32, _output_shapes=[[]], _user_specified_name=\"b\", index=1, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()}}\r\n",
      "    nullptr\r\n",
      "{name:'b' id:3 op device:{/job:localhost/replica:0/task:0/device:CPU:0} def:{{{node b}} = _Arg[T=DT_INT32, _output_shapes=[[]], _user_specified_name=\"b\", index=1, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()}}\r\n",
      "------------------------------------Inputs------------------------------------\r\n",
      "    nullptr\r\n",
      "------------------------------------Outputs------------------------------------\r\n",
      "    nullptr\r\n",
      "{name:'a' id:2 op device:{/job:localhost/replica:0/task:0/device:CPU:0} def:{{{node a}} = _Arg[T=DT_INT32, _output_shapes=[[]], _user_specified_name=\"a\", index=0, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()}}\r\n",
      "------------------------------------Inputs------------------------------------\r\n",
      "    nullptr\r\n",
      "------------------------------------Outputs------------------------------------\r\n",
      "    nullptr\r\n",
      "{name:'add' id:4 op device:{/job:localhost/replica:0/task:0/device:CPU:0} def:{{{node add}} = AddV2[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](a, b)}}\r\n",
      "------------------------------------Inputs------------------------------------\r\n",
      "    nullptr\r\n",
      "------------------------------------Outputs------------------------------------\r\n",
      "    nullptr\r\n",
      "{name:'identity_RetVal' id:5 op device:{/job:localhost/replica:0/task:0/device:CPU:0} def:{{{node identity_RetVal}} = _Retval[T=DT_INT32, index=0, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](add)}}\r\n",
      "------------------------------------Inputs------------------------------------\r\n",
      "    nullptr\r\n",
      "------------------------------------Outputs------------------------------------\r\n",
      "    nullptr\r\n",
      "{name:'_SINK' id:1 sink}\r\n",
      "------------------------------------Inputs------------------------------------\r\n",
      "    nullptr\r\n",
      "------------------------------------Outputs------------------------------------\r\n",
      "    nullptr\r\n",
      "```\r\n",
      "\r\n",
      "Obviously, the edges on the figure are completely disordered. For example, AddV2 has the nullptr edge* input in both the input and output.\r\n",
      "\r\n",
      "I have also verified the ConvertGraphDefToGraph interface and the result is the same as that described above. I'm sure this function isn't working properly.\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "\r\n",
      "ConvertGraphDefToGraph should return a valid tensorflow::Graph object, including complete input and output edge information.\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "Provide a reproducible test case that is the bare minimum necessary to generate\r\n",
      "the problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n",
      "\r\n",
      "**Other info / logs** Include any logs or source code that would be helpful to\r\n",
      "diagnose the problem. If including tracebacks, please include the full\r\n",
      "traceback. Large logs and files should be attached.\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  Shakespeare's style writing generation model cannot be exported with TensorFlow 2.3\n",
      "issue body -  [text_generation.ipynb]: https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/text_generation.ipynb\r\n",
      "[Colab notebook]: https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/text_generation.ipynb\r\n",
      "## URL(s) with the issue:\r\n",
      "\r\n",
      "- TensorFlow Tutorial: Text generation with an RNN (the one that produces Shakespeare's style writing using RNN).\r\n",
      "\r\n",
      "    https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/text_generation.ipynb\r\n",
      "\r\n",
      "- Accompanying Colab.\r\n",
      "\r\n",
      "    https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/text_generation.ipynb\r\n",
      "\r\n",
      "## Description of issue (what needs changing):\r\n",
      "\r\n",
      "With TensorFlow 2.4, the notebook [text_generation.ipynb] above works as expected (without throwing any error), but this is not the case with TensorFlow 2.3.\r\n",
      "\r\n",
      "A trivial solution is to upgrade to TensorFlow 2.4. However, this would require, when using pre-built TensorFlow, switching to CUDA 11 (cf. https://www.tensorflow.org/install/source#gpu). In a cloud environment I use from time to time, CUDA 11 is not yet supported; this is where my trouble comes from.\r\n",
      "\r\n",
      "So the notebook better works well with TensorFlow 2.3 or higher.\r\n",
      "\r\n",
      "### Clear description\r\n",
      "\r\n",
      "To reproduce the issue:\r\n",
      "\r\n",
      "1. Open the [Colab notebook].\r\n",
      "1. Select Runtime > Change runtime type > GPU.\r\n",
      "1. insert the line `!pip install 'tensorflow<2.4'` before the line `import tensorflow as tf`.\r\n",
      "1. Select Runtime > Run all.\r\n",
      "\r\n",
      "Environmental setup, text preprocessing, model building, and training run smoothly, but after the following cell in the \"Export the generator\" section, a `ValueError` is raised:\r\n",
      "\r\n",
      "```\r\n",
      "tf.saved_model.save(one_step_model, 'one_step')\r\n",
      "one_step_reloaded = tf.saved_model.load('one_step')\r\n",
      "```\r\n",
      "\r\n",
      "```\r\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7f020a19e860>, because it is not built.\r\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\r\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\r\n",
      "INFO:tensorflow:Assets written to: one_step/assets\r\n",
      "---------------------------------------------------------------------------\r\n",
      "ValueError                                Traceback (most recent call last)\r\n",
      "<ipython-input-45-5089d08e324f> in <module>()\r\n",
      "      1 tf.saved_model.save(one_step_model, 'one_step')\r\n",
      "----> 2 one_step_reloaded = tf.saved_model.load('one_step')\r\n",
      "\r\n",
      "9 frames\r\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saving/saveable_object_util.py in _add_saveable(saveables, seen_ops, saveable)\r\n",
      "    329   if saveable.op in seen_ops:\r\n",
      "    330     raise ValueError(\"The same saveable will be restored with two names: %s\" %\r\n",
      "--> 331                      saveable.name)\r\n",
      "    332   saveables.append(saveable)\r\n",
      "    333   seen_ops.add(saveable.op)\r\n",
      "\r\n",
      "ValueError: The same saveable will be restored with two names: ids_from_chars/_table/.ATTRIBUTES/table\r\n",
      "```\r\n",
      "\r\n",
      "Just so you know, this notebook is not compatible with TensorFlow 2.2 for the very reason that `StringLookup` is not supported; running it with TensorFlow 2.2 raises an exception as follows:\r\n",
      "\r\n",
      "```\r\n",
      "ids_from_chars = preprocessing.StringLookup(\r\n",
      "    vocabulary=list(vocab))\r\n",
      "```\r\n",
      "\r\n",
      "```\r\n",
      "---------------------------------------------------------------------------\r\n",
      "AttributeError                            Traceback (most recent call last)\r\n",
      "<ipython-input-9-aea607cfc6f7> in <module>()\r\n",
      "----> 1 ids_from_chars = preprocessing.StringLookup(\r\n",
      "      2     vocabulary=list(vocab))\r\n",
      "\r\n",
      "AttributeError: module 'tensorflow.keras.layers.experimental.preprocessing' has no attribute 'StringLookup'\r\n",
      "```\r\n",
      "\r\n",
      "So the `ValueError` in concern is an exception peculiar to TensorFlow 2.3. So far I have no idea why the notebook is not compatible with TensorFlow 2.3, and any help or a single hint will be very much appreciated.\n",
      "issue labels - \n",
      "TF 2.3\n",
      "comp:apis\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  A mistake code on Model Maker Guide\n",
      "issue body -  The image classification code on [official Model Maker Guide](https://tensorflow.google.cn/lite/guide/model_maker) may get a mistake on training data name.\r\n",
      "\r\n",
      "![516441612286382_ pic_hd](https://user-images.githubusercontent.com/17171866/106685499-76437700-6603-11eb-83c3-6e4098e44183.jpg)\r\n",
      "\r\n",
      "Mistake code in above red rectangle, I think should be replace with following code snippet.\r\n",
      "```\r\n",
      "model = image_classifier.create(train_data)\r\n",
      "```\r\n",
      "\n",
      "issue labels - \n",
      "comp:lite\n",
      "comp:lite-examples\n",
      "\n",
      "\n",
      "issue title -  [INTEL MKL] oneDNN partials and Dockerfiles for CentOS 7\n",
      "issue body -  This PR introduces:\r\n",
      "\r\n",
      "- Partials and Dockerfiles for oneDNN based on `CentOS 7` \r\n",
      "- Misc updates and fixes for existing oneDNN partials\r\n",
      "- Install Horovod from `git+https` which seems to be more reliable and flexible too\r\n",
      "\r\n",
      "Like before this is how you generate the new Docker files and build the images:\r\n",
      "\r\n",
      "```\r\n",
      "$ export DOCKER_BUILD_ARGS=<extra docker build args specific to your environment, like proxies>\r\n",
      "$ alias db=\"docker build ${DOCKER_BUILD_ARGS}\"\r\n",
      "$ export DOCKER_RUN_ENVS=\"extra docker run environment variables specific to your environment, like proxies\"\r\n",
      "$ alias dr=\"docker run --disable-content-trust ${DOCKER_RUN_ENVS}\"\r\n",
      "```\r\n",
      "\r\n",
      "Finally start building contaiers:\r\n",
      "\r\n",
      "```\r\n",
      "$ cd tensorflow/tools/dockerfiles\r\n",
      "$ db -t tf-tools -f tools.Dockerfile .\r\n",
      "\r\n",
      "$ alias asm_dockerfiles=\"dr --rm -u $(id -u):$(id -g) -v $(pwd):/tf tf-tools python3 assembler.py \"\r\n",
      "$ alias asm_images=\"dr --rm -v $(pwd):/tf -v /var/run/docker.sock:/var/run/docker.sock tf-tools python3 assembler.py \"\r\n",
      "$ asm_dockerfiles --release dockerfiles --construct_dockerfiles\r\n",
      "\r\n",
      "$ TF_VERSION=2.4.0 HOROVOD_VERSION=v0.21.1 && asm_images ${PARTIALS_BUILD_ARGS} --release onednn --repository intel/intel-optimized-tensorflow --arg BAZEL_VERSION=3.1.0 --arg TF_BRANCH=v${TF_VERSION} --arg TF_PACKAGE_VERSION=${TF_VERSION} --arg _TAG_PREFIX=${TF_VERSION}-centos --build_images --only_tags_matching '.*centos-7'\r\n",
      "```\r\n",
      "\r\n",
      "and this will produce the following images:\r\n",
      "```\r\n",
      "intel/intel-optimized-tensorflow:2.4.0-centos-7\r\n",
      "intel/intel-optimized-tensorflow:2.4.0-centos-7-devel\r\n",
      "intel/intel-optimized-tensorflow:2.4.0-centos-7-devel-jupyter\r\n",
      "intel/intel-optimized-tensorflow:2.4.0-centos-7-devel-mpich-horovod\r\n",
      "intel/intel-optimized-tensorflow:2.4.0-centos-7-devel-mpich-horovod-jupyter\r\n",
      "intel/intel-optimized-tensorflow:2.4.0-centos-7-devel-mpi-horovod\r\n",
      "intel/intel-optimized-tensorflow:2.4.0-centos-7-devel-mpi-horovod-jupyter\r\n",
      "intel/intel-optimized-tensorflow:2.4.0-centos-7-jupyter\r\n",
      "intel/intel-optimized-tensorflow:2.4.0-centos-7-mpich-horovod\r\n",
      "intel/intel-optimized-tensorflow:2.4.0-centos-7-mpich-horovod-jupyter\r\n",
      "intel/intel-optimized-tensorflow:2.4.0-centos-7-mpi-horovod\r\n",
      "intel/intel-optimized-tensorflow:2.4.0-centos-7-mpi-horovod-jupyter\r\n",
      "```\r\n",
      "\n",
      "issue labels - \n",
      "cla: yes\n",
      "ready to pull\n",
      "size:XL\n",
      "\n",
      "\n",
      "issue title -  Improve MicroProfiler and make per-op profiling the default in the benchmarks.\n",
      "issue body -  * Maintain state within the MicroProfiler object such that all the logging can happen external to the interpreter.\r\n",
      "* Refactor the benchmarks to make use of this new functionality.\r\n",
      "\r\n",
      "This command (keyword benchmark with xtensa Fusion F1):\r\n",
      "```\r\n",
      "make -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=fusion_f1 XTENSA_CORE=F1_190305_swupgrade test_keyword_benchmark -j8\r\n",
      "```\r\n",
      "\r\n",
      "gives the following output at tip-of-tree:\r\n",
      "```\r\n",
      "InitializeKeywordRunner() took 199262 ticks (199 ms)\r\n",
      "KeywordRunNIerations(1) took 152158 ticks (152 ms)\r\n",
      "KeywordRunNIerations(10) took 1521087 ticks (1521 ms)\r\n",
      "```\r\n",
      "\r\n",
      "and with this change:\r\n",
      "```\r\n",
      "InitializeKeywordRunner took 152689 ticks (152 ms).\r\n",
      "\r\n",
      "KeywordRunNIerations(1) took 152628 ticks (152 ms)\r\n",
      "QUANTIZE took 3753 ticks (3 ms).\r\n",
      "SVDF took 37998 ticks (37 ms).\r\n",
      "FULLY_CONNECTED took 1363 ticks (1 ms).\r\n",
      "SVDF took 18798 ticks (18 ms).\r\n",
      "FULLY_CONNECTED took 1363 ticks (1 ms).\r\n",
      "SVDF took 18798 ticks (18 ms).\r\n",
      "FULLY_CONNECTED took 1363 ticks (1 ms).\r\n",
      "SVDF took 18798 ticks (18 ms).\r\n",
      "FULLY_CONNECTED took 1363 ticks (1 ms).\r\n",
      "SVDF took 13902 ticks (13 ms).\r\n",
      "SVDF took 15822 ticks (15 ms).\r\n",
      "SVDF took 15822 ticks (15 ms).\r\n",
      "FULLY_CONNECTED took 1087 ticks (1 ms).\r\n",
      "SOFTMAX took 2037 ticks (2 ms).\r\n",
      "QUANTIZE took 361 ticks (0 ms).\r\n",
      "\r\n",
      "KeywordRunNIerations(10) took 1526280 ticks (1526 ms)\r\n",
      "```\r\n",
      "\r\n",
      "Note:\r\n",
      " * overall increase due to the additional book-keeping for profiling is minimal (470 ticks).\r\n",
      " * The initialize time has decreased because we are no longer including the time needed to compute a random input and copy it into the input buffer as part of the initialization.\r\n",
      "\r\n",
      "Partially addresses http://b/158212576\r\n",
      "\r\n",
      "\r\n",
      "For completeness, here is the output of the person_detection_benchmark:\r\n",
      "```\r\n",
      "make -f tensorflow/lite/micro/tools/make/Makefile test_person_detection_benchmark -j8\r\n",
      "```\r\n",
      "\r\n",
      "tip-of-tree:\r\n",
      "```\r\n",
      "InitializeBenchmarkRunner() took 260 ticks (0 ms)\r\n",
      "benchmark_runner->RunSingleIteration() took 31238 ticks (31 ms)\r\n",
      "PersonDetectionTenIerationsWithPerson() took 295380 ticks (295 ms)\r\n",
      "PersonDetectionTenIerationsWithoutPerson() took 288194 ticks (288 ms)\r\n",
      "```\r\n",
      "\r\n",
      "with this change:\r\n",
      "```\r\n",
      "InitializeBenchmarkRunner took 302 ticks (0 ms).\r\n",
      "\r\n",
      "WithPersonDataIterations(1) took 25590 ticks (25 ms)\r\n",
      "DEPTHWISE_CONV_2D took 903 ticks (0 ms).\r\n",
      "DEPTHWISE_CONV_2D took 887 ticks (0 ms).\r\n",
      "CONV_2D took 1378 ticks (1 ms).\r\n",
      "DEPTHWISE_CONV_2D took 453 ticks (0 ms).\r\n",
      "CONV_2D took 1051 ticks (1 ms).\r\n",
      "DEPTHWISE_CONV_2D took 987 ticks (0 ms).\r\n",
      "CONV_2D took 2043 ticks (2 ms).\r\n",
      "DEPTHWISE_CONV_2D took 225 ticks (0 ms).\r\n",
      "CONV_2D took 1071 ticks (1 ms).\r\n",
      "DEPTHWISE_CONV_2D took 495 ticks (0 ms).\r\n",
      "CONV_2D took 1907 ticks (1 ms).\r\n",
      "DEPTHWISE_CONV_2D took 118 ticks (0 ms).\r\n",
      "CONV_2D took 938 ticks (0 ms).\r\n",
      "DEPTHWISE_CONV_2D took 234 ticks (0 ms).\r\n",
      "CONV_2D took 1817 ticks (1 ms).\r\n",
      "DEPTHWISE_CONV_2D took 228 ticks (0 ms).\r\n",
      "CONV_2D took 1909 ticks (1 ms).\r\n",
      "DEPTHWISE_CONV_2D took 224 ticks (0 ms).\r\n",
      "CONV_2D took 1769 ticks (1 ms).\r\n",
      "DEPTHWISE_CONV_2D took 224 ticks (0 ms).\r\n",
      "CONV_2D took 1797 ticks (1 ms).\r\n",
      "DEPTHWISE_CONV_2D took 223 ticks (0 ms).\r\n",
      "CONV_2D took 1787 ticks (1 ms).\r\n",
      "DEPTHWISE_CONV_2D took 80 ticks (0 ms).\r\n",
      "CONV_2D took 921 ticks (0 ms).\r\n",
      "DEPTHWISE_CONV_2D took 99 ticks (0 ms).\r\n",
      "CONV_2D took 1809 ticks (1 ms).\r\n",
      "AVERAGE_POOL_2D took 8 ticks (0 ms).\r\n",
      "CONV_2D took 2 ticks (0 ms).\r\n",
      "RESHAPE took 1 ticks (0 ms).\r\n",
      "SOFTMAX took 2 ticks (0 ms).\r\n",
      "\r\n",
      "NoPersonDataIterations(1) took 25310 ticks (25 ms)\r\n",
      "DEPTHWISE_CONV_2D took 912 ticks (0 ms).\r\n",
      "DEPTHWISE_CONV_2D took 988 ticks (0 ms).\r\n",
      "CONV_2D took 1356 ticks (1 ms).\r\n",
      "DEPTHWISE_CONV_2D took 510 ticks (0 ms).\r\n",
      "CONV_2D took 1013 ticks (1 ms).\r\n",
      "DEPTHWISE_CONV_2D took 964 ticks (0 ms).\r\n",
      "CONV_2D took 2004 ticks (2 ms).\r\n",
      "DEPTHWISE_CONV_2D took 323 ticks (0 ms).\r\n",
      "CONV_2D took 1012 ticks (1 ms).\r\n",
      "DEPTHWISE_CONV_2D took 480 ticks (0 ms).\r\n",
      "CONV_2D took 1770 ticks (1 ms).\r\n",
      "DEPTHWISE_CONV_2D took 113 ticks (0 ms).\r\n",
      "CONV_2D took 882 ticks (0 ms).\r\n",
      "DEPTHWISE_CONV_2D took 266 ticks (0 ms).\r\n",
      "CONV_2D took 1766 ticks (1 ms).\r\n",
      "DEPTHWISE_CONV_2D took 219 ticks (0 ms).\r\n",
      "CONV_2D took 1743 ticks (1 ms).\r\n",
      "DEPTHWISE_CONV_2D took 217 ticks (0 ms).\r\n",
      "CONV_2D took 2004 ticks (2 ms).\r\n",
      "DEPTHWISE_CONV_2D took 217 ticks (0 ms).\r\n",
      "CONV_2D took 1640 ticks (1 ms).\r\n",
      "DEPTHWISE_CONV_2D took 206 ticks (0 ms).\r\n",
      "CONV_2D took 1677 ticks (1 ms).\r\n",
      "DEPTHWISE_CONV_2D took 52 ticks (0 ms).\r\n",
      "CONV_2D took 1083 ticks (1 ms).\r\n",
      "DEPTHWISE_CONV_2D took 110 ticks (0 ms).\r\n",
      "CONV_2D took 1771 ticks (1 ms).\r\n",
      "AVERAGE_POOL_2D took 7 ticks (0 ms).\r\n",
      "CONV_2D took 2 ticks (0 ms).\r\n",
      "RESHAPE took 1 ticks (0 ms).\r\n",
      "SOFTMAX took 2 ticks (0 ms).\r\n",
      "\r\n",
      "WithPersonDataIterations(10) took 254901 ticks (254 ms)\r\n",
      "\r\n",
      "NoPersonDataIterations(10) took 241536 ticks (241 ms)\r\n",
      "```\r\n",
      "\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:L\n",
      "\n",
      "\n",
      "issue title -  [TFLM] Keyword benchmark broken when using generated Makefile project\n",
      "issue body -  See github issue: https://github.com/tensorflow/tensorflow/issues/46860\r\n",
      "\r\n",
      "When building the keyword benchmark like this: make -f tensorflow/lite/micro/tools/make/Makefile generate_keyword_benchmark_make_project\r\n",
      "\r\n",
      "Doesn't build (see above issue). This PR fixes that.\r\n",
      "\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "size:XS\n",
      "stat:awaiting response\n",
      "\n",
      "\n",
      "issue title -  [TFLM] keyword benchmark broken when using generated Makefile projects\n",
      "issue body -  @tensorflow/micro\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu\r\n",
      "- TensorFlow installed from (source or binary): source\r\n",
      "- Tensorflow version (commit SHA if source):\r\n",
      "- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "When building the keyword benchmark project like this:\r\n",
      "make -f tensorflow/lite/micro/tools/make/Makefile  generate_keyword_benchmark_make_project\r\n",
      "\r\n",
      "I get 2 errors. One is due to micro_benchmark.h not being copied into the generated project, the other is a duplicate object error for **g_keyword_scrambled_model_data**. That happens because keyword_scrambled_model_data.cc somehow appears twice in the generated Makefile :)\r\n",
      "\r\n",
      "I will open a PR with a fix shortly.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "comp:lite\n",
      "comp:micro\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Requesting MatrixDeterminant op in tflite\n",
      "issue body -  <em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n",
      "\r\n",
      "\r\n",
      "**System information**\r\n",
      "- TensorFlow version (you are using): 2.3+\r\n",
      "- Are you willing to contribute it (Yes/No): I would need guidance / access to someone who knows the tflite codebase\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**Describe the feature and the current behavior/state.**\r\n",
      "I request that the operator MatrixDeterminant be added to the tensorflow-lite operators.  Currently, tensorflow/core/kernels/determinant_op.[cc/h] defines the standard tensorflow MatrixDeterminant operation, but there appears to be no such operation for tflite.\r\n",
      "\r\n",
      "I am trying to convert the Glow model (https://github.com/openai/glow) to tflite, but I am running into the issue where I can't convert the trained pb file and invoke it because there is no MatrixDeterminant operation.\r\n",
      "\r\n",
      "**Will this change the current api? How?**\r\n",
      "It would add an operation for MatrixDeterminant to tflite's operators.\r\n",
      "\r\n",
      "**Who will benefit with this feature?**\r\n",
      "Anyone trying to convert a model containing the MatrixDeterminant operation to tflite.\r\n",
      "\r\n",
      "**Any Other info.**\r\n",
      "\n",
      "issue labels - \n",
      "comp:lite\n",
      "comp:lite-kernels\n",
      "type:feature\n",
      "\n",
      "\n",
      "issue title -  TFLite: tools: make: remove test_delegate_providers.cc from the build resources for libtensorflow-lite.a\n",
      "issue body -  lite/kernel/test_delegate_providers.cc should not be include in\r\n",
      "libtensorflow-lite.a but it was not par of CORE_CC_EXCLUDE_SRCS because the\r\n",
      "file syntax was not recognized as an excluded file.\r\n",
      "This patch propose to exclude all the file containing in *test*.cc.\r\n",
      "\r\n",
      "Signed-off-by: Vincent ABRIOU <vincent.abriou@st.com>\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:lite\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  TF 2.5 nightly CUDA driver check failed \n",
      "issue body -  <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): linux ubuntu 18.04\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "- TensorFlow installed from (source or binary): binary\r\n",
      "- TensorFlow version: 2.5.0-dev20210202\r\n",
      "- Python version: 3.6.9\r\n",
      "- Installed using virtualenv? pip? conda?: Pip \r\n",
      "- Bazel version (if compiling from source):\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version: 11.0/8.0.4\r\n",
      "- GPU model and memory: 2 x RTX 3090 w/ 24GB\r\n",
      "\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "I have installed the tensorflow 2.5 nightly builds: \r\n",
      "```\r\n",
      "pip install tf-nightly-gpu\r\n",
      "pip install tf-nightly\r\n",
      "```\r\n",
      "into a virtual environment to try and address performance issues with using a dual RTX 3090 setup. When I try and execute the python script I get the following error from the CUDA driver: \r\n",
      "```\r\n",
      "2021-02-02 09:48:40.071972: F tensorflow/stream_executor/cuda/cuda_driver.cc:1289] Check failed: context != nullptr success should entail non-null context\r\n",
      "```\r\n",
      "\r\n",
      "I assume this might be linked to the CUDA/cuDNN revision being used but I can't find any instructions as to the versions being of CUDA/cuDNN being expected. \r\n",
      "\r\n",
      "**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n",
      "```\r\n",
      "pip install tf-nightly-gpu\r\n",
      "pip install tf-nightly\r\n",
      "\r\n",
      "**run python script** \r\n",
      "\r\n",
      "```\r\n",
      "\r\n",
      "N.B. the script is working as it executes under other versions of TF. \r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.5\n",
      "stat:awaiting response\n",
      "subtype: ubuntu/linux\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  Issue is not managed well\n",
      "issue body -  Friends, I opened this issue https://github.com/tensorflow/tensorflow/issues/46315 already 3 weeks ago and it look like it is not prioritized and no ETA.... And actually for me it looks like \"talking\" and not \"doing\" progress.\r\n",
      "I would like to understand if it is regular situation when the updates from support side come a week after ?\r\n",
      "\r\n",
      "Thanks\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:lite\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  LinearRegression Example in ForwardAccumulator docstring has an error\n",
      "issue body -  ## URL(s) with the issue:\r\n",
      "\r\n",
      "https://github.com/tensorflow/tensorflow/blob/85c8b2a817f95a3e979ecd1ed95bff1dc1335cff/tensorflow/python/eager/forwardprop.py#L234\r\n",
      "\r\n",
      "## Description of issue (what needs changing):\r\n",
      "\r\n",
      "I was trying to understand precisely how ForwardAccumulator.jvp works and was working through the example to which I have linked. From computing by hand what I believed to be the correct jvp formula, and comparing it to the linear regression code I found that in the line\r\n",
      "\r\n",
      "  ...   loss = tf.reduce_sum((dense(x) - tf.constant([1., -1.])) ** 2.)\r\n",
      "\r\n",
      "the target \r\n",
      "tf.constant([1., -1.]) \r\n",
      "which I would call 'y', should be reshaped, for example replaced by \r\n",
      "tf.constant([[1.], [-1.]]) \r\n",
      "\r\n",
      "The code as it stands introduces a multiplicative factor of n in the loss function where n is the number of samples. This same problem occurs in other places in this docstring. \r\n",
      "\r\n",
      "Would it be possible to include somewhere in this docstring a formula for the jacobian-vector-product?\r\n",
      "\n",
      "issue labels - \n",
      "comp:eager\n",
      "stat:awaiting tensorflower\n",
      "type:docs-bug\n",
      "\n",
      "\n",
      "issue title -  Creating custom model for object detection\n",
      "issue body -  How can I create a custom model from a directory containing some images????????????I dont want to download a pre trained model.I want to make my own model for object detection.How can I do that??????\n",
      "issue labels - \n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  gpt2 int8 quantization:  op_context.input->type == kTfLiteUInt8 || op_context.input->type == kTfLiteInt8 || op_context.input->type == kTfLiteInt16 || op_context.input->type == kTfLiteFloat16 was not true.Node number 15 (DEQUANTIZE) failed to prepare \n",
      "issue body -  TF: 2.4.1 \r\n",
      "Huggingface/transformers: 4.2.2\r\n",
      "Python: 3.8\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "The int8 quantization during the exporting has no error or problem. But it would throw an error when it is invoked. \r\n",
      "```\r\n",
      "RuntimeError: tensorflow/lite/kernels/dequantize.cc:61 op_context.input->type == kTfLiteUInt8 || op_context.input->type == kTfLiteInt8 || op_context.input->type == kTfLiteInt16 || op_context.input->type == kTfLiteFloat16 was not true.Node number 15 (DEQUANTIZE) failed to prepare.\r\n",
      "```\r\n",
      "Note if we only use `last_hidden_state` for tflite, this would have no problem. It seems like `tf.matmul` is a problem.\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "Provide a reproducible test case that is the bare minimum necessary to generate\r\n",
      "the problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n",
      "\r\n",
      "```\r\n",
      "import random\r\n",
      "\r\n",
      "import numpy as np\r\n",
      "import tensorflow as tf\r\n",
      "from transformers import *\r\n",
      "\r\n",
      "rng = random.Random()\r\n",
      "\r\n",
      "gpt2_model = TFGPT2Model.from_pretrained('distilgpt2')\r\n",
      "\r\n",
      "\r\n",
      "def get_tf_lm_head_tensor():\r\n",
      "    gpt2_lm_pt_model = GPT2LMHeadModel.from_pretrained('distilgpt2')\r\n",
      "    np_tensor = gpt2_lm_pt_model.lm_head.weight.detach().numpy()\r\n",
      "    np_tensor = np.transpose(np_tensor)\r\n",
      "    tf_lm_head_tensor = tf.convert_to_tensor(np_tensor)\r\n",
      "    return tf_lm_head_tensor\r\n",
      "\r\n",
      "\r\n",
      "tf_lm_head = get_tf_lm_head_tensor()\r\n",
      "\r\n",
      "@tf.function(input_signature=[tf.TensorSpec(shape=(1, None), dtype=tf.int32, name=\"input_ids\")])\r\n",
      "def serving_func(input_ids):\r\n",
      "    outputs = gpt2_model(input_ids, training=False)\r\n",
      "    last_hidden_state = outputs[0][0][-1:, :]\r\n",
      "    next_token_logits = tf.matmul(last_hidden_state, tf_lm_head)\r\n",
      "    next_token = tf.math.argmax(next_token_logits, axis=-1, output_type=tf.int32)\r\n",
      "    log_probs = tf.math.reduce_max(tf.nn.log_softmax(next_token_logits))\r\n",
      "    return {\"decoded_ids\": next_token, \"log_probs\": log_probs}\r\n",
      "\r\n",
      "\r\n",
      "tensors = []\r\n",
      "for example in range(100):\r\n",
      "    values = [[rng.randint(0, 30000) for _ in range(8)]]\r\n",
      "    tensors.append(np.array(values, dtype=np.int32))\r\n",
      "\r\n",
      "\r\n",
      "def representative_dataset_gen():\r\n",
      "    for ts in tensors:\r\n",
      "        yield [ts]\r\n",
      "\r\n",
      "\r\n",
      "converter = tf.lite.TFLiteConverter.from_concrete_functions([serving_func.get_concrete_function(\r\n",
      "    tf.TensorSpec(shape=(1, None), dtype=tf.int32, name='input_ids'))])\r\n",
      "# converter.experimental_new_converter = True\r\n",
      "converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n",
      "converter.representative_dataset = representative_dataset_gen\r\n",
      "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS,\r\n",
      "                                       tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n",
      "tflite_quant_model = converter.convert()\r\n",
      "with open(\"/tmp/model.tflite\", 'wb') as f:\r\n",
      "    f.write(tflite_quant_model)\r\n",
      "\r\n",
      "# invoke the model\r\n",
      "import tensorflow as tf\r\n",
      "\r\n",
      "# Load TFLite model and allocate tensors.\r\n",
      "interpreter = tf.lite.Interpreter(model_path=\"/tmp/model.tflite\")\r\n",
      "interpreter.allocate_tensors()\r\n",
      "\r\n",
      "# Get input and output tensors.\r\n",
      "input_details = interpreter.get_input_details()\r\n",
      "output_details = interpreter.get_output_details()\r\n",
      "\r\n",
      "import numpy as np\r\n",
      "# Test the TensorFlow Lite model on random input data.\r\n",
      "input_shape = input_details[0]['shape']\r\n",
      "input_data = np.array(np.random.random_sample(input_shape), dtype=np.int32)\r\n",
      "interpreter.set_tensor(input_details[0]['index'], input_data)\r\n",
      "\r\n",
      "interpreter.invoke()\r\n",
      "```\r\n",
      "\r\n",
      "**Other info / logs** Include any logs or source code that would be helpful to\r\n",
      "diagnose the problem. If including tracebacks, please include the full\r\n",
      "traceback. Large logs and files should be attached.\r\n",
      "\n",
      "issue labels - \n",
      "Fixed in Nightly\n",
      "ModelOptimizationToolkit\n",
      "TF 2.4\n",
      "comp:lite\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Distributed training using Parameterstrategy \n",
      "issue body -  Hi All,\r\n",
      "\r\n",
      "I am new to Tensorflow and trying to implement distributed Tensorflow using ParameterStrategy based on the [Documentation][1]. So far I have the code below\r\n",
      "\r\n",
      "  [1]: https://www.tensorflow.org/tutorials/distribute/parameter_server_training\r\n",
      "```\r\n",
      "import multiprocessing\r\n",
      "import os\r\n",
      "import portpicker\r\n",
      "import tensorflow as tf\r\n",
      "import tensorflow.keras as keras\r\n",
      "import tensorflow.keras.layers.experimental.preprocessing as kpl\r\n",
      "import tensorflow_hub as hub\r\n",
      "import numpy as np\r\n",
      "print(tf.__version__)\r\n",
      "\r\n",
      "def create_in_process_cluster(num_workers, num_ps):\r\n",
      "  \"\"\"Creates and starts local servers and returns the cluster_resolver.\"\"\"\r\n",
      "  worker_ports = [portpicker.pick_unused_port() for _ in range(num_workers)]\r\n",
      "  ps_ports = [portpicker.pick_unused_port() for _ in range(num_ps)]\r\n",
      "\r\n",
      "  cluster_dict = {}\r\n",
      "  cluster_dict[\"worker\"] = [\"localhost:%s\" % port for port in worker_ports]\r\n",
      "  if num_ps > 0:\r\n",
      "    cluster_dict[\"ps\"] = [\"localhost:%s\" % port for port in ps_ports]\r\n",
      "\r\n",
      "  cluster_spec = tf.train.ClusterSpec(cluster_dict)\r\n",
      "\r\n",
      "  # Workers need some inter_ops threads to work properly.\r\n",
      "  worker_config = tf.compat.v1.ConfigProto()\r\n",
      "  if multiprocessing.cpu_count() < num_workers + 1:\r\n",
      "    worker_config.inter_op_parallelism_threads = num_workers + 1\r\n",
      "\r\n",
      "  for i in range(num_workers):\r\n",
      "    tf.distribute.Server(\r\n",
      "        cluster_spec, job_name=\"worker\", task_index=i, config=worker_config,\r\n",
      "        protocol=\"grpc\")\r\n",
      "\r\n",
      "  for i in range(num_ps):\r\n",
      "    tf.distribute.Server(\r\n",
      "        cluster_spec, job_name=\"ps\", task_index=i, protocol=\"grpc\")\r\n",
      "\r\n",
      "  cluster_resolver = tf.distribute.cluster_resolver.SimpleClusterResolver(\r\n",
      "      cluster_spec, task_id=0, task_type=\"worker\",rpc_layer=\"grpc\")\r\n",
      "  return cluster_resolver\r\n",
      "\r\n",
      "# Set the environment variable to allow reporting worker and ps failure to the\r\n",
      "# coordinator. This is a workaround and won't be necessary in the future.\r\n",
      "os.environ[\"GRPC_FAIL_FAST\"] = \"use_caller\"\r\n",
      "\r\n",
      "NUM_WORKERS = 3\r\n",
      "NUM_PS = 2\r\n",
      "cluster_resolver = create_in_process_cluster(NUM_WORKERS, NUM_PS)\r\n",
      "\r\n",
      "variable_partitioner = (\r\n",
      "    tf.distribute.experimental.partitioners.FixedShardsPartitioner(\r\n",
      "        num_shards=NUM_PS))\r\n",
      "\r\n",
      "strategy = tf.distribute.experimental.ParameterServerStrategy(cluster_resolver)\r\n",
      "\r\n",
      "word = \"Elephant\"\r\n",
      "sentence = \"I am a sentence for which I would like to get its embedding.\"\r\n",
      "paragraph = (\r\n",
      "    \"Universal Sentence Encoder embeddings also support short paragraphs. \"\r\n",
      "    \"There is no hard limit on how long the paragraph is. Roughly, the longer \"\r\n",
      "    \"the more 'diluted' the embedding will be.\")\r\n",
      "messages = [word, sentence, paragraph]\r\n",
      "labels=[\"1\",\"2\",\"3\"]\r\n",
      "reviews = [[1,0,0],[0,1,0],[0,0,1]]\r\n",
      "\r\n",
      "encoder=hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\r\n",
      "\r\n",
      "X_train=encoder(messages)\r\n",
      "\r\n",
      "with strategy.scope():\r\n",
      "  feature_lookup_layer = kpl.StringLookup(vocabulary=messages)\r\n",
      "\r\n",
      "  label_lookup_layer = kpl.StringLookup(vocabulary=labels,\r\n",
      "                                        num_oov_indices=0,\r\n",
      "                                        mask_token=None)\r\n",
      "\r\n",
      "  raw_feature_input = keras.layers.Input(\r\n",
      "      shape=(1,), dtype=tf.string, name=\"feature\")\r\n",
      "  feature_id_input = feature_lookup_layer(raw_feature_input)\r\n",
      "  feature_preprocess_stage = keras.Model(\r\n",
      "      {\"features\": raw_feature_input}, feature_id_input)\r\n",
      "\r\n",
      "  raw_label_input = keras.layers.Input(\r\n",
      "      shape=(3,), dtype=tf.string, name=\"label\")\r\n",
      "  label_id_input = label_lookup_layer(raw_label_input)\r\n",
      "  label_preprocess_stage = keras.Model({\"label\": raw_label_input}, label_id_input)\r\n",
      "\r\n",
      "examples = {\"features\": [word,sentence,paragraph], \"label\": [[\"1\",\"0\",\"0\"],[\"0\",\"1\",\"0\"],[\"0\",\"0\",\"1\"]]}\r\n",
      "print(examples)\r\n",
      "def dataset_fn(_):\r\n",
      "  raw_dataset = tf.data.Dataset.from_tensor_slices(examples)\r\n",
      "\r\n",
      "  train_dataset = raw_dataset.map(\r\n",
      "      lambda x: (\r\n",
      "          {\"features\": feature_preprocess_stage(x[\"features\"])},\r\n",
      "          label_preprocess_stage(x[\"label\"])\r\n",
      "      )).shuffle(200).batch(32).repeat()\r\n",
      "  return train_dataset\r\n",
      "\r\n",
      "\r\n",
      "# These variables created under the `strategy.scope` will be placed on parameter\r\n",
      "# servers in a round-robin fashion.\r\n",
      "with strategy.scope():\r\n",
      "  # Create the model. The input needs to be compatible with KPLs.\r\n",
      "  model_input = keras.layers.Input(\r\n",
      "      shape=(3,), dtype=tf.int64, name=\"model_input\")\r\n",
      "\r\n",
      "  emb_layer = keras.layers.Embedding(\r\n",
      "      input_dim=len(feature_lookup_layer.get_vocabulary()), output_dim=20)\r\n",
      "  emb_output = tf.reduce_mean(emb_layer(model_input), axis=1)\r\n",
      "  dense_output = keras.layers.Dense(units=1, activation=\"sigmoid\")(emb_output)\r\n",
      "  model = keras.Model({\"features\": model_input}, dense_output)\r\n",
      "\r\n",
      "  optimizer = keras.optimizers.RMSprop(learning_rate=0.1)\r\n",
      "  accuracy = keras.metrics.Accuracy()\r\n",
      "\r\n",
      "\r\n",
      "@tf.function\r\n",
      "def step_fn(iterator):\r\n",
      "\r\n",
      "  def replica_fn(batch_data, labels):\r\n",
      "    with tf.GradientTape() as tape:\r\n",
      "      pred = model(batch_data, training=True)\r\n",
      "      per_example_loss = keras.losses.CategoricalCrossentropy(\r\n",
      "              reduction=tf.keras.losses.Reduction.NONE)(labels, pred)\r\n",
      "      loss = tf.nn.compute_average_loss(per_example_loss)\r\n",
      "      gradients = tape.gradient(loss, model.trainable_variables)\r\n",
      "\r\n",
      "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n",
      "\r\n",
      "    actual_pred = tf.cast(tf.greater(pred, 0.5), tf.int64)\r\n",
      "    accuracy.update_state(labels, actual_pred)\r\n",
      "    return loss\r\n",
      "\r\n",
      "  batch_data, labels = next(iterator)\r\n",
      "  losses = strategy.run(replica_fn, args=(batch_data, labels))\r\n",
      "  return strategy.reduce(tf.distribute.ReduceOp.SUM, losses, axis=None)\r\n",
      "\r\n",
      "\r\n",
      "coordinator = tf.distribute.experimental.coordinator.ClusterCoordinator(strategy)\r\n",
      "@tf.function\r\n",
      "def per_worker_dataset_fn():\r\n",
      "  return strategy.distribute_datasets_from_function(dataset_fn)\r\n",
      "\r\n",
      "\r\n",
      "per_worker_dataset = coordinator.create_per_worker_dataset(per_worker_dataset_fn)\r\n",
      "per_worker_iterator = iter(per_worker_dataset)\r\n",
      "\r\n",
      "num_epoches = 2\r\n",
      "steps_per_epoch = 1\r\n",
      "for i in range(num_epoches):\r\n",
      "  accuracy.reset_states()\r\n",
      "  for _ in range(steps_per_epoch):\r\n",
      "    coordinator.schedule(step_fn, args=(per_worker_iterator,))\r\n",
      "  # Wait at epoch boundaries.\r\n",
      "  coordinator.join()\r\n",
      "  print (\"Finished epoch %d, accuracy is %f.\" % (i, accuracy.result().numpy()))\r\n",
      "```\r\n",
      "\r\n",
      "In this example, I'm trying to convert the document example from binary classification to Categorical classification. But I'm getting the following error.\r\n",
      "\r\n",
      "> ValueError: Input 0 of layer dense is incompatible with the layer: : expected min_ndim=2, found ndim=1. Full shape received: (None,)\r\n",
      " \r\n",
      "I have Tensotflow `2.4.1` version  \n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:dist-strat\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  [TFLM] CEVA-DSP platform - Update makefiles for CEVA-ToolBox 18.05\n",
      "issue body -  Dealt with the issues regarding the makesfiles from PR: https://github.com/tensorflow/tensorflow/pull/46500\r\n",
      "And updated to latest CEVA-Toolbox, moving from clang 7.0.1 to clang 9.0.1 which solved some building issues for us.\r\n",
      "\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "comp:micro:ceva\n",
      "ready to pull\n",
      "size:L\n",
      "\n",
      "\n",
      "issue title -  Update tf.keras.layers.convolutional.py with extended docstring information.\n",
      "issue body -  Update tf.keras.layers.convolutional.py docstrings with better information about kernel/bias initializers and consistency throughout each of the different operations. \n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:keras\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  Update nn_ops.py\n",
      "issue body -  Fix [#46834](https://github.com/tensorflow/tensorflow/issues/46834) and [#46994](https://github.com/tensorflow/tensorflow/issues/46994)\r\n",
      "On having kernel size as 0 , the code crashes on execution. This pull request will raise a **FloatingPointError** upon getting a value of 0 for the `ksize` argument in `nn_ops`.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:ops\n",
      "prtype:bugfix\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  FATAL ERROR: tensorflow/core/framework/types.pb.h: No such file or directory\n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes. Standard C++ program used to previously compile and run with TF 1.1 (CUDA - 9.2 and CUDNN 7.2 )\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n",
      "- TensorFlow installed from (source or binary): Source\r\n",
      "- TensorFlow version (use command below): 2.3.0\r\n",
      "- Python version: 3.6.9\r\n",
      "- Bazel version (if compiling from source): 3.1.0\r\n",
      "- GCC/Compiler version (if compiling from source): 7.5\r\n",
      "- CUDA/cuDNN version: CUDA - 10.2/ CUDNN - 7.6\r\n",
      "- GPU model and memory: Quadro P5200, 32 Gb RAM\r\n",
      "\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "TF 2.3.0 successfully installs with bazel. However, while compiling a C++ program using gcc, I get the following error\r\n",
      "_**/usr/local/include/tensorflow/tensorflow/core/framework/tensor_shape.h:22:10: fatal error: tensorflow/core/framework/types.pb.h: No such file or directory**_\r\n",
      "I tried to locate the types.pb.h file and found the file in _/home/%username%/.cache/bazel/_bazel_%username%/3fa00a5b455754a3e6fd353fefb67596/execroot/org_tensorflow/bazel-out/k8-opt-exec-50AE0418/bin/tensorflow/core/framework/types.pb.h_\r\n",
      "Copying or including this folder/file did not help as additional dependencies were missing. \r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "Clone tensorflow from github and checkout v2.3.0. Run ./configure with following options (refer attachment)\r\n",
      "Bazel build with the following options:\r\n",
      "_bazel build -c opt \\\r\n",
      "            --copt=-mavx \\\r\n",
      "            --copt=-mavx2 \\\r\n",
      "            --copt=-mfma \\\r\n",
      "            --copt=-mfpmath=both \\\r\n",
      "            --copt=-msse4.2 \\\r\n",
      "            --config=cuda //tensorflow:libtensorflow_cc.so_\r\n",
      "[TF_Config.txt](https://github.com/tensorflow/tensorflow/files/5906638/TF_Config.txt)\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.3\n",
      "comp:apis\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Bug or me? Using a generator with a dataset, impossible to correctly specify shapes\n",
      "issue body -  <em>Please make sure that this is a bug. As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:bug_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10/Ubuntu 18.04/Macos\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "- TensorFlow installed from (source or binary): Binary\r\n",
      "- TensorFlow version (use command below):v2.4.0-rc4-71-g582c8d236cb 2.4.0\r\n",
      "- Python version: 3.8.5\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version: 11.1\r\n",
      "- GPU model and memory: 1080TI 8gigs\r\n",
      "\r\n",
      "You can collect some of this information using our environment capture\r\n",
      "[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n",
      "You can also obtain the TensorFlow version with:\r\n",
      "1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n",
      "2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n",
      "\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "Creating a functional model with multiple outputs seems to work fine if all of the data is in memory or being sourced from files, but using a generator it seems to be impossible to properly define the shapes.  I've tried both the deprecated `output_*` options and the current `output_signature` options to no avail.  It is entirely possible it is just me, but if so, it is possible the the documentation might be a tad incorrect. :)\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "I would expect that the shape of the data can be properly defined and used for fitting!\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "Provide a reproducible test case that is the bare minimum necessary to generate\r\n",
      "the problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n",
      "\r\n",
      "This code will compile fine but fail to fit with an error concerning the data shape.  I've redefined the data to be any of a variety of shapes... As soon as I switch to multiple outputs, this fails.\r\n",
      "\r\n",
      "```\r\n",
      "import numpy as np\r\n",
      "import tensorflow as tf\r\n",
      "from tensorflow import keras\r\n",
      "from tensorflow.keras import layers, models\r\n",
      "\r\n",
      "def generate_sample():\r\n",
      "    x = list(\"123456789\")\r\n",
      "    y = list(\"2345\")\r\n",
      "    while 1:\r\n",
      "        yield np.array(x).astype(np.float32),[np.array(y).astype(np.float32),np.array(y).astype(np.float32)]\r\n",
      "\r\n",
      "dataset = tf.data.Dataset.from_generator(generate_sample,\r\n",
      "            output_signature=(\r\n",
      "                 tf.TensorSpec(shape=(9,), dtype=tf.float32),\r\n",
      "                 tf.TensorSpec(shape=(2,4), dtype=tf.float32)\r\n",
      "\r\n",
      "            ))\r\n",
      "\r\n",
      "dataset = dataset.batch(batch_size=32)\r\n",
      "\r\n",
      "inputs = keras.Input(shape=(next(generate_sample())[0].shape))\r\n",
      "x = layers.Dense(512, activation = \"relu\")(inputs)\r\n",
      "x_outputs = layers.Dense(4, activation=\"relu\", name=\"output\")(x)\r\n",
      "y_outputs = layers.Dense(4, activation=\"relu\", name=\"output2\")(x)\r\n",
      "\r\n",
      "model = keras.Model(inputs=inputs, outputs=[x_outputs,y_outputs])\r\n",
      "model.compile(loss=\"mse\", optimizer = \"adam\", metrics=['accuracy'])\r\n",
      "history = model.fit(dataset, epochs=1, steps_per_epoch=10, validation_data=dataset, validation_steps=5)\r\n",
      "```\r\n",
      "\r\n",
      "**Other info / logs** Include any logs or source code that would be helpful to\r\n",
      "diagnose the problem. If including tracebacks, please include the full\r\n",
      "traceback. Large logs and files should be attached.\r\n",
      "\r\n",
      "```\r\n",
      "---------------------------------------------------------------------------\r\n",
      "InvalidArgumentError                      Traceback (most recent call last)\r\n",
      "<ipython-input-102-3dad39c1e2c1> in <module>\r\n",
      "----> 1 history = model.fit(dataset, epochs=1, steps_per_epoch=10, validation_data=dataset, validation_steps=5)\r\n",
      "\r\n",
      "~\\anaconda3\\envs\\SEC595\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n",
      "   1098                 _r=1):\r\n",
      "   1099               callbacks.on_train_batch_begin(step)\r\n",
      "-> 1100               tmp_logs = self.train_function(iterator)\r\n",
      "   1101               if data_handler.should_sync:\r\n",
      "   1102                 context.async_wait()\r\n",
      "\r\n",
      "~\\anaconda3\\envs\\SEC595\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py in __call__(self, *args, **kwds)\r\n",
      "    826     tracing_count = self.experimental_get_tracing_count()\r\n",
      "    827     with trace.Trace(self._name) as tm:\r\n",
      "--> 828       result = self._call(*args, **kwds)\r\n",
      "    829       compiler = \"xla\" if self._experimental_compile else \"nonXla\"\r\n",
      "    830       new_tracing_count = self.experimental_get_tracing_count()\r\n",
      "\r\n",
      "~\\anaconda3\\envs\\SEC595\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py in _call(self, *args, **kwds)\r\n",
      "    886         # Lifting succeeded, so variables are initialized and we can run the\r\n",
      "    887         # stateless function.\r\n",
      "--> 888         return self._stateless_fn(*args, **kwds)\r\n",
      "    889     else:\r\n",
      "    890       _, _, _, filtered_flat_args = \\\r\n",
      "\r\n",
      "~\\anaconda3\\envs\\SEC595\\lib\\site-packages\\tensorflow\\python\\eager\\function.py in __call__(self, *args, **kwargs)\r\n",
      "   2940       (graph_function,\r\n",
      "   2941        filtered_flat_args) = self._maybe_define_function(args, kwargs)\r\n",
      "-> 2942     return graph_function._call_flat(\r\n",
      "   2943         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\r\n",
      "   2944 \r\n",
      "\r\n",
      "~\\anaconda3\\envs\\SEC595\\lib\\site-packages\\tensorflow\\python\\eager\\function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n",
      "   1916         and executing_eagerly):\r\n",
      "   1917       # No tape is watching; skip to running the function.\r\n",
      "-> 1918       return self._build_call_outputs(self._inference_function.call(\r\n",
      "   1919           ctx, args, cancellation_manager=cancellation_manager))\r\n",
      "   1920     forward_backward = self._select_forward_and_backward_functions(\r\n",
      "\r\n",
      "~\\anaconda3\\envs\\SEC595\\lib\\site-packages\\tensorflow\\python\\eager\\function.py in call(self, ctx, args, cancellation_manager)\r\n",
      "    553       with _InterpolateFunctionError(self):\r\n",
      "    554         if cancellation_manager is None:\r\n",
      "--> 555           outputs = execute.execute(\r\n",
      "    556               str(self.signature.name),\r\n",
      "    557               num_outputs=self._num_outputs,\r\n",
      "\r\n",
      "~\\anaconda3\\envs\\SEC595\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n",
      "     57   try:\r\n",
      "     58     ctx.ensure_initialized()\r\n",
      "---> 59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n",
      "     60                                         inputs, attrs, num_outputs)\r\n",
      "     61   except core._NotOkStatusException as e:\r\n",
      "\r\n",
      "InvalidArgumentError:  Incompatible shapes: [32,2,4] vs. [32,4]\r\n",
      "\t [[node mean_squared_error/SquaredDifference (defined at <ipython-input-102-3dad39c1e2c1>:1) ]] [Op:__inference_train_function_12605]\r\n",
      "\r\n",
      "Function call stack:\r\n",
      "train_function\r\n",
      "```\n",
      "issue labels - \n",
      "TF 2.4\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  floating point exception in tf.nn.avg_pool3d and tf.nn.max_pool3dwhen ksize=0\n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n",
      "- TensorFlow installed from (source or binary): binary\r\n",
      "- TensorFlow version (use command below):2.1.0\r\n",
      "- Python version:3.7.6\r\n",
      "- Bazel version (if compiling from source):N/A\r\n",
      "- GCC/Compiler version (if compiling from source):N/A\r\n",
      "- CUDA/cuDNN version:N/A\r\n",
      "- GPU model and memory:N/A\r\n",
      "\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "floating point exception in `tf.nn.avg_pool3d` and `tf.nn.max_pool3d` when `ksize`=0\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "expect no crash\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "\r\n",
      "The code crashes in nightly version too. Check out the [gist](https://colab.research.google.com/drive/10O4Qn2S4uW-40jLDHvlUBN_vlMsQkUo-?usp=sharing)\r\n",
      "~~~python\r\n",
      "tf.nn.avg_pool3d(input=tf.ones((1,1,1,1,1)), strides=1, ksize=0, padding='VALID')\r\n",
      "~~~\r\n",
      "\r\n",
      "~~~python\r\n",
      "tf.nn.max_pool3d(input=tf.ones((1,1,1,1,1)), strides=1, ksize=0, padding='VALID')\r\n",
      "~~~\r\n",
      "Output:\r\n",
      "~~~python\r\n",
      "Floating point exception (core dumped)\r\n",
      "~~~\n",
      "issue labels - \n",
      "TF 2.1\n",
      "comp:apis\n",
      "stat:awaiting tensorflower\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Error when converting a resnetv1_50 based model trained with Tensorflow.\n",
      "issue body -  ### 1. System information\r\n",
      "\r\n",
      "- OS Platform and Distribution: Windows 10\r\n",
      "- TensorFlow installation (pip package or built from source): pip\r\n",
      "- TensorFlow library (version, if pip package or github SHA, if built from source): Tensorflow 1.15\r\n",
      "\r\n",
      "### 2. Code\r\n",
      "\r\n",
      "```\r\n",
      "from tensorflow import lite\r\n",
      "## Training model code is omitted here ##\r\n",
      "\r\n",
      "saver.save(sess, path_to_save, global_step=it)\r\n",
      "        \r\n",
      "# Converting a GraphDef from session.\r\n",
      "converter = lite.TFLiteConverter.from_session(sess, list(batch.values()), posenet.output_tensors)\r\n",
      "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\n",
      "converter.allow_custom_ops=True\r\n",
      "tflite_model = converter.convert()\r\n",
      "open(\"./converted_model.tflite\", \"wb\").write(tflite_model)\r\n",
      "```\r\n",
      "\r\n",
      "\r\n",
      "#### Option B: Paste your code here or provide a link to a custom end-to-end colab\r\n",
      "\r\n",
      "```\r\n",
      "[Link to my notebook in drive](https://drive.google.com/file/d/1xnmr69QOCmUByZbDQNTBLBp0RQbbKYqb/view?usp=sharing)\r\n",
      "```\r\n",
      "\r\n",
      "### 3. Failure after conversion\r\n",
      "If the conversion is successful, but the generated model is wrong, then state what is wrong:\r\n",
      "The conversion didn't work.\r\n",
      "\r\n",
      "### 5. (optional) Any other info / logs\r\n",
      "**These are my inputs:**\r\n",
      "[<tf.Tensor 'fifo_queue_Dequeue:0' shape=(1, 200, 200, 3) dtype=float32>,\r\n",
      "<tf.Tensor 'fifo_queue_Dequeue:1' shape=(1, 26, 26, 21) dtype=float32>,\r\n",
      "<tf.Tensor 'fifo_queue_Dequeue:2' shape=(1, 26, 26, 21) dtype=float32>,\r\n",
      "<tf.Tensor 'fifo_queue_Dequeue:3' shape=(1, 26, 26, 42) dtype=float32>,\r\n",
      "<tf.Tensor 'fifo_queue_Dequeue:4' shape=(1, 26, 26, 42) dtype=float32>]\r\n",
      "\r\n",
      "**These are my outputs:**\r\n",
      "[<tf.Tensor 'pose/part_pred/block4/BiasAdd:0' shape=(1, 26, 26, 21) dtype=float32>,\r\n",
      "<tf.Tensor 'pose/locref_pred/block4/BiasAdd:0' shape=(1, 26, 26, 42) dtype=float32>]\r\n",
      "\r\n",
      "\r\n",
      "```\r\n",
      "I'm showing the error output below, any help will be useful:\r\n",
      "`---------------------------------------------------------------------------\r\n",
      "ConverterError                            Traceback (most recent call last)\r\n",
      "<ipython-input-10-458b03f29263> in <module>\r\n",
      "     30         converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\n",
      "     31         converter.allow_custom_ops=True\r\n",
      "---> 32         tflite_model = converter.convert()\r\n",
      "     33 #         open(\"./converted_model.tflite\", \"wb\").write(tflite_model)\r\n",
      "     34 #         with open('model.tflite', 'wb') as f:\r\n",
      "\r\n",
      "~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\lite\\python\\lite.py in convert(self)\r\n",
      "    981           input_tensors=self._input_tensors,\r\n",
      "    982           output_tensors=self._output_tensors,\r\n",
      "--> 983           **converter_kwargs)\r\n",
      "    984     else:\r\n",
      "    985       result = _toco_convert_graph_def(\r\n",
      "\r\n",
      "~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\lite\\python\\convert.py in toco_convert_impl(input_data, input_tensors, output_tensors, enable_mlir_converter, *args, **kwargs)\r\n",
      "    447       input_data.SerializeToString(),\r\n",
      "    448       debug_info_str=debug_info_str,\r\n",
      "--> 449       enable_mlir_converter=enable_mlir_converter)\r\n",
      "    450   return data\r\n",
      "    451 \r\n",
      "\r\n",
      "~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\lite\\python\\convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n",
      "    198       stdout = _try_convert_to_unicode(stdout)\r\n",
      "    199       stderr = _try_convert_to_unicode(stderr)\r\n",
      "--> 200       raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\n",
      "    201   finally:\r\n",
      "    202     # Must manually cleanup files.\r\n",
      "\r\n",
      "ConverterError: See console for info.\r\n",
      "El sistema no puede encontrar la ruta especificada.\r\n",
      "2021-02-01 17:29:45.263535: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_100.dll'; dlerror: cudart64_100.dll not found\r\n",
      "2021-02-01 17:29:45.264019: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n",
      "c:\\users\\pipita\\anaconda3\\envs\\aws_train\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n",
      "c:\\users\\pipita\\anaconda3\\envs\\aws_train\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n",
      "c:\\users\\pipita\\anaconda3\\envs\\aws_train\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n",
      "c:\\users\\pipita\\anaconda3\\envs\\aws_train\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n",
      "c:\\users\\pipita\\anaconda3\\envs\\aws_train\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n",
      "c:\\users\\pipita\\anaconda3\\envs\\aws_train\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n",
      "2021-02-01 17:29:48.157783: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: FIFOQueueV2\r\n",
      "2021-02-01 17:29:48.158185: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n",
      "2021-02-01 17:29:48.290435: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: QueueDequeueV2\r\n",
      "2021-02-01 17:29:48.322668: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 685 operators, 1055 arrays (0 quantized)\r\n",
      "2021-02-01 17:29:48.343643: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 683 operators, 1054 arrays (0 quantized)\r\n",
      "2021-02-01 17:29:48.367174: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 683 operators, 1054 arrays (0 quantized)\r\n",
      "2021-02-01 17:29:48.582688: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 92 operators, 231 arrays (0 quantized)\r\n",
      "2021-02-01 17:29:48.586252: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 92 operators, 231 arrays (0 quantized)\r\n",
      "2021-02-01 17:29:48.588560: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 92 operators, 231 arrays (0 quantized)\r\n",
      "2021-02-01 17:29:48.593331: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 10880000 bytes, theoretical optimal value: 7680000 bytes.\r\n",
      "2021-02-01 17:29:48.594149: I tensorflow/lite/toco/toco_tooling.cc:439] Estimated count of arithmetic ops: 10608315996 ops, equivalently 5304157998 MACs\r\n",
      "2021-02-01 17:29:48.594470: I tensorflow/lite/toco/toco_tooling.cc:454] Number of parameters: 24644870`\r\n",
      "\r\n",
      "```\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 1.15\n",
      "TFLiteConverter\n",
      "stalled\n",
      "stat:awaiting response\n",
      "\n",
      "\n",
      "issue title -  TF 2.4.1: Mirrored Strategy not providing performance boost RTX 3090 \n",
      "issue body -  <em>Please make sure that this is a bug. As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:bug_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  Yes\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "- TensorFlow installed from (source or binary): binary\r\n",
      "- TensorFlow version (use command below): 2.4.1\r\n",
      "- Python version: 3.6.9\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version: 11.0/8.0.4.30\r\n",
      "- GPU model and memory: 2x RTX 3090/24GB\r\n",
      "\r\n",
      "You can collect some of this information using our environment capture\r\n",
      "[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n",
      "You can also obtain the TensorFlow version with:\r\n",
      "1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n",
      "2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n",
      "\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "\r\n",
      "I have a dataset created from an image directory (tf.keras.preprocessing.image_dataset_from_directory()). I then use a mirrored strategy (tf.distribute.MirroredStrategy) to create and compile my model then I try training the model using model.fit(). The call to model.fit causes an error message to inform me that the dataset is unshardable. I am therefore not receiving a performance increase from the second RTX 3090 GPU that I have just installed in the machine. \r\n",
      "\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "\r\n",
      "When I used this approach using Tensorflow 2.3.1 and 2 x RTX 2060s, I did not generate any error messages about sharding the dataset, the log printed that it was running with a mirrored strategy, and listed the GPUs being used, and there was a significant performance boost\r\n",
      "\r\n",
      "**Standalone code snippet to reproduce the issue**\r\n",
      "\r\n",
      "```\r\n",
      "\r\n",
      "def get_compiled_model(num_classes, dim_x, dim_y, dim_z):\r\n",
      "\r\n",
      "        data_augmentation = keras.Sequential(\r\n",
      "          [\r\n",
      "                layers.experimental.preprocessing.RandomFlip(\"horizontal\", input_shape=(dim_x, dim_y, dim_z)),\r\n",
      "                layers.experimental.preprocessing.RandomRotation(0.1),\r\n",
      "                layers.experimental.preprocessing.RandomZoom(0.1),\r\n",
      "          ]\r\n",
      "        )\r\n",
      "\r\n",
      "\r\n",
      "        model = Sequential([\r\n",
      "          data_augmentation,\r\n",
      "          layers.experimental.preprocessing.Rescaling(1./255),\r\n",
      "          layers.Conv2D(2, 3, padding='same', activation='relu'),\r\n",
      "          layers.MaxPooling2D(),\r\n",
      "          #layers.Conv2D(32, 3, padding='same', activation='relu'),\r\n",
      "          #layers.MaxPooling2D(),\r\n",
      "          #layers.Conv2D(64, 3, padding='same', activation='relu'),\r\n",
      "          #layers.MaxPooling2D(),\r\n",
      "          layers.Dropout(0.2),\r\n",
      "          layers.Flatten(),\r\n",
      "          layers.Dense(6, activation='relu'),\r\n",
      "          layers.Dropout(0.2),\r\n",
      "          layers.Dense(num_classes)\r\n",
      "        ])\r\n",
      "        model.compile(\r\n",
      "                #optimizer=tf.keras.optimizers.Adam(),\r\n",
      "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n",
      "                optimizer=tf.keras.optimizers.Adam(),\r\n",
      "                metrics=['accuracy'],\r\n",
      "        )\r\n",
      "\r\n",
      "        return model\r\n",
      "\r\n",
      "\r\n",
      "## create data set \r\n",
      "\r\n",
      "        train_ds = tf.keras.preprocessing.image_dataset_from_directory(\r\n",
      "          data_dir,\r\n",
      "          validation_split=0.2,\r\n",
      "          subset=\"training\",\r\n",
      "          seed=123,\r\n",
      "          image_size=(50, 50),\r\n",
      "          batch_size=2048,\r\n",
      "          color_mode='grayscale'\r\n",
      "        )\r\n",
      "        val_ds = tf.keras.preprocessing.image_dataset_from_directory(\r\n",
      "          data_dir,\r\n",
      "          validation_split=0.2,\r\n",
      "          subset=\"validation\",\r\n",
      "          seed=123,\r\n",
      "          image_size=(50, 50),\r\n",
      "          batch_size=2048,\r\n",
      "          color_mode='grayscale'\r\n",
      "        )\r\n",
      "\r\n",
      "\r\n",
      "## create model with mirrored strategy \r\n",
      "        mirrored_strategy = tf.distribute.MirroredStrategy()\r\n",
      "\r\n",
      "        with mirrored_strategy.scope():\r\n",
      "\r\n",
      "                model = get_compiled_model(2, 50, 50, 1):\r\n",
      "\r\n",
      "\r\n",
      "## train model \r\n",
      "\r\n",
      "        history = model.fit(\r\n",
      "          train_ds,\r\n",
      "          validation_data=val_ds,\r\n",
      "          epochs=5000,\r\n",
      "          initial_epoch=0,\r\n",
      "        )\r\n",
      "\r\n",
      "```\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:dist-strat\n",
      "regression issue\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:performance\n",
      "\n",
      "\n",
      "issue title -  TFLM: Add new Cortex M target for running on a FVP\n",
      "issue body -  * Adds new target for running on a fixed virtual platform based on Arm\r\n",
      "  Corstone-300 software.\r\n",
      "* Adds test script for running with FVP.\r\n",
      "* Adds new download scripts.\r\n",
      "* Adds new CI script.\r\n",
      "* Adds readme file.\r\n",
      "\r\n",
      "\r\n",
      "This is fixing: https://github.com/tensorflow/tensorflow/issues/46829\r\n",
      "\r\n",
      "The CI script takes about 5 minutes to run for me.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "comp:micro:arm\n",
      "ready to pull\n",
      "size:L\n",
      "\n",
      "\n",
      "issue title -  tensorflow 2.4 ParameterServerStrategy + Estimator will be stuck\n",
      "issue body -  <em>Please make sure that this is a bug. As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:bug_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "- TensorFlow installed from (source or binary): binary\r\n",
      "- TensorFlow version (use command below): 2.4\r\n",
      "- Python version:\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version:\r\n",
      "- GPU model and memory:\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "tf version: 2.4\r\n",
      "cluster spec:\r\n",
      "ParameterServerStrategyV2 is now connecting to cluster with cluster_spec: ClusterSpec({'chief': ['tensorflow-tess-search-rk2-mlp-339212-chief-0.mlp.svc:2222'], 'evaluator': ['tensorflow-tess-search-rk2-mlp-339212-evaluator-0.mlp.svc:2222'], 'ps': ['tensorflow-tess-search-rk2-mlp-339212-ps-0.mlp.svc:2222', 'tensorflow-tess-search-rk2-mlp-339212-ps-1.mlp.svc:2222', 'tensorflow-tess-search-rk2-mlp-339212-ps-2.mlp.svc:2222', 'tensorflow-tess-search-rk2-mlp-339212-ps-3.mlp.svc:2222', 'tensorflow-tess-search-rk2-mlp-339212-ps-4.mlp.svc:2222', 'tensorflow-tess-search-rk2-mlp-339212-ps-5.mlp.svc:2222', 'tensorflow-tess-search-rk2-mlp-339212-ps-6.mlp.svc:2222', 'tensorflow-tess-search-rk2-mlp-339212-ps-7.mlp.svc:2222', 'tensorflow-tess-search-rk2-mlp-339212-ps-8.mlp.svc:2222', 'tensorflow-tess-search-rk2-mlp-339212-ps-9.mlp.svc:2222', 'tensorflow-tess-search-rk2-mlp-339212-ps-10.mlp.svc:2222', 'tensorflow-tess-search-rk2-mlp-339212-ps-11.mlp.svc:2222', 'tensorflow-tess-search-rk2-mlp-339212-ps-12.mlp.svc:2222', 'tensorflow-tess-search-rk2-mlp-339212-ps-13.mlp.svc:2222', 'tensorflow-tess-search-rk2-mlp-339212-ps-14.mlp.svc:2222', 'tensorflow-tess-search-rk2-mlp-339212-ps-15.mlp.svc:2222'], 'worker': ['tensorflow-tess-search-rk2-mlp-339212-worker-0.mlp.svc:2222', 'tensorflow-tess-search-rk2-mlp-339212-worker-1.mlp.svc:2222', 'tensorflow-tess-search-rk2-mlp-339212-worker-2.mlp.svc:2222', 'tensorflow-tess-search-rk2-mlp-339212-worker-3.mlp.svc:2222', 'tensorflow-tess-search-rk2-mlp-339212-worker-4.mlp.svc:2222', 'tensorflow-tess-search-rk2-mlp-339212-worker-5.mlp.svc:2222', 'tensorflow-tess-search-rk2-mlp-339212-worker-6.mlp.svc:2222', 'tensorflow-tess-search-rk2-mlp-339212-worker-7.mlp.svc:2222', 'tensorflow-tess-search-rk2-mlp-339212-worker-8.mlp.svc:2222', 'tensorflow-tess-search-rk2-mlp-339212-worker-9.mlp.svc:2222', 'tensorflow-tess-search-rk2-mlp-339212-worker-10.mlp.svc:2222', 'tensorflow-tess-search-rk2-mlp-339212-worker-11.mlp.svc:2222', 'tensorflow-tess-search-rk2-mlp-339212-worker-12.mlp.svc:2222', 'tensorflow-tess-search-rk2-mlp-339212-worker-13.mlp.svc:2222', 'tensorflow-tess-search-rk2-mlp-339212-worker-14.mlp.svc:2222', 'tensorflow-tess-search-rk2-mlp-339212-worker-15.mlp.svc:2222']})\r\n",
      "1、tf.compat.v1.distribute.experimental.ParameterServerStrategy + Estimator :  Error reported to Coordinator: 'NoneType' object has no attribute 'extended'\r\n",
      "example code:\r\n",
      "strategy = tf.compat.v1.distribute.experimental.ParameterServerStrategy()\r\n",
      "model_config = tf.estimator.RunConfig(\r\n",
      "        train_distribute=strategy,\r\n",
      "        eval_distribute=None,\r\n",
      "        log_step_count_steps=config[\"parameters\"][\"log_steps\"],\r\n",
      "        save_summary_steps=config[\"parameters\"][\"save_summary_steps\"],\r\n",
      "        save_checkpoints_steps=config[\"parameters\"][\"save_checkpoints_steps\"],\r\n",
      "        save_checkpoints_secs=None,\r\n",
      "        keep_checkpoint_max=config[\"parameters\"][\"keep_checkpoint_max\"],\r\n",
      " )\r\n",
      "estimator = tf.estimator.Estimator(\r\n",
      "        model_fn=model_fn, model_dir=model_dir, params=config, config=model_config\r\n",
      ")\r\n",
      "![image](https://user-images.githubusercontent.com/13100437/106465747-5ada4e00-64d5-11eb-8ddc-11fb45695a88.png)\r\n",
      "\r\n",
      "2、tf.distribute.experimental.ParameterServerStrategy + Estimator: the training job will be stuck\r\n",
      "example code:\r\n",
      "os.environ[\"GRPC_FAIL_FAST\"] = \"use_caller\"\r\n",
      "cluster_resolver = tf.distribute.cluster_resolver.TFConfigClusterResolver()\r\n",
      " variable_partitioner = (\r\n",
      "    tf.distribute.experimental.partitioners.FixedShardsPartitioner(\r\n",
      "    num_shards=ps_number))\r\n",
      "strategy = tf.distribute.experimental.ParameterServerStrategy(cluster_resolver=cluster_resolver,\r\n",
      "    variable_partitioner=variable_partitioner)\r\n",
      "model_config = tf.estimator.RunConfig(\r\n",
      "        train_distribute=strategy,\r\n",
      "        eval_distribute=None,\r\n",
      "        log_step_count_steps=config[\"parameters\"][\"log_steps\"],\r\n",
      "        save_summary_steps=config[\"parameters\"][\"save_summary_steps\"],\r\n",
      "        save_checkpoints_steps=config[\"parameters\"][\"save_checkpoints_steps\"],\r\n",
      "        save_checkpoints_secs=None,\r\n",
      "        keep_checkpoint_max=config[\"parameters\"][\"keep_checkpoint_max\"],\r\n",
      "    )\r\n",
      "estimator = tf.estimator.Estimator(\r\n",
      "        model_fn=model_fn, model_dir=model_dir, params=config, config=model_config\r\n",
      ")\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:dist-strat\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Estimator: how to write asset files into the assets directory hermetically when save model\n",
      "issue body -  System information\r\n",
      "\r\n",
      "- python 3.6.8\r\n",
      "- tensorflow-gpu 1.15.0\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "I have an asset file which needs to be saved within the exported model. I use estimator to export model as follows:\r\n",
      "```\r\n",
      "predictor = tf.estimator.Estimator()\r\n",
      "assets_extra = {'index.idx': '/local/path/to/index.idx'}\r\n",
      "servable_model_path = predictor.export_savedmodel(\r\n",
      "    model_path,\r\n",
      "    export_input_fn,\r\n",
      "    assets_extra,\r\n",
      "    as_text=True)\r\n",
      "```\r\n",
      "\r\n",
      "The parameter `assets_extra` will copy the asset file `index.idx` to the export directory. And the expected result is that the tensor in the graph which contain the directory of `index.idx` should be binded to the path of `index.idx` within the export directory. Actually,  the directory of `index.idx` in the graph dosen't change, but still `/local/path/to/index.idx`. If I move the exported model to another machine which don't contain `/local/path/to/index.idx`, the model can't find the `index.idx`.\r\n",
      "\r\n",
      "So how can I bind the tensor value to the path of `index.idx` within the export directory?\n",
      "issue labels - \n",
      "TF 1.15\n",
      "comp:apis\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  RaggedTensor not supported as model input in generator and sequence\n",
      "issue body -  <em>Please make sure that this is a bug. As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:bug_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- tf-nightly 2.5.0 (2021020101)\r\n",
      "- colab\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "generators and sequences (objects that implement the keras.utils.Sequence interface) that return a RaggedTensor as model input trigger an exception\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "generators and sequences should support any CompositeTensor\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "https://colab.research.google.com/drive/1Qxov6zoWEnVcGwAH67cWiqaIjGcUTwbk?usp=sharing\r\n",
      "\r\n",
      "This notebook creates a model that uses ragged tensors and verifies that when a tensor is passed to ```model.fit``` the behaviour is as expected. It then attempts to return the save value from a generator. The generator works if the tensor is converted from ragged to dense (although that changes the computed value). Passing a ragged tensor directly to the tf.data.Dataset API also seems to work. \r\n",
      "\r\n",
      "**Other info / logs** Include any logs or source code that would be helpful to\r\n",
      "diagnose the problem. If including tracebacks, please include the full\r\n",
      "traceback. Large logs and files should be attached.\r\n",
      "\r\n",
      "```\r\n",
      "InvalidArgumentError:  TypeError: `generator` yielded an element that could not be converted to the expected type. The expected type was int32, but the yielded element was <tf.RaggedTensor [[[1, 2], [3, 4]], [[5, 6]]]>.\r\n",
      "TypeError: int() argument must be a string, a bytes-like object or a number, not 'RaggedTensor'\r\n",
      "\r\n",
      "\r\n",
      "The above exception was the direct cause of the following exception:\r\n",
      "\r\n",
      "\r\n",
      "Traceback (most recent call last):\r\n",
      "\r\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 912, in generator_py_func\r\n",
      "    dtype=dtype.as_numpy_dtype))\r\n",
      "\r\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/script_ops.py\", line 209, in _convert\r\n",
      "    result = np.asarray(value, dtype=dtype, order=\"C\")\r\n",
      "\r\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/numpy/core/_asarray.py\", line 83, in asarray\r\n",
      "    return array(a, dtype, copy=False, order=order)\r\n",
      "\r\n",
      "ValueError: setting an array element with a sequence.\r\n",
      "\r\n",
      "\r\n",
      "During handling of the above exception, another exception occurred:\r\n",
      "\r\n",
      "\r\n",
      "Traceback (most recent call last):\r\n",
      "\r\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/script_ops.py\", line 249, in __call__\r\n",
      "    ret = func(*args)\r\n",
      "\r\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py\", line 625, in wrapper\r\n",
      "    return func(*args, **kwargs)\r\n",
      "\r\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 921, in generator_py_func\r\n",
      "    sys.exc_info()[2])\r\n",
      "\r\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/six.py\", line 702, in reraise\r\n",
      "    raise value.with_traceback(tb)\r\n",
      "\r\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 912, in generator_py_func\r\n",
      "    dtype=dtype.as_numpy_dtype))\r\n",
      "\r\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/script_ops.py\", line 209, in _convert\r\n",
      "    result = np.asarray(value, dtype=dtype, order=\"C\")\r\n",
      "\r\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/numpy/core/_asarray.py\", line 83, in asarray\r\n",
      "    return array(a, dtype, copy=False, order=order)\r\n",
      "\r\n",
      "TypeError: `generator` yielded an element that could not be converted to the expected type. The expected type was int32, but the yielded element was <tf.RaggedTensor [[[1, 2], [3, 4]], [[5, 6]]]>.\r\n",
      "\r\n",
      "\r\n",
      "\t [[{{node PyFunc}}]]\r\n",
      "\t [[IteratorGetNext]] [Op:__inference_train_function_1924]\r\n",
      "\r\n",
      "Function call stack:\r\n",
      "train_function\r\n",
      "```\n",
      "issue labels - \n",
      "TF 2.5\n",
      "comp:keras\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Running TensorFlow Profiler on CPU only machine but training on GPU machine?\n",
      "issue body -  Can I trainng a model on GPU machine and generating the profile files, but use a CPU only machine to load the files and render the profile webpage correctly?\n",
      "issue labels - \n",
      "comp:tensorboard\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  ConvLSTM2D with CUDNN crashes\n",
      "issue body -  System Information: Windows 10\r\n",
      "tensorflow version: 2.5.0-dev20201217\r\n",
      "python version: 3.7.9\r\n",
      "cuda version : 10.2\r\n",
      "\r\n",
      "\r\n",
      "On using the above model, python crashes after\r\n",
      "\r\n",
      "`model = models.Sequential(\r\n",
      "\t[\r\n",
      "\t\tlayers.Input(\r\n",
      "\t\t\tshape=(timesteps, width, height, channels)\r\n",
      "\t\t),\r\n",
      "\t\tlayers.ConvLSTM2D(\r\n",
      "\t\t\tfilters=32, kernel_size=(3, 3), padding=\"same\", return_sequences=True, dropout=0.1, recurrent_dropout=0.1\r\n",
      "\t\t),\r\n",
      "\t\tlayers.MaxPool3D(\r\n",
      "\t\t\tpool_size=(1, 2, 2), strides=(1, 2, 2), padding=\"same\"\r\n",
      "\t\t),\r\n",
      "\t\tlayers.BatchNormalization(),\r\n",
      "\t\tlayers.ConvLSTM2D(\r\n",
      "\t\t\tfilters=16, kernel_size=(3, 3), padding=\"same\", return_sequences=True, dropout=0.1, recurrent_dropout=0.1\r\n",
      "\t\t),\r\n",
      "\t\tlayers.MaxPool3D(\r\n",
      "\t\t\tpool_size=(1, 2, 2), strides=(1, 2, 2), padding=\"same\"\r\n",
      "\t\t),\r\n",
      "\t\tlayers.BatchNormalization(),\r\n",
      "\t\tlayers.ConvLSTM2D(\r\n",
      "\t\t\tfilters=8, kernel_size=(3, 3), padding=\"same\", return_sequences=False, dropout=0.1, recurrent_dropout=0.1\r\n",
      "\t\t),\r\n",
      "\t\tlayers.MaxPool2D(\r\n",
      "\t\t\tpool_size=(2, 2), strides=(2, 2), padding=\"same\"\r\n",
      "\t\t),\r\n",
      "\t\tlayers.BatchNormalization(),\r\n",
      "\t\tlayers.Flatten(),\r\n",
      "\t\tlayers.Dense(192, activation='relu'),\r\n",
      "\t\tlayers.Dense(action_num, activation='softmax')\r\n",
      "\t]\r\n",
      ")\r\n",
      "`\r\n",
      "On using the above model, python crashes after\r\n",
      "\r\n",
      ">2021-02-01 10:03:12.227916: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:127] None of the MLIR optimization passes are enabled (registered 2)\r\n",
      "Epoch 1/300\r\n",
      "2021-02-01 10:03:21.950076: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n",
      "2021-02-01 10:03:22.694349: I tensorflow/stream_executor/cuda/cuda_dnn.cc:334] Loaded cuDNN version 8005\r\n",
      "2021-02-01 10:03:23.281554: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 0\r\n",
      "\r\n",
      ">2021-02-01 10:03:23.769410: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 0\r\n",
      "\r\n",
      ">2021-02-01 10:03:23.867774: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n",
      "2021-02-01 10:03:24.624172: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n",
      "\r\n",
      "on further examining the system logs found the following\r\n",
      "\r\n",
      ">Faulting application name: python.exe, version: 3.7.9150.1013, time stamp: 0x5f3ad38e\r\n",
      "Faulting module name: _pywrap_tensorflow_internal.pyd, version: 0.0.0.0, time stamp: 0x5fdb260c\r\n",
      "Exception code: 0xc00000fd\r\n",
      "Fault offset: 0x00000000094ae988\r\n",
      "Faulting process id: 0x35dc\r\n",
      "Faulting application start time: 0x01d6f85196812501\r\n",
      "Faulting application path: C:\\Users\\Scorp\\AppData\\Local\\Programs\\Python\\Python37\\python.exe\r\n",
      "Faulting module path: C:\\Users\\Scorp\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\_pywrap_tensorflow_internal.pyd\r\n",
      "\r\n",
      "keras config\r\n",
      "\r\n",
      "{\r\n",
      "    \"floatx\": \"float32\",\r\n",
      "    \"epsilon\": 1e-07,\r\n",
      "    \"backend\": \"tensorflow\",\r\n",
      "    \"image_data_format\": \"channels_last\"\r\n",
      "}\r\n",
      "\r\n",
      "PFA the code and files\r\n",
      "\r\n",
      "https://github.com/sivi299/lstm\n",
      "issue labels - \n",
      "TF 2.5\n",
      "comp:gpu\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  problematic args description table of aixs in tf.nn.softmax\n",
      "issue body -  ## URL(s) with the issue:\r\n",
      "https://www.tensorflow.org/api_docs/python/tf/nn/softmax#args\r\n",
      "\r\n",
      "document version: 2.4.1\r\n",
      "\r\n",
      "## Description of issue (what needs changing):\r\n",
      "\r\n",
      "In the args description table, it says axis default to -1,\r\n",
      "while the code below 'view alias' uses: axis=None.\r\n",
      "\r\n",
      "and according to its equivalent code, axis deafaults to None in the function reduce_sum.\r\n",
      "\r\n",
      "### so which one is right?\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "comp:apis\n",
      "stat:awaiting response\n",
      "type:docs-bug\n",
      "\n",
      "\n",
      "issue title -  error trying to download tensorflow with pip\n",
      "issue body -  \r\n",
      "**System information**\r\n",
      "-MacOs Big sur(11.1)\r\n",
      "- TensorFlow installed from pip \r\n",
      "- Python version: Python 3.9.1\r\n",
      "- pip version: 21.01\r\n",
      "\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "when i want to download Tensor-flow with pip this error occurs. it says: \r\n",
      "ERROR: Could not find a version that satisfies the requirement tensorflow\r\n",
      "ERROR: No matching distribution found for tensorflow\r\n",
      "(attachment)\r\n",
      "\r\n",
      "<img width=\"569\" alt=\"Bildschirmfoto 2021-01-31 um 17 09 14\" src=\"https://user-images.githubusercontent.com/22261997/106390097-0ce71c80-63e7-11eb-971e-5b6472516600.png\">\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "stat:awaiting response\n",
      "subtype:macOS\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  The chained `==` operator throws error when converting to TFLite from `tf.function`.\n",
      "issue body -  ### 1. System information\r\n",
      "\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 18.04 (Google Colab)**\r\n",
      "- TensorFlow installation (pip package or built from source):  **pip package (Google Colab)**\r\n",
      "- TensorFlow library (version, if pip package or github SHA, if built from source): **2.4.1 (Google Colab)**\r\n",
      "\r\n",
      "### 2. Code\r\n",
      "\r\n",
      "#### Option B: Paste your code here:\r\n",
      "Consider the scenario, in which one would like to zero-out (or filter) a tensor, based on multiple equality checks:\r\n",
      "```python\r\n",
      "import tensorflow as tf\r\n",
      "\r\n",
      "SHAPE = (10, )\r\n",
      "\r\n",
      "@tf.function(\r\n",
      "    input_signature=[tf.TensorSpec(shape=SHAPE, dtype=tf.int32)]\r\n",
      ")\r\n",
      "def my_function(inputs):\r\n",
      "    \r\n",
      "    # Let's say that we want to zero-out all values BUT 1 and 2 from our input tensor.\r\n",
      "\r\n",
      "    filtered_inputs = tf.where(\r\n",
      "        (inputs == 1) | (inputs == 2),\r\n",
      "        inputs,\r\n",
      "        0\r\n",
      "    )\r\n",
      "    return filtered_inputs\r\n",
      "```\r\n",
      "\r\n",
      "Let us call this function on a mock input:\r\n",
      "\r\n",
      "```python\r\n",
      "mock_inputs = tf.random.uniform(shape=SHAPE, minval=1, maxval=5, dtype=tf.int32)\r\n",
      "my_function(mock_inputs)\r\n",
      "```\r\n",
      "It works (so far). Now let us convert this to Tensorflow Lite as per [official documentation](https://www.tensorflow.org/lite/convert#convert_concrete_functions_)\r\n",
      "\r\n",
      "```python\r\n",
      "concrete_func = my_function.get_concrete_function()\r\n",
      "converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\r\n",
      "converter.experimental_new_converter = True\r\n",
      "tflite_model = converter.convert()   # ERROR.\r\n",
      "```\r\n",
      "The above will fail with rather long error message that ends with\r\n",
      "```python\r\n",
      "<unknown>:0: error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):\r\n",
      "\ttf.Equal {device = \"\", incompatible_shape_error = false}\r\n",
      "```\r\n",
      "Now, there is a workaround: one can use the `tf.math.equal` instead of the `==` operator:\r\n",
      "\r\n",
      "```python\r\n",
      "@tf.function(\r\n",
      "    input_signature=[tf.TensorSpec(shape=SHAPE, dtype=tf.int32)]\r\n",
      ")\r\n",
      "def my_workaround_function(inputs):\r\n",
      "\r\n",
      "    # Use Tensorflow ops instead of Python ops\r\n",
      "\r\n",
      "    filtered_inputs = tf.where(\r\n",
      "        (tf.math.equal(inputs, 1)) | (tf.math.equal(inputs, 2)),\r\n",
      "        inputs,\r\n",
      "        0\r\n",
      "    )\r\n",
      "    return filtered_inputs\r\n",
      "```\r\n",
      "And the above can be safely converted to TFLite:\r\n",
      "\r\n",
      "```python\r\n",
      "concrete_func = my_workaround_function.get_concrete_function()\r\n",
      "converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\r\n",
      "converter.experimental_new_converter = True\r\n",
      "tflite_model = converter.convert()  # OK. \r\n",
      "```\r\n",
      "\r\n",
      "I am opening the issue as I am unsure if this is a intended or known behaviour. For example, some people might be stuck on this situation, without figuring out the workaround. Also: as per the [Tensorflow documentation](https://www.tensorflow.org/guide/function#autograph_transformations), the `tf.function` is intended to work with simple Python ops.\r\n",
      "\r\n",
      "If this is somewhat intended, let me know and I will close the issue (though I would greatly appreciate some explanation if possible).\r\n",
      "  \r\n",
      "Best regards,\r\n",
      "Sebastian\r\n",
      "\r\n",
      "### 5. (optional) Any other info / logs\r\n",
      "\r\n",
      "Here is a full error message:\r\n",
      "```python\r\n",
      "Exception: /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1750:0: error: 'tf.Equal' op is neither a custom op nor a flex op\r\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201:0: note: called from\r\n",
      "<ipython-input-54-3ccb02792638>:14:0: note: called from\r\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py:973:0: note: called from\r\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py:634:0: note: called from\r\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py:990:0: note: called from\r\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py:3206:0: note: called from\r\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py:3361:0: note: called from\r\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py:2969:0: note: called from\r\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py:726:0: note: called from\r\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1750:0: error: 'tf.Equal' op is neither a custom op nor a flex op\r\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201:0: note: called from\r\n",
      "<ipython-input-54-3ccb02792638>:14:0: note: called from\r\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py:973:0: note: called from\r\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py:634:0: note: called from\r\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py:990:0: note: called from\r\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py:3206:0: note: called from\r\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py:3361:0: note: called from\r\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py:2969:0: note: called from\r\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py:726:0: note: called from\r\n",
      "<unknown>:0: error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):\r\n",
      "\ttf.Equal {device = \"\", incompatible_shape_error = false}\r\n",
      "\r\n",
      "\r\n",
      "During handling of the above exception, another exception occurred:\r\n",
      "\r\n",
      "ConverterError                            Traceback (most recent call last)\r\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n",
      "    214       return model_str\r\n",
      "    215     except Exception as e:\r\n",
      "--> 216       raise ConverterError(str(e))\r\n",
      "    217 \r\n",
      "    218   if distutils.spawn.find_executable(_toco_from_proto_bin) is None:\r\n",
      "\r\n",
      "ConverterError: /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1750:0: error: 'tf.Equal' op is neither a custom op nor a flex op\r\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201:0: note: called from\r\n",
      "<ipython-input-54-3ccb02792638>:14:0: note: called from\r\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py:973:0: note: called from\r\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py:634:0: note: called from\r\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py:990:0: note: called from\r\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py:3206:0: note: called from\r\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py:3361:0: note: called from\r\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py:2969:0: note: called from\r\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py:726:0: note: called from\r\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1750:0: error: 'tf.Equal' op is neither a custom op nor a flex op\r\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201:0: note: called from\r\n",
      "<ipython-input-54-3ccb02792638>:14:0: note: called from\r\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py:973:0: note: called from\r\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py:634:0: note: called from\r\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py:990:0: note: called from\r\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py:3206:0: note: called from\r\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py:3361:0: note: called from\r\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py:2969:0: note: called from\r\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py:726:0: note: called from\r\n",
      "<unknown>:0: error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):\r\n",
      "\ttf.Equal {device = \"\", incompatible_shape_error = false}\r\n",
      "```\n",
      "issue labels - \n",
      "TFLiteConverter\n",
      "\n",
      "\n",
      "issue title -  Problem with jit compiler:\n",
      "issue body -  When trying your example programm:\r\n",
      "import tensorflow as tf\r\n",
      "import tensorflow_probability as tfp\r\n",
      "\r\n",
      "# Pretend to load synthetic data set.\r\n",
      "features = tfp.distributions.Normal(loc=0., scale=1.).sample(int(100e3))\r\n",
      "labels = tfp.distributions.Bernoulli(logits=1.618 * features).sample()\r\n",
      "\r\n",
      "# Specify model.\r\n",
      "model = tfp.glm.Bernoulli()\r\n",
      "\r\n",
      "# Fit model given data.\r\n",
      "coeffs, linear_response, is_converged, num_iter = tfp.glm.fit(\r\n",
      "    model_matrix=features[:, tf.newaxis],\r\n",
      "    response=tf.cast(labels, dtype=tf.float32),\r\n",
      "    model=model)\r\n",
      "# ==> coeffs is approximately [1.618] (We're golden!)\r\n",
      "\r\n",
      "I get the following log message:\r\n",
      "2021-01-31 13:51:01.420629: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"D:/PyProjects/PILCO-master/other.py\", line 2, in <module>\r\n",
      "    import tensorflow_probability as tfp\r\n",
      "  File \"C:\\Users\\Martin\\Anaconda3\\lib\\site-packages\\tensorflow_probability\\__init__.py\", line 20, in <module>\r\n",
      "    from tensorflow_probability import substrates\r\n",
      "  File \"C:\\Users\\Martin\\Anaconda3\\lib\\site-packages\\tensorflow_probability\\substrates\\__init__.py\", line 21, in <module>\r\n",
      "    from tensorflow_probability.python.internal import all_util\r\n",
      "  File \"C:\\Users\\Martin\\Anaconda3\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py\", line 142, in <module>\r\n",
      "    dir(globals()[pkg_name])  # Forces loading the package from its lazy loader.\r\n",
      "  File \"C:\\Users\\Martin\\Anaconda3\\lib\\site-packages\\tensorflow_probability\\python\\internal\\lazy_loader.py\", line 61, in __dir__\r\n",
      "    module = self._load()\r\n",
      "  File \"C:\\Users\\Martin\\Anaconda3\\lib\\site-packages\\tensorflow_probability\\python\\internal\\lazy_loader.py\", line 44, in _load\r\n",
      "    module = importlib.import_module(self.__name__)\r\n",
      "  File \"C:\\Users\\Martin\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\r\n",
      "  File \"C:\\Users\\Martin\\Anaconda3\\lib\\site-packages\\tensorflow_probability\\python\\experimental\\__init__.py\", line 35, in <module>\r\n",
      "    from tensorflow_probability.python.experimental import bijectors\r\n",
      "  File \"C:\\Users\\Martin\\Anaconda3\\lib\\site-packages\\tensorflow_probability\\python\\experimental\\bijectors\\__init__.py\", line 17, in <module>\r\n",
      "    from tensorflow_probability.python.bijectors.ldj_ratio import inverse_log_det_jacobian_ratio\r\n",
      "  File \"C:\\Users\\Martin\\Anaconda3\\lib\\site-packages\\tensorflow_probability\\python\\bijectors\\__init__.py\", line 23, in <module>\r\n",
      "    from tensorflow_probability.python.bijectors.absolute_value import AbsoluteValue\r\n",
      "  File \"C:\\Users\\Martin\\Anaconda3\\lib\\site-packages\\tensorflow_probability\\python\\bijectors\\absolute_value.py\", line 23, in <module>\r\n",
      "    from tensorflow_probability.python.bijectors import bijector\r\n",
      "  File \"C:\\Users\\Martin\\Anaconda3\\lib\\site-packages\\tensorflow_probability\\python\\bijectors\\bijector.py\", line 35, in <module>\r\n",
      "    from tensorflow_probability.python.math import gradient\r\n",
      "  File \"C:\\Users\\Martin\\Anaconda3\\lib\\site-packages\\tensorflow_probability\\python\\math\\__init__.py\", line 30, in <module>\r\n",
      "    from tensorflow_probability.python.math.generic import log1mexp\r\n",
      "  File \"C:\\Users\\Martin\\Anaconda3\\lib\\site-packages\\tensorflow_probability\\python\\math\\generic.py\", line 151, in <module>\r\n",
      "    _kahan_reduction, _kahan_reduce_bwd, _kahan_reduce_tangents)\r\n",
      "  File \"C:\\Users\\Martin\\Anaconda3\\lib\\site-packages\\tensorflow_probability\\python\\internal\\variadic_reduce.py\", line 122, in make_variadic_reduce\r\n",
      "    @tf.function(jit_compile=True)\r\n",
      "TypeError: function() got an unexpected keyword argument 'jit_compile'\r\n",
      "\n",
      "issue labels - \n",
      "comp:apis\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  updated punctuations\n",
      "issue body -  Punctuations are corrected in the readme file.\n",
      "issue labels - \n",
      "cla: yes\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  suffering with the problem : tensorflow.python.framework.errors_impl.NotFoundError: NewRandomAccessFile failed to Create/Open: HelmetDetection/frozen_inference_graph.pb : The system cannot find the path specified. ; No such process\n",
      "issue body -  <em>Please make sure that this is a bug. As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:bug_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n",
      "![issue1](https://user-images.githubusercontent.com/75311266/106377536-a8669600-63c3-11eb-865a-9fdfbb9b10f7.png)\r\n",
      "![issue1](https://user-images.githubusercontent.com/75311266/106377540-b1effe00-63c3-11eb-9385-ec6b9997a32e.png)\r\n",
      "\r\n",
      "\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "- TensorFlow installed from (source or binary):\r\n",
      "- TensorFlow version (use command below):\r\n",
      "- Python version:\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version:\r\n",
      "- GPU model and memory:\r\n",
      "\r\n",
      "You can collect some of this information using our environment capture\r\n",
      "[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n",
      "You can also obtain the TensorFlow version with:\r\n",
      "1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n",
      "2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n",
      "\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "Provide a reproducible test case that is the bare minimum necessary to generate\r\n",
      "the problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n",
      "\r\n",
      "**Other info / logs** Include any logs or source code that would be helpful to\r\n",
      "diagnose the problem. If including tracebacks, please include the full\r\n",
      "traceback. Large logs and files should be attached.\r\n",
      "\n",
      "issue labels - \n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -      TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type int64 of argument 'x'.\n",
      "issue body -  [](# -*- coding: utf-8 -*-\r\n",
      "\"\"\"Untitled42.ipynb\r\n",
      "\r\n",
      "Automatically generated by Colaboratory.\r\n",
      "\r\n",
      "Original file is located at\r\n",
      "    https://colab.research.google.com/drive/1LJOb_cY2_aK9adr8i6jNn9QcFzwDZBS8\r\n",
      "\"\"\"\r\n",
      "\r\n",
      "import numpy as np\r\n",
      "import pandas as pd\r\n",
      "import os\r\n",
      "import random, re, math\r\n",
      "import tensorflow as tf, tensorflow.keras.backend as K\r\n",
      "import tensorflow_addons as tfa\r\n",
      "from tensorflow.keras.layers import Dense\r\n",
      "from tensorflow.keras.models import Model\r\n",
      "from tensorflow.keras import optimizers\r\n",
      "from tensorflow.keras.models import Sequential\r\n",
      "import tensorflow.keras.layers as L\r\n",
      "from tensorflow.keras.applications import ResNet152V2, InceptionResNetV2, InceptionV3, Xception, VGG19\r\n",
      "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D,GlobalMaxPooling2D\r\n",
      "from tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n",
      "from sklearn.model_selection import train_test_split\r\n",
      "from tensorflow.keras.callbacks import ReduceLROnPlateau , EarlyStopping , ModelCheckpoint , LearningRateScheduler\r\n",
      "from keras import regularizers\r\n",
      "\r\n",
      "import matplotlib.pyplot as plt\r\n",
      "\r\n",
      "!pip install efficientnet\r\n",
      "import efficientnet.tfkeras as efn\r\n",
      "\r\n",
      "from google.colab import files\r\n",
      "files.upload()\r\n",
      "\r\n",
      "from google.colab import files\r\n",
      "files.upload()\r\n",
      "\r\n",
      "! mkdir -p ~/.kaggle\r\n",
      "! cp kaggle.json ~/.kaggle/\r\n",
      "#change the permission\r\n",
      "!chmod 600 ~/.kaggle/kaggle.json\r\n",
      "\r\n",
      "!kaggle datasets download -d andrewmvd/ocular-disease-recognition-odir5k\r\n",
      "\r\n",
      "from zipfile import ZipFile\r\n",
      "file_name = \"ocular-disease-recognition-odir5k.zip\"\r\n",
      "with ZipFile(file_name, 'r') as zip:\r\n",
      "  zip.extractall()\r\n",
      "  print('done')\r\n",
      "\r\n",
      "AUTO = tf.data.experimental.AUTOTUNE\r\n",
      "try:\r\n",
      "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\r\n",
      "    print('Running on TPU ', tpu.master())\r\n",
      "except ValueError:\r\n",
      "    tpu = None\r\n",
      "\r\n",
      "if tpu:\r\n",
      "    tf.config.experimental_connect_to_cluster(tpu)\r\n",
      "    tf.tpu.experimental.initialize_tpu_system(tpu)\r\n",
      "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\r\n",
      "else:\r\n",
      "    strategy = tf.distribute.get_strategy()\r\n",
      "\r\n",
      "print(\"REPLICAS: \", strategy.num_replicas_in_sync)\r\n",
      "\r\n",
      "GCS_DS_PATH = '/content/ODIR-5K'\r\n",
      "\r\n",
      "train = pd.read_csv('/content/new_df_oc (1).csv')\r\n",
      "train_paths = train.filename.apply(lambda x: GCS_DS_PATH+ '/ODIR-5K/ODIR-5K/Training Images/' + x).values\r\n",
      "train_labels = train.target.values\r\n",
      "\r\n",
      "train_paths\r\n",
      "\r\n",
      "train.head(10)\r\n",
      "\r\n",
      "train=train.drop(columns=['D','C','A','M','G','O'],axis=1)\r\n",
      "\r\n",
      "train=train[((train['N']== 1) | (train['H'] == 1))]\r\n",
      "\r\n",
      "train\r\n",
      "\r\n",
      "train,valid = train_test_split(train,test_size = 0.2,random_state = 42)\r\n",
      "\r\n",
      "BATCH_SIZE = 8* strategy.num_replicas_in_sync\r\n",
      "img_size = 512\r\n",
      "EPOCHS = 1\r\n",
      "SEED = 42\r\n",
      "\r\n",
      "def decode_image(filename, label=None, image_size=(img_size,img_size)):\r\n",
      "    bits = tf.io.read_file(filename)\r\n",
      "    image = tf.image.decode_jpeg(bits, channels=3) \r\n",
      "    image = tf.image.resize(image, image_size)\r\n",
      "    image = tf.cast(image, tf.float32)\r\n",
      "    image = tf.image.per_image_standardization(image)\r\n",
      "    if label is None:\r\n",
      "        return image\r\n",
      "    else:\r\n",
      "        return image, label\r\n",
      "    \r\n",
      "def preprocess(df,test=False):\r\n",
      "    paths = df.filename.apply(lambda x: GCS_DS_PATH + '/ODIR-5K/Training Images/' + x).values\r\n",
      "    labels = df.loc[:, ['N', 'H']].values\r\n",
      "    if test==False:\r\n",
      "        return paths,labels\r\n",
      "    else:\r\n",
      "        return paths\r\n",
      "    \r\n",
      "def data_augment(image, label=None, seed=SEED):\r\n",
      "    image = tf.image.random_flip_left_right(image, seed=seed)\r\n",
      "    image = tf.image.random_flip_up_down(image, seed=seed)\r\n",
      "           \r\n",
      "    if label is None:\r\n",
      "        return image\r\n",
      "    else:\r\n",
      "        return image, label\r\n",
      "\r\n",
      "def get_mat(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\r\n",
      "    rotation = math.pi * rotation / 180.\r\n",
      "    shear = math.pi * shear / 180.\r\n",
      "\r\n",
      "    c1 = tf.math.cos(rotation)\r\n",
      "    s1 = tf.math.sin(rotation)\r\n",
      "    one = tf.constant([1],dtype='float32')\r\n",
      "    zero = tf.constant([0],dtype='float32')\r\n",
      "    rotation_matrix = tf.reshape( tf.concat([c1,s1,zero, -s1,c1,zero, zero,zero,one],axis=0),[3,3] )\r\n",
      "\r\n",
      "    c2 = tf.math.cos(shear)\r\n",
      "    s2 = tf.math.sin(shear)\r\n",
      "    \r\n",
      "    shear_matrix = tf.reshape( tf.concat([one,s2,zero, zero,c2,zero, zero,zero,one],axis=0),[3,3] )    \r\n",
      "    zoom_matrix = tf.reshape( tf.concat([one/height_zoom,zero,zero, zero,one/width_zoom,zero, zero,zero,one],axis=0),[3,3] )\r\n",
      "    shift_matrix = tf.reshape( tf.concat([one,zero,height_shift, zero,one,width_shift, zero,zero,one],axis=0),[3,3] )\r\n",
      "    \r\n",
      "    return K.dot(K.dot(rotation_matrix, shear_matrix), K.dot(zoom_matrix, shift_matrix))\r\n",
      "\r\n",
      "def transform(image,label=None):\r\n",
      "    DIM = img_size\r\n",
      "    XDIM = DIM%2 \r\n",
      "    \r\n",
      "    rot = 15. * tf.random.normal([1],dtype='float32')\r\n",
      "    shr = 5. * tf.random.normal([1],dtype='float32') \r\n",
      "    h_zoom = 1.0 + tf.random.normal([1],dtype='float32')/10.\r\n",
      "    w_zoom = 1.0 + tf.random.normal([1],dtype='float32')/10.\r\n",
      "    h_shift = 8. * tf.random.normal([1],dtype='float32') \r\n",
      "    w_shift = 8. * tf.random.normal([1],dtype='float32') \r\n",
      "  \r\n",
      "    m = get_mat(rot,shr,h_zoom,w_zoom,h_shift,w_shift) \r\n",
      "\r\n",
      "    x = tf.repeat( tf.range(DIM//2,-DIM//2,-1), DIM )\r\n",
      "    y = tf.tile( tf.range(-DIM//2,DIM//2),[DIM] )\r\n",
      "    z = tf.ones([DIM*DIM],dtype='int32')\r\n",
      "    idx = tf.stack( [x,y,z] )\r\n",
      "    \r\n",
      "    idx2 = K.dot(m,tf.cast(idx,dtype='float32'))\r\n",
      "    idx2 = K.cast(idx2,dtype='int32')\r\n",
      "    idx2 = K.clip(idx2,-DIM//2+XDIM+1,DIM//2)\r\n",
      "              \r\n",
      "    idx3 = tf.stack( [DIM//2-idx2[0,], DIM//2-1+idx2[1,]] )\r\n",
      "    d = tf.gather_nd(image,tf.transpose(idx3))\r\n",
      "    \r\n",
      "    if label is None:\r\n",
      "        return tf.reshape(d,[DIM,DIM,3])\r\n",
      "    else:\r\n",
      "        return tf.reshape(d,[DIM,DIM,3]),label\r\n",
      "\r\n",
      "train_dataset = (tf.data.Dataset\r\n",
      "    .from_tensor_slices(preprocess(train))\r\n",
      "    .map(decode_image, num_parallel_calls=AUTO)\r\n",
      "    #.map(data_augment, num_parallel_calls=AUTO)\r\n",
      "    .map(transform,num_parallel_calls=AUTO)\r\n",
      "    .shuffle(SEED)\r\n",
      "    .batch(BATCH_SIZE)\r\n",
      "    .repeat()\r\n",
      "    .prefetch(AUTO))\r\n",
      "\r\n",
      "test_dataset= (tf.data.Dataset\r\n",
      "    .from_tensor_slices(preprocess(valid))\r\n",
      "    .map(decode_image, num_parallel_calls=AUTO)\r\n",
      "    .batch(BATCH_SIZE)\r\n",
      "    .cache()\r\n",
      "    .prefetch(AUTO))\r\n",
      "\r\n",
      "LR_START = 0.00001\r\n",
      "LR_MAX = 0.00005 * strategy.num_replicas_in_sync\r\n",
      "LR_MIN = 0.00001\r\n",
      "LR_RAMPUP_EPOCHS = 5\r\n",
      "LR_SUSTAIN_EPOCHS = 0\r\n",
      "LR_EXP_DECAY = .8\r\n",
      "\r\n",
      "def lrfn(epoch):\r\n",
      "    if epoch < LR_RAMPUP_EPOCHS:\r\n",
      "        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\r\n",
      "    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\r\n",
      "        lr = LR_MAX\r\n",
      "    else:\r\n",
      "        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\r\n",
      "    return lr\r\n",
      "    \r\n",
      "lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)\r\n",
      "\r\n",
      "rng = [i for i in range(EPOCHS)]\r\n",
      "y = [lrfn(x) for x in rng]\r\n",
      "plt.plot(rng, y)\r\n",
      "print(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\".format(y[0], max(y), y[-1]))\r\n",
      "\r\n",
      "def categorical_focal_loss(gamma=2., alpha=.25):\r\n",
      "    def categorical_focal_loss_fixed(y_true, y_pred):\r\n",
      "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\r\n",
      "        epsilon = K.epsilon()\r\n",
      "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\r\n",
      "        cross_entropy = -y_true * K.log(y_pred)\r\n",
      "        loss = alpha * K.pow(1 - y_pred, gamma) * cross_entropy\r\n",
      "        return K.sum(loss, axis=1)\r\n",
      "    return categorical_focal_loss_fixed\r\n",
      "\r\n",
      "with strategy.scope():\r\n",
      "    enet = efn.EfficientNetB7(input_shape=(img_size, img_size, 3),weights='noisy-student',include_top=False)\r\n",
      "\r\n",
      "with strategy.scope():\r\n",
      "    enet.trainable = True\r\n",
      "\r\n",
      "with strategy.scope():\r\n",
      "    ef7 =tf.keras.Sequential()\r\n",
      "    ef7.add(enet)\r\n",
      "    ef7.add(tf.keras.layers.MaxPooling2D())\r\n",
      "    ef7.add(tf.keras.layers.Conv2D(2048,3,padding='same'))\r\n",
      "    ef7.add(tf.keras.layers.BatchNormalization())\r\n",
      "    ef7.add(tf.keras.layers.ReLU())\r\n",
      "    ef7.add(tf.keras.layers.GlobalAveragePooling2D())\r\n",
      "    ef7.add(tf.keras.layers.Flatten())\r\n",
      "\r\n",
      "    ef7.add(tf.keras.layers.Dense(1024,activation='relu'))\r\n",
      "    ef7.add(tf.keras.layers.BatchNormalization())\r\n",
      "    ef7.add(tf.keras.layers.LeakyReLU())\r\n",
      "    ef7.add(tf.keras.layers.Dropout(0.25))\r\n",
      "\r\n",
      "    ef7.add(tf.keras.layers.Dense(512,activation='relu'))\r\n",
      "    ef7.add(tf.keras.layers.BatchNormalization())\r\n",
      "    ef7.add(tf.keras.layers.LeakyReLU())\r\n",
      "    ef7.add(tf.keras.layers.Dropout(0.15))\r\n",
      "    ef7.add(tf.keras.layers.Dense(2,activation='softmax'))\r\n",
      "    ef7.compile(\r\n",
      "                optimizer=tf.optimizers.Adam(lr=0.0001),\r\n",
      "                loss=categorical_focal_loss(gamma=2., alpha=.25),\r\n",
      "                metrics=['categorical_accuracy',\r\n",
      "                        tf.keras.metrics.Recall(),\r\n",
      "                        tf.keras.metrics.Precision(),   \r\n",
      "                        tf.keras.metrics.AUC(),\r\n",
      "                        tfa.metrics.F1Score(num_classes=2, average=\"macro\")\r\n",
      "                       ])\r\n",
      "\r\n",
      "h7=ef7.fit(\r\n",
      "    train_dataset,\r\n",
      "    steps_per_epoch=train_labels.shape[0] // BATCH_SIZE,\r\n",
      "    callbacks=[lr_callback],\r\n",
      "    epochs=EPOCHS)\r\n",
      "\r\n",
      ")\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "TypeError: in user code:\r\n",
      "\r\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\r\n",
      "        return step_function(self, iterator)\r\n",
      "    <ipython-input-20-4c42ef206b9f>:6 categorical_focal_loss_fixed  *\r\n",
      "        cross_entropy = -y_true * K.log(y_pred)\r\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\r\n",
      "        raise e\r\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\r\n",
      "        return func(x, y, name=name)\r\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1496 _mul_dispatch\r\n",
      "        return multiply(x, y, name=name)\r\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\r\n",
      "        return target(*args, **kwargs)\r\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:518 multiply\r\n",
      "        return gen_math_ops.mul(x, y, name)\r\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:6078 mul\r\n",
      "        \"Mul\", x=x, y=y, name=name)\r\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\r\n",
      "        inferred_from[input_arg.type_attr]))\r\n",
      "\r\n",
      "    TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type int64 of argument 'x'.](url)\r\n",
      "\r\n",
      "\r\n",
      "[Pls help me on this finding the solution of this error ](url)\n",
      "issue labels - \n",
      "comp:keras\n",
      "comp:tpus\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Encountered \"Segmentation fault (core dumped)\" when convert keras model to tflite model\n",
      "issue body -  I‘m trying to convert a pytorch model (SSD) to tflite model. The model transformation path is: pytorch mode >> onnx model >> keras model >> tflite mode. \r\n",
      "\r\n",
      "### 1. System information\r\n",
      "\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 16.04**\r\n",
      "- TensorFlow installation (pip package or built from source): **pytorch1.6.0+cu101, onnx1.8.0, onnx2keras0.0.24, tensorflow2.3.0, cudatoolkit10.1, cudnn7.6.5** installed via \"**conda install**\" command\r\n",
      "\r\n",
      "\r\n",
      "### 2. Code\r\n",
      "\r\n",
      "pytorch source code in [pytorch-ssd](https://github.com/qfgaohao/pytorch-ssd) and my converting code as follows:\r\n",
      "\r\n",
      "\r\n",
      "```\r\n",
      "from vision.ssd.mobilenet_v2_ssd_lite import create_mobilenetv2_ssd_lite\r\n",
      "import os\r\n",
      "import torch\r\n",
      "import onnx\r\n",
      "import tensorflow as tf\r\n",
      "from onnx2keras import onnx_to_keras\r\n",
      "import numpy as np\r\n",
      "\r\n",
      "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\r\n",
      "\r\n",
      "torch_path = \" ... pretrained weights path ...\"\r\n",
      "tf_lite_path = \" ... output path ... \"\r\n",
      "\r\n",
      "create_net = lambda num: create_mobilenetv2_ssd_lite(num, is_test=True, device='cpu')\r\n",
      "net = create_net(21)\r\n",
      "\r\n",
      "state_dict = torch.load(torch_path, map_location=torch.device('cpu'))\r\n",
      "net.load_state_dict(state_dict, strict=True)\r\n",
      "\r\n",
      "net.eval()\r\n",
      "image_size = 300\r\n",
      "\r\n",
      "input_np = np.random.uniform(0, 1, (1, 3, image_size, image_size))\r\n",
      "input = torch.FloatTensor(input_np)\r\n",
      "\r\n",
      "torch.onnx.export(\r\n",
      "        model=net,\r\n",
      "        args=input,\r\n",
      "        f=\" ... onnx output path ... \",\r\n",
      "        export_params=True,\r\n",
      "        do_constant_folding=False\r\n",
      "        verbose=False,\r\n",
      "        input_names=['input'],\r\n",
      "        opset_version=10,\r\n",
      "        output_names=['output'])\r\n",
      "\r\n",
      "onnx_model = onnx.load(' .. onnx output path ... ')\r\n",
      "keras_model = onnx_to_keras(onnx_model, ['input'])\r\n",
      "\r\n",
      "print(\"======1\")\r\n",
      "\r\n",
      "converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\r\n",
      "converter.experimental_new_converter = True\r\n",
      "print(\"======2\")\r\n",
      "\r\n",
      "tflite_model = converter.convert() \r\n",
      "print(\"======3\")\r\n",
      "with open(tf_lite_path, 'wb') as f:\r\n",
      "        f.write(tflite_model)\r\n",
      "print(\"=====4\")\r\n",
      "```\r\n",
      "\r\n",
      "\r\n",
      "\"====1\" and \"====2\" are printed normally, but Segmentation fault occured before \"====3\". No error messages are shown before Segmentation fault occured. Some of the log information is as follows (Total log over 10,000 lines):\r\n",
      "\r\n",
      "```\r\n",
      "Tensor(\"inputs/0:0\", shape=(None, 3000, 2), dtype=float32) Tensor(\"inputs/1:0\", shape=(1, 3000, 2), dtype=float32)\r\n",
      "INFO:tensorflow:Assets written to: /tmp/tmpt6z4tlht/assets\r\n",
      "INFO:tensorflow:Assets written to: /tmp/tmpt6z4tlht/assets\r\n",
      "Tensor(\"functional_1/444/FusedBatchNormV3:0\", shape=(None, 16, 150, 150), dtype=float32) Tensor(\"functional_1/446_const2/Const:0\", shape=(), dtype=float32)\r\n",
      "Tensor(\"functional_1/469/BiasAdd:0\", shape=(None, 16), dtype=float32) Tensor(\"functional_1/471_const2/Const:0\", shape=(), dtype=float32)\r\n",
      "Tensor(\"functional_1/522/BiasAdd:0\", shape=(None, 96), dtype=float32) Tensor(\"functional_1/524_const2/Const:0\", shape=(), dtype=float32)\r\n",
      "Tensor(\"functional_1/558/BiasAdd:0\", shape=(None, 240), dtype=float32) Tensor(\"functional_1/560_const2/Const:0\", shape=(), dtype=float32)\r\n",
      "Tensor(\"functional_1/595/BiasAdd:0\", shape=(None, 240), dtype=float32) Tensor(\"functional_1/597_const2/Const:0\", shape=(), dtype=float32)\r\n",
      "Tensor(\"functional_1/615/FusedBatchNormV3:0\", shape=(None, 120, 19, 19), dtype=float32) Tensor(\"functional_1/617_const2/Const:0\", shape=(), dtype=float32)\r\n",
      "Tensor(\"functional_1/637/BiasAdd:0\", shape=(None, 120), dtype=float32) Tensor(\"functional_1/639_const2/Const:0\", shape=(), dtype=float32)\r\n",
      "Tensor(\"functional_1/653/FusedBatchNormV3:0\", shape=(None, 48, 19, 19), dtype=float32) Tensor(\"functional_1/655_const2/Const:0\", shape=(), dtype=float32)\r\n",
      "Tensor(\"functional_1/661/FusedBatchNormV3:0\", shape=(None, 144, 19, 19), dtype=float32) Tensor(\"functional_1/663_const2/Const:0\", shape=(), dtype=float32)\r\n",
      "Tensor(\"functional_1/683/BiasAdd:0\", shape=(None, 144), dtype=float32) Tensor(\"functional_1/685_const2/Const:0\", shape=(), dtype=float32)\r\n",
      "Tensor(\"functional_1/699/FusedBatchNormV3:0\", shape=(None, 48, 19, 19), dtype=float32) Tensor(\"functional_1/701_const2/Const:0\", shape=(), dtype=float32)\r\n",
      "Tensor(\"functional_1/746/FusedBatchNormV3:0\", shape=(None, 288, 19, 19), dtype=float32) Tensor(\"functional_1/748_const2/Const:0\", shape=(), dtype=float32)\r\n",
      "Tensor(\"functional_1/768/BiasAdd:0\", shape=(None, 288), dtype=float32) Tensor(\"functional_1/770_const2/Const:0\", shape=(), dtype=float32)\r\n",
      "Tensor(\"functional_1/784/FusedBatchNormV3:0\", shape=(None, 96, 10, 10), dtype=float32) Tensor(\"functional_1/786_const2/Const:0\", shape=(), dtype=float32)\r\n",
      "Tensor(\"functional_1/792/FusedBatchNormV3:0\", shape=(None, 576, 10, 10), dtype=float32) Tensor(\"functional_1/794_const2/Const:0\", shape=(), dtype=float32)\r\n",
      "Tensor(\"functional_1/814/BiasAdd:0\", shape=(None, 576), dtype=float32) Tensor(\"functional_1/816_const2/Const:0\", shape=(), dtype=float32)\r\n",
      "Tensor(\"functional_1/830/FusedBatchNormV3:0\", shape=(None, 96, 10, 10), dtype=float32) Tensor(\"functional_1/832_const2/Const:0\", shape=(), dtype=float32)\r\n",
      "Tensor(\"functional_1/839/FusedBatchNormV3:0\", shape=(None, 576, 10, 10), dtype=float32) Tensor(\"functional_1/841_const2/Const:0\", shape=(), dtype=float32)\r\n",
      "Tensor(\"functional_1/861/BiasAdd:0\", shape=(None, 576), dtype=float32) Tensor(\"functional_1/863_const2/Const:0\", shape=(), dtype=float32)\r\n",
      "Tensor(\"functional_1/877/FusedBatchNormV3:0\", shape=(None, 96, 10, 10), dtype=float32) Tensor(\"functional_1/879_const2/Const:0\", shape=(), dtype=float32)\r\n",
      "Tensor(\"functional_1/899/BiasAdd:0\", shape=(None, 576), dtype=float32) Tensor(\"functional_1/901_const2/Const:0\", shape=(), dtype=float32)\r\n",
      "Tensor(\"functional_1/914/FusedBatchNormV3:0\", shape=(None, 576, 10, 10), dtype=float32) Tensor(\"functional_1/916_const2/Const:0\", shape=(), dtype=float32)\r\n",
      "Tensor(\"functional_1/921/BiasAdd:0\", shape=(None, 1280, 10, 10), dtype=float32) Tensor(\"functional_1/923_const2/Const:0\", shape=(), dtype=float32)\r\n",
      "Tensor(\"functional_1/708/FusedBatchNormV3:0\", shape=(None, 288, 19, 19), dtype=float32) Tensor(\"functional_1/710_const2/Const:0\", shape=(), dtype=float32)\r\n",
      "Tensor(\"functional_1/1115/mul:0\", shape=(None, 3000, 2), dtype=float32) Tensor(\"functional_1/1117_const2/Const:0\", shape=(1, 3000, 2), dtype=float32)\r\n",
      "2021-01-31 14:22:38.501393: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\r\n",
      "2021-01-31 14:22:38.501533: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n",
      "2021-01-31 14:22:38.502841: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\n",
      "pciBusID: 0000:06:00.0 name: TITAN RTX computeCapability: 7.5\r\n",
      "coreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.62GiB deviceMemoryBandwidth: 625.94GiB/s\r\n",
      "2021-01-31 14:22:38.502879: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n",
      "2021-01-31 14:22:38.502931: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n",
      "2021-01-31 14:22:38.502954: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n",
      "2021-01-31 14:22:38.502973: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n",
      "2021-01-31 14:22:38.502993: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n",
      "2021-01-31 14:22:38.503024: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n",
      "2021-01-31 14:22:38.503046: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n",
      "2021-01-31 14:22:38.504682: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n",
      "2021-01-31 14:22:38.504715: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n",
      "2021-01-31 14:22:38.504725: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n",
      "2021-01-31 14:22:38.504733: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n",
      "2021-01-31 14:22:38.506316: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 21686 MB memory) -> physical GPU (device: 0, name: TITAN RTX, pci bus id: 0000:06:00.0, compute capability: 7.5)\r\n",
      "2021-01-31 14:22:38.537184: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: graph_to_optimize\r\n",
      "2021-01-31 14:22:38.537202: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.006ms.\r\n",
      "2021-01-31 14:22:38.537219: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n",
      "2021-01-31 14:22:40.299627: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:313] Ignored output_format.\r\n",
      "2021-01-31 14:22:40.299667: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:316] Ignored drop_control_dependency.\r\n",
      "2021-01-31 14:22:40.546372: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\n",
      "pciBusID: 0000:06:00.0 name: TITAN RTX computeCapability: 7.5\r\n",
      "coreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.62GiB deviceMemoryBandwidth: 625.94GiB/s\r\n",
      "2021-01-31 14:22:40.546422: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n",
      "2021-01-31 14:22:40.546476: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n",
      "2021-01-31 14:22:40.546498: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n",
      "2021-01-31 14:22:40.546519: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n",
      "2021-01-31 14:22:40.546539: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n",
      "2021-01-31 14:22:40.546558: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n",
      "2021-01-31 14:22:40.546579: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n",
      "2021-01-31 14:22:40.548140: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n",
      "2021-01-31 14:22:40.548184: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n",
      "2021-01-31 14:22:40.548194: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n",
      "2021-01-31 14:22:40.548202: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n",
      "2021-01-31 14:22:40.549800: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 21686 MB memory) -> physical GPU (device: 0, name: TITAN RTX, pci bus id: 0000:06:00.0, compute capability: 7.5)\r\n",
      "Segmentation fault (core dumped)\r\n",
      "```\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.3\n",
      "TFLiteConverter\n",
      "comp:lite\n",
      "stalled\n",
      "stat:awaiting response\n",
      "\n",
      "\n",
      "issue title -  StyleGAN2 is not converging in 4x RTX 3090\n",
      "issue body -  <em>Please make sure that this is an issue related to performance of TensorFlow.\r\n",
      "As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:performance_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 20.04\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n",
      "- TensorFlow installed from (source or binary): 2.4rc02\r\n",
      "- TensorFlow version (use command below): 2.x\r\n",
      "- Python version: 3.8\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version: 11.1\r\n",
      "- GPU model and memory: 4x RTX 3090(24 GB)\r\n",
      "\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "I have converted StyleGAN 2 official code(Written in Tensorflow 1.x) to TensorFlow 2.x. It is converging properly till some point, but then it started to decrease the quality. \r\n",
      "\r\n",
      "First I observed a mode collapsing then Eventually, the resulting images were a completely distorted type, kind of noise only. \r\n",
      "I'm using data having many categories.\r\n",
      "\r\n",
      "Anyone knows why it is happing and the solution. Thanks in advance.\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:gpu\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  [RNN] LSTM with TimeDistributed layer converts successfully but fails when invoking\n",
      "issue body -  ### 1. System information\r\n",
      "\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n",
      "- TensorFlow installation (pip package or built from source): pip\r\n",
      "- TensorFlow library (version, if pip package or github SHA, if built from source): tf-nightly\r\n",
      "\r\n",
      "### 2. Code\r\n",
      "\r\n",
      "https://colab.research.google.com/drive/1zgiZN6K1YsT70w-uWshBQA2ESuHSASn3?usp=sharing\r\n",
      "\r\n",
      "### 3. Failure after conversion\r\n",
      "\r\n",
      "- Model fails when invoking interpreter.\r\n",
      "- When set `batch_size` to 1 during building model, everything works. See `with_converted_lstm.tflite` for example.\r\n",
      "\n",
      "issue labels - \n",
      "TFLiteConverter\n",
      "\n",
      "\n",
      "issue title -  Multi GPU training - works on Intel CPU but failed on AMD CPU\n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:N/A\r\n",
      "- TensorFlow installed from (source or binary):binary and source with -march=native\r\n",
      "- TensorFlow version (use command below):2.4\r\n",
      "- Python version:3.6.9\r\n",
      "- Bazel version (if compiling from source):3.1.0\r\n",
      "- GCC/Compiler version (if compiling from source):7.5.0\r\n",
      "- CUDA/cuDNN version:11.0/8.0.4\r\n",
      "- GPU model and memory: 2x GTX1080  Titan/12GB\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "With the standard tensorflow-gpu 2.4 distribution installed using\r\n",
      "- pip3 install tensorflow-gpu==2.4\"\r\n",
      "\r\n",
      "When the same script is executed on the following two systems:\r\n",
      "- System 1: Intel CPU i7 6850k / 2x Titan (Pascal) 12GB / Ubuntu 18.04\r\n",
      "- System 2: AMD Threadripper 1950x / 2x Titan (Pascal) 12GB / Ubuntu 18.04\r\n",
      "\r\n",
      "The script works on System 1 but stuck on System 2. The script is stuck on the following line with both GPU at 100% utilization:\r\n",
      "```\r\n",
      "model.fit(train_dataset, epochs=12, callbacks=callbacks)\r\n",
      "```\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "The multi-gpu training should work on the systems with AMD cpus. System can run the same script successfully with only one GPU assigned to the runtime (export CUDA_VISIBLE_DEVICES=\"0\"). The problem might be CPU related. \r\n",
      "\r\n",
      "I have also tried Tensorflow compiled from source (version 2.4/GPU with -march=native) and the results are the same (stuck).\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "https://github.com/tensorflow/docs/blob/master/site/en/tutorials/distribute/keras.ipynb\r\n",
      "\r\n",
      "**Other info / logs** Include any logs or source code that would be helpful to\r\n",
      "diagnose the problem. If including tracebacks, please include the full\r\n",
      "traceback. Large logs and files should be attached.\r\n",
      "\r\n",
      "System 1 (Intel CPU) runs the entire script successfully:\r\n",
      "```\r\n",
      "Epoch 1/12\r\n",
      "INFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1\r\n",
      "INFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1\r\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n",
      "INFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1\r\n",
      "INFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1\r\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n",
      "  3/469 [..............................] - ETA: 2:40 - loss: 2.2512 - accuracy: 0.1276WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0065s vs `on_train_batch_begin` time: 0.0727s). Check your callbacks.\r\n",
      "WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0065s vs `on_train_batch_begin` time: 0.0727s). Check your callbacks.\r\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0065s vs `on_train_batch_end` time: 0.0402s). Check your callbacks.\r\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0065s vs `on_train_batch_end` time: 0.0402s). Check your callbacks.\r\n",
      "```\r\n",
      "\r\n",
      "System 2 (AMD Threadripper CPU) gets stuck on the model.fit line:\r\n",
      "```\r\n",
      "model.fit(train_dataset, epochs=12, callbacks=callbacks)\r\n",
      "Epoch 1/12\r\n",
      "INFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1\r\n",
      "INFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1\r\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n",
      "INFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1\r\n",
      "INFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1\r\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n",
      "```\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:dist-strat\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  ImageDataGenerator - Caching not effective when using tf.data.Dataset.from_generator\n",
      "issue body -  <em>Please make sure that this is a bug. As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:bug_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Manjaro Linux 20.2.1\r\n",
      "- TensorFlow installed from (source or binary): pip\r\n",
      "- TensorFlow version (use command below): 2.4.1\r\n",
      "- Python version: 3.8.5\r\n",
      "- CUDA/cuDNN version: 11.0\r\n",
      "\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "\r\n",
      "When creating a dataset generator using ImageDataGenerator, as the documentation specifies it:\r\n",
      "```\r\n",
      "The data will be looped over (in batches).\r\n",
      "```\r\n",
      "The dataset generator is then used in `tf.data.Dataset.from_generator`, on which is then called `.cache()`: we have a problem here, since the `cache()` function caches the data, as the documentation specifies it:\r\n",
      "```\r\n",
      "The first time the dataset is iterated over, its elements will be cached either in the specified file or in memory.\r\n",
      "Subsequent iterations will use the cached data.\r\n",
      "```\r\n",
      "The problem is that the dataset never ends since this is the way ImageDataGenerator works.\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "\r\n",
      "There should be a callback or something telling `tf.data.Dataset` that the dataset is OVER when getting the last batch each time, so that data can be cached properly.\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:data\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Shuffle Buffer Filled\n",
      "issue body -  When I run the model_main_tf2.py script to train a centernet_resnet50_v1_fpn_512x512 model with a dataset that is of the size 25GB in the tfrecord format.\r\n",
      "\r\n",
      "Is the dataset the main reason for this issue?\r\n",
      "\r\n",
      "the error is\r\n",
      "\r\n",
      "2021-01-30 19:37:30.641972: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\r\n",
      "2021-01-30 19:37:30.642004: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n",
      "2021-01-30 19:37:41.087656: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n",
      "2021-01-30 19:37:41.087854: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\r\n",
      "2021-01-30 19:37:41.087870: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\r\n",
      "2021-01-30 19:37:41.087898: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (kingsman-desktop): /proc/driver/nvidia/version does not exist\r\n",
      "2021-01-30 19:37:41.088899: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n",
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\r\n",
      "W0130 19:37:41.089709 140284268705600 cross_device_ops.py:1321] There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\r\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\r\n",
      "I0130 19:37:41.089887 140284268705600 mirrored_strategy.py:350] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\r\n",
      "INFO:tensorflow:Maybe overwriting train_steps: None\r\n",
      "I0130 19:37:41.093331 140284268705600 config_util.py:552] Maybe overwriting train_steps: None\r\n",
      "INFO:tensorflow:Maybe overwriting use_bfloat16: False\r\n",
      "I0130 19:37:41.093431 140284268705600 config_util.py:552] Maybe overwriting use_bfloat16: False\r\n",
      "WARNING:tensorflow:From /home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/object_detection/model_lib_v2.py:523: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "rename to distribute_datasets_from_function\r\n",
      "W0130 19:37:43.446819 140284268705600 deprecation.py:339] From /home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/object_detection/model_lib_v2.py:523: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "rename to distribute_datasets_from_function\r\n",
      "INFO:tensorflow:Reading unweighted datasets: ['/home/kingsman/deepl/records/train.tfrecord']\r\n",
      "I0130 19:37:43.468255 140284268705600 dataset_builder.py:163] Reading unweighted datasets: ['/home/kingsman/deepl/records/train.tfrecord']\r\n",
      "INFO:tensorflow:Reading record datasets for input file: ['/home/kingsman/deepl/records/train.tfrecord']\r\n",
      "I0130 19:37:43.468415 140284268705600 dataset_builder.py:80] Reading record datasets for input file: ['/home/kingsman/deepl/records/train.tfrecord']\r\n",
      "INFO:tensorflow:Number of filenames to read: 1\r\n",
      "I0130 19:37:43.468508 140284268705600 dataset_builder.py:81] Number of filenames to read: 1\r\n",
      "WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\r\n",
      "W0130 19:37:43.468611 140284268705600 dataset_builder.py:88] num_readers has been reduced to 1 to match input file shards.\r\n",
      "WARNING:tensorflow:From /home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/object_detection/builders/dataset_builder.py:105: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`.\r\n",
      "W0130 19:37:43.471664 140284268705600 deprecation.py:339] From /home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/object_detection/builders/dataset_builder.py:105: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`.\r\n",
      "WARNING:tensorflow:From /home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/object_detection/builders/dataset_builder.py:237: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use `tf.data.Dataset.map()\r\n",
      "W0130 19:37:43.492702 140284268705600 deprecation.py:339] From /home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/object_detection/builders/dataset_builder.py:237: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use `tf.data.Dataset.map()\r\n",
      "WARNING:tensorflow:From /home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py:201: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\r\n",
      "W0130 19:37:51.294010 140284268705600 deprecation.py:339] From /home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py:201: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\r\n",
      "WARNING:tensorflow:From /home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/object_detection/inputs.py:281: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use `tf.cast` instead.\r\n",
      "W0130 19:37:54.907917 140284268705600 deprecation.py:339] From /home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/object_detection/inputs.py:281: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use `tf.cast` instead.\r\n",
      "2021-01-30 19:37:58.317565: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n",
      "2021-01-30 19:37:58.341385: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3399920000 Hz\r\n",
      "2021-01-30 19:38:09.704606: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 68 of 2048\r\n",
      "2021-01-30 19:38:18.681284: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 162 of 2048\r\n",
      "2021-01-30 19:38:28.579701: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 314 of 2048\r\n",
      "2021-01-30 19:38:34.514916: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:230] Shuffle buffer filled.\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/eager/context.py\", line 2113, in execution_mode\r\n",
      "    yield\r\n",
      "  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 733, in _next_internal\r\n",
      "    output_shapes=self._flat_output_shapes)\r\n",
      "  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 2579, in iterator_get_next\r\n",
      "    _ops.raise_from_not_ok_status(e, name)\r\n",
      "  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 6862, in raise_from_not_ok_status\r\n",
      "    six.raise_from(core._status_to_exception(e.code, message), None)\r\n",
      "  File \"<string>\", line 3, in raise_from\r\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: Input is empty.\r\n",
      "\t [[{{node case/cond/else/_10/case/cond/cond_jpeg/else/_105/case/cond/cond_jpeg/decode_image/DecodeImage}}]]\r\n",
      "\t [[MultiDeviceIteratorGetNextFromShard]]\r\n",
      "\t [[RemoteCall]] [Op:IteratorGetNext]\r\n",
      "\r\n",
      "During handling of the above exception, another exception occurred:\r\n",
      "\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"object_detection/model_main_tf2.py\", line 113, in <module>\r\n",
      "    tf.compat.v1.app.run()\r\n",
      "  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n",
      "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n",
      "  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/absl/app.py\", line 300, in run\r\n",
      "    _run_main(main, args)\r\n",
      "  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/absl/app.py\", line 251, in _run_main\r\n",
      "    sys.exit(main(argv))\r\n",
      "  File \"object_detection/model_main_tf2.py\", line 110, in main\r\n",
      "    record_summaries=FLAGS.record_summaries)\r\n",
      "  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/object_detection/model_lib_v2.py\", line 566, in train_loop\r\n",
      "    unpad_groundtruth_tensors)\r\n",
      "  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/object_detection/model_lib_v2.py\", line 344, in load_fine_tune_checkpoint\r\n",
      "    features, labels = iter(input_dataset).next()\r\n",
      "  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/distribute/input_lib.py\", line 628, in next\r\n",
      "    return self.__next__()\r\n",
      "  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/distribute/input_lib.py\", line 632, in __next__\r\n",
      "    return self.get_next()\r\n",
      "  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/distribute/input_lib.py\", line 663, in get_next\r\n",
      "    self._iterators[i].get_next_as_list_static_shapes(new_name))\r\n",
      "  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/distribute/input_lib.py\", line 1619, in get_next_as_list_static_shapes\r\n",
      "    return self._format_data_list_with_options(self._iterator.get_next())\r\n",
      "  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py\", line 585, in get_next\r\n",
      "    result.append(self._device_iterators[i].get_next())\r\n",
      "  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 800, in get_next\r\n",
      "    return self._next_internal()\r\n",
      "  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 739, in _next_internal\r\n",
      "    return structure.from_compatible_tensor_list(self._element_spec, ret)\r\n",
      "  File \"/usr/lib/python3.6/contextlib.py\", line 99, in __exit__\r\n",
      "    self.gen.throw(type, value, traceback)\r\n",
      "  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/eager/context.py\", line 2116, in execution_mode\r\n",
      "    executor_new.wait()\r\n",
      "  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/eager/executor.py\", line 69, in wait\r\n",
      "    pywrap_tfe.TFE_ExecutorWaitForAllPendingNodes(self._handle)\r\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: Input is empty.\r\n",
      "\t [[{{node case/cond/else/_10/case/cond/cond_jpeg/else/_105/case/cond/cond_jpeg/decode_image/DecodeImage}}]]\r\n",
      "\t [[MultiDeviceIteratorGetNextFromShard]]\r\n",
      "\t [[RemoteCall]]\r\n",
      "\n",
      "issue labels - \n",
      "comp:model\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  thankyou\n",
      "issue body -  thankyou all of you .\r\n",
      "I have solved my error by just installing tensorflow==1.5\r\n",
      "by the command \"pip install tensorflow==1.5\"\n",
      "issue labels - \n",
      "TF 1.15\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  Failed when trying to install TensorFlow 2.x into my laptop. Ubuntu 18.04(Desktop). From source\n",
      "issue body -  **System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04(Desktop)\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None\r\n",
      "- TensorFlow installed from (source or binary): Source code\r\n",
      "- TensorFlow version: 2.2\r\n",
      "- Python version: 3.7.5\r\n",
      "- Installed using virtualenv? pip? conda?: conda\r\n",
      "- Bazel version (if compiling from source): 3.7.2\r\n",
      "- GCC/Compiler version (if compiling from source): clang 11.0.0\r\n",
      "- CUDA/cuDNN version: CUDA 11.2 / cuDNN 8.0.4\r\n",
      "- GPU model and memory: GTX 1050 ti / 4 GB\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "Hey, guys. I'm trying to install TensorFlow into my laptop(Dell G3 3579) but failed. There is the error information list below(**Any other info / logs**).\r\n",
      "\r\n",
      "**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n",
      "```bash\r\n",
      "sudo bazel build --config=opt --config=v2  --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n",
      "```\r\n",
      "\r\n",
      "**Any other info / logs**\r\n",
      "```bash\r\n",
      "(base) river@river-G3-3579:~/Downloads/Ubuntu_installation_essential_components/tensorflow$ sudo bazel build --config=opt --config=v2  --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n",
      "[sudo] password for river: \r\n",
      "Starting local Bazel server and connecting to it...\r\n",
      "WARNING: The following configs were expanded more than once: [cuda_clang, using_cuda, v2]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\n",
      "INFO: Options provided by the client:\r\n",
      "  Inherited 'common' options: --isatty=1 --terminal_columns=141\r\n",
      "INFO: Reading rc options for 'build' from /home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/.bazelrc:\r\n",
      "  Inherited 'common' options: --experimental_repo_remote_exec\r\n",
      "INFO: Reading rc options for 'build' from /home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/.bazelrc:\r\n",
      "  'build' options: --apple_platform_type=macos --define framework_shared_object=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2\r\n",
      "INFO: Reading rc options for 'build' from /home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/.tf_configure.bazelrc:\r\n",
      "  'build' options: --action_env PYTHON_BIN_PATH=/usr/local/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.7/site-packages --python_path=/usr/local/bin/python3 --config=xla --config=tensorrt --action_env TF_CUDA_VERSION=11.2 --action_env TF_CUDNN_VERSION=8 --action_env TF_TENSORRT_VERSION=7 --action_env TF_NCCL_VERSION=2.7 --action_env TF_CUDA_PATHS=/usr/local/cuda-11.2/,/usr/local/cuda-11.2/targets/x86_64-linux/include/,/usr/local/include/,/usr/local/lib/,/home/river/Documents/OS_unitility/TensorRT/include/,/home/river/Documents/OS_unitility/TensorRT/util --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda-11.2 --action_env TF_CUDA_COMPUTE_CAPABILITIES=6.1 --action_env LD_LIBRARY_PATH=:/usr/local/jdk1.8/lib:/usr/local/jdk1.8/jre/lib:/usr/local/cuda-11.2/lib64:/usr/local/cuda-11.2/extras/Debugger/lib64:/usr/local/cuda-11.2/nvvm/lib64:/usr/local/cuda-11.2/nvvm-prev/lib64 --config=cuda_clang --action_env CLANG_CUDA_COMPILER_PATH=/usr/local/bin/clang --config=cuda_clang --action_env TF_CONFIGURE_IOS=0\r\n",
      "INFO: Found applicable config definition build:short_logs in file /home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\n",
      "INFO: Found applicable config definition build:v2 in file /home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\n",
      "INFO: Found applicable config definition build:xla in file /home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/.bazelrc: --define=with_xla_support=true\r\n",
      "INFO: Found applicable config definition build:tensorrt in file /home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/.bazelrc: --action_env TF_NEED_TENSORRT=1\r\n",
      "INFO: Found applicable config definition build:cuda_clang in file /home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/.bazelrc: --config=using_cuda --define=using_cuda_clang=true --define=using_clang=true --action_env TF_CUDA_CLANG=1\r\n",
      "INFO: Found applicable config definition build:using_cuda in file /home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1\r\n",
      "INFO: Found applicable config definition build:cuda_clang in file /home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/.bazelrc: --config=using_cuda --define=using_cuda_clang=true --define=using_clang=true --action_env TF_CUDA_CLANG=1\r\n",
      "INFO: Found applicable config definition build:using_cuda in file /home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1\r\n",
      "INFO: Found applicable config definition build:opt in file /home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/.tf_configure.bazelrc: --copt=-Wno-sign-compare --host_copt=-Wno-sign-compare --define with_default_optimizations=true\r\n",
      "INFO: Found applicable config definition build:v2 in file /home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\n",
      "INFO: Found applicable config definition build:cuda in file /home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\n",
      "INFO: Found applicable config definition build:using_cuda in file /home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1\r\n",
      "INFO: Found applicable config definition build:linux in file /home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\n",
      "INFO: Found applicable config definition build:dynamic_kernels in file /home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\n",
      "INFO: Repository local_config_cuda instantiated at:\r\n",
      "  /home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/WORKSPACE:15:10: in <toplevel>\r\n",
      "  /home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/tensorflow/workspace2.bzl:13:20: in workspace\r\n",
      "  /home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/tensorflow/workspace.bzl:95:19: in tf_repositories\r\n",
      "Repository rule cuda_configure defined at:\r\n",
      "  /home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/third_party/gpus/cuda_configure.bzl:1424:33: in <toplevel>\r\n",
      "ERROR: An error occurred during the fetch of repository 'local_config_cuda':\r\n",
      "   Traceback (most recent call last):\r\n",
      "\tFile \"/home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1394, column 38, in _cuda_autoconf_impl\r\n",
      "\t\t_create_local_cuda_repository(repository_ctx)\r\n",
      "\tFile \"/home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/third_party/gpus/cuda_configure.bzl\", line 977, column 35, in _create_local_cuda_repository\r\n",
      "\t\tcuda_config = _get_cuda_config(repository_ctx, find_cuda_config_script)\r\n",
      "\tFile \"/home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/third_party/gpus/cuda_configure.bzl\", line 666, column 30, in _get_cuda_config\r\n",
      "\t\tconfig = find_cuda_config(repository_ctx, find_cuda_config_script, [\"cuda\", \"cudnn\"])\r\n",
      "\tFile \"/home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/third_party/gpus/cuda_configure.bzl\", line 643, column 41, in find_cuda_config\r\n",
      "\t\texec_result = _exec_find_cuda_config(repository_ctx, script_path, cuda_libraries)\r\n",
      "\tFile \"/home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/third_party/gpus/cuda_configure.bzl\", line 637, column 19, in _exec_find_cuda_config\r\n",
      "\t\treturn execute(repository_ctx, [python_bin, \"-c\", decompress_and_execute_cmd])\r\n",
      "\tFile \"/home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/third_party/remote_config/common.bzl\", line 219, column 13, in execute\r\n",
      "\t\tfail(\r\n",
      "Error in fail: Repository command failed\r\n",
      "script.py:124: DeprecationWarning: invalid escape sequence \\d\r\n",
      "  match = re.match(\"#define %s +(\\d+)\" % name, line)\r\n",
      "script.py:260: DeprecationWarning: invalid escape sequence \\d\r\n",
      "  pattern = \"Cuda compilation tools, release \\d+\\.\\d+, V(\\d+\\.\\d+\\.\\d+)\"\r\n",
      "script.py:553: DeprecationWarning: invalid escape sequence \\w\r\n",
      "  match = re.match(\"^(/[^/ ]*)+/lib/\\w+-linux-gnu/?$\", os.environ[env_name])\r\n",
      "script.py:123: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/cuda-11.2/include/cuda.h' mode='r' encoding='utf-8'>\r\n",
      "  for line in io.open(path, \"r\", encoding=\"utf-8\").readlines():\r\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\r\n",
      "script.py:123: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/cuda-11.2/include/cublas_api.h' mode='r' encoding='utf-8'>\r\n",
      "  for line in io.open(path, \"r\", encoding=\"utf-8\").readlines():\r\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\r\n",
      "script.py:123: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/cuda-11.2/include/cusolver_common.h' mode='r' encoding='utf-8'>\r\n",
      "  for line in io.open(path, \"r\", encoding=\"utf-8\").readlines():\r\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\r\n",
      "script.py:123: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/cuda-11.2/include/curand.h' mode='r' encoding='utf-8'>\r\n",
      "  for line in io.open(path, \"r\", encoding=\"utf-8\").readlines():\r\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\r\n",
      "script.py:123: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/cuda-11.2/include/cufft.h' mode='r' encoding='utf-8'>\r\n",
      "  for line in io.open(path, \"r\", encoding=\"utf-8\").readlines():\r\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\r\n",
      "script.py:123: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/cuda-11.2/include/cusparse.h' mode='r' encoding='utf-8'>\r\n",
      "  for line in io.open(path, \"r\", encoding=\"utf-8\").readlines():\r\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\r\n",
      "script.py:123: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/cuda-11.2/include/cudnn.h' mode='r' encoding='utf-8'>\r\n",
      "  for line in io.open(path, \"r\", encoding=\"utf-8\").readlines():\r\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\r\n",
      "script.py:123: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/cuda-11.2/include/cudnn_version.h' mode='r' encoding='utf-8'>\r\n",
      "  for line in io.open(path, \"r\", encoding=\"utf-8\").readlines():\r\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\r\n",
      "INFO: Repository rules_cc instantiated at:\r\n",
      "  /home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/WORKSPACE:15:10: in <toplevel>\r\n",
      "  /home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/tensorflow/workspace2.bzl:13:20: in workspace\r\n",
      "  /home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/tensorflow/workspace.bzl:1001:20: in tf_repositories\r\n",
      "Repository rule tf_http_archive defined at:\r\n",
      "  /home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/third_party/repo.bzl:131:34: in <toplevel>\r\n",
      "INFO: Repository bazel_skylib instantiated at:\r\n",
      "  /home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/WORKSPACE:15:10: in <toplevel>\r\n",
      "  /home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/tensorflow/workspace2.bzl:13:20: in workspace\r\n",
      "  /home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/tensorflow/workspace.bzl:1065:20: in tf_repositories\r\n",
      "Repository rule tf_http_archive defined at:\r\n",
      "  /home/river/Downloads/Ubuntu_installation_essential_components/tensorflow/third_party/repo.bzl:131:34: in <toplevel>\r\n",
      "ERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': no such package '@local_config_cuda//cuda': Repository command failed\r\n",
      "script.py:124: DeprecationWarning: invalid escape sequence \\d\r\n",
      "  match = re.match(\"#define %s +(\\d+)\" % name, line)\r\n",
      "script.py:260: DeprecationWarning: invalid escape sequence \\d\r\n",
      "  pattern = \"Cuda compilation tools, release \\d+\\.\\d+, V(\\d+\\.\\d+\\.\\d+)\"\r\n",
      "script.py:553: DeprecationWarning: invalid escape sequence \\w\r\n",
      "  match = re.match(\"^(/[^/ ]*)+/lib/\\w+-linux-gnu/?$\", os.environ[env_name])\r\n",
      "script.py:123: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/cuda-11.2/include/cuda.h' mode='r' encoding='utf-8'>\r\n",
      "  for line in io.open(path, \"r\", encoding=\"utf-8\").readlines():\r\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\r\n",
      "script.py:123: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/cuda-11.2/include/cublas_api.h' mode='r' encoding='utf-8'>\r\n",
      "  for line in io.open(path, \"r\", encoding=\"utf-8\").readlines():\r\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\r\n",
      "script.py:123: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/cuda-11.2/include/cusolver_common.h' mode='r' encoding='utf-8'>\r\n",
      "  for line in io.open(path, \"r\", encoding=\"utf-8\").readlines():\r\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\r\n",
      "script.py:123: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/cuda-11.2/include/curand.h' mode='r' encoding='utf-8'>\r\n",
      "  for line in io.open(path, \"r\", encoding=\"utf-8\").readlines():\r\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\r\n",
      "script.py:123: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/cuda-11.2/include/cufft.h' mode='r' encoding='utf-8'>\r\n",
      "  for line in io.open(path, \"r\", encoding=\"utf-8\").readlines():\r\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\r\n",
      "script.py:123: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/cuda-11.2/include/cusparse.h' mode='r' encoding='utf-8'>\r\n",
      "  for line in io.open(path, \"r\", encoding=\"utf-8\").readlines():\r\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\r\n",
      "script.py:123: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/cuda-11.2/include/cudnn.h' mode='r' encoding='utf-8'>\r\n",
      "  for line in io.open(path, \"r\", encoding=\"utf-8\").readlines():\r\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\r\n",
      "script.py:123: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/cuda-11.2/include/cudnn_version.h' mode='r' encoding='utf-8'>\r\n",
      "  for line in io.open(path, \"r\", encoding=\"utf-8\").readlines():\r\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\r\n",
      "WARNING: Target pattern parsing failed.\r\n",
      "ERROR: no such package '@local_config_cuda//cuda': Repository command failed\r\n",
      "script.py:124: DeprecationWarning: invalid escape sequence \\d\r\n",
      "  match = re.match(\"#define %s +(\\d+)\" % name, line)\r\n",
      "script.py:260: DeprecationWarning: invalid escape sequence \\d\r\n",
      "  pattern = \"Cuda compilation tools, release \\d+\\.\\d+, V(\\d+\\.\\d+\\.\\d+)\"\r\n",
      "script.py:553: DeprecationWarning: invalid escape sequence \\w\r\n",
      "  match = re.match(\"^(/[^/ ]*)+/lib/\\w+-linux-gnu/?$\", os.environ[env_name])\r\n",
      "script.py:123: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/cuda-11.2/include/cuda.h' mode='r' encoding='utf-8'>\r\n",
      "  for line in io.open(path, \"r\", encoding=\"utf-8\").readlines():\r\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\r\n",
      "script.py:123: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/cuda-11.2/include/cublas_api.h' mode='r' encoding='utf-8'>\r\n",
      "  for line in io.open(path, \"r\", encoding=\"utf-8\").readlines():\r\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\r\n",
      "script.py:123: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/cuda-11.2/include/cusolver_common.h' mode='r' encoding='utf-8'>\r\n",
      "  for line in io.open(path, \"r\", encoding=\"utf-8\").readlines():\r\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\r\n",
      "script.py:123: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/cuda-11.2/include/curand.h' mode='r' encoding='utf-8'>\r\n",
      "  for line in io.open(path, \"r\", encoding=\"utf-8\").readlines():\r\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\r\n",
      "script.py:123: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/cuda-11.2/include/cufft.h' mode='r' encoding='utf-8'>\r\n",
      "  for line in io.open(path, \"r\", encoding=\"utf-8\").readlines():\r\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\r\n",
      "script.py:123: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/cuda-11.2/include/cusparse.h' mode='r' encoding='utf-8'>\r\n",
      "  for line in io.open(path, \"r\", encoding=\"utf-8\").readlines():\r\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\r\n",
      "script.py:123: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/cuda-11.2/include/cudnn.h' mode='r' encoding='utf-8'>\r\n",
      "  for line in io.open(path, \"r\", encoding=\"utf-8\").readlines():\r\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\r\n",
      "script.py:123: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/cuda-11.2/include/cudnn_version.h' mode='r' encoding='utf-8'>\r\n",
      "  for line in io.open(path, \"r\", encoding=\"utf-8\").readlines():\r\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\r\n",
      "INFO: Elapsed time: 7.982s\r\n",
      "INFO: 0 processes.\r\n",
      "FAILED: Build did NOT complete successfully (0 packages loaded)\r\n",
      "    currently loading: tensorflow/tools/pip_package\r\n",
      "    Fetching @local_config_tensorrt; fetching\r\n",
      "```\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.2\n",
      "stat:awaiting response\n",
      "subtype: ubuntu/linux\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  Fix region control flow to functional pass with incompatible cast.\n",
      "issue body -  Fixes https://github.com/tensorflow/tensorflow/issues/46656.\r\n",
      "\r\n",
      "When matching call args, do not skip incompatible cast. Otherwise, there will be incompatible args passed to build control flow op.\r\n",
      "\r\n",
      "Also note that the https://github.com/tensorflow/tensorflow/issues/46656 is broken after https://github.com/tensorflow/tensorflow/commit/f9818f12c332e8a35dbc9042af55649be64f30c4 (so it works in 2.4.x). Before f9818f12c332e8a35dbc9042af55649be64f30c4, the single call op with few operations is inlined so it won't fall into\r\n",
      "\r\n",
      "https://github.com/tensorflow/tensorflow/blob/e323dc9a05193c76ba5db1c71d0ef019be676224/tensorflow/compiler/mlir/tensorflow/transforms/region_control_flow_to_functional.cc#L287-L291\n",
      "issue labels - \n",
      "cla: yes\n",
      "prtype:bugfix\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  Trying to train a custom object detection model\n",
      "issue body -  Hey guys!\r\n",
      "I have been trying to train a model of Object detection using the the Tensorflow models that you guys provide and while i have been trying to do this for the last 3 weeks there are some issues which are coming again and again like there is nor attribute input to convolution box predictor as those issues are resolved now there is a new issue of checkpoint which i am not able to solve or find a way to solve it i would request to update me on this one \r\n",
      "![Problem1](https://user-images.githubusercontent.com/51516774/106319221-1e211380-6297-11eb-8395-bf5d3a09478a.jpg)\r\n",
      "\n",
      "issue labels - \n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  Change 'must a Tensor' to 'must be a Tensor' in TypeError message\n",
      "issue body -  This is very minor. There was a small typo in a TypeError message.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:ops\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Refactor fake_quant.h from reference_ops.h\n",
      "issue body -  PR2 for issue #46783.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  ValueError: Input 0 of layer encoder1_ is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (None, 64, 4, 850)\n",
      "issue body -            inputs_p = Input(shape=(5,4,850)) #4 time steps, 5 features , num_sds\r\n",
      "          decoder_inputs = Input(shape=(5,4,850)) #1time step, 5 features , num_sds\r\n",
      "          neighbhourhood = Input(shape=(4,850,850)) #1time step, 5 features , num_sds\r\n",
      "\r\n",
      "\r\n",
      "          encoder_inputs = MyModel_conv_accross_time()(inputs_p , neighbhourhood)\r\n",
      "\r\n",
      "\r\n",
      "          enc = LSTM(cfg['units'],activation='tanh',return_state=True,return_sequences=True,name='encoder1_') \r\n",
      "          decoder_outputs, state_h, state_c  = enc(encoder_inputs)\r\n",
      "\r\n",
      "\r\n",
      "ValueError: Input 0 of layer encoder1_ is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (None, 64, 4, 850)\r\n",
      "\r\n",
      "\r\n",
      "MyModel_conv_accross_time  returns results with shape (batch,64,4,850)\n",
      "issue labels - \n",
      "comp:keras\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Refactor ParseFakeQuant in lite/core/api/flatbuffer_conversions.cc/h\n",
      "issue body -  PR1 for issue #46783.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  Do not use XLA. Build configuration\n",
      "issue body -  Hi, \r\n",
      "\r\n",
      "this is related to building TensorFlow from source, but in case it's my wrongdoing I didn't want to mark it as a bug.\r\n",
      "\r\n",
      "Tensorflow 2.2.1\r\n",
      "Keras 2.3.1\r\n",
      "\r\n",
      "I have a VAE model in which decoder is autoregressive and and one of the layer looks like this:\r\n",
      "\r\n",
      "```python\r\n",
      "class AutoregressiveGRU(layers.Layer):\r\n",
      "\r\n",
      "    def __init__(\r\n",
      "            self,\r\n",
      "            output_dim: int,\r\n",
      "            output_len: int,\r\n",
      "            recurrent: layers.Recurrent,\r\n",
      "            **kwargs,\r\n",
      "    ):\r\n",
      "        self.output_dim = output_dim\r\n",
      "        self.output_len = output_len\r\n",
      "        self.initial_state = None\r\n",
      "        self.recurrent = recurrent\r\n",
      "        super(AutoregressiveGRU, self).__init__(**kwargs)\r\n",
      "\r\n",
      "    def build(self, input_shape):\r\n",
      "        super(AutoregressiveGRU, self).build(input_shape)\r\n",
      "\r\n",
      "    def call(self, x):\r\n",
      "        outputs = []\r\n",
      "        current_output = backend.zeros_like(backend.repeat(x, 1))\r\n",
      "        current_state = x\r\n",
      "        for _ in range(self.output_len):\r\n",
      "            current_output, current_state = self.recurrent(\r\n",
      "                current_output,\r\n",
      "                initial_state=current_state,\r\n",
      "            )\r\n",
      "            outputs.append(current_output)\r\n",
      "        result = layers.concatenate(outputs, axis=1)\r\n",
      "        result = backend.reshape(result, (-1, self.output_len, self.output_dim))\r\n",
      "        return result\r\n",
      "```\r\n",
      "\r\n",
      "I am calculating some higher-order gradients and I am getting errors such as:\r\n",
      "\r\n",
      "```shell\r\n",
      "InvalidArgumentError: Operation 'gradients_2/autoregressive_gru_1_3/gru_3/while_grad/autoregressive_gru_1_3/gru_3/while_grad' has no attr named '_XlaCompile'.\r\n",
      "\r\n",
      "```\r\n",
      "\r\n",
      "I looked at [XLA known issues](https://www.tensorflow.org/xla/known_issues) and found out that this might be related to the for-loop that is not bounded to constant number of iterations. I have rewritten the `for-loop` in that layer with `tf.while_loop` that has `maximum_iteration` parameter set, but the problem (error mentioned above) persists:\r\n",
      "\r\n",
      "```python\r\n",
      "    def call(self, x):\r\n",
      "        outputs = []\r\n",
      "        current_output = backend.zeros_like(backend.repeat(x, 1))\r\n",
      "        current_state = x\r\n",
      "        i = tf.constant(0)\r\n",
      "        c = lambda i, a, b, ta: i < 25\r\n",
      "        def turn(i, current_statex, current_outputx, ta):\r\n",
      "            current_output_cur, current_state_cur = self.recurrent(\r\n",
      "                current_outputx,\r\n",
      "                initial_state=current_statex,\r\n",
      "            )\r\n",
      "            ta.write(i, current_output_cur)\r\n",
      "            tf.add(i, 1)\r\n",
      "            return [i, current_state_cur, current_output_cur, ta]\r\n",
      "\r\n",
      "        i, a, b, ta =  tf.while_loop(c, turn, [i, current_state, current_output, tf.TensorArray(tf.float32, size=25)], maximum_iterations=tf.constant(25))\r\n",
      "        result = ta.stack()\r\n",
      "        return backend.reshape(result, (-1, self.output_len, self.output_dim))\r\n",
      "```\r\n",
      "\r\n",
      "I decided to drop XLA all together and so I compiled TensorFlow from source setting:\r\n",
      "```\r\n",
      "build:xla --action_env=TF_ENABLE_XLA=0\r\n",
      "build:xla --define=with_xla_support=false\r\n",
      "```\r\n",
      "\r\n",
      "And it still  comes with the same error. Is there something wrong with what I am doing?\r\n",
      "\r\n",
      "I apologize but I am not able to put the entire model here for various reasons. If I remove the autoregressive layer, everything works.\r\n",
      "\r\n",
      "EDIT:\r\n",
      "```\r\n",
      "tf.config.list_physical_devices()\r\n",
      "```\r\n",
      "now returns only cpu `physical_device:CPU` ano no `physical_device:XLA_CPU`. So why is XLA even used during training? \r\n",
      "```\r\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\r\n",
      "```\n",
      "issue labels - \n",
      "TF 2.2\n",
      "comp:xla\n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  tf-nightly ResNet50V2 building error\n",
      "issue body -  **System information**\r\n",
      "Colab\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "Tf-nightly notebook:\r\n",
      "https://colab.research.google.com/drive/1-A_C9bQL0cRdJ5F2ETlJl9GaZmukYVaq?usp=sharing\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "Tf 2.4.1 notebook:\r\n",
      "https://colab.research.google.com/drive/1lRlGfQljmqAwOsP_npItl_aElCI9KPMD?usp=sharing\n",
      "issue labels - \n",
      "TF 2.5\n",
      "comp:keras\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Win10: ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\n",
      "issue body -  <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Home 15063\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n",
      "- TensorFlow installed from (source or binary): binary\r\n",
      "- TensorFlow version: 2.4.1\r\n",
      "- Python version: 3.8 (64-bit)\r\n",
      "- Installed using virtualenv? pip? conda?: pip inside virtualenv\r\n",
      "- Bazel version (if compiling from source): N/A\r\n",
      "- GCC/Compiler version (if compiling from source): N/A\r\n",
      "- CUDA/cuDNN version: no cuda (Intel HD Graphics)\r\n",
      "- GPU model and memory: Intel HD Graphics Integrated (Memory: N/A)\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "It looks like i am unable to import tensorflow. this is my first time starting with tensorflow and i was trying this code:\r\n",
      "\r\n",
      "```python\r\n",
      "import tensorflow as tf\r\n",
      "\r\n",
      "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist\r\n",
      "\r\n",
      "x_train = tf.keras.utils.normalize(x_train, axis=1)\r\n",
      "x_test = tf.keras.utils.normalize(x_test, axis=1)\r\n",
      "\r\n",
      "model = tf.keras.models.Sequential()\r\n",
      "model.add(tf.keras.layers.Flatten())\r\n",
      "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\r\n",
      "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\r\n",
      "model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))\r\n",
      "\r\n",
      "model.compile(optimizer=\"adam\",\r\n",
      "              loss=\"sparse_categorical_crossentropy\",\r\n",
      "              metrics=[\"accuracy\"])\r\n",
      "\r\n",
      "model.fit(x_train, y_train, epochs=3)\r\n",
      "\r\n",
      "model.save(\"m\\\\mnist_model.model\")\r\n",
      "```\r\n",
      "\r\n",
      "**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n",
      "```\r\n",
      "venv\\Scripts\\activate.bat\r\n",
      "pip install numpy tensorflow\r\n",
      "python main.py\r\n",
      "```\r\n",
      "\r\n",
      "\r\n",
      "**Any other info / logs**\r\n",
      "```\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"D:\\Xcodz\\tensorflowmachinelearning\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n",
      "    from tensorflow.python._pywrap_tensorflow_internal import *\r\n",
      "ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\r\n",
      "\r\n",
      "During handling of the above exception, another exception occurred:\r\n",
      "\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"D:/Xcodz/tensorflowmachinelearning/main.py\", line 1, in <module>\r\n",
      "    import tensorflow as tf\r\n",
      "  File \"D:\\Xcodz\\tensorflowmachinelearning\\venv\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n",
      "    from tensorflow.python.tools import module_util as _module_util\r\n",
      "  File \"D:\\Xcodz\\tensorflowmachinelearning\\venv\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 39, in <module>\r\n",
      "    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\r\n",
      "  File \"D:\\Xcodz\\tensorflowmachinelearning\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 83, in <module>\r\n",
      "    raise ImportError(msg)\r\n",
      "ImportError: Traceback (most recent call last):\r\n",
      "  File \"D:\\Xcodz\\tensorflowmachinelearning\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n",
      "    from tensorflow.python._pywrap_tensorflow_internal import *\r\n",
      "ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\r\n",
      "\r\n",
      "\r\n",
      "Failed to load the native TensorFlow runtime.\r\n",
      "\r\n",
      "See https://www.tensorflow.org/install/errors\r\n",
      "\r\n",
      "for some common reasons and solutions.  Include the entire stack trace\r\n",
      "above this error message when asking for help.\r\n",
      "```\n",
      "issue labels - \n",
      "TF 2.4\n",
      "stat:awaiting response\n",
      "subtype:windows\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  Add Torch (Flash light) \"Android\"\n",
      "issue body -  Hi,\r\n",
      "I wanna add Torch to this app. sometime light is not good  in area so I have to use flash light in app\r\n",
      "But I can't use this code in app \r\n",
      "mCameraManager.setTorchMode(mCameraId, true);\r\n",
      "The error is: camera \"0\" is in use\r\n",
      "Or flash light just blink for a second\r\n",
      "How can I use this feature in app?\n",
      "issue labels - \n",
      "stat:awaiting response\n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  Op with name (StatefulPartitionedCall/sequential/conv2d/BiasAdd) and type (Conv) kernel not found in CPUExecutionProvider\n",
      "issue body -  I followed the practices described in to build onnxruntime for all 4 targets (in android) https://github.com/microsoft/onnxruntime/blob/master/docs/ONNX_Runtime_for_Mobile_Platforms.md\r\n",
      "\r\n",
      "1. converted onnx to ort files with basic optimization via --optimization_level basic  & another ort files allowing onnx optimization in ort.\r\n",
      "\r\n",
      "2. Tried Release build with \r\n",
      "./build.sh --config Release--android --android_sdk_path /Android --android_ndk_path /Android/ndk/21.1.6352462/ --android_abi x86 --android_api 29 --minimal_build extended --use_nnapi  --disable_exceptions --build_shared_lib --skip_tests --include_ops_by_config <config file produced by step 1>\r\n",
      "(For all ABI types)\r\n",
      "3. My config files contain \r\n",
      "`ai.onnx;9;MatMul,Sub`\r\n",
      "`ai.onnx;12;Add,Conv,Gemm,MaxPool,Mul,Relu,Reshape,Softmax,Transpose`\r\n",
      "\r\n",
      "I am running on Api 30 on x86 emulator but getting exception on both types of (ort: one with basioc optimization & one without optimization)\r\n",
      "`exception of type Ort::Exception: Failed to find kernel for Conv(11) (node StatefulPartitionedCall/sequential/conv2d/BiasAdd). Op with name (StatefulPartitionedCall/sequential/conv2d/BiasAdd) and type (Conv) kernel not found in CPUExecutionProvider. No matching hash for 8328794455908578232\"`\r\n",
      "Is there kernel named \"BiasAdd\" ? \r\n",
      "\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "comp:lite\n",
      "stat:awaiting tensorflower\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Enabling GPU support for Tensorflow in virtualenv\n",
      "issue body -  I am attempting to setup `Tensorflow` with GPU support on virtualenv. \r\n",
      "I have a native version with system interpreter - `Tensorflow 2.4.1`, which works fine and does see the `GPU`. \r\n",
      "And  in virtualenv I use `Tensorflow 2.0.0` with Python 3.7.9. \r\n",
      "\r\n",
      "When running in the interpreter on the system with `Tensorflow 2.4.1` I receive following greeting message : \r\n",
      "\r\n",
      "`I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0`\r\n",
      "\r\n",
      "And the command `tf.config.experimental.list_physical_devices(\"GPU\")` shows my GPU. \r\n",
      "However perforing the same actions in virtualenv I do not receive this greeting message, and the output of the command above is an empty list. \r\n",
      "\r\n",
      "**System information**\r\n",
      "- OS Platform and Distribution : (Linux Ubuntu 20.04):\r\n",
      "- TensorFlow version: 2.0.0\r\n",
      "- Python version: 3.7.9\r\n",
      "- Installed using pip in virtualenv\r\n",
      "- CUDA/cuDNN version: 11.0\r\n",
      "- GPU model and memory: RTX 2080Ti\r\n",
      "\r\n",
      "I've installed the `Tensorflow 2.0.0` via pip inside virtualenv. And the `CUDA` setup was performed as described here \r\n",
      "https://www.tensorflow.org/install/gpu?hl=ur with some changes for `Ubuntu 20.04`.\r\n",
      "\r\n",
      "My question is : what needs to be done in order to enable GPU support inside virtualenv? \r\n",
      "\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:gpu\n",
      "stat:awaiting response\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  Micro: port op FAKE_QUANT from Lite\n",
      "issue body -  @tensorflow/micro\r\n",
      "\r\n",
      "**System information**\r\n",
      "Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n",
      "TensorFlow installed from (source or binary): source\r\n",
      "Tensorflow version (commit SHA if source): master\r\n",
      "Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Sparkfun Edge\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "I am about to port The TF Lite kernel op FAKE_QUANT to TFLite Micro.\r\n",
      "\r\n",
      "**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n",
      "PR 1: refactor flatbuffer_conversions parsing function\r\n",
      "PR 2: refactor reference implementation from lite/kernels/internal/reference/reference_ops.h into its own header with only the changes to pass internal CI build checks.\r\n",
      "PR 3: copy the kernel from lite to micro any make the micro op and its testing code to work.\r\n",
      "\n",
      "issue labels - \n",
      "comp:lite\n",
      "comp:micro\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  tf.keras.Model.fit() training works fine but custom training loop fails for identical model, optimizer, and loss function\n",
      "issue body -  <em>Please make sure that this is a bug. As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:bug_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10, Anaconda\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "- TensorFlow installed from (source or binary): Binary\r\n",
      "- TensorFlow version (use command below): 2.4.0\r\n",
      "- Python version: 3.8.0\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version:\r\n",
      "- GPU model and memory:\r\n",
      "\r\n",
      "You can collect some of this information using our environment capture\r\n",
      "[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n",
      "You can also obtain the TensorFlow version with:\r\n",
      "1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n",
      "2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n",
      "\r\n",
      "v2.4.0-rc4-71-g582c8d236cb 2.4.0\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "Code\r\n",
      "`from tensorflow.python.eager import backprop\r\n",
      "import tensorflow as tf\r\n",
      "import numpy as np\r\n",
      "import math\r\n",
      "\r\n",
      "print(\"TensorFlow version: {}\".format(tf.__version__))\r\n",
      "print(\"Eager execution: {}\".format(tf.executing_eagerly()))\r\n",
      "\r\n",
      "\r\n",
      "#Equation to generate data\r\n",
      "#y = x * 0.1\r\n",
      "def calulate(x):\r\n",
      "    return sum([i*0.1 for i in x])\r\n",
      "\r\n",
      "#Generate samples in batch\r\n",
      "def samples(size):\r\n",
      "    x = np.random.random((size, 4))\r\n",
      "    y = np.zeros((size))\r\n",
      "    for i in range(size):\r\n",
      "        y[i] = calulate(x[i])\r\n",
      "    return x,y\r\n",
      "\r\n",
      "#Model\r\n",
      "model = tf.keras.Sequential([\r\n",
      "  tf.keras.layers.Dense(1, input_shape=(4,))\r\n",
      "])\r\n",
      "\r\n",
      "#loss function and optimizer\r\n",
      "loss = tf.keras.losses.MeanSquaredError(reduction=\"auto\")\r\n",
      "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\r\n",
      "\r\n",
      "#Validation function\r\n",
      "def validate():\r\n",
      "    x = np.random.random((5, 4))\r\n",
      "    y = [calulate(i) for i in x]\r\n",
      "    e = model.predict(x)\r\n",
      "    print(\"{} ==> {} , {}\".format(x, y, e))\r\n",
      "\r\n",
      "#Model.fit\r\n",
      "def train_default(epoch) :\r\n",
      "    x, y = samples(64000)\r\n",
      "    model.compile(optimizer=optimizer, loss=loss)\r\n",
      "    history = model.fit(x, y, epochs=epoch, validation_split = 0.2)\r\n",
      "\r\n",
      "\r\n",
      "#Custom train\r\n",
      "def train_custom(steps):\r\n",
      "    x = []\r\n",
      "    y = []\r\n",
      "    for _ in range(2000):\r\n",
      "        xe, ye = samples(32)\r\n",
      "        x.append(xe)\r\n",
      "        y.append(ye)\r\n",
      "\r\n",
      "    for episod in range(steps):\r\n",
      "        episod_loss = tf.keras.metrics.Mean()\r\n",
      "        for i in range(2000):\r\n",
      "            with tf.GradientTape() as tape:\r\n",
      "                e = model(x[i], training=True)\r\n",
      "                l = loss(y[i], e)\r\n",
      "            grads = tape.gradient(l, model.trainable_weights)\r\n",
      "            #optimizer.minimize(l, model.trainable_variables, tape=tape)\r\n",
      "            optimizer.apply_gradients(zip(grads, model.trainable_weights))\r\n",
      "            episod_loss.update_state(l)\r\n",
      "        print(\"episod: {} loss: {}\".format(episod, episod_loss.result()))\r\n",
      "        print(model.trainable_weights)\r\n",
      "\r\n",
      "\r\n",
      "#Tensorflow extension\r\n",
      "loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\r\n",
      "mae_metric = tf.keras.metrics.MeanAbsoluteError(name=\"mae\")\r\n",
      "\r\n",
      "class CustomModel(tf.keras.Model):\r\n",
      "    def train_step(self, data):\r\n",
      "        x, y = data\r\n",
      "\r\n",
      "        with tf.GradientTape() as tape:\r\n",
      "            y_pred = self(x, training=True)  # Forward pass\r\n",
      "            l = loss(y, y_pred)\r\n",
      "\r\n",
      "        #Compute gradients\r\n",
      "        trainable_vars = self.trainable_variables\r\n",
      "        gradients = tape.gradient(l, trainable_vars)\r\n",
      "\r\n",
      "        #Update weights\r\n",
      "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\r\n",
      "\r\n",
      "        #Compute our own metrics\r\n",
      "        loss_tracker.update_state(l)\r\n",
      "        mae_metric.update_state(y, y_pred)\r\n",
      "        return {\"loss\": loss_tracker.result(), \"mae\": mae_metric.result()}\r\n",
      "\r\n",
      "    @property\r\n",
      "    def metrics(self):\r\n",
      "        return [loss_tracker, mae_metric]\r\n",
      "\r\n",
      "def train_extended(steps):\r\n",
      "    #Construct an instance of CustomModel\r\n",
      "    inputs = tf.keras.Input(shape=(4,))\r\n",
      "    outputs = tf.keras.layers.Dense(1)(inputs)\r\n",
      "    model = CustomModel(inputs, outputs)\r\n",
      "\r\n",
      "    #We don't passs a loss or metrics here.\r\n",
      "    model.compile(optimizer=optimizer)\r\n",
      "\r\n",
      "    x = []\r\n",
      "    y = []\r\n",
      "    for _ in range(2000):\r\n",
      "        xe, ye = samples(32)\r\n",
      "        x.append(xe)\r\n",
      "        y.append(ye)\r\n",
      "\r\n",
      "    for episod in range(steps):\r\n",
      "        for i in range(2000):\r\n",
      "            model.fit(x[i], y[i], epochs=1, verbose= 0)\r\n",
      "        print(\"episod: {}\".format(episod))\r\n",
      "        print(model.trainable_variables)\r\n",
      "\r\n",
      "\r\n",
      "##Works\r\n",
      "#train_default(5)\r\n",
      "#validate()\r\n",
      "\r\n",
      "#Works\r\n",
      "#train_extended(5)\r\n",
      "#validate()\r\n",
      "\r\n",
      "#Don't work\r\n",
      "train_custom(5)\r\n",
      "validate()`\r\n",
      "**Describe the expected behavior**\r\n",
      "train_custom function should also works like other \r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "Provide a reproducible test case that is the bare minimum necessary to generate\r\n",
      "the problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n",
      "\r\n",
      "**Other info / logs** Include any logs or source code that would be helpful to\r\n",
      "diagnose the problem. If including tracebacks, please include the full\r\n",
      "traceback. Large logs and files should be attached.\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:keras\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  cudnn 8.1.0 support ( + Bazel 4.0.0 )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "issue body -  <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n",
      "- TensorFlow installed from (source or binary): source\r\n",
      "- TensorFlow version: master 2.5.0\r\n",
      "- Python version: 3.9.1\r\n",
      "- Installed using virtualenv? pip? conda?: N/A\r\n",
      "- Bazel version (if compiling from source): 3.7.2\r\n",
      "- GCC/Compiler version (if compiling from source): Visual Studio 2019\r\n",
      "- CUDA/cuDNN version: 11.2/8.1.0\r\n",
      "- GPU model and memory: RTX3090 GDDR6 24GB\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "build failed\r\n",
      "**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n",
      "bazel build\r\n",
      "\r\n",
      "**Any other info / logs**\r\n",
      "Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n",
      "\r\n",
      "```\r\n",
      "ERROR: D:/repo/tensorflow/tensorflow/compiler/xla/service/cpu/BUILD:496:11: C++ compilation of rule '//tensorflow/compiler/xla/service/cpu:dot_op_emitter' failed (Exit 2): python.exe failed: error executing command\r\n",
      "```\r\n",
      "\r\n",
      "```\r\n",
      "cl : Command line warning D9035 : option 'experimental:preprocessor' has been deprecated and will be removed in a future release\r\n",
      "cl : Command line warning D9036 : use 'Zc:preprocessor' instead of 'experimental:preprocessor'\r\n",
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29333\\include\\utility(246): warning C4244: 'initializing': conversion from '_Ty' to '_Ty1', possible loss of data\r\n",
      "        with\r\n",
      "        [\r\n",
      "            _Ty=uint64_t\r\n",
      "        ]\r\n",
      "        and\r\n",
      "        [\r\n",
      "            _Ty1=unsigned int\r\n",
      "        ]\r\n",
      "external/llvm-project/llvm/include\\llvm/ADT/SmallBitVector.h(725): note: see reference to function template instantiation 'std::pair<unsigned int,llvm::ArrayRef<uint64_t>>::pair<unsigned __int64,llvm::ArrayRef<uint64_t>,0>(std::pair<unsigned __int64,llvm::ArrayRef<uint64_t>> &&) noexcept' being compiled\r\n",
      "external/llvm-project/llvm/include\\llvm/ADT/SmallBitVector.h(724): note: see reference to function template instantiation 'std::pair<unsigned int,llvm::ArrayRef<uint64_t>>::pair<unsigned __int64,llvm::ArrayRef<uint64_t>,0>(std::pair<unsigned __int64,llvm::ArrayRef<uint64_t>> &&) noexcept' being compiled\r\n",
      "external/llvm-project/mlir/include\\mlir/Dialect/Linalg/Transforms/CodegenStrategy.h(74): error C2668: 'mlir::linalg::sfinae_enqueue': ambiguous call to overloaded function\r\n",
      "external/llvm-project/mlir/include\\mlir/Dialect/Linalg/Transforms/CodegenStrategy.h(51): note: could be 'void mlir::linalg::sfinae_enqueue<mlir::linalg::LinalgTilingPattern,LinalgOpType,mlir::linalg::LinalgTilingOptions>(mlir::OwningRewritePatternList &,OptionsType,mlir::MLIRContext *,llvm::StringRef,mlir::linalg::LinalgTransformationFilter)'\r\n",
      "        with\r\n",
      "        [\r\n",
      "            LinalgOpType=mlir::linalg::GenericOp,\r\n",
      "            OptionsType=mlir::linalg::LinalgTilingOptions\r\n",
      "        ]\r\n",
      "external/llvm-project/mlir/include\\mlir/Dialect/Linalg/Transforms/CodegenStrategy.h(38): note: or       'void mlir::linalg::sfinae_enqueue<mlir::linalg::LinalgTilingPattern,LinalgOpType,mlir::linalg::LinalgTilingOptions,std::enable_if<false,void>>(mlir::OwningRewritePatternList &,OptionsType,mlir::MLIRContext *,llvm::StringRef,mlir::linalg::LinalgTransformationFilter)'\r\n",
      "        with\r\n",
      "        [\r\n",
      "            LinalgOpType=mlir::linalg::GenericOp,\r\n",
      "            OptionsType=mlir::linalg::LinalgTilingOptions\r\n",
      "        ]\r\n",
      "external/llvm-project/mlir/include\\mlir/Dialect/Linalg/Transforms/CodegenStrategy.h(75): note: while trying to match the argument list '(mlir::OwningRewritePatternList, mlir::linalg::LinalgTilingOptions, mlir::MLIRContext *, std::string, mlir::linalg::LinalgTransformationFilter)'\r\n",
      "external/llvm-project/mlir/include\\mlir/Dialect/Linalg/Transforms/CodegenStrategy.h(72): note: while compiling class template member function 'mlir::OwningRewritePatternList mlir::linalg::Tile<LinalgOpType>::buildRewritePatterns(mlir::MLIRContext *,mlir::linalg::LinalgTransformationFilter)'\r\n",
      "        with\r\n",
      "        [\r\n",
      "            LinalgOpType=mlir::linalg::GenericOp\r\n",
      "        ]\r\n",
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29333\\include\\memory(2536): note: see reference to class template instantiation 'mlir::linalg::Tile<LinalgOpType>' being compiled\r\n",
      "        with\r\n",
      "        [\r\n",
      "            LinalgOpType=mlir::linalg::GenericOp\r\n",
      "        ]\r\n",
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29333\\include\\memory(2535): note: while compiling class template member function 'void std::default_delete<mlir::linalg::Tile<LinalgOpType>>::operator ()(_Ty *) noexcept const'\r\n",
      "        with\r\n",
      "        [\r\n",
      "            LinalgOpType=mlir::linalg::GenericOp,\r\n",
      "            _Ty=mlir::linalg::Tile<mlir::linalg::GenericOp>\r\n",
      "        ]\r\n",
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29333\\include\\memory(2647): note: see reference to function template instantiation 'void std::default_delete<mlir::linalg::Tile<LinalgOpType>>::operator ()(_Ty *) noexcept const' being compiled\r\n",
      "        with\r\n",
      "        [\r\n",
      "            LinalgOpType=mlir::linalg::GenericOp,\r\n",
      "            _Ty=mlir::linalg::Tile<mlir::linalg::GenericOp>\r\n",
      "        ]\r\n",
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29333\\include\\memory(2574): note: see reference to class template instantiation 'std::default_delete<mlir::linalg::Tile<LinalgOpType>>' being compiled\r\n",
      "        with\r\n",
      "        [\r\n",
      "            LinalgOpType=mlir::linalg::GenericOp\r\n",
      "        ]\r\n",
      "external/llvm-project/mlir/include\\mlir/Dialect/Linalg/Transforms/CodegenStrategy.h(160): note: see reference to class template instantiation 'std::unique_ptr<mlir::linalg::Tile<LinalgOpType>,std::default_delete<mlir::linalg::Tile<LinalgOpType>>>' being compiled\r\n",
      "        with\r\n",
      "        [\r\n",
      "            LinalgOpType=mlir::linalg::GenericOp\r\n",
      "        ]\r\n",
      "tensorflow/compiler/xla/service/cpu/dot_op_emitter.cc(325): note: see reference to function template instantiation 'mlir::linalg::CodegenStrategy &mlir::linalg::CodegenStrategy::tile<mlir::linalg::GenericOp>(mlir::linalg::LinalgTilingOptions,mlir::linalg::LinalgTransformationFilter::FilterFunction)' being compiled\r\n",
      "```\n",
      "issue labels - \n",
      "TF 2.5\n",
      "subtype:windows\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  TFLite Converter Version  InCompatibility Issue\n",
      "issue body -  \r\n",
      "\r\n",
      "### 2. Code\r\n",
      "\r\n",
      "Please find this [notebook](https://colab.research.google.com/drive/1slj7OwIZ8M567BDUyNLUI7w-Eag7b55R?usp=sharing)\r\n",
      "\r\n",
      "### 3. Failure after conversion\r\n",
      "\r\n",
      "Model conversion working with `tensorflow-2.3.0` and `tf-nightly` versions and not working with `tensorflow=2.4`(default colab version). Similary model inference is working only with `tensorflow-2.3.0` and not working with both `tensorflow-2.4` and `tf-nightly`\r\n",
      "\r\n",
      "CC: @abattery @khanhlvg \n",
      "issue labels - \n",
      "TF 2.4\n",
      "TFLiteConverter\n",
      "comp:lite\n",
      "regression issue\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  fixed issue template link\n",
      "issue body -  When creating a new GitHub issue using the \"bug\" template, the `GitHub Policy` link results in a `404`.\r\n",
      "I am assuming it is meant to link to `ISSUE_TEMPLATE.md` instead of `ISSUES.md`.\n",
      "issue labels - \n",
      "cla: yes\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  flattening operation using tf.reshape inside @tf.function graph raises ValueError\n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): `No`\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `Windows 10 19042.746`\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "- TensorFlow installed from (source or binary): `binary`\r\n",
      "- TensorFlow version (use command below): `2.4.1`\r\n",
      "- Python version: `3.8.5`\r\n",
      "- CUDA/cuDNN version: `11.0/8.0`\r\n",
      "- GPU model and memory: `2x RTX2080 12GB`\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "Calling function wrapped with @tf.function and reshaping operation properly reshapes tensor with value `-1` for flattening raises `ValueError`\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "Calling function wrapped with @tf.function and reshaping operation properly reshapes tensor with value `-1` for flattening converts tensor into `1D` shape\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "```python\r\n",
      "@tf.function\r\n",
      "def tf_function_wrapped_function():\r\n",
      "  reshaped = tf.reshape(tf.ones((64, 64)), -1)\r\n",
      "```\r\n",
      "\r\n",
      "**Full TraceBack**\r\n",
      "\r\n",
      "```python\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \".\\tests\\test_tf_dark_com_preds.py\", line 36, in <module>\r\n",
      "    test_tf_reshape()\r\n",
      "  File \".\\tests\\test_tf_dark_com_preds.py\", line 32, in test_get_com_preds\r\n",
      "    assert test_tf_reshape()\r\n",
      "  File \"C:\\Users\\iam\\.conda\\envs\\alyce\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 828, in __call__\r\n",
      "    result = self._call(*args, **kwds)\r\n",
      "  File \"C:\\Users\\iam\\.conda\\envs\\alyce\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 871, in _call\r\n",
      "    self._initialize(args, kwds, add_initializers_to=initializers)\r\n",
      "  File \"C:\\Users\\iam\\.conda\\envs\\alyce\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 725, in _initialize\r\n",
      "    self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n",
      "  File \"C:\\Users\\iam\\.conda\\envs\\alyce\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2969, in _get_concrete_function_internal_garbage_collected\r\n",
      "    graph_function, _ = self._maybe_define_function(args, kwargs)\r\n",
      "  File \"C:\\Users\\iam\\.conda\\envs\\alyce\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 3361, in _maybe_define_function\r\n",
      "    graph_function = self._create_graph_function(args, kwargs)\r\n",
      "  File \"C:\\Users\\iam\\.conda\\envs\\alyce\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 3196, in _create_graph_function\r\n",
      "    func_graph_module.func_graph_from_py_func(\r\n",
      "  File \"C:\\Users\\iam\\.conda\\envs\\alyce\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\", line 990, in func_graph_from_py_func\r\n",
      "    func_outputs = python_func(*func_args, **func_kwargs)\r\n",
      "  File \"C:\\Users\\iam\\.conda\\envs\\alyce\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 634, in wrapped_fn\r\n",
      "    out = weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n",
      "  File \"C:\\Users\\iam\\.conda\\envs\\alyce\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\", line 977, in wrapper\r\n",
      "    raise e.ag_error_metadata.to_exception(e)\r\n",
      "ValueError: in user code:\r\n",
      "    .\\tests\\test_tf_reshape.py:27 tf_function_wrapped_function  *\r\n",
      "        reshaped = tf.reshape(tf.ones((64, 64)), -1)\r\n",
      "    C:\\Users\\iam\\.conda\\envs\\alyce\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper  **\r\n",
      "        return target(*args, **kwargs)\r\n",
      "    C:\\Users\\iam\\.conda\\envs\\alyce\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:195 reshape\r\n",
      "        result = gen_array_ops.reshape(tensor, shape, name)\r\n",
      "    C:\\Users\\iam\\.conda\\envs\\alyce\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py:8376 reshape\r\n",
      "        _, _, _op, _outputs = _op_def_library._apply_op_helper(\r\n",
      "    C:\\Users\\iam\\.conda\\envs\\alyce\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:748 _apply_op_helper\r\n",
      "        op = g._create_op_internal(op_type_name, inputs, dtypes=None,\r\n",
      "    C:\\Users\\iam\\.conda\\envs\\alyce\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:590 _create_op_internal\r\n",
      "        return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\r\n",
      "    C:\\Users\\iam\\.conda\\envs\\alyce\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:3528 _create_op_internal\r\n",
      "        ret = Operation(\r\n",
      "    C:\\Users\\iam\\.conda\\envs\\alyce\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:2015 __init__\r\n",
      "        self._c_op = _create_c_op(self._graph, node_def, inputs,\r\n",
      "    C:\\Users\\iam\\.conda\\envs\\alyce\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1856 _create_c_op\r\n",
      "        raise ValueError(str(e))\r\n",
      "\r\n",
      "    ValueError: Shape must be rank 1 but is rank 0 for '{{node Reshape}} = Reshape[T=DT_FLOAT, Tshape=DT_INT32](ones, Reshape/shape)' with input shapes: [64,64], [].\r\n",
      "```\r\n",
      "**Findings**\r\n",
      "\r\n",
      "1. This example raises `ValueError`\r\n",
      "```python\r\n",
      "@tf.function\r\n",
      "def tf_function_wrapped_function():\r\n",
      "  reshaped = tf.reshape(tf.ones((64, 64)), 64*64)\r\n",
      "```\r\n",
      "\r\n",
      "2. This example do not raise any error\r\n",
      "```python\r\n",
      "@tf.function\r\n",
      "def tf_function_wrapped_function():\r\n",
      "  reshaped = tf.reshape(tf.ones((64, 64)), [1, 64*64])\r\n",
      "```\r\n",
      "\r\n",
      "3. This example raises `ValueError`\r\n",
      "```python\r\n",
      "@tf.function\r\n",
      "def tf_function_wrapped_function():\r\n",
      "  reshaped = tf.reshape(tf.ones((64, 64, 64)), 64*64)\r\n",
      "```\r\n",
      "\r\n",
      "4. This example do not raise any error\r\n",
      " ```python\r\n",
      "def tf_function_wrapped_function():\r\n",
      "  reshaped = tf.reshape(tf.ones((64, 64)), -1)\r\n",
      "```\r\n",
      "\r\n",
      "5. This is an alternative way that I passed this ValueError\r\n",
      " ```python\r\n",
      "@tf.function\r\n",
      "def tf_function_wrapped_function():\r\n",
      "  reshaped = tf.reshape(tf.ones((64, 64)), (1, 64*64))[0]\r\n",
      "```\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:autograph\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  hexagon\n",
      "issue body -  This template is for miscellaneous issues not covered by the other issue categories.\r\n",
      "\r\n",
      "For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n",
      "\r\n",
      "If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n",
      "\r\n",
      "For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n",
      "\n",
      "issue labels - \n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  Switch to single-pip-package and install nvprune\n",
      "issue body -  - nvprune is now required for building TF\n",
      "- since 'tensorflow' is now the GPU package, I switched the -gpu images to install 'tensorflow' and the non-gpu images to install 'tensorflow-cpu'.\n",
      "issue labels - \n",
      "cla: yes\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  Time series tutorial throws error when selecting multiple columns\n",
      "issue body -  **Tensorflow version is 2.4.0**.\r\n",
      "\r\n",
      "## URL(s) with the issue:\r\n",
      "https://www.tensorflow.org/tutorials/structured_data/time_series\r\n",
      "\r\n",
      "Issue is also listed as a [StackOverflow question](https://stackoverflow.com/questions/65944671/how-to-select-multiple-label-columns-for-the-tensorflow-time-series-tutorial).\r\n",
      "\r\n",
      "\r\n",
      "## Description of issue (what needs changing):\r\n",
      "The current time series tutorial only works when specifying one label, or leaving `label_columns=None` which defaults to all columns as labels. When selecting multiple columns in the WindowGenerator, such as:\r\n",
      "```python\r\n",
      "OUT_STEPS = 24\r\n",
      "multi_window = WindowGenerator(input_width=24,\r\n",
      "                               label_width=OUT_STEPS,\r\n",
      "                               shift=OUT_STEPS,label_columns=['T (degC)','p (mbar)'])\r\n",
      "```\r\n",
      "\r\n",
      "the tutorial throws the error:\r\n",
      "```\r\n",
      "ValueError: Dimensions must be equal, but are 19 and 2 for '{{node mean_squared_error/SquaredDifference}} = SquaredDifference[T=DT_FLOAT](feed_back/transpose, IteratorGetNext:1)' with input shapes: [?,24,19], [?,24,2].\r\n",
      "```\r\n",
      "### Clear description\r\n",
      "\r\n",
      "As someone who is new to the framework, it is not clear how to adapt the tutorial to address an arbitrary subset of columns as labels, given the error provided.\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:keras\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Using MaxPooling1D with 4D and higher Tensors\n",
      "issue body -  <em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n",
      "\r\n",
      "\r\n",
      "**System information**\r\n",
      "- TensorFlow version (you are using): 2.3.1\r\n",
      "- Are you willing to contribute it (Yes/No): Not sure?\r\n",
      "\r\n",
      "**Describe the feature and the current behavior/state.**\r\n",
      "\r\n",
      "Currently, MaxPooling1D layer only takes 3D tensor as input, and data_format argument specifies if features are first or last.\r\n",
      "If tensor is 4D, an error gets thrown. However, the same API can be used to handle higher-dimensional data, e. 4D tensors, and there are use-cases for that. For example, when trying to implement multiple instance learning, data is organized in bags of instances, and for time series would become 4D.\r\n",
      "\r\n",
      "```\r\n",
      "n = 6\r\n",
      "sample_size = 300\r\n",
      "code_size = 50\r\n",
      "learning_rate = 0.001\r\n",
      "n_bags= None\r\n",
      "\r\n",
      "# autoencoder: n_bags X n_instances_in_bag X n_samples (timesteps) X n_measurements\r\n",
      "input_window = Input(shape=(n_bags,sample_size, n)) \r\n",
      "x = Conv1D(filters=40, kernel_size=21, activation='relu', padding='valid')(input_window)\r\n",
      "x = MaxPooling1D(pool_size=2)(x)\r\n",
      "```\r\n",
      "\r\n",
      "Currently, this throws an error:\r\n",
      "```\r\n",
      "ValueError: Input 0 of layer max_pooling1d_4 is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: [None, None, 280, 40]\r\n",
      "```\r\n",
      "\r\n",
      "but there is no reason why this can't work. Note that Conv1D layer works just fine with this.\r\n",
      "\r\n",
      "**Will this change the current api? How?**\r\n",
      "I believe the API can remain the same\r\n",
      "\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "comp:keras\n",
      "stat:awaiting tensorflower\n",
      "type:feature\n",
      "\n",
      "\n",
      "issue title -  Fix the 9KB binary size increase.\n",
      "issue body -  https://github.com/tensorflow/tensorflow/commit/9ee7896d229278f582a3a381c6d22ec0559d9765 added a function that was unused for the TFLM build yet still contributed to binary size increase.\r\n",
      "\r\n",
      "This change moves that function inside #ifndef TF_LITE_STATIC_MEMORY and so avoids the size increase.\r\n",
      "\r\n",
      "Manually tested with the following commands:\r\n",
      "```\r\n",
      "make -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=fusion_f1 XTENSA_CORE=F1_190305_swupgrade keyword_benchmark -j8 BUILD_TYPE=release\r\n",
      "xt-size tensorflow/lite/micro/tools/make/gen/xtensa_fusion_f1_release/bin/keyword_benchmark\r\n",
      "```\r\n",
      "\r\n",
      "Before this change:\r\n",
      "```\r\n",
      "   text\t   data\t    bss\t    dec\r\n",
      "  67192\t  44512\t  24968\t 136672\r\n",
      "```\r\n",
      "\r\n",
      "After this change:\r\n",
      "```\r\n",
      "   text\t   data\t    bss\t    dec\r\n",
      "  62184\t  40180\t  24872\t 127236\r\n",
      "```\r\n",
      "\r\n",
      "Fixes http://b/178731766\r\n",
      "\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  PoseNet model readme points to the wrong paper reference\n",
      "issue body -  ## URL(s) with the issue:\r\n",
      "\r\n",
      "Please provide a link to the documentation entry, for example:\r\n",
      "\r\n",
      "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/pose_estimation/overview.md\r\n",
      "or\r\n",
      "https://www.tensorflow.org/lite/models/pose_estimation/overview\r\n",
      "\r\n",
      "## Description of issue (what needs changing):\r\n",
      "In Further Reading the reference is pointing to the wrong PoseNet paper. It points to\r\n",
      "https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Kendall_PoseNet_A_Convolutional_ICCV_2015_paper.pdf\r\n",
      "which is also called PoseNet but it is about camera relocalization, not body pose detection.\r\n",
      "\r\n",
      "### Correct links\r\n",
      "\r\n",
      "According to this blog post\r\n",
      "https://medium.com/tensorflow/real-time-human-pose-estimation-in-the-browser-with-tensorflow-js-7dd0bc881cd5\r\n",
      "I think the correct papers are here:\r\n",
      "https://arxiv.org/abs/1701.01779\r\n",
      "and\r\n",
      "https://arxiv.org/abs/1803.08225\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "comp:lite\n",
      "comp:lite-examples\n",
      "type:docs-bug\n",
      "\n",
      "\n",
      "issue title -  Fix the hifimini build.\n",
      "issue body -  https://github.com/tensorflow/tensorflow/pull/46712 broke the hifimini build and is the reason why the Xtensa build badge is currently red.\r\n",
      "\r\n",
      "This change fixes the hifimini build.\r\n",
      "\r\n",
      "Manually tested that all the tests pass with:\r\n",
      "```\r\n",
      "make -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=hifimini XTENSA_CORE=mini1m1m_RG test -j8\r\n",
      "```\r\n",
      "\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  numpy() is not working on Tensor object in TF2.x\n",
      "issue body -  **Please consider the following code snippet**\r\n",
      "\r\n",
      "from tensorflow.keras.layers import Layer\r\n",
      "\r\n",
      "class SimpleDense(Layer):\r\n",
      "    \r\n",
      "    def __init__(self, units=32):\r\n",
      "        '''Initializes the instance attributes'''\r\n",
      "        super(SimpleDense, self).__init__()\r\n",
      "        self.units = units\r\n",
      "\r\n",
      "    def build(self, input_shape):\r\n",
      "        '''Create the state of the layer (weights)'''\r\n",
      "        # initialize the weights\r\n",
      "        w_init = tf.random_normal_initializer()\r\n",
      "        self.w = tf.Variable(name=\"kernel\",\r\n",
      "            initial_value=w_init(shape=(input_shape[-1], self.units),\r\n",
      "                                 dtype='float32'),\r\n",
      "            trainable=True)\r\n",
      "\r\n",
      "        # initialize the biases\r\n",
      "        b_init = tf.zeros_initializer()\r\n",
      "        self.b = tf.Variable(name=\"bias\",\r\n",
      "            initial_value=b_init(shape=(self.units,), dtype='float32'),\r\n",
      "            trainable=True)\r\n",
      "\r\n",
      "    def call(self, inputs):\r\n",
      "        '''Defines the computation from inputs to outputs'''\r\n",
      "        print(inputs.numpy().shape)\r\n",
      "        print(self.w.numpy().shape)\r\n",
      "        print(tf.matmul(inputs,self.w).numpy().shape)\r\n",
      "        print(self.b.numpy().shape)\r\n",
      "        print((tf.matmul(inputs,self.w)+self.b).numpy().shape)\r\n",
      "        return tf.matmul(inputs, self.w) + self.b\r\n",
      "\r\n",
      "my_dense = SimpleDense(units=1)\r\n",
      "x = tf.ones((3, 2))\r\n",
      "y = my_dense(x)   # I am able to execute all the print of the call method.\r\n",
      "\r\n",
      "xs = np.array([-1.0,  0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\r\n",
      "ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float)\r\n",
      "\r\n",
      "my_layer = SimpleDense(units=1)\r\n",
      "model = tf.keras.Sequential([my_layer])\r\n",
      "model.compile(optimizer='sgd', loss='mean_squared_error')\r\n",
      "model.fit(xs, ys, epochs=500,verbose=0)  # I am not able to execute any of the print of the call method.\r\n",
      "\r\n",
      "- There error is following.\r\n",
      "AttributeError: in converted code:\r\n",
      "        print(inputs.numpy().shape)\r\n",
      "\r\n",
      "    AttributeError: 'Tensor' object has no attribute 'numpy'\r\n",
      "\r\n",
      "_**I am not able to understand the above error as per my knowledge Tensor object has numpy() attribute available in tf2.x version and I am using TF2.0**_\r\n",
      "\r\n",
      "Please Explain why i am getting the above error. It will be very helpful.\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "comp:keras\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  MHA not attending the specified dimension\n",
      "issue body -  Hi, it looks like if I add a channel dimension, the number of parameters in the Dense layer is no longer correct:\r\n",
      "This is ok:\r\n",
      "\r\n",
      "inputs = Input(shape=[1800])\r\n",
      "y = MultiHeadAttention(num_heads=10,\r\n",
      "key_dim=10,\r\n",
      "use_bias=False,\r\n",
      "attention_axes = 0,\r\n",
      ")(inputs,inputs)\r\n",
      "model = Model(inputs=inputs, outputs=y)\r\n",
      "print(model.summary())  # This shows that we have 720,000  parameters\r\n",
      "This is not:\r\n",
      "\r\n",
      "inputs = Input(shape=[1800,1]). # added channel dim here\r\n",
      "y = MultiHeadAttention(num_heads=10,\r\n",
      "key_dim=10,\r\n",
      "use_bias=False,\r\n",
      "attention_axes = 0,  #if this is set to None the same error persists\r\n",
      ")(inputs,inputs)\r\n",
      "model = Model(inputs=inputs, outputs=y)\r\n",
      "print(model.summary()) # this is not correct, it shows 400 parameters, although I specify to attend the first axis\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:keras\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  [tf.data] eager mode support for experimental benchmarks part1\n",
      "issue body -  This PR is a follow up of https://github.com/tensorflow/tensorflow/pull/46320 and extends the eager mode support to a subset of experimental benchmarks.\r\n",
      "\r\n",
      "Sample result:\r\n",
      "\r\n",
      "```console\r\n",
      "entry {\r\n",
      "  name: \"CsvDatasetBenchmark.csv_strings_map_decode_csv_with_cols_256.eager\"\r\n",
      "  iters: 10\r\n",
      "  wall_time: 0.0020652782201766966\r\n",
      "  extras {\r\n",
      "    key: \"num_elements\"\r\n",
      "    value {\r\n",
      "      double_value: 5000.0\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "\r\n",
      "#####\r\n",
      "\r\n",
      "entry {\r\n",
      "  name: \"OptimizationBenchmark.map_fusion_noopt_chain_length_50.eager\"\r\n",
      "  iters: 10\r\n",
      "  wall_time: 2.3424625396728516e-05\r\n",
      "  extras {\r\n",
      "    key: \"num_elements\"\r\n",
      "    value {\r\n",
      "      double_value: 100.0\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "\r\n",
      "entry {\r\n",
      "  name: \"OptimizationBenchmark.map_fusion_opt_chain_length_50.eager\"\r\n",
      "  iters: 10\r\n",
      "  wall_time: 5.298852920532227e-06\r\n",
      "  extras {\r\n",
      "    key: \"num_elements\"\r\n",
      "    value {\r\n",
      "      double_value: 100.0\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "\r\n",
      "```\r\n",
      "\r\n",
      "cc: @jsimsa \n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:data\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  Multiheadattention layer dimensions\n",
      "issue body -  ## This works ok\r\n",
      "inputs = Input(shape=[1800])\r\n",
      "y = MultiHeadAttention(num_heads=10,\r\n",
      "key_dim=10,\r\n",
      "use_bias=False,\r\n",
      "attention_axes = 0,\r\n",
      ")(inputs,inputs)\r\n",
      "model = Model(inputs=inputs, outputs=y)\r\n",
      "print(model.summary())\r\n",
      "\r\n",
      "## This is not working properly:\r\n",
      "\r\n",
      "inputs = Input(shape=[1800,1]). # added channel dim here\r\n",
      "y = MultiHeadAttention(num_heads=10,\r\n",
      "key_dim=10,\r\n",
      "use_bias=False,\r\n",
      "attention_axes = 0,\r\n",
      ")(inputs,inputs)\r\n",
      "model = Model(inputs=inputs, outputs=y)\r\n",
      "print(model.summary())\n",
      "issue labels - \n",
      "comp:keras\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  \"minlength\" attribute for tf.math.bincount not honored during run_eagerly=False\n",
      "issue body -  -Microsoft Windows 10 version 10.0.18363 Build 18363\r\n",
      "-pip install tensorflow\r\n",
      "-TensorFlow v2.4.0-49-g85c8b2a817f 2.4.1\r\n",
      "-Python 3.7.6\r\n",
      "\r\n",
      "I am doing a Monte Carlo simulation inside my custom metric to determine the probabilities over 10k samples. I have used the minlength and maxlength attributes of the `tf.math.bincount` such that the size of the tensor `result` should be same as tensor `mus`\r\n",
      "Upon using `tf.math.bincount` in a custom metric like below,\r\n",
      "\r\n",
      "```\r\n",
      "    distr = tfp.distributions.Normal(loc=mus, scale=sigmas)\r\n",
      "    samples_min = tf.math.argmin(distr.sample((10000,)), axis=1)\r\n",
      "    result = tf.math.bincount(tf.cast(tf.reshape(samples_min, (10000,)), dtype=tf.int32), minlength=mus.shape[0], maxlength=mus.shape[0], dtype=tf.float32) / tf.constant(10000, dtype=tf.float32)\r\n",
      "\r\n",
      "    filter1 = tf.where(tf.squeeze(y_public_estimate) < result, tf.ones_like(result), tf.zeros_like(result))\r\n",
      "\r\n",
      "```\r\n",
      "Now if I run the above code with `model.compile(run_eagerly=False...` it throws below error,  and so I tried to debug the issue with `model.compile(run_eagerly=True...` and it doesn't produce any error ever. \r\n",
      "\r\n",
      "I suspect the `InvalidArgumentError` is generated in the less-than operation between ` tf.squeeze(y_public_estimate)` and `result `  where their shapes are mismatched, but it only happens when `run_eagerly=False` not when `run_eagerly=True` and btw the length of the tensor `y_public_estimate` is same as tensor `mus`, so I am thinking, maybe the `minlength` or `maxlength` attribute in `tf.math.bincount` does not seem to work with `run_eagerly=False`\r\n",
      "\r\n",
      "```\r\n",
      "InvalidArgumentError                      Traceback (most recent call last)\r\n",
      "<ipython-input-99-3eba1530fe17> in <module>\r\n",
      "     10 \r\n",
      "     11 model = build_model(continuous, categoricals_map)\r\n",
      "---> 12 history = model.fit(train_generator, validation_data=valid_generator, epochs=900, callbacks=[terminate_onnan, check_pointer, early_stop], verbose=1, workers=NUM_CORES)\r\n",
      "\r\n",
      "e:\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n",
      "   1098                 _r=1):\r\n",
      "   1099               callbacks.on_train_batch_begin(step)\r\n",
      "-> 1100               tmp_logs = self.train_function(iterator)\r\n",
      "   1101               if data_handler.should_sync:\r\n",
      "   1102                 context.async_wait()\r\n",
      "\r\n",
      "e:\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py in __call__(self, *args, **kwds)\r\n",
      "    826     tracing_count = self.experimental_get_tracing_count()\r\n",
      "    827     with trace.Trace(self._name) as tm:\r\n",
      "--> 828       result = self._call(*args, **kwds)\r\n",
      "    829       compiler = \"xla\" if self._experimental_compile else \"nonXla\"\r\n",
      "    830       new_tracing_count = self.experimental_get_tracing_count()\r\n",
      "\r\n",
      "e:\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py in _call(self, *args, **kwds)\r\n",
      "    853       # In this case we have created variables on the first call, so we run the\r\n",
      "    854       # defunned version which is guaranteed to never create variables.\r\n",
      "--> 855       return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\r\n",
      "    856     elif self._stateful_fn is not None:\r\n",
      "    857       # Release the lock early so that multiple threads can perform the call\r\n",
      "\r\n",
      "e:\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py in __call__(self, *args, **kwargs)\r\n",
      "   2941        filtered_flat_args) = self._maybe_define_function(args, kwargs)\r\n",
      "   2942     return graph_function._call_flat(\r\n",
      "-> 2943         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\r\n",
      "   2944 \r\n",
      "   2945   @property\r\n",
      "\r\n",
      "e:\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n",
      "   1917       # No tape is watching; skip to running the function.\r\n",
      "   1918       return self._build_call_outputs(self._inference_function.call(\r\n",
      "-> 1919           ctx, args, cancellation_manager=cancellation_manager))\r\n",
      "   1920     forward_backward = self._select_forward_and_backward_functions(\r\n",
      "   1921         args,\r\n",
      "\r\n",
      "e:\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py in call(self, ctx, args, cancellation_manager)\r\n",
      "    558               inputs=args,\r\n",
      "    559               attrs=attrs,\r\n",
      "--> 560               ctx=ctx)\r\n",
      "    561         else:\r\n",
      "    562           outputs = execute.execute_with_cancellation(\r\n",
      "\r\n",
      "e:\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n",
      "     58     ctx.ensure_initialized()\r\n",
      "     59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n",
      "---> 60                                         inputs, attrs, num_outputs)\r\n",
      "     61   except core._NotOkStatusException as e:\r\n",
      "     62     if name is not None:\r\n",
      "\r\n",
      "InvalidArgumentError:  Incompatible shapes: [18] vs. [17]\r\n",
      "\t [[node Less (defined at <ipython-input-95-93a59f364a63>:39) ]] [Op:__inference_train_function_130872]\r\n",
      "\r\n",
      "Errors may have originated from an input operation.\r\n",
      "Input Source operations connected to node Less:\r\n",
      " truediv (defined at <ipython-input-95-93a59f364a63>:37)\r\n",
      "\r\n",
      "Function call stack:\r\n",
      "train_function\r\n",
      "```\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:ops\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  update\n",
      "issue body -  \n",
      "issue labels - \n",
      "cla: no\n",
      "invalid\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  [TFL] Fix BatchMatMul constant RHS.\n",
      "issue body -  Fixes #46724. For constant RHS and !adj_y, transpose of RHS only occurs once, so the allocation should be persistent across runs. Old commit does this, but the allocation is accidentally overridden to kTfLiteArenaRw for constant RHS.\r\n",
      "\r\n",
      "https://github.com/tensorflow/tensorflow/blob/7c0a3f12dd00b0aeb9d2a91d157b0e9d1a4c1250/tensorflow/lite/kernels/batch_matmul.cc#L193-L199\r\n",
      "\r\n",
      "/cc @abattery for visibility.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:lite\n",
      "prtype:bugfix\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  Improve rendering for tf.distribute.cluster_resolver.TPUClusterResolver in tpu_strategy API docs\n",
      "issue body -  This should fix the auto-linking and rendering\r\n",
      "\r\n",
      "<img width=\"500\" alt=\"image\" src=\"https://user-images.githubusercontent.com/19637339/106074345-9a82ed80-6103-11eb-9074-0c5433eba08d.png\">\r\n",
      "\n",
      "issue labels - \n",
      "cla: yes\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Add in optimizations for softmax for Fusion F1.\n",
      "issue body -  Confirmed that the test passes with:\r\n",
      "```\r\n",
      "make -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=fusion_f1 XTENSA_CORE=F1_190305_swupgrade test_kernel_softmax_test -j8\r\n",
      "```\r\n",
      "\r\n",
      "However, the latency improvement is only ~1000 ticks, as tested with:\r\n",
      "```\r\n",
      "make -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=fusion_f1 XTENSA_CORE=F1_190305_swupgrade run_keyword_benchmark -j8\r\n",
      "```\r\n",
      "\r\n",
      "Since Softmax is currently a small fraction of the overall keyword_benchmark latency we will focus on the latency of only this particular OP.\r\n",
      "\r\n",
      "With the optimized implementation:\r\n",
      "```\r\n",
      "SOFTMAX took 749 ticks (0 ms).\r\n",
      "```\r\n",
      "\r\n",
      "Reference implementation:\r\n",
      "```\r\n",
      "SOFTMAX took 2052 ticks (2 ms).\r\n",
      "```\r\n",
      "\r\n",
      "And with the LUT hifimini implementation (for completeness):\r\n",
      "```\r\n",
      "SOFTMAX took 1142 ticks (1 ms).\r\n",
      "```\r\n",
      "\r\n",
      "The gain of ~1500 ticks ticks is worth merging because after all the optimizations (e.g.  #47098), this will mean a ~5% improvement for the keyword benchmark.\r\n",
      "\r\n",
      "And the benefits might be more significant for other models too.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  Instructions to add git hook for code-style checks.\n",
      "issue body -  Installing such a hook should enable fixing formatting and other style issues faster.\r\n",
      "\r\n",
      "Also, updated the pigweed patch to have the format_code error message give a command that can be copy-pasted into the terminal to fix the formatting errors.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  Fix crash when tf.sequence_mask takes a non-integer lengths\n",
      "issue body -  This PR tries to address the issue raised in #46698 where\r\n",
      "tf.sequence_mask will crash abruptly if lengths is not passed\r\n",
      "with an integer tensor.\r\n",
      "\r\n",
      "This PR applies a dtype check and throw out ValueError to avoid\r\n",
      "program crash.\r\n",
      "\r\n",
      "This PR fixes #46698.\r\n",
      "\r\n",
      "Signed-off-by: Yong Tang <yong.tang.github@outlook.com>\n",
      "issue labels - \n",
      "cla: yes\n",
      "prtype:bugfix\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  Fix crash when invalid keepdims value is being passed to tf.math.reduce_prod\n",
      "issue body -  This PR tries to address the issue raised in #46700 where\r\n",
      "tf.math.reduce_prod will crash if keepdims is being passed\r\n",
      "with a non-boolean value (e.g. numpy value)\r\n",
      "\r\n",
      "The issue was that keepdims is passed through pywrap\r\n",
      "which can not interprete numpy values, thus crashes.\r\n",
      "\r\n",
      "A way to detect the type mismatch before being passed\r\n",
      "to pywrap is to use `bool(keepdims)` to give python a chance\r\n",
      "to convert to bool (and throw out error when appropriate).\r\n",
      "\r\n",
      "This PR also fixes all reduce_ ops.\r\n",
      "\r\n",
      "This PR fixes #46700.\r\n",
      "\r\n",
      "Signed-off-by: Yong Tang <yong.tang.github@outlook.com>\n",
      "issue labels - \n",
      "cla: yes\n",
      "prtype:bugfix\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  Difference between regular and fast nms in TFLite \n",
      "issue body -  What is the difference between regular and fast Non-Max-Suppression (NMS) in TFLite ? is there any documentation on that?\r\n",
      "I mean it terms of accuracy/performance. I understand there is a difference in the time complexity, but why do they yield different results? \r\n",
      "\r\n",
      "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/detection_postprocess.cc\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "comp:lite\n",
      "stat:awaiting tensorflower\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  ImportError: DLL load failed while importing _pywrap_tensorflow_internal:\n",
      "issue body -  <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n",
      "- TensorFlow installed from (source or binary): \r\n",
      "- TensorFlow version: 2.4.1\r\n",
      "- Python version:3.8\r\n",
      "- Installed using virtualenv? pip? conda?:pip\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version:\r\n",
      "- GPU model and memory:\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "I am not able to install tensorflow using pip on anaconda (& sypder)\r\n",
      "It has worked on my other Computer but not on the Windows10, i have uninstalled and reinstalled everything several times, but it just wont work. I can install other libraries, but not tensorflow - i do not understand why.\r\n",
      "\r\n",
      "I know this post is a duplicate, but the answers in previous posts didnt help me since i am new to coding and everything....Sorry for bothering though\r\n",
      "And no i do not want to use google collab, but install & use tensorflow on my computer\r\n",
      "\r\n",
      "I would be very happy if someone could help me!!\r\n",
      "\r\n",
      "\r\n",
      "**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n",
      "ImportError: Traceback (most recent call last):\r\n",
      "  File \"C:\\Users\\go-pa\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n",
      "    from tensorflow.python._pywrap_tensorflow_internal import *\r\n",
      "ImportError: DLL load failed while importing _pywrap_tensorflow_internal: Das angegebene Modul wurde nicht gefunden.\r\n",
      "\r\n",
      "\r\n",
      "Failed to load the native TensorFlow runtime.\r\n",
      "\r\n",
      "See https://www.tensorflow.org/install/errors\r\n",
      "\r\n",
      "for some common reasons and solutions.  Include the entire stack trace\r\n",
      "above this error message when asking for help.\r\n",
      "\r\n",
      "**Any other info / logs**\r\n",
      "Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "stalled\n",
      "stat:awaiting response\n",
      "subtype:windows\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  bqml kmeans -> tflite: Call_once op doesn't support multiple subgraphs with inputs\n",
      "issue body -  Filing on request of tflite group\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 & macOS 11.0.1\r\n",
      "- TensorFlow installed from (source or binary): pip tf-nightly \r\n",
      "- TensorFlow version (use command below): 2.5.0-dev20210123 (git: v1.12.1-49539-g18d8bcbe72b)\r\n",
      "- Python version: 3.7\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "Export saved model from bqml to GCS. Load saved model and test:\r\n",
      "![Screen Shot 2021-01-26 at 12 21 34 PM](https://user-images.githubusercontent.com/5230786/106028324-ccf80080-6099-11eb-9e45-a24188eee451.png)\r\n",
      "\r\n",
      "create tflite model using from_saved_model:\r\n",
      "![Screen Shot 2021-01-26 at 12 26 28 PM](https://user-images.githubusercontent.com/5230786/106028363-da14ef80-6099-11eb-8aa4-d7a2c13c848b.png)\r\n",
      "\r\n",
      "load tflite model with interpreter to test:\r\n",
      "![Screen Shot 2021-01-26 at 12 29 29 PM](https://user-images.githubusercontent.com/5230786/106028401-e39e5780-6099-11eb-93a4-3c229194501a.png)\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "Load tflite model and make predictions using interpreter to validate successful conversion\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "toy saved model from bqml k-means, notebook to repro, and tflite model can be found here: https://drive.google.com/drive/folders/1HfAU7ZK6CHvRFl_hSfw42RzlAfvXS-PJ?usp=sharing\r\n",
      "\n",
      "issue labels - \n",
      "TFLiteConverter\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  RaggedTensor processing float type  causes memory leak\n",
      "issue body -  <em>Please make sure that this is an issue related to performance of TensorFlow.\r\n",
      "As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:performance_template</em>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**System information**\r\n",
      "- TF 1.15.3\r\n",
      "- Linux\r\n",
      "- python 3.7\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "text_input_flatten is a 3D tensor, [batch_size, seq_length, embedding_size], dtype=tf.float32\r\n",
      "text_mask is a 2D tensor, [batch_size, seq_length], dtype=tf.bool\r\n",
      "\r\n",
      "text_input_ragged = tf.RaggedTensor.from_tensor(text_input_flatten)\r\n",
      "text_input_ragged = tf.ragged.boolean_mask(text_input_ragged, text_mask, name=\"ragged_text_input\")\r\n",
      "new_text_input = text_input_ragged.to_tensor(default_value=0)\r\n",
      "new_length = tf.shape(new_text_input)[1]\r\n",
      "new_text_input = tf.pad(new_text_input, [[0, 0], [0, seq_length * emb_size - new_length], [0, 0]], constant_values=0)\r\n",
      "\r\n",
      "the memory usage is increasing rapidly, finally, the program will crash.\r\n",
      "When I changed the text_input_flatten dtype to tf.int32, it works normally.\r\n",
      "\r\n",
      "![memory](https://user-images.githubusercontent.com/8108725/106013911-e7f95d80-60f7-11eb-97e5-b4fe31c38bfc.png)\r\n",
      "**Describe the expected behavior**\n",
      "issue labels - \n",
      "TF 1.15\n",
      "comp:ops\n",
      "type:performance\n",
      "\n",
      "\n",
      "issue title -  How to create representative_dataset with multiple inputs on TensorFlow Lite Converter?\n",
      "issue body -  I cannot find any examples online, if I was to simulate data would it look like this?\r\n",
      "\r\n",
      "```\r\n",
      "def representative_dataset():\r\n",
      "    for _ in range(100):\r\n",
      "      data1, data2 = np.random.rand(1, 256), np.random.rand(1, 50)\r\n",
      "      yield [data1.astype(np.float32), data2.astype(np.float32)]\r\n",
      "```\n",
      "issue labels - \n",
      "TFLiteConverter\n",
      "stat:awaiting tensorflower\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  cleanup `c/experimental/gradients/` part 4\n",
      "issue body -  @saxenasaurabh I have a lot of things that I would like to discuss with you, please take a look at it. Thank you ! 😄 \r\n",
      "\r\n",
      "---\r\n",
      "\r\n",
      "\r\n",
      "8f2852f: IMO, `mnist_grad_test` doesn't do anything really interesting. We have \r\n",
      "- 1 grad test for `MatMul` ( added in `math_grad_test`).\r\n",
      "- 2 forward tests which test `ops` and aren't related to gradients.\r\n",
      "- An another test for `ops` ( `ScalarMul` ).\r\n",
      "- 1 `MNIST` training test but it only checks for the `status`.\r\n",
      "\r\n",
      "So I removed it. I also removed `mnist_gradients_testutil` and `gradients_util` which are no longer used.\r\n",
      "\r\n",
      "---\r\n",
      "\r\n",
      "20db3b8: I am trying to add templates for `TestTensorHandleWithDims` and `TestScalarTensorHandle`. I think it won't break anything because these headers are used for testing only.\r\n",
      "\r\n",
      "---\r\n",
      "\r\n",
      "I am working on a bare minimum infrastructure for getting model trained fully in C++ ( optimizer, loss, metrics, ... ). Currently, I follow the interface of `model` in `keras`. You could see it here: https://github.com/vnvo2409/tensorflow/commit/9e42f7ea613c73b303829bfc833bc81d474d9218. With this infrastructure, `MNIST` achieves 96% accuracy ( with a real dataset downloaded from http://yann.lecun.com/exdb/mnist/ ) in `mnist_grad_test`. Hopefully we could use it for `resnet` soon. Below are the problems I have when building it.\r\n",
      "\r\n",
      "---\r\n",
      "\r\n",
      "How to pass `DataType` to `ops` ( e.g https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/cast#summary ). We might need it in the future. For an example, `MNIST` metric is essentially `tf.mean(tf.cast(tf.equal()))`\r\n",
      "\r\n",
      "---\r\n",
      "\r\n",
      "Should we rewrite the `ops` to something like \r\n",
      "```cpp\r\n",
      "Status Equal(AbstractContext* ctx, AbstractTensorHandle* const x,\r\n",
      "             AbstractTensorHandle* const y,\r\n",
      "             absl::Span<AbstractTensorHandle*> outputs, const char* name,\r\n",
      "             bool incompatible_shape_error = false);\r\n",
      "```\r\n",
      "and getting rid of this pattern\r\n",
      "```cpp\r\n",
      "Status Log1p(AbstractContext* ctx,\r\n",
      "             absl::Span<AbstractTensorHandle* const> inputs,\r\n",
      "             absl::Span<AbstractTensorHandle*> outputs, const char* name);\r\n",
      "```\r\n",
      "\r\n",
      "IMO, the former pattern is clearer and could help us avoid some confusion. An example, consider [`ops::Concat`](https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/concat#summary) ( because it both has `InputList` and `Input` ). \r\n",
      "\r\n",
      "---\r\n",
      "\r\n",
      "When I used `AddInputList` for adding input to `ops::Concat`, it threw https://github.com/tensorflow/tensorflow/blob/c5012222b1d9accc07ea8fa4140c66039883ba2b/tensorflow/core/common_runtime/eager/eager_operation.cc#L403\r\n",
      "\r\n",
      "`AddInputList` for `AddN` is working fine though.\r\n",
      "\r\n",
      "---\r\n",
      "\r\n",
      "[`ops::ApplyGradientDescent`](https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/apply-gradient-descent#summary) requires a `TF_FLOAT_REF` instead of `TF_FLOAT`. How to use it ?\r\n",
      "\r\n",
      "---\r\n",
      "\r\n",
      "Memory leaks: These code will leak because we lost access to the output of `MatMul` and `Relu` will replace it with another tensor without calling `Unref`. Is there an intuitive solution for this problem ? Currently, we could wrap it inside a `AbstractTensorHandlePtr` or save the pointer manually before passing it around but both ways require manual effort.\r\n",
      "\r\n",
      "https://github.com/tensorflow/tensorflow/blob/c5012222b1d9accc07ea8fa4140c66039883ba2b/tensorflow/c/eager/mnist_gradients_testutil.cc#L120-L129\r\n",
      "\r\n",
      "---\r\n",
      "\r\n",
      "Thank you for your time once again ! 😄 \n",
      "issue labels - \n",
      "cla: yes\n",
      "ready to pull\n",
      "size:XL\n",
      "\n",
      "\n",
      "issue title -  Cash on tflite::Interpreter::AllocateTensors()\n",
      "issue body -  I'm trying to make an audio classifier as [android YAMNet](https://farmaker47.medium.com/classification-of-sounds-using-android-mobile-phone-and-the-yamnet-ml-model-539bc199540) with C++ language on JNI, however, I got the crash on tflite::Interpreter::AllocateTensors. And my tflite version maybe 2.3.\r\n",
      "\r\n",
      "Here is my code snippet\r\n",
      "```\r\n",
      "  bool YAMNet::Classify(const std::vector<float> &wave_data, const int sample_rate,\r\n",
      "                          const int top_k, std::vector<Recognition> &recognitions) {\r\n",
      "\r\n",
      "        static const int kDefaultSampleRate = 16000;// Hz\r\n",
      "        if (kDefaultSampleRate != sample_rate) {\r\n",
      "            Trace_Err(\"Error: YAMNet input must be 16kHz.\");\r\n",
      "            assert(false);\r\n",
      "            return false;\r\n",
      "        }\r\n",
      "\r\n",
      "        std::vector<int> input_tensor_indices;\r\n",
      "        input_tensor_indices = interpreter_ -> inputs();\r\n",
      "        interpreter_->ResizeInputTensor(input_tensor_indices[input_index_of_wave_],\r\n",
      "                                        {1, (int)wave_data.size()});\r\n",
      "\r\n",
      "        interpreter_->AllocateTensors();\r\n",
      "...\r\n",
      "```\r\n",
      "And then, I got the crash as following:\r\n",
      "```\r\n",
      "2021-01-27 18:15:36.478 20012-20012/com.tomato.ketchup A/libc: Fatal signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x28 in tid 20012 (.tomato.ketchup), pid 20012 (.tomato.ketchup)\r\n",
      "2021-01-27 18:15:36.839 20012-20055/com.tomato.ketchup E/ketchup: ###### CSeedUdp::ProcessSend sendto() failed, errno=101(Network is unreachable), fd=58\r\n",
      "2021-01-27 18:15:36.859 20105-20105/? A/DEBUG: *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***\r\n",
      "2021-01-27 18:15:36.859 20105-20105/? A/DEBUG: Build fingerprint: 'xiaomi/wayne/wayne:9/PKQ1.180904.001/V11.0.5.0.PDCCNXM:user/release-keys'\r\n",
      "2021-01-27 18:15:36.859 20105-20105/? A/DEBUG: Revision: '0'\r\n",
      "2021-01-27 18:15:36.859 20105-20105/? A/DEBUG: ABI: 'arm64'\r\n",
      "2021-01-27 18:15:36.859 20105-20105/? A/DEBUG: pid: 20012, tid: 20012, name: .tomato.ketchup  >>> com.tomato.ketchup <<<\r\n",
      "2021-01-27 18:15:36.859 20105-20105/? A/DEBUG: signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x28\r\n",
      "2021-01-27 18:15:36.859 20105-20105/? A/DEBUG: Cause: null pointer dereference\r\n",
      "2021-01-27 18:15:36.859 20105-20105/? A/DEBUG:     x0  0000000000000000  x1  0000000000000000  x2  0000007c4b800000  x3  0000000000000004\r\n",
      "2021-01-27 18:15:36.859 20105-20105/? A/DEBUG:     x4  0000000000000096  x5  0000007c633a3848  x6  00003e8000000001  x7  00003e8000000001\r\n",
      "2021-01-27 18:15:36.859 20105-20105/? A/DEBUG:     x8  0000000000000000  x9  0000007c3abfe358  x10 00000000000000d9  x11 0000007c4b896650\r\n",
      "2021-01-27 18:15:36.859 20105-20105/? A/DEBUG:     x12 0000000000000008  x13 0000000000000000  x14 0000000000000011  x15 0000000000000001\r\n",
      "2021-01-27 18:15:36.859 20105-20105/? A/DEBUG:     x16 0000007c4fa760f0  x17 0000007c4f973910  x18 00000000ffffffff  x19 0000000000000000\r\n",
      "2021-01-27 18:15:36.859 20105-20105/? A/DEBUG:     x20 0000000000000000  x21 0000007c4b86ff80  x22 0000007c4fa25993  x23 0000000000000000\r\n",
      "2021-01-27 18:15:36.859 20105-20105/? A/DEBUG:     x24 0000000000000000  x25 0000000000000000  x26 0000007cf1aaa5e0  x27 0000007c4fa71a08\r\n",
      "2021-01-27 18:15:36.859 20105-20105/? A/DEBUG:     x28 0000007c3abfe360  x29 0000007fdb66e470\r\n",
      "2021-01-27 18:15:36.859 20105-20105/? A/DEBUG:     sp  0000007fdb66e410  lr  0000007c4f974fc4  pc  0000007c4f975018\r\n",
      "2021-01-27 18:15:37.182 20105-20105/? A/DEBUG: backtrace:\r\n",
      "2021-01-27 18:15:37.182 20105-20105/? A/DEBUG:     #00 pc 0000000000235018  /data/app/com.tomato.ketchup-V1Z97Zn_3BrgXuDk3RSqeA==/base.apk (offset 0xedbb000) (tflite::Subgraph::ModifyGraphWithDelegate(TfLiteDelegate*)+252)\r\n",
      "2021-01-27 18:15:37.182 20105-20105/? A/DEBUG:     #01 pc 0000000000238b98  /data/app/com.tomato.ketchup-V1Z97Zn_3BrgXuDk3RSqeA==/base.apk (offset 0xedbb000) (tflite::Interpreter::AllocateTensors()+220)\r\n",
      "```\r\n",
      "\r\n",
      "Dose anyone have any idea on this?\n",
      "issue labels - \n",
      "TF 2.3\n",
      "comp:lite\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Reshape before Softmax leads runtime error:Unnecessary dynamic-sized tensors\n",
      "issue body -  ### 1. System information\r\n",
      "\r\n",
      "- OS: Win10 & Ubuntu 1804\r\n",
      "- TensorFlow installation: Pip\r\n",
      "- TensorFlow library: 2.4 & 2.4.1\r\n",
      "\r\n",
      "### 2. Code\r\n",
      "\r\n",
      "`\r\n",
      "import tensorflow as tf\r\n",
      "from tensorflow import keras\r\n",
      "from tensorflow.keras import layers\r\n",
      "\r\n",
      "def convert_to_lite(model,out_path,enable_selected_tf_ops=False):\r\n",
      "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n",
      "    out_model = converter.convert()\r\n",
      "    with open(out_path, \"wb\") as fp:\r\n",
      "        fp.write(out_model)\r\n",
      "\r\n",
      "def failed_on_android_gpu_for_dynamic_size():\r\n",
      "    ipt = layers.Input((256,64,3))\r\n",
      "    x = ipt\r\n",
      "    x = layers.Reshape((16384,3))(x)\r\n",
      "    x = layers.Softmax(axis=1)(x)\r\n",
      "    x = layers.Flatten()(x)\r\n",
      "    x = layers.Dense(1)(x)\r\n",
      "    model = keras.Model(inputs=[ipt],outputs=[x])\r\n",
      "    convert_to_lite(model,'failed_on_android_gpu.tflite')\r\n",
      "\r\n",
      "def work_reshape_only():\r\n",
      "    ipt = layers.Input((256,64,3))\r\n",
      "    x = ipt\r\n",
      "    x = layers.Reshape((16384,3))(x)\r\n",
      "    # x = layers.Softmax(axis=1)(x)\r\n",
      "    x = layers.Flatten()(x)\r\n",
      "    x = layers.Dense(1)(x)\r\n",
      "    model = keras.Model(inputs=[ipt],outputs=[x])\r\n",
      "    convert_to_lite(model,'work_reshape_only.tflite')\r\n",
      "[Reshape_before_Softmax_leads_runtime_error.zip](https://github.com/tensorflow/tensorflow/files/5879355/Reshape_before_Softmax_leads_runtime_error.zip)\r\n",
      "\r\n",
      "\r\n",
      "def work_softmax_only():\r\n",
      "    ipt = layers.Input((256,64,3))\r\n",
      "    x = ipt\r\n",
      "    # x = layers.Reshape((16384,3))(x)\r\n",
      "    x = layers.Softmax(axis=1)(x)\r\n",
      "    x = layers.Flatten()(x)\r\n",
      "    x = layers.Dense(1)(x)\r\n",
      "    model = keras.Model(inputs=[ipt],outputs=[x])\r\n",
      "    convert_to_lite(model,'work_softmax_only.tflite')\r\n",
      "\r\n",
      "if __name__ == \"__main__\":\r\n",
      "    failed_on_android_gpu_for_dynamic_size()\r\n",
      "    work_reshape_only()\r\n",
      "    work_softmax_only()\r\n",
      "`\r\n",
      "### 5. (optional) Any other info / logs\r\n",
      "Run the 'failed_on_android_gpu.tflite' on android with gpu delegate cause exception:\r\n",
      "Internal error: Failed to apply delegate: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.\r\n",
      "The other 2 tflite files which with Reshape/Softmax only works fine, so the dynamic-size tensor is unnecessary.\r\n",
      "\r\n",
      "I also tested nn.softmax or math.reduced_sum, has same problem. Maybe 'reduce_sum_with_dims' is the real \r\n",
      "\r\n",
      "problem.\n",
      "issue labels - \n",
      "TF 2.4\n",
      "TFLiteConverter\n",
      "\n",
      "\n",
      "issue title -  `tf.matmul` and `tf.tensordot` behave different in converted concrete function in TensorFlowLite \n",
      "issue body -  ### 1. System information\r\n",
      "\r\n",
      "- OS Platform and Distribution: MacOS 10.15 and Ubuntu 18.04 LTS on Colab machine\r\n",
      "- TensorFlow installation (pip package or built from source): pip\r\n",
      "- TensorFlow library (version, if pip package or github SHA, if built from source): `tensorflow                    2.4.0`\r\n",
      "\r\n",
      "### 2. Code\r\n",
      "\r\n",
      "This notebook demonstrates the bug with the simplest example I came up with.\r\n",
      "\r\n",
      "https://colab.research.google.com/gist/ebraraktas/ab87170deb38eae979b37795015e44bc\r\n",
      "\r\n",
      "### 3. Failure after conversion\r\n",
      "\r\n",
      "I implemented RFFT for TFLite using `tf.matmul` and saved the module concrete function. But invoking saved tflite model repeatedly returns different results. However, replacing `tf.matmul` with `tf.tensordot` fixes the strange behavior. Therefore, I have prepared the notebook above to demonstrate the bug. I have realized interesting cases which change the behavior:\r\n",
      "\r\n",
      "- If negative sign is removed from output returned from `DummyMatmul` or `DummyTensordot` (`result` variable), outputs are same\r\n",
      "- If we use `tf.Module` directly, outputs are same (colab demo shows it)\r\n",
      "- Somehow, size of the right hand side matrix matters (colab demo shows it)\r\n",
      "- Difference occurs after first iteration (colab demo shows it), and for some inputs it gets larger with every iteration \r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:lite\n",
      "comp:lite-kernels\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -   An error occurs when processing data delivered to tensorflow serving.\n",
      "issue body -  <em>Please make sure that this is a bug. As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:bug_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10\r\n",
      "- TensorFlow installed from (source or binary): pip\r\n",
      "- TensorFlow version (use command below): 2.4\r\n",
      "- Python version: 3.7.9\r\n",
      "- CUDA/cuDNN version: None\r\n",
      "- GPU model and memory: None\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "An error occurs when transferring data that is confirmed to be operating normally to tensorflow serving\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "When transmitting data confirmed to be working normally to TensorFlow Serving, it should operate normally\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "This Is my model's signature\r\n",
      "```\r\n",
      "signature_def['serving_default']:\r\n",
      "  The given SavedModel SignatureDef contains the following input(s):\r\n",
      "    inputs['age'] tensor_info:\r\n",
      "        dtype: DT_FLOAT\r\n",
      "        shape: (-1, 1)\r\n",
      "        name: serving_default_age:0\r\n",
      "    inputs['balance'] tensor_info:\r\n",
      "        dtype: DT_FLOAT\r\n",
      "        shape: (-1, 1)\r\n",
      "        name: serving_default_balance:0\r\n",
      "    inputs['campaign'] tensor_info:\r\n",
      "        dtype: DT_FLOAT\r\n",
      "        shape: (-1, 1)\r\n",
      "        name: serving_default_campaign:0\r\n",
      "    inputs['contact'] tensor_info:\r\n",
      "        dtype: DT_STRING\r\n",
      "        shape: (-1, 1)\r\n",
      "        name: serving_default_contact:0\r\n",
      "    inputs['day'] tensor_info:\r\n",
      "        dtype: DT_FLOAT\r\n",
      "        shape: (-1, 1)\r\n",
      "        name: serving_default_day:0\r\n",
      "    inputs['default'] tensor_info:\r\n",
      "        dtype: DT_STRING\r\n",
      "        shape: (-1, 1)\r\n",
      "        name: serving_default_default:0\r\n",
      "    inputs['duration'] tensor_info:\r\n",
      "        dtype: DT_FLOAT\r\n",
      "        shape: (-1, 1)\r\n",
      "        name: serving_default_duration:0\r\n",
      "    inputs['education'] tensor_info:\r\n",
      "        dtype: DT_STRING\r\n",
      "        shape: (-1, 1)\r\n",
      "        name: serving_default_education:0\r\n",
      "    inputs['housing'] tensor_info:\r\n",
      "        dtype: DT_STRING\r\n",
      "        shape: (-1, 1)\r\n",
      "        name: serving_default_housing:0\r\n",
      "    inputs['job'] tensor_info:\r\n",
      "        dtype: DT_STRING\r\n",
      "        shape: (-1, 1)\r\n",
      "        name: serving_default_job:0\r\n",
      "    inputs['loan'] tensor_info:\r\n",
      "        dtype: DT_STRING\r\n",
      "        shape: (-1, 1)\r\n",
      "        name: serving_default_loan:0\r\n",
      "    inputs['marital'] tensor_info:\r\n",
      "        dtype: DT_STRING\r\n",
      "        shape: (-1, 1)\r\n",
      "        name: serving_default_marital:0\r\n",
      "    inputs['month'] tensor_info:\r\n",
      "        dtype: DT_STRING\r\n",
      "        shape: (-1, 1)\r\n",
      "        name: serving_default_month:0\r\n",
      "    inputs['pdays'] tensor_info:\r\n",
      "        dtype: DT_FLOAT\r\n",
      "        shape: (-1, 1)\r\n",
      "        name: serving_default_pdays:0\r\n",
      "    inputs['poutcome'] tensor_info:\r\n",
      "        dtype: DT_STRING\r\n",
      "        shape: (-1, 1)\r\n",
      "        name: serving_default_poutcome:0\r\n",
      "    inputs['previous'] tensor_info:\r\n",
      "        dtype: DT_FLOAT\r\n",
      "        shape: (-1, 1)\r\n",
      "        name: serving_default_previous:0\r\n",
      "    inputs['seq'] tensor_info:\r\n",
      "        dtype: DT_FLOAT\r\n",
      "        shape: (-1, 5)\r\n",
      "        name: serving_default_seq:0\r\n",
      "    inputs['y'] tensor_info:\r\n",
      "        dtype: DT_STRING\r\n",
      "        shape: (-1, 1)\r\n",
      "        name: serving_default_y:0\r\n",
      "  The given SavedModel SignatureDef contains the following output(s):\r\n",
      "    outputs['dense_1'] tensor_info:\r\n",
      "        dtype: DT_FLOAT\r\n",
      "        shape: (-1, 171)\r\n",
      "        name: StatefulPartitionedCall:0\r\n",
      "  Method name is: tensorflow/serving/predict\r\n",
      "```\r\n",
      "load the model and predict with sample_data works fine\r\n",
      "\r\n",
      "```\r\n",
      "loded_model = tf.keras.models.load_model(model_path, compile=True)\r\n",
      "sample_data = {\r\n",
      "        'seq': np.expand_dims(np.array([110, 115, 49, 71, 50]), axis=0),\r\n",
      "        'age': np.expand_dims(np.array(32), axis=0),\r\n",
      "        'job': np.expand_dims(np.array('management'), axis=0),\r\n",
      "        'marital': np.expand_dims(np.array('married'), axis=0),\r\n",
      "        'education': np.expand_dims(np.array('primary'), axis=0),\r\n",
      "        'default': np.expand_dims(np.array('no'), axis=0),\r\n",
      "        'balance': np.expand_dims(np.array(67), axis=0),\r\n",
      "        'housing': np.expand_dims(np.array('no'), axis=0),\r\n",
      "        'loan': np.expand_dims(np.array('no'), axis=0),\r\n",
      "        'contact': np.expand_dims(np.array('unknown'), axis=0),\r\n",
      "        'day': np.expand_dims(np.array(24), axis=0),\r\n",
      "        'month': np.expand_dims(np.array('jun'), axis=0),\r\n",
      "        'duration': np.expand_dims(np.array(9), axis=0),\r\n",
      "        'campaign': np.expand_dims(np.array(1), axis=0),\r\n",
      "        'pdays': np.expand_dims(np.array(-1), axis=0),\r\n",
      "        'previous': np.expand_dims(np.array(0), axis=0),\r\n",
      "        'poutcome': np.expand_dims(np.array('unknown'), axis=0),\r\n",
      "        'y': np.expand_dims(np.array('no'), axis=0)\r\n",
      "    }\r\n",
      "loded_model(sample_data)\r\n",
      "<tf.Tensor: shape=(1, 171), dtype=float32, numpy=\r\n",
      "array([[5.27819619e-02, 7.91415647e-02, 2.05127634e-02, 4.92982864e-02,\r\n",
      "        8.63614008e-02, 5.65255731e-02, 4.22200933e-02, 6.70532882e-02,\r\n",
      "        2.94332802e-02, 5.92661090e-02, 3.92056666e-02, 6.91237226e-02,\r\n",
      "        1.33277355e-02, 8.19469616e-03, 5.87656125e-02, 5.18164076e-02,\r\n",
      "        6.20538630e-02, 9.00777951e-02, 4.28347066e-02, 2.19823830e-02,\r\n",
      "        1.49062146e-07, 2.01256057e-07, 1.53366202e-07, 1.27340172e-07,\r\n",
      "        1.86510434e-07, 1.98491620e-07, 1.82937896e-07, 1.32476117e-07,\r\n",
      "        1.31851834e-07, 1.40986472e-07, 1.99838468e-07, 1.58420590e-07,\r\n",
      "        1.17166692e-07, 9.00412758e-08, 1.95417840e-07, 1.31040650e-07,\r\n",
      "        1.60380964e-07, 1.07113330e-07, 1.39082644e-07, 1.42982330e-07,\r\n",
      "        1.21180648e-07, 1.54600315e-07, 1.02207764e-07, 2.41572224e-07,\r\n",
      "        1.50535001e-07, 1.23603499e-07, 1.76933810e-07, 1.29627537e-07,\r\n",
      "        2.25440814e-07, 2.21079134e-07, 1.40433457e-07, 1.21948943e-07,\r\n",
      "        1.12487960e-07, 1.82974887e-07, 1.94294685e-07, 1.49468548e-07,\r\n",
      "        9.63142881e-08, 1.64101479e-07, 8.75015900e-08, 1.78585822e-07,\r\n",
      "        1.18338065e-07, 1.34275794e-07, 1.94435557e-07, 9.40549114e-08,\r\n",
      "        7.68365709e-08, 9.09561138e-08, 2.07228780e-07, 1.23126767e-07,\r\n",
      "        1.52446063e-07, 1.45874452e-07, 1.97502175e-07, 1.05272626e-07,\r\n",
      "        2.01461347e-07, 2.27837347e-07, 2.44192165e-07, 1.04354719e-07,\r\n",
      "        1.47167256e-07, 1.86789734e-07, 1.95981102e-07, 1.83554263e-07,\r\n",
      "        1.42765145e-07, 9.52495824e-08, 2.68968222e-07, 1.77189975e-07,\r\n",
      "        9.28885981e-08, 1.18023948e-07, 1.40940770e-07, 1.22296697e-07,\r\n",
      "        1.37569558e-07, 1.37506873e-07, 8.40115391e-08, 1.25378747e-07,\r\n",
      "        2.05835846e-07, 1.08780618e-07, 5.62952067e-08, 1.59157153e-07,\r\n",
      "        1.29730566e-07, 1.50789603e-07, 1.63185859e-07, 1.54063102e-07,\r\n",
      "        1.42086819e-07, 1.31032266e-07, 1.95424562e-07, 8.60203855e-08,\r\n",
      "        2.31950693e-07, 1.25126334e-07, 2.37775524e-07, 1.93945539e-07,\r\n",
      "        7.71898456e-08, 1.51705976e-07, 1.75589378e-07, 2.89475594e-07,\r\n",
      "        1.35602036e-07, 1.80231439e-07, 2.15656939e-07, 1.21509544e-07,\r\n",
      "        1.46606226e-07, 1.69434031e-07, 8.56758717e-08, 1.21774391e-07,\r\n",
      "        2.94665796e-07, 1.73765784e-07, 1.44480254e-07, 1.25448224e-07,\r\n",
      "        1.26027047e-07, 1.53907735e-07, 1.21600536e-07, 1.80686797e-07,\r\n",
      "        1.34774865e-07, 1.55523523e-07, 1.34890200e-07, 1.37089174e-07,\r\n",
      "        1.40633560e-07, 1.36167046e-07, 1.17650742e-07, 1.60875459e-07,\r\n",
      "        1.26850381e-07, 1.48223222e-07, 1.11685516e-07, 1.30754032e-07,\r\n",
      "        1.99773112e-07, 9.96520626e-08, 1.44601273e-07, 2.22673521e-07,\r\n",
      "        1.37150110e-07, 9.27915593e-08, 1.32079876e-07, 1.45855807e-07,\r\n",
      "        1.98044447e-07, 8.85873419e-08, 1.73917329e-07, 9.99040353e-08,\r\n",
      "        2.03707074e-07, 9.97928140e-08, 1.81921166e-07, 1.35442946e-07,\r\n",
      "        1.58751504e-07, 1.62741699e-07, 1.55544441e-07, 1.72405649e-07,\r\n",
      "        8.22981931e-08, 2.17250758e-07, 1.59198137e-07, 1.26780478e-07,\r\n",
      "        1.79329419e-07, 1.99787223e-07, 2.10311356e-07, 1.92279757e-07,\r\n",
      "        1.51305883e-07, 1.15883971e-07, 1.22970349e-07]], dtype=float32)>\r\n",
      "\r\n",
      "```\r\n",
      "\r\n",
      "However, an error occurs when serving with same data.\r\n",
      "\r\n",
      "```\r\n",
      "def processing_input(seq = list, age=int, job=str, marital=str, education=str, default=str, balance=int,\r\n",
      "                         housing=str, loan=str, contact=str, day=int, month=str, duration=int,campaign=int, pdays=int, previous=int,poutcome=str,y=str):\r\n",
      "    input = {\r\n",
      "        'seq': np.expand_dims(np.array(seq), axis=0).tolist(),\r\n",
      "        'age': np.expand_dims(np.array(age), axis=0).tolist(),\r\n",
      "        'job': np.expand_dims(np.array(job), axis=0).tolist(),\r\n",
      "        'marital': np.expand_dims(np.array(marital), axis=0).tolist(),\r\n",
      "        'education': np.expand_dims(np.array(education), axis=0).tolist(),\r\n",
      "        'default': np.expand_dims(np.array(default), axis=0).tolist(),\r\n",
      "        'balance': np.expand_dims(np.array(balance), axis=0).tolist(),\r\n",
      "        'housing': np.expand_dims(np.array(housing), axis=0).tolist(),\r\n",
      "        'loan': np.expand_dims(np.array(loan), axis=0).tolist(),\r\n",
      "        'contact': np.expand_dims(np.array(contact), axis=0).tolist(),\r\n",
      "        'day': np.expand_dims(np.array(day), axis=0).tolist(),\r\n",
      "        'month': np.expand_dims(np.array(month), axis=0).tolist(),\r\n",
      "        'duration': np.expand_dims(np.array(duration), axis=0).tolist(),\r\n",
      "        'campaign': np.expand_dims(np.array(campaign), axis=0).tolist(),\r\n",
      "        'pdays': np.expand_dims(np.array(pdays), axis=0).tolist(),\r\n",
      "        'previous': np.expand_dims(np.array(previous), axis=0).tolist(),\r\n",
      "        'poutcome': np.expand_dims(np.array(poutcome), axis=0).tolist(),\r\n",
      "        'y': np.expand_dims(np.array(y), axis=0).tolist()\r\n",
      "    }\r\n",
      "    data = json.dumps({'inputs': input})\r\n",
      "    return data\r\n",
      "\r\n",
      "data = processing_input([0,0,0,152,151],32,'tech','none','string','no',3,'no','no','unknown',3,'june',2,1,5,2,'name','no')\r\n",
      "json_response = requests.post(f'http://{model_adr}/v1/models/{model_name}:predict', data=data)\r\n",
      "prediction = json.loads(str(json_response.content, 'utf-8'))\r\n",
      "\r\n",
      "```\r\n",
      "and i get \r\n",
      "\r\n",
      "```\r\n",
      "{'error': 'Invalid reduction dimension (1 for input with 1 dimension(s)\\n\\t [[{{node model/category_encoding_8/Min}}]]'}\r\n",
      "```\r\n",
      "\r\n",
      "I put data that can be accepted as input in the model, why doesn't it work at serving time?\r\n",
      "\r\n",
      "\r\n",
      "**Other info / logs** Include any logs or source code that would be helpful to\r\n",
      "diagnose the problem. If including tracebacks, please include the full\r\n",
      "traceback. Large logs and files should be attached.\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Add uint32 and uint64 support for PopulationCount\n",
      "issue body -  PopulationCount is part of the bitwise operations (BitwiseAnd/Or/Xor/etc)\r\n",
      "though unlike other bitwise operatios, PopulationCount does not\r\n",
      "have uint32 and uint64 support.\r\n",
      "\r\n",
      "This PR add uint32 and uint64 support and fixes #46676.\r\n",
      "\r\n",
      "Signed-off-by: Yong Tang <yong.tang.github@outlook.com>\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:core\n",
      "prtype:bugfix\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  Port int8 and float versions of space_to_batch to TFLM\n",
      "issue body -  Commit 1 copies the TFLite operator into TFLM\r\n",
      "Commit 2 implements basic float, int8 and error checking tests along with float and int8 implementations of space_to_batch_nd\r\n",
      "\r\n",
      "This version requires that the flat size of input matches output, since TFLM does not support tensor resizing.\r\n",
      "\r\n",
      "#45693\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "prtype:bugfix\n",
      "ready to pull\n",
      "size:L\n",
      "\n",
      "\n",
      "issue title -  Add reference fallback for TARGET_ARCH!=hifimini\n",
      "issue body -  A follow-up change will add in calls to the XaNNLib to get an optimized softmax implementation.\r\n",
      "\r\n",
      "Tested that keyword_benchmark has a latency increase of ~1000 ticks for the Fusion F1.\r\n",
      "\r\n",
      "This latency improvement does not matter since the current change is setting the stage for pulling in the optimized implementation.\r\n",
      "\r\n",
      "This command:\r\n",
      "```\r\n",
      "make -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=fusion_f1 XTENSA_CORE=F1_190305_swupgrade test_keyword_benchmark -j8\r\n",
      "```\r\n",
      "\r\n",
      "before this change:\r\n",
      "```\r\n",
      "InitializeKeywordRunner() took 279548 ticks (279 ms)\r\n",
      "KeywordRunNIerations(1) took 151249 ticks (151 ms)\r\n",
      "KeywordRunNIerations(10) took 1511997 ticks (1511 ms)\r\n",
      "```\r\n",
      "\r\n",
      "after this change:\r\n",
      "```\r\n",
      "InitializeKeywordRunner() took 201464 ticks (201 ms)\r\n",
      "KeywordRunNIerations(1) took 152158 ticks (152 ms)\r\n",
      "KeywordRunNIerations(10) took 1521087 ticks (1521 ms)\r\n",
      "```\r\n",
      "\r\n",
      "Progress towards http://b/177457688\r\n",
      "\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  micro: prepare to port operator FLOOR_DIV kernel from lite with test\n",
      "issue body -  Implement skeleton (non-working) code for operator and test.\r\n",
      "Header files changed.\r\n",
      "Namespaces changed.\r\n",
      "Some original code deleted.\r\n",
      "Some original code modified.\r\n",
      "\r\n",
      "This represents PR step 4 of the work to port operator FLOOR_DIV as tracked in Issue #45657\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:lite\n",
      "comp:micro\n",
      "prtype:bugfix\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  Refactor softmax to share code between reference and optimized implementations.\n",
      "issue body -  \n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:L\n",
      "\n",
      "\n",
      "issue title -  Converting a model with Int8 fake quant nodes from a saved model file fails\n",
      "issue body -  **System information**\r\n",
      "\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.15.6\r\n",
      "- TensorFlow installed from (source or binary): TensorFlow 2.4.1 PyPI pip package.\r\n",
      "- TensorFlow version (or github SHA if from source): 2.4.1.\r\n",
      "\r\n",
      "**Command used to run the converter or code if you’re using the Python API**\r\n",
      "\r\n",
      "Suppose `keras_model` is a Keras model containing QAT fake quantiser ops, like `tf.quantization.fake_quant_with_min_max_vars`. Then, the following conversion code using `from_saved_model` fails to output a valid Int8 model and throws an error.\r\n",
      "\r\n",
      "```python\r\n",
      "with open(\"converted_saved_model.tflite\", \"wb\") as f:\r\n",
      "    tf.keras.models.save_model(keras_model, \"tmp-saved-model\", save_format=\"tf\")\r\n",
      "    converter = tf.lite.TFLiteConverter.from_saved_model(\"tmp-saved-model\")\r\n",
      "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n",
      "    converter.inference_input_type = tf.int8\r\n",
      "    converter.inference_output_type = tf.int8\r\n",
      "    f.write(converter.convert())\r\n",
      "```\r\n",
      "\r\n",
      "Note that `from_keras_model` works correctly.\r\n",
      "\r\n",
      "See a minimal reproduction with the following colab notebook: https://colab.research.google.com/drive/1s3AkxetmaIHKcB3M69cBSVbOfhNx8106?usp=sharing\r\n",
      "\r\n",
      "**The output from the converter invocation**\r\n",
      "\r\n",
      "The above snippet fails with:\r\n",
      "\r\n",
      "```\r\n",
      "ValueError: The inference_input_type and inference_output_type must be tf.float32.\r\n",
      "```\r\n",
      "\r\n",
      "For more detail, see the linked colab notebook.\r\n",
      "\r\n",
      "**Also, please include a link to the saved model or GraphDef**\r\n",
      "\r\n",
      "Can be found in the linked colab notebook.\r\n",
      "\r\n",
      "**Failure details**\r\n",
      "\r\n",
      "Attempting to convert and setting the inference input/output type to int8 causes the error above.\r\n",
      "\r\n",
      "Attempting to convert without setting those types (such that they default to float) doesn't cause an error but does yield a broken Int8 model with lots of dangling quant/dequant ops pairs:\r\n",
      "\r\n",
      "<img width=\"500\" alt=\"image\" src=\"https://user-images.githubusercontent.com/7688302/105897373-f9673a80-600f-11eb-95ad-683671cbf825.png\">\n",
      "issue labels - \n",
      "ModelOptimizationToolkit\n",
      "TF 2.4\n",
      "comp:lite\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  x\n",
      "issue body -  x\n",
      "issue labels - \n",
      "invalid\n",
      "type:performance\n",
      "\n",
      "\n",
      "issue title -  Consider `FakeQuantWithMinMaxArgs` to be a train-time quant op in the TFLite Converter\n",
      "issue body -  `tf.quantization.fake_quant_with_min_max_vars` is commonly used for training-time Int8 fake-quantisation in TF, and when used correctly will cause the TFLite converter to output int8 (rather than float) ops.\r\n",
      "\r\n",
      "`tf.quantization.fake_quant_with_min_max_vars` requires a min/max quantisation scale to be passed in as TF variables. This means that the scales can be updated during training. However, one could conceivably instead use `tf.quantization.fake_quant_with_min_max_args`, which is the same op with the exception that the min/max scales are passed in as constants rather than variables.\r\n",
      "\r\n",
      "At the moment, attempting to convert an model that contains only `tf.quantization.fake_quant_with_min_max_args` ops does not work correctly, because the function `contains_training_quant_op` incorrectly returns false, which means that `is_training_time_int8_allow_float` returns false:\r\n",
      "\r\n",
      "https://github.com/tensorflow/tensorflow/blob/2833b3d9457aa04de808578bbc0fa70ec136c63f/tensorflow/lite/python/lite.py#L249-L251\r\n",
      "\r\n",
      "This PR fixes the problem by updating `contains_training_quant_op` to recognise `FakeQuantWithMinMaxArgs` and `FakeQuantWithMinMaxArgsPerChannel` as training-time quant ops.\n",
      "issue labels - \n",
      "ModelOptimizationToolkit\n",
      "TFLiteConverter\n",
      "cla: yes\n",
      "comp:lite\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  [TFLM] Add headers and API for optimized kernels for CEVA-DSP BX1 and SP500\n",
      "issue body -  Added common headers and a scratch allocation file for CEVA platforms.\r\n",
      "\r\n",
      "These are linked against an internal CEVA static lib containing all the optimized code. Since the library is not open source (or part of a third_party_download), we are currently adding the headers directly to the TFLM repository.\r\n",
      "\r\n",
      "This is the same reason why these headers match the CEVA API and style (as opposed to the Google C++ style) and are not reviewed beyond looking at license compliance.\r\n",
      "\r\n",
      "In the future, there might be value in pulling in the library and headers as a combined third_party_download. If that turns out to be useful, we would remove these headers from the TFLM repository.\r\n",
      "\r\n",
      "Relevant github issue: https://github.com/tensorflow/tensorflow/issues/45607\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "comp:micro:ceva\n",
      "ready to pull\n",
      "size:XL\n",
      "\n",
      "\n",
      "issue title -  tf.math.reduce_prod aborts when keepdims contain large values\n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n",
      "- TensorFlow installed from (source or binary): binary\r\n",
      "- TensorFlow version (use command below):2.1.0\r\n",
      "- Python version:3.7.6\r\n",
      "- Bazel version (if compiling from source):N/A\r\n",
      "- GCC/Compiler version (if compiling from source):N/A\r\n",
      "- CUDA/cuDNN version:N/A\r\n",
      "- GPU model and memory:N/A\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "`tf.math.reduce_prod` aborts when `keepdims` contain large values\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "expect an exception message if the input is not expected, instead of crash.\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "~~~python\r\n",
      "import tensorflow as tf\r\n",
      "import numpy as np\r\n",
      "tf.math.reduce_prod(input_tensor=1, keepdims=np.array([63600, 1], dtype=np.float16))\r\n",
      "~~~\r\n",
      "\r\n",
      "\r\n",
      "Output:\r\n",
      "~~~python\r\n",
      "2021-01-26 17:02:24.497049: F ./tensorflow/python/eager/pywrap_tensor_conversion.h:58] Check failed: !PyErr_Occurred()\r\n",
      "Aborted (core dumped)\r\n",
      "~~~\n",
      "issue labels - \n",
      "TF 2.1\n",
      "comp:ops\n",
      "stat:awaiting tensorflower\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  tf.math.segment_min abortion when segment_ids contains large value\n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n",
      "- TensorFlow installed from (source or binary): binary\r\n",
      "- TensorFlow version (use command below):2.1.0\r\n",
      "- Python version:3.7.6\r\n",
      "- Bazel version (if compiling from source):N/A\r\n",
      "- GCC/Compiler version (if compiling from source):N/A\r\n",
      "- CUDA/cuDNN version:N/A\r\n",
      "- GPU model and memory:N/A\r\n",
      "\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "`tf.math.segment_min` abortion when `segment_ids` contains large value\r\n",
      "**Describe the expected behavior**\r\n",
      "expect an exception message if the input is not expected, instead of crash.\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "~~~python\r\n",
      "import tensorflow as tf\r\n",
      "import numpy as np\r\n",
      "tf.math.segment_min(data=np.ones((1,2,1,1,1), dtype=np.uint16), segment_ids=[5053997376933981534])\r\n",
      "~~~\r\n",
      "\r\n",
      "output:\r\n",
      "~~~python\r\n",
      "2021-01-26 15:49:10.870684: F tensorflow/core/framework/tensor_shape.cc:405] Check failed: 0 <= new_num_elements (0 vs. -8338749319841588546)\r\n",
      "Aborted (core dumped)\r\n",
      "~~~\n",
      "issue labels - \n",
      "TF 2.1\n",
      "comp:ops\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  fixed the output description of bucketized column to one-hot encoded value\n",
      "issue body -  \n",
      "issue labels - \n",
      "cla: yes\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  At least read the BogoMIPS figure on ARM\n",
      "issue body -  Use the capitalization of BogoMIPS that is used by ARM architecture CPUs.\n",
      "issue labels - \n",
      "cla: yes\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Cannot copy between a TensorFlowLite tensor with shape [1, 1917, 4] and a Java object with shape [1, 10,4]\n",
      "issue body -  Im useing tensorflow - object detection my questions is like this but!\r\n",
      "\r\n",
      "https://stackoverflow.com/questions/57393407/java-lang-illegalargumentexception-cannot-copy-between-a-tensorflowlite-tensor\r\n",
      "\r\n",
      "**java.lang.IllegalArgumentException: Cannot copy between a TensorFlowLite tensor with shape [1, 1917, 4] and a Java object with shape [1, 10,4].**\r\n",
      "\r\n",
      "a guy answerd to this questions and sayed your should change in python side! but he didnt talk about which lines or which .py codes Im useing mobile_ssd_v2_float and too many people cant solve it! Is there any answers for this question?\r\n",
      "\r\n",
      "this is pic of my .tflite file form Netron\r\n",
      "![yez6b](https://user-images.githubusercontent.com/42836468/105861333-d9317e80-6003-11eb-91a4-452a60b7e632.png)\r\n",
      "\n",
      "issue labels - \n",
      "comp:lite\n",
      "comp:lite-examples\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  [ROCm] Update script install_pip_packages.sh for Python 2.7\n",
      "issue body -  Currently ROCm TF cotnainers use the `install_pip_packages.sh` script to install the Python pip package-manager.\r\n",
      "* https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/Dockerfile.rocm#L102\r\n",
      "* https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/install/install_pip_packages.sh#L20-L23\r\n",
      "\r\n",
      "Starting 01/23/2021, we started getting the following error while building TF containers for ROCM CI (when calling `get-pip.py` for Python2)\r\n",
      "\r\n",
      "```\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"get-pip.py\", line 24226, in <module>\r\n",
      "    main()\r\n",
      "  File \"get-pip.py\", line 199, in main\r\n",
      "    bootstrap(tmpdir=tmpdir)\r\n",
      "  File \"get-pip.py\", line 82, in bootstrap\r\n",
      "    from pip._internal.cli.main import main as pip_entry_point\r\n",
      "  File \"/tmp/tmpWkL0gn/pip.zip/pip/_internal/cli/main.py\", line 60\r\n",
      "    sys.stderr.write(f\"ERROR: {exc}\")\r\n",
      "```\r\n",
      "\r\n",
      "The cause seems to be an update to the `get-pip.py` script, which now picks the version `pip-21.0` (previously it was `pip-20.3.4`).\r\n",
      "`pip-21.0` drops support for Python2.7 (as indicated by the following warning message)\r\n",
      "```\r\n",
      "DEPRECATION: Python 2.7 reached the end of its life on January 1st, 2020.\r\n",
      "Please upgrade your Python as Python 2.7 is no longer maintained.\r\n",
      "pip 21.0 will drop support for Python 2.7 in January 2021.\r\n",
      "More details about Python 2 support in pip can be found at\r\n",
      "https://pip.pypa.io/en/latest/development/release-process/#python-2-support\r\n",
      "pip 21.0 will remove support for this functionality.\r\n",
      "```\r\n",
      "\r\n",
      "It seems that there is now a Python 2.7 specific version of the `get-pip.py` script that we need to use, and that is what this commit does\r\n",
      "\r\n",
      "Note: Although I am filing this PR as a ROCm specific PR, this issue + fix is probably applicable to all users of the `install_pip_packages.sh` script.\r\n",
      "\r\n",
      "---------------------------------------------------------------------------\r\n",
      "\r\n",
      "/cc @cheshire @chsigg @nvining-work \r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:gpu\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  predictions = model(inputs) outputs nan\n",
      "issue body -  Hi\r\n",
      "\r\n",
      "This is a great project and is very useful, but I have encountered an error with model predicting\r\n",
      "\r\n",
      "I have used an lstm as the model input. Once I run window.plot(model=lstm_model) the output of predictions is just a 2D array with \"nan\" as each value. The shape of this array is 2, 79, 1\r\n",
      "\r\n",
      "Please help\r\n",
      "\r\n",
      "P.S. Here's my code for reference\r\n",
      "\r\n",
      "print(\"Initiating the time series forcasting module\")\r\n",
      "\r\n",
      "print(\"Importing modules\")\r\n",
      "import pandas as pd\r\n",
      "import os\r\n",
      "import matplotlib as mpl\r\n",
      "import matplotlib.pyplot as plt\r\n",
      "import numpy as np\r\n",
      "import seaborn as sns\r\n",
      "import tensorflow as tf\r\n",
      "\r\n",
      "print(\"Collecting data\")\r\n",
      "df = pd.read_csv(\"data.csv\")\r\n",
      "dates = pd.to_datetime((df.pop(\"Date\")), format = \"%Y/%m/%d\")\r\n",
      "\r\n",
      "print(\"Splitting data\")\r\n",
      "column_indices = {name: i for i, name in enumerate(df.columns)}\r\n",
      "n = len(df)\r\n",
      "train_df = df[0:int(n*0.9)]\r\n",
      "test_df = df[int(n*0.9):]\r\n",
      "print(str(len(train_df)) + \" plots for training\")\r\n",
      "print(str(len(test_df)) + \" plots for testing\")\r\n",
      "num_features = df.shape[1]\r\n",
      "\r\n",
      "print(\"Normalizing data\")\r\n",
      "train_mean = train_df.mean()\r\n",
      "train_std = train_df.std()\r\n",
      "train_df = (train_df - train_mean) / train_std\r\n",
      "test_df = (test_df - train_mean) / train_std\r\n",
      "df_std = (df - train_mean) / train_std\r\n",
      "\r\n",
      "print(\"Creating classes\")\r\n",
      "class WindowGenerator():\r\n",
      "    def __init__(self, input_width, label_width, shift,\r\n",
      "                label_columns=None, train_df=train_df, test_df=test_df):\r\n",
      "        print(\"Generating windows\")\r\n",
      "        self.train_df = train_df\r\n",
      "        self.test_df = test_df\r\n",
      "        \r\n",
      "        self.label_columns = label_columns\r\n",
      "        if label_columns is not None:\r\n",
      "            self.label_column_indices = {name: i for i, name in\r\n",
      "                                         enumerate(label_columns)}\r\n",
      "        self.column_indices = {name: i for i, name in\r\n",
      "                               enumerate(train_df.columns)}\r\n",
      "        self.input_width = input_width\r\n",
      "        self.label_width = label_width\r\n",
      "        self.shift = shift\r\n",
      "        \r\n",
      "        self.total_window_size = input_width + shift\r\n",
      "        self.input_slice = slice(0, input_width)\r\n",
      "        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\r\n",
      "        \r\n",
      "        self.label_start = self.total_window_size - self.label_width\r\n",
      "        self.labels_slice = slice(self.label_start, None)\r\n",
      "        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\r\n",
      "    \r\n",
      "    def __repr__(self):\r\n",
      "        return \"\\n\".join([\"Total window size: \" + str(self.total_window_size),\r\n",
      "                          \"Input indices: \" + str(self.input_indices),\r\n",
      "                          \"Label indices: \" + str(self.label_indices),\r\n",
      "                          \"Label column names: \" + str(self.label_columns)])\r\n",
      "    \r\n",
      "    def split_window(self, features):\r\n",
      "        print(\"Splitting windows\")\r\n",
      "        inputs = features[:, self.input_slice, :]\r\n",
      "        labels = features[:, self.labels_slice, :]\r\n",
      "        if self.label_columns is not None:\r\n",
      "            labels = tf.stack(\r\n",
      "                [labels[:, :, self.column_indices[name]] for name in self.label_columns],\r\n",
      "                axis=-1)\r\n",
      "        inputs.set_shape([None, self.input_width, None])\r\n",
      "        labels.set_shape([None, self.label_width, None])\r\n",
      "        return inputs, labels\r\n",
      "    \r\n",
      "    def plot(self, plot_col, model=None, max_subplots=3):\r\n",
      "        print(\"Plotting data\")\r\n",
      "        inputs, labels = self.example\r\n",
      "        plt.figure(figsize=(12,8))\r\n",
      "        plot_col_index = self.column_indices[plot_col]\r\n",
      "        max_n = min(max_subplots, len(inputs))\r\n",
      "        for n in range(max_n):\r\n",
      "            plt.subplot(3,1,n+1)\r\n",
      "            plt.ylabel(plot_col)\r\n",
      "            plt.plot(self.input_indices, inputs[n, :, plot_col_index],\r\n",
      "                     label=\"Inputs\", marker=\".\", zorder=-10)\r\n",
      "            if self.label_columns:\r\n",
      "                label_col_index = self.label_column_indices.get(plot_col, None)\r\n",
      "            else:\r\n",
      "                label_col_index = plot_col_index\r\n",
      "                \r\n",
      "            if plot_col_index is None:\r\n",
      "                continue\r\n",
      "            \r\n",
      "            plt.scatter(self.label_indices, labels[n, :, label_col_index],\r\n",
      "                        edgecolors=\"k\", label=\"Labels\", c='#2ca02c', s=64)\r\n",
      "            \r\n",
      "            if model is not None:\r\n",
      "                predictions = model(inputs)\r\n",
      "                plt.scatter(self.label_indices, predictions[n, :, label_col_index],\r\n",
      "                    marker='X', edgecolors='k', label='Predictions',\r\n",
      "                    c='#ff7f0e', s=64)\r\n",
      "            if n == 0:\r\n",
      "                plt.legend()\r\n",
      "        plt.xlabel(\"Date\")\r\n",
      "        plt.show()\r\n",
      "    \r\n",
      "    def make_ds(self,data):\r\n",
      "        print(\"Generating dataset\")\r\n",
      "        data = np.array(data, dtype=np.float32)\r\n",
      "        ds = tf.keras.preprocessing.timeseries_dataset_from_array(\r\n",
      "            data=data,\r\n",
      "            targets=None,\r\n",
      "            sequence_length=self.total_window_size,\r\n",
      "            sequence_stride=1,\r\n",
      "            shuffle=True,\r\n",
      "            batch_size=32,)\r\n",
      "        ds = ds.map(self.split_window)\r\n",
      "        return ds\r\n",
      "    \r\n",
      "    @property\r\n",
      "    def train(self):\r\n",
      "        return self.make_ds(self.train_df)\r\n",
      "    \r\n",
      "    @property\r\n",
      "    def test(self):\r\n",
      "        return self.make_ds(self.test_df)\r\n",
      "    \r\n",
      "    @property\r\n",
      "    def example(self):\r\n",
      "        result = getattr(self, \"_example\", None)\r\n",
      "        if result is None:\r\n",
      "            result = next(iter(self.train))\r\n",
      "            self._example = result\r\n",
      "        return result\r\n",
      "    \r\n",
      "class Baseline(tf.keras.Model):\r\n",
      "  def __init__(self, label_index=None):\r\n",
      "    print(\"Creating baseline model\")\r\n",
      "    super().__init__()\r\n",
      "    self.label_index = label_index\r\n",
      "\r\n",
      "  def call(self, inputs):\r\n",
      "    if self.label_index is None:\r\n",
      "      return inputs\r\n",
      "    result = inputs[:, :, self.label_index]\r\n",
      "    return result[:, :, tf.newaxis]\r\n",
      "    \r\n",
      "wide_window = WindowGenerator(\r\n",
      "    input_width=24, label_width=24, shift=24,\r\n",
      "    label_columns=['ConfirmedCases'])\r\n",
      "\r\n",
      "def compile_and_fit(model, window, patience=2, MAX_EPOCHS=20):\r\n",
      "  model.compile(loss=tf.losses.MeanSquaredError(),\r\n",
      "                optimizer=tf.optimizers.Adam(),\r\n",
      "                metrics=[tf.metrics.MeanAbsoluteError()])\r\n",
      "\r\n",
      "  history = model.fit(window.train, epochs=MAX_EPOCHS)\r\n",
      "  return history\r\n",
      "\r\n",
      "lstm_model = tf.keras.models.Sequential([\r\n",
      "    tf.keras.layers.LSTM(32, return_sequences=True),\r\n",
      "    tf.keras.layers.Dense(units=1)\r\n",
      "])\r\n",
      "\r\n",
      "history = compile_and_fit(lstm_model, wide_window)\r\n",
      "\r\n",
      "wide_window.plot(model=lstm_model, plot_col=\"ConfirmedCases\")\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.3\n",
      "stalled\n",
      "stat:awaiting response\n",
      "\n",
      "\n",
      "issue title -  Bazel cannot grab libstdc++ when paths for them are not default.\n",
      "issue body -  <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "- TensorFlow installed from (source or binary): source\r\n",
      "- TensorFlow version: master branch\r\n",
      "- Python version: 3.6\r\n",
      "- Installed using virtualenv? pip? conda?: conda\r\n",
      "- Bazel version (if compiling from source): 3.7.2\r\n",
      "- GCC/Compiler version (if compiling from source): 8.3.0\r\n",
      "- CUDA/cuDNN version: 10.1 / 7.6.5\r\n",
      "- GPU model and memory: Titan Xp\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "During the installation, bazel cannot link the path for LD_LIBRARY_PATH.\r\n",
      "Errors occur when bazel compiles `tensorflow/compiler/...`\r\n",
      "\r\n",
      "**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n",
      "1. Install tensorflow from source with non-default path for LD_LIBRARY_PATH. (e.g. ~/opt/gcc/8.3.0/lib64)\r\n",
      "\r\n",
      "**Any other info / logs**\r\n",
      "Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n",
      "\r\n",
      "I added some lines into `tensorflow/core/kernels/mlir_generated/build_defs.bzl` to fix this and it works.\r\n",
      "```\r\n",
      "+++ b/tensorflow/core/kernels/mlir_generated/build_defs.bzl\r\n",
      "@@ -53,6 +53,7 @@ def _gen_mlir_op_impl(ctx):\r\n",
      "                 ctx.outputs.out.path,\r\n",
      "             )\r\n",
      "         ),\r\n",
      "+        use_default_shell_env=True,\r\n",
      "     )\r\n",
      " \r\n",
      " _gen_mlir_op_rule = rule(\r\n",
      "@@ -114,6 +115,7 @@ def _gen_kernel_fatbin_impl(ctx):\r\n",
      "             \"--enable_ftz=%s\" % (ctx.attr.data_type == \"f32\"),\r\n",
      "         ],\r\n",
      "         mnemonic = \"compile\",\r\n",
      "+        use_default_shell_env=True,\r\n",
      "     )\r\n",
      "```\r\n",
      "\n",
      "issue labels - \n",
      "subtype: ubuntu/linux\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  Fix inference equation and make it more readable\n",
      "issue body -  Replace the denominator in the equation with its square root.\r\n",
      "And make the multiplication of gamma a little more readable.\r\n",
      "\r\n",
      "Signed-off-by: Suraj Upadhyay <usuraj35@gmail.com>\r\n",
      "\r\n",
      "FIxes #46522 \n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:keras\n",
      "ready to pull\n",
      "size:XS\n",
      "type:docs-bug\n",
      "\n",
      "\n",
      "issue title -  [TF2] Converted quantized TFLite model suffers severe precision/recall drop\n",
      "issue body -  **System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS High Sierra\r\n",
      "- TensorFlow installed from (source or binary): Source\r\n",
      "- TensorFlow version (or github SHA if from source): 2.5.0-dev20201209\r\n",
      "- Trained on custom dataset using  https://github.com/tensorflow/models/blob/master/research/object_detection/configs/tf2/ssd_mobilenet_v2_320x320_coco17_tpu-8.config \r\n",
      "- tflite_runtime version: 2.5.0\r\n",
      "\r\n",
      "\r\n",
      "**Command used to run the converter or code if you’re using the Python API**\r\n",
      "If possible, please share a link to Colab/Jupyter/any notebook.\r\n",
      "\r\n",
      "```\r\n",
      "converter = tf.lite.TFLiteConverter.from_saved_model('/Path/To/Tensorflow/Model/saved_model')\r\n",
      "def representative_data_gen():\r\n",
      "  image_urls = glob.glob(os.path.join(\"/Path/To/Imageset\",'*'))[:250]\r\n",
      "\r\n",
      "  for image in image_urls:\r\n",
      "    try:\r\n",
      "      img = cv2.imread(image)\r\n",
      "      img = cv2.resize(img, (300, 300))\r\n",
      "      img = cv2.cvtColor(img.copy(), cv2.COLOR_BGR2RGB)\r\n",
      "      img = img/255.0 \r\n",
      "      img = img.astype(np.float32)\r\n",
      "      image_list.append(img)\r\n",
      "    except Exception as e:\r\n",
      "        print(f'{str(e)} : {image}')\r\n",
      "        continue\r\n",
      "\r\n",
      "  image_list = np.array(image_list)\r\n",
      "  img = tf.data.Dataset.from_tensor_slices(image_list).batch(1)\r\n",
      "  for i in img.take(250):\r\n",
      "    yield [i]\r\n",
      "\r\n",
      "converter.representative_dataset = representative_data_gen\r\n",
      "converter.optimizations = [tf.lite.Optimize.DEFAULT] # size and latency\r\n",
      "converter.inference_input_type = tf.uint8\r\n",
      "converter.inference_output_type = tf.uint8\r\n",
      "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8, tf.lite.OpsSet.TFLITE_BUILTINS]\r\n",
      "converter.target_spec.supported_types = [tf.int8]\r\n",
      "tflite_quant_model = converter.convert()\r\n",
      "\r\n",
      "#Save\r\n",
      "tflite_models_dir = pathlib.Path(\"/filepath/quantized_export_tflite_uint8\")\r\n",
      "tflite_models_dir.mkdir(exist_ok=True, parents=True)\r\n",
      "tflite_model_quant_file = tflite_models_dir/\"quantized_model.tflite\"\r\n",
      "tflite_model_quant_file.write_bytes(tflite_quant_model)\r\n",
      "```\r\n",
      "\r\n",
      "**The output from the converter invocation**\r\n",
      "\r\n",
      "```\r\n",
      "# Compiles tflite model successfully.\r\n",
      "```\r\n",
      "\r\n",
      "**Failure details**\r\n",
      "\r\n",
      "The Tensorflow model used is a custom trained version of the MobileNet SSD V2 model using the Tensorflow Object Detection API (TF V2). This model is to be deployed on the Google Coral Dev board, and as such requires full integer quantisation (falling back to float32 for the final custom op) when converting to TFLite format. To export the model after training, export_tflite_graph_tf2.py is used (https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tf2.md) , before the quantization step is carried out. Once the post training quantization is complete, the resulting converted .tflite model runs successfully, however upon evaluation (using coco metrics), suffers a severe drop (>50% decrease) in performance compared to the original model, which achieves very high precision/recall when evaluated in the same manner. \r\n",
      "\r\n",
      "The code used to generate inference is below:\r\n",
      "\r\n",
      "```\r\n",
      "# Load TFLite model and allocate tensors.\r\n",
      "interpreter = tf.lite.Interpreter(model_path=\"quantized_model.tflite\")\r\n",
      "interpreter.allocate_tensors()\r\n",
      "\r\n",
      "input_details = interpreter.get_input_details()\r\n",
      "output_details = interpreter.get_output_details()\r\n",
      "width = input_details[0]['shape'][2]\r\n",
      "height = input_details[0]['shape'][1]\r\n",
      "\r\n",
      "# Load image\r\n",
      "img = cv2.imread(image)\r\n",
      "(H, W) = img.shape[:2] # Used to multiply normalized output bounding box co-ords to return to image coords\r\n",
      "img = cv2.cvtColor(img.copy(), cv2.COLOR_BGR2RGB)\r\n",
      "img = cv2.resize(img, (width, height))\r\n",
      "\r\n",
      "if input_details[0][\"dtype\"] == np.uint8:\r\n",
      "        input_scale, input_zero_point = input_details[0][\"quantization\"]\r\n",
      "        img = img / input_scale + input_zero_point\r\n",
      "\r\n",
      "img = np.expand_dims(img, axis=0)\r\n",
      "img = np.uint8(img)\r\n",
      "\r\n",
      "# Generate inference\r\n",
      "interpreter.set_tensor(input_details[0]['index'], img)\r\n",
      "interpreter.invoke()\r\n",
      "\r\n",
      "# Output\r\n",
      "boxes = interpreter.get_tensor(output_details[0]['index'])[0]\r\n",
      "classes = interpreter.get_tensor(output_details[1]['index'])[0]\r\n",
      "scores = interpreter.get_tensor(output_details[2]['index'])[0]\r\n",
      "\r\n",
      "```\n",
      "issue labels - \n",
      "ModelOptimizationToolkit\n",
      "TF 2.5\n",
      "comp:lite\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Model's allocation of node fails after conversion to TFLite\n",
      "issue body -  **System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Amazon Linux 2\r\n",
      "- TensorFlow installed from (source or binary): pip\r\n",
      "- TensorFlow version (or github SHA if from source): tf-nightly (2.5.0)\r\n",
      "\r\n",
      "\r\n",
      "**Command used to run the converter or code if you’re using the Python API**\r\n",
      "If possible, please share a link to Colab/Jupyter/any notebook.\r\n",
      "\r\n",
      "```\r\n",
      "  model_concrete_function = model.inference_decode.get_concrete_function()\r\n",
      "  converter = tf.lite.TFLiteConverter.from_concrete_functions(\r\n",
      "      [model_concrete_function]\r\n",
      "  )\r\n",
      "  converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\r\n",
      "  \r\n",
      "  if args.quantized:\r\n",
      "      converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n",
      "      output_file_name = os.path.join(args.outdir, 'model_quant.tflite')\r\n",
      "  else:\r\n",
      "      output_file_name = os.path.join(args.outdir, 'model.tflite')\r\n",
      "  \r\n",
      "  tflite_model = converter.convert()\r\n",
      "```\r\n",
      "\r\n",
      "**The output from the converter invocation**\r\n",
      "\r\n",
      "```\r\n",
      "2021-01-26 09:30:17.656931: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\r\n",
      "2021-01-26 09:30:17.656958: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n",
      "/home/dmmatwic/anaconda3/envs/tflite_x86/lib/python3.8/site-packages/tensorflow_addons/utils/ensure_tf_install.py:37: UserWarning: You are currently using a nightly version of TensorFlow (2.5.0-dev20210125).\r\n",
      "TensorFlow Addons offers no support for the nightly versions of TensorFlow. Some things might work, some other might not.\r\n",
      "If you encounter a bug, do not file an issue on GitHub.\r\n",
      "warnings.warn(\r\n",
      "2021-01-26 09:30:20.220470: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\r\n",
      "2021-01-26 09:30:20.220498: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\r\n",
      "2021-01-26 09:30:20.220517: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (dev-dsk-dmmatwic-1b-a5a0da5a.eu-west-1.amazon.com): /proc/driver/nvidia/version does not exist\r\n",
      "2021-01-26 09:30:20.220718: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA\r\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n",
      "WARNING:tensorflow:From /home/dmmatwic/anaconda3/envs/tflite_x86/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:5039: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "The validate_indices argument has no effect. Indices are always validated on CPU and never validated on GPU.\r\n",
      "2021-01-26 09:30:22,167 (deprecation:528) WARNING: From /home/dmmatwic/anaconda3/envs/tflite_x86/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:5039: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "The validate_indices argument has no effect. Indices are always validated on CPU and never validated on GPU.\r\n",
      "2021-01-26 09:30:22.977251: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\r\n",
      "2021-01-26 09:30:22.977359: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n",
      "2021-01-26 09:30:22.995718: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2500000000 Hz\r\n",
      "2021-01-26 09:30:23.040599: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:935] Optimization results for grappler item: graph_to_optimize\r\n",
      "function_optimizer: Graph size after: 900 nodes (122), 1564 edges (136), time = 10.878ms.\r\n",
      "function_optimizer: Graph size after: 900 nodes (0), 1564 edges (0), time = 9.301ms.\r\n",
      "Optimization results for grappler item: while_body_3883\r\n",
      "function_optimizer: function_optimizer did nothing. time = 0.006ms.\r\n",
      "function_optimizer: function_optimizer did nothing. time = 0.002ms.\r\n",
      "Optimization results for grappler item: while_body_4324\r\n",
      "function_optimizer: function_optimizer did nothing. time = 0.004ms.\r\n",
      "function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n",
      "Optimization results for grappler item: while_cond_3882\r\n",
      "function_optimizer: function_optimizer did nothing. time = 0.004ms.\r\n",
      "function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n",
      "Optimization results for grappler item: while_cond_4323\r\n",
      "function_optimizer: function_optimizer did nothing. time = 0.003ms.\r\n",
      "function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n",
      "2021-01-26 09:30:24,125 (lite:659) INFO: Using new converter: If you encounter a problem please file a bug. You can opt-out by setting experimental_new_converter=False\r\n",
      "2021-01-26 09:30:24.216881: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:332] Ignored output_format.\r\n",
      "2021-01-26 09:30:24.216915: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:335] Ignored drop_control_dependency.\r\n",
      "2021-01-26 09:30:24.305204: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var MLIR_CRASH_REPRODUCER_DIRECTORY to enable.\r\n",
      "2021-01-26 09:30:25.025058: I tensorflow/lite/tools/optimize/quantize_weights.cc:233] Skipping quantization of tensor arg5 because it has no allocated buffer.\r\n",
      "2021-01-26 09:30:25.026738: I tensorflow/lite/tools/optimize/quantize_weights.cc:233] Skipping quantization of tensor arg5 because it has no allocated buffer.\r\n",
      "2021-01-26 09:30:25,077 (convert_model:102) INFO: Model of size 10.273849 MBs saved to model_tflite/model_quant.tflite\r\n",
      "```\r\n",
      "\r\n",
      "**Also, please include a link to the saved model or GraphDef**\r\n",
      "\r\n",
      "```\r\n",
      "No link because it's internal company's model\r\n",
      "```\r\n",
      "\r\n",
      "**Failure details**\r\n",
      "If the conversion is successful, but the generated model is wrong,\r\n",
      "state what is wrong:\r\n",
      " I get\r\n",
      "```\r\n",
      "ERROR: tensorflow/lite/kernels/kernel_util.cc:404 d1 == d2 || d1 == 1 || d2 == 1 was not true.\r\n",
      "ERROR: Node number 3 (ADD) failed to prepare.\r\n",
      "```\r\n",
      "when running\r\n",
      "\r\n",
      "\r\n",
      "**TfLiteInterpreterAllocateTensors(interpreter)**\r\n",
      "\r\n",
      "in TFLite C++ API.\r\n",
      "The conversion to tflite model runs fine.\r\n",
      "\r\n",
      "The node looks like that in Netron\r\n",
      "\r\n",
      "![add_node](https://user-images.githubusercontent.com/32575801/105830725-268efb00-5fc6-11eb-804f-18d16651d07d.png)\r\n",
      "\r\n",
      "\r\n",
      "I'm not sure what exactly does this error mean. As far as I can see these conditions are met, with [1, 128] and [1, 128, 128] d1 is in fact equal to d2, while d1 and d2 also being 1? Unless I misunderstood how it works. This happens during tensors allocation, so I understand that this has nothing to do with input data shapes? Thanks :)\n",
      "issue labels - \n",
      "TF 2.5\n",
      "comp:lite\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  get \"Tensor(\"args_0:0\", shape=(), dtype=string)\" when using tf.data.TextLineDataset.map()\n",
      "issue body -  <em>Please make sure that this is a bug. As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:bug_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux version 3.16.0-7-amd64 (debian-kernel@lists.debian.org) (gcc version 4.9.2 (Debian 4.9.2-10+deb8u1) ) #1 SMP Debian 3.16.59-1 (2018-10-03)\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "- TensorFlow installed from (source or binary): pip\r\n",
      "- TensorFlow version (use command below): unknown 1.15.3\r\n",
      "- Python version: 3.6\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version: no gpu used\r\n",
      "- GPU model and memory: no gpu used\r\n",
      "\r\n",
      "You can collect some of this information using our environment capture\r\n",
      "[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n",
      "You can also obtain the TensorFlow version with:\r\n",
      "1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n",
      "2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n",
      "\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "\r\n",
      "I am reading data from a hdfs path using TextLineDataset. I use `for` to iter over the dataset everything goes well but when it comes to `map`, all I get is `Tensor(\"args_0:0\", shape=(), dtype=string)` (I don'y know there it comes from). The issue can be reproduced with tf ver 2.4.1.\r\n",
      "\r\n",
      "Here is the code:\r\n",
      "\r\n",
      "```python\r\n",
      "path = 'hdfs://path-to-file'\r\n",
      "dataset = tf.data.Dataset.list_files(path)\r\n",
      "dataset = tf.data.TextLineDataset(dataset)\r\n",
      "for line in dataset:\r\n",
      "    print(line)\r\n",
      "```\r\n",
      "\r\n",
      "output using `for`:\r\n",
      "\r\n",
      "```\r\n",
      "...\r\n",
      "tf.Tensor(b'1\\t[1, 15, 1907, 190706, 19070605, 161, \"nan\", \"nan\", \"nan\", 2, 7, 37, \"nan\", \"nan\", 1, 0, \"nan\", \"nan\", 1819, 181903, 18190301, \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", 0, 1, 1, 1, 0, 0, \"201\", \"2.486379972076975\", \"1\", \"0\", \"0.0\", 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]', shape=(), dtype=string)\r\n",
      "tf.Tensor(b'0\\t[1, 13, 1909, 190911, 19091101, 195, \"nan\", \"nan\", \"nan\", 2, \"nan\", \"nan\", \"nan\", \"nan\", 1, 0, \"nan\", \"nan\", 1909, 190901, 19090101, \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]', shape=(), dtype=string)\r\n",
      "...\r\n",
      "```\r\n",
      "\r\n",
      "---\r\n",
      "\r\n",
      "```python\r\n",
      "def parse_line(line):\r\n",
      "    print(line)\r\n",
      "    return line\r\n",
      "\r\n",
      "path = 'hdfs://path-to-file'\r\n",
      "dataset = tf.data.Dataset.list_files(path)\r\n",
      "dataset = tf.data.TextLineDataset(dataset).map(lambda x: parse_line(x))\r\n",
      "```\r\n",
      "\r\n",
      "output using `map`: (this is all I get)\r\n",
      "\r\n",
      "```\r\n",
      "Tensor(\"args_0:0\", shape=(), dtype=string)\r\n",
      "```\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "\r\n",
      "map func should get element in the dataset rather than `Tensor(\"args_0:0\", shape=(), dtype=string)`\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "Provide a reproducible test case that is the bare minimum necessary to generate\r\n",
      "the problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n",
      "\r\n",
      "provided above\r\n",
      "\r\n",
      "**Other info / logs** Include any logs or source code that would be helpful to\r\n",
      "diagnose the problem. If including tracebacks, please include the full\r\n",
      "traceback. Large logs and files should be attached.\r\n",
      "\r\n",
      "no log\r\n",
      "\n",
      "issue labels - \n",
      "TF 1.15\n",
      "comp:data\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  how can i use Sequential to achieve the process of Forward propagation\n",
      "issue body -  I really want know how to use Sequential to achieve the process of Forward propagation,like sess,run\r\n",
      "\n",
      "issue labels - \n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  Port int8 and float versions of batch_to_space to TFLM\n",
      "issue body -  Commit 1 copies the TFLite operator into TFLM\r\n",
      "Commit 2 implements basic float, int8 and error checking tests along with float and int8 implementations of batch_to_space_nd\r\n",
      "\r\n",
      "This version requires that the flat size of input matches output, since TFLM does not support tensor resizing.\r\n",
      "\r\n",
      "https://github.com/tensorflow/tensorflow/issues/45693\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  genop: backport fix for template error\n",
      "issue body -  Rewrite a conditional in a template to be syntactically valid.\r\n",
      "\r\n",
      "PiperOrigin-RevId: 284055201\r\n",
      "Change-Id: Ie0129e7553857076e60e4d5d85e912a7893168d9\n",
      "issue labels - \n",
      "cla: yes\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Update lite/micro/tools/make/Makefile for exp.cc and exp_test.cc\n",
      "issue body -  PR6 for issue #45415.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "prtype:bugfix\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  tf.raw_ops.PopulationCount for uint32 not supported but documented\n",
      "issue body -  \r\n",
      "**System information**\r\n",
      "\r\n",
      "- OS Platform and Distribution:  (Intel Linux Ubuntu 20.04):\r\n",
      "- TensorFlow installed from (source or binary): python pip\r\n",
      "- TensorFlow version: '2.4.1'\r\n",
      "- Python version: Python 3.8.6\r\n",
      "- CPU\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "tf.raw_ops.PopulationCount of array with type uint32 fails.\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "In api documentation for raw_ops.PopulationCount for Arg x: \r\n",
      "A Tensor. Must be one of the following types: int8, int16, int32, int64, uint8, uint16, uint32, uint64. \r\n",
      "\r\n",
      "So this is either a documentation error or more likely a bug, because feature is important on uint32.\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "a = numpy.array([3], dtype = numpy,uint32)\r\n",
      "tf.raw_ops.PopulationCount(x=a)\r\n",
      "\r\n",
      "**Other info / logs** Include any logs or source code that would be helpful to\r\n",
      "diagnose the problem. If including tracebacks, please include the full\r\n",
      "traceback. Large logs and files should be attached.\r\n",
      "\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"<stdin>\", line 1, in <module>\r\n",
      "  File \"/home/rst/PYTHON/ng3py/lib/python3.8/site-packages/tensorflow/python/util/tf_export.py\", line 404, in wrapper\r\n",
      "    return f(**kwargs)\r\n",
      "  File \"/home/rst/PYTHON/ng3py/lib/python3.8/site-packages/tensorflow/python/ops/gen_bitwise_ops.py\", line 547, in population_count\r\n",
      "    return population_count_eager_fallback(\r\n",
      "  File \"/home/rst/PYTHON/ng3py/lib/python3.8/site-packages/tensorflow/python/ops/gen_bitwise_ops.py\", line 570, in population_count_eager_fallback\r\n",
      "    _result = _execute.execute(b\"PopulationCount\", 1, inputs=_inputs_flat,\r\n",
      "  File \"/home/rst/PYTHON/ng3py/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\r\n",
      "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n",
      "tensorflow.python.framework.errors_impl.NotFoundError: Could not find device for node: {{node PopulationCount}} = PopulationCount[T=DT_UINT32]\r\n",
      "All kernels registered for op PopulationCount:\r\n",
      "  device='GPU'; T in [DT_INT64]\r\n",
      "  device='GPU'; T in [DT_INT32]\r\n",
      "  device='GPU'; T in [DT_INT16]\r\n",
      "  device='GPU'; T in [DT_UINT16]\r\n",
      "  device='GPU'; T in [DT_INT8]\r\n",
      "  device='GPU'; T in [DT_UINT8]\r\n",
      "  device='CPU'; T in [DT_INT64]\r\n",
      "  device='CPU'; T in [DT_INT32]\r\n",
      "  device='CPU'; T in [DT_INT16]\r\n",
      "  device='CPU'; T in [DT_UINT16]\r\n",
      "  device='CPU'; T in [DT_INT8]\r\n",
      "  device='CPU'; T in [DT_UINT8]\r\n",
      " [Op:PopulationCount]\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:ops\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  [TFTRT - Dynamic Shape Phase 3] Add Dynamic Shape Testing for ConvertAddN and DebugString templated for nested numerical vectors\n",
      "issue body -  @bixia1 @tfeher for review\r\n",
      "\r\n",
      "Feature Tracker: #45481\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:gpu:tensorrt\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  [Tensorflow Lite] Build static framework for iOS\n",
      "issue body -  \r\n",
      "**System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS Catalina\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "- TensorFlow installed from (source or binary): source\r\n",
      "- TensorFlow version: 2.5 (nightly)\r\n",
      "- Python version: N/A\r\n",
      "- Installed using virtualenv? pip? conda?: N/A\r\n",
      "- Bazel version (if compiling from source): 3.7\r\n",
      "- GCC/Compiler version (if compiling from source): Xcode 12.3\r\n",
      "- CUDA/cuDNN version: N/A\r\n",
      "- GPU model and memory: N/A\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "I am trying to cross compile Tensorflow Lite for iOS. I would like to build a **static** framework. Here is the build command I used:\r\n",
      "\r\n",
      "`bazel build --config=ios_arm64 -c opt //tensorflow/lite/ios:TensorFlowLiteC_framework`\r\n",
      "\r\n",
      "The `TensorFlowLiteC_framework` target is defined in `tensorflow/tensorflow/lite/ios/BUILD.apple` as \r\n",
      "\r\n",
      "```\r\n",
      "tflite_ios_static_framework(\r\n",
      "    name = \"TensorFlowLiteC_framework\",\r\n",
      "    hdrs = [\r\n",
      "        \":c_api.h\",\r\n",
      "        \":common.h\",\r\n",
      "        \":xnnpack_delegate.h\",\r\n",
      "        \"//tensorflow/lite/c:c_api_types.h\",\r\n",
      "    ],\r\n",
      "    allowlist_symbols_file = \":allowlist_TensorFlowLiteC.txt\",\r\n",
      "    bundle_name = \"TensorFlowLiteC\",\r\n",
      "    minimum_os_version = TFL_MINIMUM_OS_VERSION,\r\n",
      "    deps = [\r\n",
      "        \":tensorflow_lite_c\",\r\n",
      "    ],\r\n",
      ")\r\n",
      "```\r\n",
      "\r\n",
      "I had expected the resulting framework (please see the file attached below) to be a static framework, but it appears to be a dynamic framework instead. Inside the `TensorFlowLiteC.framework` folder, there is a binary file `TensorFlowLiteC`. If I do `file TensorFlowLiteC`, I get:\r\n",
      "\r\n",
      "```\r\n",
      "TensorFlowLiteC: Mach-O universal binary with 1 architecture: [arm64:Mach-O 64-bit object arm64]\r\n",
      "TensorFlowLiteC (for architecture arm64):\tMach-O 64-bit object arm64\r\n",
      "```\r\n",
      "This appears to be a dynamic lib file to me. As far as I know, if this was a static archive, I should have gotten: `current ar archive`.\r\n",
      "\r\n",
      "[TensorFlowLiteC.framework.zip](https://github.com/tensorflow/tensorflow/files/5870061/TensorFlowLiteC.framework.zip)\r\n",
      "\r\n",
      "Is there a way to actually build Tensorflow Lite into an actual static framework for iOS?\r\n",
      "\r\n",
      "**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n",
      "\r\n",
      "`bazel build --config=ios_arm64 -c opt //tensorflow/lite/ios:TensorFlowLiteC_framework`\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.5\n",
      "comp:lite\n",
      "stat:awaiting response\n",
      "subtype:macOS\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  Tensorflow does not work with RTX 3070 on Windows\n",
      "issue body -  **System information**\r\n",
      "- Code attached below\r\n",
      "- OS: Windows 10\r\n",
      "- TensorFlow installed from binary (`pip3 install tensorflow`)\r\n",
      "- TensorFlow version: tried latest stable v2.4.0-49-g85c8b2a817f 2.4.1\r\n",
      "- Python 3.6.8 (tags/v3.6.8:3c6b436a57, Dec 24 2018, 00:16:47) [MSC v.1916 64 bit (AMD64)] on win32\r\n",
      "- CUDA/cuDNN version: cuda_11.2.0_460.89_win10\\cudnn-11.1-v8.0.5.39\r\n",
      "- GPU drivers: 460.89\r\n",
      "- GPU model and memory: seems to be recognized correctly by TF- GeForce RTX 3070 computeCapability: 8.6 coreClock: 1.725GHz coreCount: 46 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "Getting error:\r\n",
      "```\r\n",
      "2021-01-25 21:36:01.042433: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n",
      "Epoch 1/500\r\n",
      "2021-01-25 21:36:03.304809: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n",
      "2021-01-25 21:36:03.880223: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n",
      "2021-01-25 21:36:03.911531: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n",
      "2021-01-25 21:36:04.515409: E tensorflow/stream_executor/cuda/cuda_dnn.cc:336] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\r\n",
      "2021-01-25 21:36:04.515498: E tensorflow/stream_executor/cuda/cuda_dnn.cc:340] Error retrieving driver version: Unimplemented: kernel reported driver version not implemented on Windows\r\n",
      "2021-01-25 21:36:04.515607: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at cudnn_rnn_ops.cc:1514 : Unknown: Fail to find the dnn implementation.\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"<input>\", line 1, in <module>\r\n",
      "  File \"C:\\Users\\Aleksander\\.IntelliJIdea2018.3\\config\\plugins\\python\\helpers\\pydev\\_pydev_bundle\\pydev_umd.py\", line 197, in runfile\r\n",
      "    pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script\r\n",
      "  File \"C:\\Users\\Aleksander\\.IntelliJIdea2018.3\\config\\plugins\\python\\helpers\\pydev\\_pydev_imps\\_pydev_execfile.py\", line 18, in execfile\r\n",
      "    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\r\n",
      "  File \"C:/Workspace_GpwScan/dnn/sandbox/reproduce_issue.py\", line 110, in <module>\r\n",
      "    callbacks=[checkpoint, tensorboard])\r\n",
      "  File \"C:\\Workspace_GpwScan\\stubs\\tensorflow\\python\\keras\\engine\\training.py\", line 1100, in fit\r\n",
      "    tmp_logs = self.train_function(iterator)\r\n",
      "  File \"C:\\Workspace_GpwScan\\stubs\\tensorflow\\python\\eager\\def_function.py\", line 828, in __call__\r\n",
      "    result = self._call(*args, **kwds)\r\n",
      "  File \"C:\\Workspace_GpwScan\\stubs\\tensorflow\\python\\eager\\def_function.py\", line 888, in _call\r\n",
      "    return self._stateless_fn(*args, **kwds)\r\n",
      "  File \"C:\\Workspace_GpwScan\\stubs\\tensorflow\\python\\eager\\function.py\", line 2943, in __call__\r\n",
      "    filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\r\n",
      "  File \"C:\\Workspace_GpwScan\\stubs\\tensorflow\\python\\eager\\function.py\", line 1919, in _call_flat\r\n",
      "    ctx, args, cancellation_manager=cancellation_manager))\r\n",
      "  File \"C:\\Workspace_GpwScan\\stubs\\tensorflow\\python\\eager\\function.py\", line 560, in call\r\n",
      "    ctx=ctx)\r\n",
      "  File \"C:\\Workspace_GpwScan\\stubs\\tensorflow\\python\\eager\\execute.py\", line 60, in quick_execute\r\n",
      "    inputs, attrs, num_outputs)\r\n",
      "tensorflow.python.framework.errors_impl.UnknownError:    Fail to find the dnn implementation.\r\n",
      "\t [[{{node CudnnRNN}}]]\r\n",
      "\t [[sequential/lstm/PartitionedCall]] [Op:__inference_train_function_8782]\r\n",
      "Function call stack:\r\n",
      "train_function -> train_function -> train_function\r\n",
      "```\r\n",
      "[tf_2.4.1_issue_on_3070.txt](https://github.com/tensorflow/tensorflow/files/5869537/tf_2.4.1_issue_on_3070.txt)\r\n",
      "\r\n",
      "\r\n",
      "Tried also with latest ```nightly 2.5.0.dev20210125``` ending up with error:\r\n",
      "```\r\n",
      "2021-01-25 21:31:05.429799: E tensorflow/stream_executor/dnn.cc:618] CUDNN_STATUS_EXECUTION_FAILED\r\n",
      "in tensorflow/stream_executor/cuda/cuda_dnn.cc(1975): 'cudnnRNNBackwardData( cudnn.handle(), rnn_desc.handle(), model_dims.max_seq_length, output_desc.handles(), output_data.opaque(), output_desc.handles(), output_backprop_data.opaque(), output_h_desc.handle(), output_h_backprop_data.opaque(), output_c_desc.handle(), output_c_backprop_data.opaque(), rnn_desc.params_handle(), params.opaque(), input_h_desc.handle(), input_h_data.opaque(), input_c_desc.handle(), input_c_data.opaque(), input_desc.handles(), input_backprop_data->opaque(), input_h_desc.handle(), input_h_backprop_data->opaque(), input_c_desc.handle(), input_c_backprop_data->opaque(), workspace.opaque(), workspace.size(), reserve_space_data->opaque(), reserve_space_data->size())'\r\n",
      "2021-01-25 21:31:05.430291: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at cudnn_rnn_ops.cc:1926 : Internal: Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 1, 128, 1, 128, 256, 128] \r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"<input>\", line 1, in <module>\r\n",
      "  File \"C:\\Users\\Aleksander\\.IntelliJIdea2018.3\\config\\plugins\\python\\helpers\\pydev\\_pydev_bundle\\pydev_umd.py\", line 197, in runfile\r\n",
      "    pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script\r\n",
      "  File \"C:\\Users\\Aleksander\\.IntelliJIdea2018.3\\config\\plugins\\python\\helpers\\pydev\\_pydev_imps\\_pydev_execfile.py\", line 18, in execfile\r\n",
      "    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\r\n",
      "  File \"C:/Workspace_GpwScan/dnn/sandbox/reproduce_issue.py\", line 108, in <module>\r\n",
      "    callbacks=[checkpoint, tensorboard])\r\n",
      "  File \"C:\\Workspace_GpwScan\\stubs\\tensorflow\\python\\keras\\engine\\training.py\", line 1134, in fit\r\n",
      "    tmp_logs = self.train_function(iterator)\r\n",
      "  File \"C:\\Workspace_GpwScan\\stubs\\tensorflow\\python\\eager\\def_function.py\", line 818, in __call__\r\n",
      "    result = self._call(*args, **kwds)\r\n",
      "  File \"C:\\Workspace_GpwScan\\stubs\\tensorflow\\python\\eager\\def_function.py\", line 846, in _call\r\n",
      "    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\r\n",
      "  File \"C:\\Workspace_GpwScan\\stubs\\tensorflow\\python\\eager\\function.py\", line 2994, in __call__\r\n",
      "    filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\r\n",
      "  File \"C:\\Workspace_GpwScan\\stubs\\tensorflow\\python\\eager\\function.py\", line 1939, in _call_flat\r\n",
      "    ctx, args, cancellation_manager=cancellation_manager))\r\n",
      "  File \"C:\\Workspace_GpwScan\\stubs\\tensorflow\\python\\eager\\function.py\", line 569, in call\r\n",
      "    ctx=ctx)\r\n",
      "  File \"C:\\Workspace_GpwScan\\stubs\\tensorflow\\python\\eager\\execute.py\", line 60, in quick_execute\r\n",
      "    inputs, attrs, num_outputs)\r\n",
      "tensorflow.python.framework.errors_impl.InternalError:    Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 1, 128, 1, 128, 256, 128] \r\n",
      "\t [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]\r\n",
      "\t [[Adam/gradients/PartitionedCall_2]] [Op:__inference_train_function_8936]\r\n",
      "Function call stack:\r\n",
      "train_function -> train_function -> train_function\r\n",
      "```\r\n",
      "[tf_nightly_issue_on_3070.txt](https://github.com/tensorflow/tensorflow/files/5869438/tf_nightly_issue_on_3070.txt)\r\n",
      "\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "The script was working on my old gtx 980 and CUDA 10.\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "```\r\n",
      "import datetime\r\n",
      "import os\r\n",
      "\r\n",
      "import pandas as pd\r\n",
      "from numpy import reshape\r\n",
      "\r\n",
      "import tensorflow as tf\r\n",
      "\r\n",
      "EPOCHS = 500\r\n",
      "BATCH_SIZE = 256\r\n",
      "TEST_SET_RATIO = 0.2\r\n",
      "\r\n",
      "LEARNING_RATE = 0.001\r\n",
      "DECAY = 3e-5\r\n",
      "LOSS_FUNC = 'categorical_crossentropy'\r\n",
      "DROPOUT = 0.2\r\n",
      "OUTPUT_PATH = \"e:\\\\ml\"\r\n",
      "\r\n",
      "RNN_SEQ_LEN = 128  # number of RNN/LSTM sequence features\r\n",
      "L_AMOUNT = 2  # number of labels\r\n",
      "\r\n",
      "MIN_ACC_TO_SAVE_MODEL = 0.6\r\n",
      "\r\n",
      "\r\n",
      "def create_model():\r\n",
      "    new_model = tf.keras.models.Sequential()\r\n",
      "\r\n",
      "    # NETWORK INPUT\r\n",
      "    new_model.add(tf.keras.layers.LSTM(RNN_SEQ_LEN, input_shape=TR_FEATURES.shape[1:], return_sequences=True))\r\n",
      "    new_model.add(tf.keras.layers.Dropout(DROPOUT))\r\n",
      "    new_model.add(tf.keras.layers.BatchNormalization())\r\n",
      "\r\n",
      "    new_model.add(tf.keras.layers.LSTM(RNN_SEQ_LEN, return_sequences=True))\r\n",
      "    new_model.add(tf.keras.layers.Dropout(DROPOUT / 2))\r\n",
      "    new_model.add(tf.keras.layers.BatchNormalization())\r\n",
      "\r\n",
      "    new_model.add(tf.keras.layers.LSTM(RNN_SEQ_LEN))\r\n",
      "    new_model.add(tf.keras.layers.Dropout(DROPOUT))\r\n",
      "    new_model.add(tf.keras.layers.BatchNormalization())\r\n",
      "\r\n",
      "    # NETWORK OUTPUT\r\n",
      "    new_model.add(tf.keras.layers.Dense(L_AMOUNT, activation=tf.keras.activations.softmax))\r\n",
      "\r\n",
      "    opt = tf.keras.optimizers.Adam(LEARNING_RATE, decay=DECAY)\r\n",
      "    new_model.compile(optimizer=opt,\r\n",
      "                      loss=LOSS_FUNC,\r\n",
      "                      metrics=['accuracy'])\r\n",
      "\r\n",
      "    print(new_model.summary())\r\n",
      "    return new_model\r\n",
      "\r\n",
      "\r\n",
      "class CustomModelCheckpoint(tf.keras.callbacks.ModelCheckpoint):\r\n",
      "    def __init__(self, fp, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', save_freq='epoch', **kwargs):\r\n",
      "        super().__init__(fp, monitor, verbose, save_best_only, save_weights_only, mode, save_freq, **kwargs)\r\n",
      "\r\n",
      "    def on_epoch_end(self, epoch, logs=None):\r\n",
      "        print(\"\\n-------------------------------------------------------------------------------------------------------\")\r\n",
      "        print(f\"epoch: {epoch}, training_acc: {round(float(logs['accuracy']), 4)}, validation_acc: {round(float(logs['val_accuracy']), 4)}\")\r\n",
      "        print(\"-------------------------------------------------------------------------------------------------------\\n\")\r\n",
      "\r\n",
      "        if MIN_ACC_TO_SAVE_MODEL <= logs['accuracy']:\r\n",
      "            super().on_epoch_end(epoch, logs)\r\n",
      "\r\n",
      "\r\n",
      "if __name__ == '__main__':\r\n",
      "    data_filename = 'input_data.csv'\r\n",
      "    print(\"Loading data file: %s\" % data_filename)\r\n",
      "    dataset = pd.read_csv(data_filename, delimiter=',', header=None)\r\n",
      "    dataset = dataset.drop(columns=[0, 1, 2, 3, 4, 5, 6]).values  # drop columns with additional information\r\n",
      "\r\n",
      "    test_set_size = int(len(dataset) * TEST_SET_RATIO)\r\n",
      "    print(\"Test set split at: %d\" % test_set_size)\r\n",
      "\r\n",
      "    train_data = dataset[:-test_set_size]\r\n",
      "    test_data = dataset[-test_set_size:]  # use most recent data for validation (extract before shuffle)\r\n",
      "\r\n",
      "    TR_F = train_data[:, 0:RNN_SEQ_LEN]\r\n",
      "    TS_F = test_data[:, 0:RNN_SEQ_LEN]\r\n",
      "\r\n",
      "    TR_L = train_data[:, RNN_SEQ_LEN:RNN_SEQ_LEN + L_AMOUNT]\r\n",
      "    TS_L = test_data[:, RNN_SEQ_LEN:RNN_SEQ_LEN + L_AMOUNT]\r\n",
      "\r\n",
      "    TR_FEATURES = reshape(TR_F, (len(TR_F), RNN_SEQ_LEN, 1))\r\n",
      "    TS_FEATURES = reshape(TS_F, (len(TS_F), RNN_SEQ_LEN, 1))\r\n",
      "\r\n",
      "    model = create_model()\r\n",
      "\r\n",
      "    TRAINING_TIMESTAMP = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\r\n",
      "    model_name = \"sscce_%s\" % TRAINING_TIMESTAMP\r\n",
      "    os.mkdir(\"%s\\\\models\\\\%s\" % (OUTPUT_PATH, model_name))\r\n",
      "    filepath = \"%s\\\\models\\\\%s\\\\%s--{epoch:02d}-{val_accuracy:.3f}.model\" % (OUTPUT_PATH, model_name, model_name)\r\n",
      "    checkpoint = CustomModelCheckpoint(filepath,\r\n",
      "                                       monitor='val_accuracy',\r\n",
      "                                       verbose=1,\r\n",
      "                                       save_best_only=True,\r\n",
      "                                       mode='max')\r\n",
      "\r\n",
      "    log_dir = \"%s\\\\logs\\\\fit\\\\%s.model\" % (OUTPUT_PATH, model_name)\r\n",
      "    tensorboard = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, profile_batch=0)\r\n",
      "\r\n",
      "    model.fit(x=TR_FEATURES,\r\n",
      "              y=TR_L,\r\n",
      "              epochs=EPOCHS,\r\n",
      "              batch_size=BATCH_SIZE,\r\n",
      "              shuffle=True,\r\n",
      "              validation_data=(TS_FEATURES, TS_L),\r\n",
      "              callbacks=[checkpoint, tensorboard])\r\n",
      "```\r\n",
      "\r\n",
      "DATA FILE SAMPLE: [input_data.zip](https://github.com/tensorflow/tensorflow/files/5869582/input_data.zip)\r\n",
      " \r\n",
      "**Other info / logs**\r\n",
      "\r\n",
      "Providing also path to CUDA 11.0 installation because without it getting errors like:\r\n",
      "\r\n",
      "```\r\n",
      "2021-01-25 21:44:15.989317: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cusolver64_10.dll'; dlerror: cusolver64_10.dll not found\r\n",
      "```\r\n",
      "\r\n",
      "Full win sys PATH:\r\n",
      "\r\n",
      "```\r\n",
      "Path=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.2\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.2\\libnvvp;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.0\\bin;C:\\cudnn-11.1-v8.0.5.39\\bin;C:\\Python36\\Scripts\\;C:\\Python36;C:\\ProgramData\\DockerDesktop\\version-bin;C:\\Program Files\\Docker\\Docker\\Resources\\bin;c:\\Java\\jdk1.8.0_144_x86;C:\\gradle-6.0.1\\bin;C:\\SVN\\bin;C:\\MinGW\\bin;C:\\WinAVR-20100110\\;c:\\avrdude\\;c:\\Android\\sdk\\platform-tools;C:\\adb\\;C:\\TortoiseGit\\bin;C:\\Git4Windows\\cmd;c:\\sqlite-tools-win32-x86-3130000\\;C:\\WINDOWS\\System32;C:\\WINDOWS;C:\\WINDOWS\\System32\\wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\Program Files (x86)\\Bitvise SSH Client;C:\\Program Files (x86)\\Windows Live\\Shared;C:\\WINDOWS\\system32;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\OpenSSH\\;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\NVIDIA Corporation\\Nsight Compute 2020.3.0\\;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR\r\n",
      "```\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "I was trying different combinations of cuda/cudnn/tensorflow just for the sake of it but actually only `cuda_11.2.0_460.89_win10` comes with win nvidia GPU divers version high enough to support RTX 30xx series.\r\n",
      "Still - there is no `cudnn` build designated particularly for `CUDA 11.2` yet... Maybe this is an issue...\r\n",
      "\r\n",
      "Any idea how to make it working all together?\n",
      "issue labels - \n",
      "TF 2.4\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Fix in resource var AssignVariableOp Compute\n",
      "issue body -  Add check in AssignVariableOp in resource_var_ops.cc similar to XlaAssignVariableOp::Compute in xla_device_ops.cc\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:core\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Extract reference for operator BATCH_MATMUL to standalone header\n",
      "issue body -  Move the reference implementation to its own header so that micro\r\n",
      "can use it without the unrelated depedencies of reference_ops.h.\r\n",
      "\r\n",
      "PR step 2 for issue #46504\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:lite\n",
      "comp:micro\n",
      "prtype:bugfix\n",
      "ready to pull\n",
      "size:XL\n",
      "\n",
      "\n",
      "issue title -  Update vexriscv + zephyr makefiles.\n",
      "issue body -  Tested that the following commands work:\r\n",
      "```\r\n",
      "make -f tensorflow/lite/micro/tools/make/Makefile TARGET=zephyr_vexriscv OPTIMIZED_KERNEL_DIR=vexriscv third_party_downloads\r\n",
      "make -f tensorflow/lite/micro/tools/make/Makefile TARGET=zephyr_vexriscv OPTIMIZED_KERNEL_DIR=vexriscv microlite\r\n",
      "```\r\n",
      "\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  Extract a function for parsing operator BATCH_MATMUL\n",
      "issue body -  Extract the parsing out of a switch statement case to create a\r\n",
      "standalone function which can be called by the micro op resolver.\r\n",
      "\r\n",
      "PR step 1 for issue #46504\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:lite\n",
      "comp:micro\n",
      "prtype:bugfix\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  Updates to the TFLM Makfile that are not backwards compatible\n",
      "issue body -  **System information**\r\n",
      "\r\n",
      "-   OS Platform and Distribution: Linux Ubuntu 20.10 \r\n",
      "-   TensorFlow version: 2.4.1\r\n",
      "-   Python version: 3.8\r\n",
      "-   CPU Intel Core i7\r\n",
      "-   GPU: NVIDIA GeForce GTX\r\n",
      "-   STM32CubeIDE-Lnx: 1.3.0\r\n",
      "-   STM32 Nucleo-64 development board with STM32F401RE MCU\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "\r\n",
      "I would like to run the Makefile in the Tensorflow Lite for Microcontrollers directory in order to run inference on ST microcontroller above. My problem is that I cannot use the \r\n",
      "TAGS =”portable_optimized” command line option /cc @advaitjain\r\n",
      "\r\n",
      "**Source code / logs**\r\n",
      "\r\n",
      "tensorflow/lite/micro/tools/make/Makefile:61: *** The TAGS command line option is no longer supported in the TFLM Makefile..\r\n",
      "\r\n",
      "Thanks in advance.\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:micro\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  neuro-symbolic\n",
      "issue body -  Please go to Stack Overflow for help and support:\r\n",
      "\r\n",
      "https://stackoverflow.com/questions/tagged/tensorflow\r\n",
      "\r\n",
      "If you open a GitHub issue, here is our policy:\r\n",
      "\r\n",
      "1.  It must be a bug, a feature request, or a significant problem with the\r\n",
      "    documentation (for small docs fixes please send a PR instead).\r\n",
      "2.  The form below must be filled out.\r\n",
      "3.  It shouldn't be a TensorBoard issue. Those go\r\n",
      "    [here](https://github.com/tensorflow/tensorboard/issues).\r\n",
      "\r\n",
      "**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n",
      "\r\n",
      "------------------------\r\n",
      "\r\n",
      "### System information\r\n",
      "\r\n",
      "-   **Have I written custom code (as opposed to using a stock example script\r\n",
      "    provided in TensorFlow)**:\r\n",
      "-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n",
      "-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n",
      "    happens on a mobile device**:\r\n",
      "-   **TensorFlow installed from (source or binary)**:\r\n",
      "-   **TensorFlow version (use command below)**:\r\n",
      "-   **Python version**:\r\n",
      "-   **Bazel version (if compiling from source)**:\r\n",
      "-   **GCC/Compiler version (if compiling from source)**:\r\n",
      "-   **CUDA/cuDNN version**:\r\n",
      "-   **GPU model and memory**:\r\n",
      "-   **Exact command to reproduce**:\r\n",
      "\r\n",
      "You can collect some of this information using our environment capture script:\r\n",
      "\r\n",
      "https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n",
      "\r\n",
      "You can obtain the TensorFlow version with:\r\n",
      "\r\n",
      "```bash\r\n",
      "python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n",
      "```\r\n",
      "\r\n",
      "### Describe the problem\r\n",
      "Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n",
      "\r\n",
      "### Source code / logs\r\n",
      "Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n",
      "\n",
      "issue labels - \n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  Tensorflow 2.4 Compile\n",
      "issue body -  <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "- TensorFlow installed from (source or binary):  with command  pip install tensorflow-gpu==2.4.0\r\n",
      "- TensorFlow version: 2.4.0\r\n",
      "- Python version: Python 3.6.9\r\n",
      "- Installed using virtualenv? pip? conda?: pip\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "- GCC/Compiler version (if compiling from source): \r\n",
      "- CUDA/cuDNN version: cuda :11.0,  cuDNN :  8.0.2\r\n",
      "- Driver Version: 450.102.04\r\n",
      "- GPU model and memory: Gigabyte Nvidia GForce 1660 6GB\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "\r\n",
      "I execute the basic command \r\n",
      "import tensorflow as tf\r\n",
      "print(tf.test.gpu_device_name())\r\n",
      "\r\n",
      "to see if tensorflow 2.4 is execute in my ubuntu i have all installed than we need for this version like cuda version, cuddn version and driver version. What maybe is the problem ? \r\n",
      "With tensorflow 2.3 compile successfully but he cannnot load the cuddn library and cuda.\r\n",
      "\r\n",
      "**Any other info / logs**\r\n",
      "From compiler i get this message : \r\n",
      "2021-01-25 16:27:37.386741: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"test.py\", line 1, in <module>\r\n",
      "    import tensorflow as tf\r\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py\", line 435, in <module>\r\n",
      "    _ll.load_library(_main_dir)\r\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/load_library.py\", line 153, in load_library\r\n",
      "    py_tf.TF_LoadLibrary(lib)\r\n",
      "tensorflow.python.framework.errors_impl.NotFoundError: /home/antreas/.local/lib/python3.6/site-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow8OpKernel11TraceStringEPNS_15OpKernelContextEb\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "stat:awaiting response\n",
      "subtype: ubuntu/linux\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  Breaking change of TFLite model definition in TensorFlow 2.4.0: OperatorCode.BuiltinCode\n",
      "issue body -  (source issue: https://github.com/jackwish/tflite/issues/9)\r\n",
      "\r\n",
      "**This change breaks software stacks that depend on the built TFLite model parser, e.g. [tvm](https://github.com/apache/tvm), [tflite2onnx](https://github.com/jackwish/tflite2onnx), and etc.**\r\n",
      "\r\n",
      "Starting from [TensorFlow 2.4.0](https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/lite/schema/schema.fbs#L220), the `BuiltinOperator` switches to `int32` from `byte`. As a result, the `OperatorCode.builtin_code` is now an `int32` too, and code like `op_code.BuiltinCode()` is broken. \r\n",
      "\r\n",
      "```\r\n",
      "// An OperatorCode can be an enum value (BuiltinOperator) if the operator is a\r\n",
      "// builtin, or a string if the operator is custom.\r\n",
      "table OperatorCode {\r\n",
      "  // This field is for backward compatibility. This field will be used when\r\n",
      "  // the value of the extended builtin_code field has less than\r\n",
      "  // BulitinOperator_PLACEHOLDER_FOR_GREATER_OP_CODES.\r\n",
      "  deprecated_builtin_code:byte;\r\n",
      "  custom_code:string;\r\n",
      "\r\n",
      "  // The version of the operator. The version need to be bumped whenever new\r\n",
      "  // parameters are introduced into an op.\r\n",
      "  version:int = 1;\r\n",
      "\r\n",
      "  // This field is introduced for resolving op builtin code shortage problem\r\n",
      "  // (the original BuiltinOperator enum field was represented as a byte).\r\n",
      "  // This field will be used when the value of the extended builtin_code field\r\n",
      "  // has greater than BulitinOperator_PLACEHOLDER_FOR_GREATER_OP_CODES.\r\n",
      "  builtin_code:BuiltinOperator;\r\n",
      "}\r\n",
      "``` \r\n",
      "\r\n",
      "For any code that uses `op_code.BuiltinCode()`, a workaround like the below (or [this PR](https://github.com/jackwish/tflite/pull/10/files)) is needed.\r\n",
      "\r\n",
      "```py\r\n",
      "\r\n",
      "if op_code.BuiltinCode() < BuiltinOperator.PLACEHOLDER_FOR_GREATER_OP_CODES:\r\n",
      "    opc = op_code.DeprecatedBuiltinCode()\r\n",
      "else:\r\n",
      "    opc = op_code.BuiltinCode()\r\n",
      "```\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:lite\n",
      "stat:awaiting tensorflower\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Reshape after conversion has two dynamic shapes, fails inference\n",
      "issue body -  **System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Amazon Linux 2\r\n",
      "- TensorFlow installed from (source or binary): With pip install tensorflow\r\n",
      "- TensorFlow version (or github SHA if from source): 2.4.0\r\n",
      "\r\n",
      "\r\n",
      "**Command used to run the converter or code if you’re using the Python API**\r\n",
      "\r\n",
      "    model_concrete_function = model.inference_encode.get_concrete_function()\r\n",
      "    converter = tf.lite.TFLiteConverter.from_concrete_functions(\r\n",
      "        [model_concrete_function]\r\n",
      "    )\r\n",
      "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\r\n",
      "\r\n",
      "    if args.quantized:\r\n",
      "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n",
      "        output_file_name = os.path.join(args.outdir, 'model_quant.tflite')\r\n",
      "    else:\r\n",
      "        output_file_name = os.path.join(args.outdir, 'model.tflite')\r\n",
      "\r\n",
      "    tflite_model = converter.convert()\r\n",
      "\r\n",
      "**The output from the converter invocation**\r\n",
      "\r\n",
      "```\r\n",
      "2021-01-25 12:34:22.930736: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\r\n",
      "2021-01-25 12:34:22.930757: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n",
      "2021-01-25 12:34:25.599105: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n",
      "2021-01-25 12:34:25.599281: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\r\n",
      "2021-01-25 12:34:25.599294: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\r\n",
      "2021-01-25 12:34:25.599313: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (dev-dsk-dmmatwic-1b-a5a0da5a.eu-west-1.amazon.com): /proc/driver/nvidia/version does not exist\r\n",
      "2021-01-25 12:34:25.599516: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\r\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n",
      "2021-01-25 12:34:25.600785: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n",
      "2021-01-25 12:34:28.607374: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\r\n",
      "2021-01-25 12:34:28.607500: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n",
      "2021-01-25 12:34:28.607690: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n",
      "2021-01-25 12:34:28.627718: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2500000000 Hz\r\n",
      "2021-01-25 12:34:28.634934: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:928] Optimization results for grappler item: graph_to_optimize\r\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.005ms.\r\n",
      "  function_optimizer: function_optimizer did nothing. time = 0ms.\r\n",
      "\r\n",
      "2021-01-25 12:34:29,117 (lite:619) INFO: Using new converter: If you encounter a problem please file a bug. You can opt-out by setting experimental_new_converter=False\r\n",
      "2021-01-25 12:34:29.139440: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:316] Ignored output_format.\r\n",
      "2021-01-25 12:34:29.139470: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:319] Ignored drop_control_dependency.\r\n",
      "2021-01-25 12:34:29.183040: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n",
      "2021-01-25 12:34:29,771 (convert:102) INFO: Model of size 6.231964 MBs saved to model_tflite/model.tflite\r\n",
      "```\r\n",
      "\r\n",
      "**Also, please include a link to the saved model or GraphDef**\r\n",
      "I can't include a model since it's company's internal model.\r\n",
      "\r\n",
      "**Failure details**\r\n",
      "If the conversion is successful, but the generated model is wrong,\r\n",
      "state what is wrong:\r\n",
      "\r\n",
      "Reshape nodes have shapes of [-1, -1, 128] instead of [1, -1, 128] and inference therefore fails\r\n",
      "**Here's a screenshot of this part from Netron, the selected node is the Reshape node** \r\n",
      "![Untitled 2](https://user-images.githubusercontent.com/32575801/105708210-61385b00-5f14-11eb-8e8d-4be25947043c.png)\r\n",
      "\r\n",
      "These nodes are part of tf.keras.layers.Conv1D, which is created with\r\n",
      "**tf.keras.layers.Conv1D(filters=128, kernel_size=3, padding=\"same\")**\r\n",
      "\r\n",
      "Inference log\r\n",
      "**ERROR: tensorflow/lite/kernels/reshape.cc:58 stretch_dim != -1 (0 != -1)\r\n",
      "ERROR: Node number 69 (RESHAPE) failed to prepare.**\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "TFLiteConverter\n",
      "comp:lite\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Third-party Tensorflow 2.4.0\n",
      "issue body -  Hello TensorFlow team,\r\n",
      "I'm looking for version 2.4.0 of Tensorflow for all used third-party with the version status. Is there a document that refers to all third-party versions and versions?\r\n",
      "Best regards,\r\n",
      "Fabian\n",
      "issue labels - \n",
      "TF 2.4\n",
      "stalled\n",
      "stat:awaiting response\n",
      "\n",
      "\n",
      "issue title -  Error in prediction:Invalid argument: Default MaxPoolingOp only supports NHWC on device type CPU for TensorFlow C++ API Reference\n",
      "issue body -  <em>Please make sure that this is a bug. As per our\r\n",
      "\r\n",
      "**System information**\r\n",
      "- OS Platform ：Ubuntu 16.04\r\n",
      "- TensorFlow installed from source :\r\n",
      "- TensorFlow version (use command below): TensorFlow Core v2.4.1  TensorFlow C++ API Reference\r\n",
      "- Python version: 3.5\r\n",
      "- Bazel version : release: release 1.2.1\r\n",
      "- GCC/Compiler version (if compiling from source): gcc (Ubuntu 5.5.0-12ubuntu1~16.04) 5.5.0 20171010\r\n",
      "\r\n",
      "\r\n",
      "LOG OUTPUT\r\n",
      "\r\n",
      "2021-01-25 10:39:38.260369: E external/org_tensorflow/tensorflow/core/common_runtime/executor.cc:661] Executor failed to create kernel. Invalid argument: Default MaxPoolingOp only supports NHWC on device type CPU\r\n",
      "         [[{{node v_}}]]\r\n",
      "2021-01-25 10:39:38.261762: E external/org_tensorflow/tensorflow/core/common_runtime/executor.cc:661] Executor failed to create kernel. Invalid argument: Default MaxPoolingOp only supports NHWC on device type CPU\r\n",
      "         [[{{node v_}}]]\r\n",
      "Error in prediction:Invalid argument: Default MaxPoolingOp only supports NHWC on device type CPU\r\n",
      "         [[{{node v_}}]]\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:apis\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Issue with tflite converter in tf-nightly:  'tf.If' op 'then_branch' input type tensor<?xf32> is incompatible with input type tensor<?xi64> at index 0\n",
      "issue body -  **System information**\r\n",
      "- OS Platform and Distribution: Linux Ubuntu 18.04.5\r\n",
      "- Python version: 3.7.4\r\n",
      "- TensorFlow version: tf-nightly-2.5.0.dev20210124\r\n",
      "\r\n",
      "Hello, I have a new issue with the tflite converter which is happening only when using nightly (it isn't happening when using tensorflow main branch). I tried as much as I can to create the minimal code to reproduce: \r\n",
      "\r\n",
      "    !pip install tf-nightly\r\n",
      "    import tensorflow as tf\r\n",
      "    import os\r\n",
      "    import numpy as np\r\n",
      "    \r\n",
      "    class EXAMPLE(tf.Module):\r\n",
      "    \r\n",
      "        @tf.function(input_signature=[tf.TensorSpec(shape=[100], dtype=tf.float32)])\r\n",
      "        def calculate(self, x):\r\n",
      "        \r\n",
      "          maxima_ind = tf.where(x > 0.8)\r\n",
      "          maxima_ind = tf.gather(maxima_ind,0,axis=1)\r\n",
      "    \r\n",
      "          maxima_ind = tf.cast(maxima_ind, dtype=tf.float32)\r\n",
      "    \r\n",
      "          if len(maxima_ind) > 10:\r\n",
      "              maxima_ind = tf.cast(maxima_ind, dtype=tf.float32)\r\n",
      "    \r\n",
      "          return maxima_ind\r\n",
      "          \r\n",
      "    to_export = EXAMPLE()\r\n",
      "    np.random.seed(54)\r\n",
      "    buffer_size = 100\r\n",
      "    x1  = tf.convert_to_tensor(np.random.rand(buffer_size).astype('float32'))\r\n",
      "    \r\n",
      "    \r\n",
      "    solution = to_export.calculate(x1)\r\n",
      "    print(solution)\r\n",
      "    models_dir = '/content/model_example/'\r\n",
      "    tf.saved_model.save(to_export, models_dir)\r\n",
      "    imported = tf.saved_model.load(models_dir)\r\n",
      "    converter = tf.lite.TFLiteConverter.from_saved_model(models_dir) # path to the SavedModel directory\r\n",
      "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n",
      "                                            tf.lite.OpsSet.SELECT_TF_OPS]\r\n",
      "    \r\n",
      "    tflite_model = converter.convert()\r\n",
      "    with open(models_dir + 'model_example.tflite', 'wb') as f:\r\n",
      "      f.write(tflite_model)\r\n",
      "\r\n",
      "When running it, I'm getting this error: \r\n",
      "\r\n",
      "    Exception                                 Traceback (most recent call last)\r\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n",
      "        216                                                  debug_info_str,\r\n",
      "    --> 217                                                  enable_mlir_converter)\r\n",
      "        218       return model_str\r\n",
      "    \r\n",
      "    4 frames\r\n",
      "    Exception: <unknown>:0: error: loc(\"cond@__inference_calculate_3155\"): 'tf.If' op 'then_branch' input type tensor<?xf32> is incompatible with input type tensor<?xi64> at index 0\r\n",
      "    \r\n",
      "    \r\n",
      "    During handling of the above exception, another exception occurred:\r\n",
      "    \r\n",
      "        ConverterError                            Traceback (most recent call last)\r\n",
      "        /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n",
      "            218       return model_str\r\n",
      "            219     except Exception as e:\r\n",
      "        --> 220       raise ConverterError(str(e))\r\n",
      "            221 \r\n",
      "            222   if distutils.spawn.find_executable(_toco_from_proto_bin) is None:\r\n",
      "        \r\n",
      "        ConverterError: <unknown>:0: error: loc(\"cond@__inference_calculate_3155\"): 'tf.If' op 'then_branch' input type tensor<?xf32> is incompatible with input type tensor<?xi64> at index 0\r\n",
      "\r\n",
      "You can also use this gist (which include the above code): https://colab.research.google.com/gist/AiaHaruv/6cac62614ef85f76db8bcfcfa0b8baae/conversion_bug.ipynb\r\n",
      "I understand there is a thing with the types but I'm using casting before the if statement and inside of it I'm actually doing nothing, and it isn't happening in the main branch..  \r\n",
      "Any help will be appreciated. Thank you \r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.5\n",
      "TFLiteConverter\n",
      "comp:lite\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  google.protobuf.text_format.ParseError: 161:14 : Message type \"object_detection.protos.Optimizer\" has no field named \"i\".\n",
      "issue body -  sparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/aarju/anaconda3/envs/demo_model/lib:/usr/lib/x86_64-linux-gnu:/home/aarju/anaconda3/envs/caffe/lib:/usr/lib/x86_64-linux-gnu\r\n",
      "2021-01-25 14:30:04.810886: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/aarju/anaconda3/envs/demo_model/lib:/usr/lib/x86_64-linux-gnu:/home/aarju/anaconda3/envs/caffe/lib:/usr/lib/x86_64-linux-gnu\r\n",
      "2021-01-25 14:30:04.810902: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\n",
      "Skipping registering GPU devices...\r\n",
      "2021-01-25 14:30:04.811643: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\r\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n",
      "2021-01-25 14:30:04.814596: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n",
      "2021-01-25 14:30:04.814714: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n",
      "2021-01-25 14:30:04.814742: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]\r\n",
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\r\n",
      "W0125 14:30:04.820366 140452454930176 cross_device_ops.py:1321] There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\r\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\r\n",
      "I0125 14:30:04.821369 140452454930176 mirrored_strategy.py:350] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"model_main_tf2.py\", line 113, in <module>\r\n",
      "    tf.compat.v1.app.run()\r\n",
      "  File \"/home/aarju/anaconda3/envs/demo_model/lib/python3.8/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n",
      "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n",
      "  File \"/home/aarju/anaconda3/envs/demo_model/lib/python3.8/site-packages/absl/app.py\", line 300, in run\r\n",
      "    _run_main(main, args)\r\n",
      "  File \"/home/aarju/anaconda3/envs/demo_model/lib/python3.8/site-packages/absl/app.py\", line 251, in _run_main\r\n",
      "    sys.exit(main(argv))\r\n",
      "  File \"model_main_tf2.py\", line 104, in main\r\n",
      "    model_lib_v2.train_loop(\r\n",
      "  File \"/home/aarju/anaconda3/envs/demo_model/lib/python3.8/site-packages/object_detection/model_lib_v2.py\", line 466, in train_loop\r\n",
      "    configs = get_configs_from_pipeline_file(\r\n",
      "  File \"/home/aarju/anaconda3/envs/demo_model/lib/python3.8/site-packages/object_detection/utils/config_util.py\", line 139, in get_configs_from_pipeline_file\r\n",
      "    text_format.Merge(proto_str, pipeline_config)\r\n",
      "  File \"/home/aarju/anaconda3/envs/demo_model/lib/python3.8/site-packages/google/protobuf/text_format.py\", line 728, in Merge\r\n",
      "    return MergeLines(\r\n",
      "  File \"/home/aarju/anaconda3/envs/demo_model/lib/python3.8/site-packages/google/protobuf/text_format.py\", line 802, in MergeLines\r\n",
      "    return parser.MergeLines(lines, message)\r\n",
      "  File \"/home/aarju/anaconda3/envs/demo_model/lib/python3.8/site-packages/google/protobuf/text_format.py\", line 827, in MergeLines\r\n",
      "    self._ParseOrMerge(lines, message)\r\n",
      "  File \"/home/aarju/anaconda3/envs/demo_model/lib/python3.8/site-packages/google/protobuf/text_format.py\", line 849, in _ParseOrMerge\r\n",
      "    self._MergeField(tokenizer, message)\r\n",
      "  File \"/home/aarju/anaconda3/envs/demo_model/lib/python3.8/site-packages/google/protobuf/text_format.py\", line 974, in _MergeField\r\n",
      "    merger(tokenizer, message, field)\r\n",
      "  File \"/home/aarju/anaconda3/envs/demo_model/lib/python3.8/site-packages/google/protobuf/text_format.py\", line 1048, in _MergeMessageField\r\n",
      "    self._MergeField(tokenizer, sub_message)\r\n",
      "  File \"/home/aarju/anaconda3/envs/demo_model/lib/python3.8/site-packages/google/protobuf/text_format.py\", line 974, in _MergeField\r\n",
      "    merger(tokenizer, message, field)\r\n",
      "  File \"/home/aarju/anaconda3/envs/demo_model/lib/python3.8/site-packages/google/protobuf/text_format.py\", line 1048, in _MergeMessageField\r\n",
      "    self._MergeField(tokenizer, sub_message)\r\n",
      "  File \"/home/aarju/anaconda3/envs/demo_model/lib/python3.8/site-packages/google/protobuf/text_format.py\", line 939, in _MergeField\r\n",
      "    raise tokenizer.ParseErrorPreviousToken(\r\n",
      "google.protobuf.text_format.ParseError: 161:14 : Message type \"object_detection.protos.Optimizer\" has no field named \"i\".\r\n",
      "\r\n",
      "\r\n",
      "### System information\r\n",
      "\r\n",
      "\r\n",
      "-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n",
      "\r\n",
      "-   **TensorFlow installed from (source or binary)**:\r\n",
      "-   **TensorFlow version (use command below)**:: 2.4\r\n",
      "-   **Python version**: : 3.7\r\n",
      "-   **Bazel version (if compiling from source)**:\r\n",
      "-   **GCC/Compiler version (if compiling from source)**:\r\n",
      "-   **CUDA/cuDNN version**: 10.2\r\n",
      "-   **GPU model and memory**:\r\n",
      "-   **Exact command to reproduce**:\r\n",
      "\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  micro: minor fix: Alphabetical order in build file\n",
      "issue body -  Change the ordering such that the alphabetical order is correct again\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:lite\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Keras model could save to h5 but not savedModel, call() missing argument\n",
      "issue body -  I am using TF 2.2, the model is a keras model written in `tf.keras.engine.training_v1.Model`. I could save the model to h5 using `model.save(save_format='h5)`, but if save using savedModel by just `model.save()`. the error is \r\n",
      "```\r\n",
      "TypeError: call() missing 1 required positional argument: 'state'\r\n",
      "```\r\n",
      "How could it be?.. The big question here is that: why is there some keras model which work well and could saved to .h5 but have this error when saved to SavedModel\r\n",
      "\r\n",
      "Full stack:\r\n",
      "```\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/litchy/anaconda3/envs/rec/lib/python3.6/contextlib.py\", line 99, in __exit__\r\n",
      "    self.gen.throw(type, value, traceback)\r\n",
      "  File \"/home/litchy/anaconda3/envs/rec/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\", line 423, in learning_phase_scope\r\n",
      "    yield\r\n",
      "  File \"/home/litchy/anaconda3/envs/rec/lib/python3.6/site-packages/tensorflow/python/keras/saving/saved_model/save.py\", line 78, in save\r\n",
      "    save_lib.save(model, filepath, signatures, options)\r\n",
      "  File \"/home/litchy/anaconda3/envs/rec/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py\", line 951, in save\r\n",
      "    obj, export_dir, signatures, options, meta_graph_def)\r\n",
      "  File \"/home/litchy/anaconda3/envs/rec/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py\", line 1008, in _build_meta_graph\r\n",
      "    checkpoint_graph_view)\r\n",
      "  File \"/home/litchy/anaconda3/envs/rec/lib/python3.6/site-packages/tensorflow/python/saved_model/signature_serialization.py\", line 75, in find_function_to_export\r\n",
      "    functions = saveable_view.list_functions(saveable_view.root)\r\n",
      "  File \"/home/litchy/anaconda3/envs/rec/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py\", line 143, in list_functions\r\n",
      "    self._serialization_cache)\r\n",
      "  File \"/home/litchy/anaconda3/envs/rec/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 1656, in _list_functions_for_serialization\r\n",
      "    Model, self)._list_functions_for_serialization(serialization_cache)\r\n",
      "  File \"/home/litchy/anaconda3/envs/rec/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer_v1.py\", line 2439, in _list_functions_for_serialization\r\n",
      "    .list_functions_for_serialization(serialization_cache))\r\n",
      "  File \"/home/litchy/anaconda3/envs/rec/lib/python3.6/site-packages/tensorflow/python/keras/saving/saved_model/base_serialization.py\", line 87, in list_functions_for_serialization\r\n",
      "    fns = self.functions_to_serialize(serialization_cache)\r\n",
      "  File \"/home/litchy/anaconda3/envs/rec/lib/python3.6/site-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py\", line 77, in functions_to_serialize\r\n",
      "    serialization_cache).functions_to_serialize)\r\n",
      "  File \"/home/litchy/anaconda3/envs/rec/lib/python3.6/site-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py\", line 92, in _get_serialized_attributes\r\n",
      "    serialization_cache)\r\n",
      "  File \"/home/litchy/anaconda3/envs/rec/lib/python3.6/site-packages/tensorflow/python/keras/saving/saved_model/model_serialization.py\", line 53, in _get_serialized_attributes_internal\r\n",
      "    serialization_cache))\r\n",
      "  File \"/home/litchy/anaconda3/envs/rec/lib/python3.6/site-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py\", line 101, in _get_serialized_attributes_internal\r\n",
      "    functions = save_impl.wrap_layer_functions(self.obj, serialization_cache)\r\n",
      "  File \"/home/litchy/anaconda3/envs/rec/lib/python3.6/site-packages/tensorflow/python/keras/saving/saved_model/save_impl.py\", line 153, in wrap_layer_functions\r\n",
      "    original_fns = _replace_child_layer_functions(layer, serialization_cache)\r\n",
      "  File \"/home/litchy/anaconda3/envs/rec/lib/python3.6/site-packages/tensorflow/python/keras/saving/saved_model/save_impl.py\", line 272, in _replace_child_layer_functions\r\n",
      "    serialization_cache).functions)\r\n",
      "  File \"/home/litchy/anaconda3/envs/rec/lib/python3.6/site-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py\", line 92, in _get_serialized_attributes\r\n",
      "    serialization_cache)\r\n",
      "  File \"/home/litchy/anaconda3/envs/rec/lib/python3.6/site-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py\", line 101, in _get_serialized_attributes_internal\r\n",
      "    functions = save_impl.wrap_layer_functions(self.obj, serialization_cache)\r\n",
      "  File \"/home/litchy/anaconda3/envs/rec/lib/python3.6/site-packages/tensorflow/python/keras/saving/saved_model/save_impl.py\", line 153, in wrap_layer_functions\r\n",
      "    original_fns = _replace_child_layer_functions(layer, serialization_cache)\r\n",
      "  File \"/home/litchy/anaconda3/envs/rec/lib/python3.6/site-packages/tensorflow/python/keras/saving/saved_model/save_impl.py\", line 272, in _replace_child_layer_functions\r\n",
      "    serialization_cache).functions)\r\n",
      "  File \"/home/litchy/anaconda3/envs/rec/lib/python3.6/site-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py\", line 92, in _get_serialized_attributes\r\n",
      "    serialization_cache)\r\n",
      "  File \"/home/litchy/anaconda3/envs/rec/lib/python3.6/site-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py\", line 101, in _get_serialized_attributes_internal\r\n",
      "    functions = save_impl.wrap_layer_functions(self.obj, serialization_cache)\r\n",
      "  File \"/home/litchy/anaconda3/envs/rec/lib/python3.6/site-packages/tensorflow/python/keras/saving/saved_model/save_impl.py\", line 191, in wrap_layer_functions\r\n",
      "    fn.get_concrete_function()\r\n",
      "  File \"/home/litchy/anaconda3/envs/rec/lib/python3.6/site-packages/tensorflow/python/keras/saving/saved_model/save_impl.py\", line 546, in get_concrete_function\r\n",
      "    self.call_collection.add_trace(*args, **kwargs)\r\n",
      "  File \"/home/litchy/anaconda3/envs/rec/lib/python3.6/site-packages/tensorflow/python/keras/saving/saved_model/save_impl.py\", line 421, in add_trace\r\n",
      "    fn.get_concrete_function(*args, **kwargs)\r\n",
      "  File \"/home/litchy/anaconda3/envs/rec/lib/python3.6/site-packages/tensorflow/python/keras/saving/saved_model/save_impl.py\", line 547, in get_concrete_function\r\n",
      "    return super(LayerCall, self).get_concrete_function(*args, **kwargs)\r\n",
      "  File \"/home/litchy/anaconda3/envs/rec/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 959, in get_concrete_function\r\n",
      "    concrete = self._get_concrete_function_garbage_collected(*args, **kwargs)\r\n",
      "  File \"/home/litchy/anaconda3/envs/rec/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 865, in _get_concrete_function_garbage_collected\r\n",
      "    self._initialize(args, kwargs, add_initializers_to=initializers)\r\n",
      "  File \"/home/litchy/anaconda3/envs/rec/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 506, in _initialize\r\n",
      "    *args, **kwds))\r\n",
      "  File \"/home/litchy/anaconda3/envs/rec/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 2446, in _get_concrete_function_internal_garbage_collected\r\n",
      "    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n",
      "  File \"/home/litchy/anaconda3/envs/rec/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 2777, in _maybe_define_function\r\n",
      "    graph_function = self._create_graph_function(args, kwargs)\r\n",
      "  File \"/home/litchy/anaconda3/envs/rec/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 2667, in _create_graph_function\r\n",
      "    capture_by_value=self._capture_by_value),\r\n",
      "  File \"/home/litchy/anaconda3/envs/rec/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\", line 981, in func_graph_from_py_func\r\n",
      "    func_outputs = python_func(*func_args, **func_kwargs)\r\n",
      "  File \"/home/litchy/anaconda3/envs/rec/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 441, in wrapped_fn\r\n",
      "    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n",
      "  File \"/home/litchy/anaconda3/envs/rec/lib/python3.6/site-packages/tensorflow/python/keras/saving/saved_model/save_impl.py\", line 524, in wrapper\r\n",
      "    ret = method(*args, **kwargs)\r\n",
      "  File \"/home/litchy/anaconda3/envs/rec/lib/python3.6/site-packages/tensorflow/python/keras/saving/saved_model/save_impl.py\", line 566, in call_and_return_conditional_losses\r\n",
      "    return layer_call(inputs, *args, **kwargs), layer.get_losses_for(inputs)\r\n",
      "TypeError: call() missing 1 required positional argument: 'state'\r\n",
      "\r\n",
      "```\r\n",
      "The model is not short, so I am not posting it. If you need I would post it.\n",
      "issue labels - \n",
      "TF 2.2\n",
      "comp:keras\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Update load.py\n",
      "issue body -  Simple fix for keras model load for raggedTensor. Otherwise there is `AttributeError` about `raggedTensorSpec` not having `name`\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:keras\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  How can I build this for Intel GPU (to use OpenCL)\n",
      "issue body -  <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n",
      "- TensorFlow installed from (source or binary): Source \r\n",
      "- TensorFlow version: Release 2.5.1\r\n",
      "- Python version: Python 2.7.15+\r\n",
      "- Installed using virtualenv? pip? conda?: \r\n",
      "- Bazel version (if compiling from source):\r\n",
      "- GCC/Compiler version (if compiling from source): gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\r\n",
      "- CUDA/cuDNN version: NA\r\n",
      "- GPU model and memory: Intel GPU \r\n",
      "\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "Compiled with -DCL_DELEGATE_NO_GL flags in Makefile however I get the following error : \r\n",
      "./tensorflow/lite/delegates/gpu/api.h:262:17: error: expected unqualified-id before ‘int’\r\n",
      "   virtual absl::Status SetInputShape(index,const Dimensions& dimensions) = 0;\r\n",
      "\r\n",
      "**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n",
      "1> Add -DCL_DELEGATE_NO_GL in Makefile \r\n",
      "2> Added tensorflow/lite/delegates/gpu/*.cc to the list of sources in Makefile \r\n",
      "3> Included /usr/include to add cl.h \r\n",
      "\r\n",
      "**Any other info / logs**\r\n",
      "Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n",
      "COMPLETE ERROR LOGS: \r\n",
      "In file included from /usr/include/EGL/eglplatform.h:130:0,\r\n",
      "                 from /usr/include/EGL/egl.h:39,\r\n",
      "                 from ./tensorflow/lite/delegates/gpu/gl/portable_gl31.h:21,\r\n",
      "                 from ./tensorflow/lite/delegates/gpu/api.h:50,\r\n",
      "                 from tensorflow/lite/delegates/gpu/api.cc:16:\r\n",
      "./tensorflow/lite/delegates/gpu/api.h:262:17: error: expected unqualified-id before ‘int’\r\n",
      "   virtual absl::Status SetInputShape(index,const Dimensions& dimensions) = 0;\r\n",
      "                 ^\r\n",
      "./tensorflow/lite/delegates/gpu/api.h:271:17: error: expected unqualified-id before ‘int’\r\n",
      "   virtual absl::Status SetInputObjectDef(int index, ObjectDef def) = 0;\r\n",
      "                 ^\r\n",
      "./tensorflow/lite/delegates/gpu/api.h:272:17: error: expected unqualified-id before ‘int’\r\n",
      "   virtual absl::Status SetOutputObjectDef(int index, ObjectDef def) = 0;\r\n",
      "                 ^\r\n",
      "./tensorflow/lite/delegates/gpu/api.h:273:17: error: expected unqualified-id before ‘int’\r\n",
      "   virtual absl::Status SetAllInputObjectDefsTo(ObjectDef def) {\r\n",
      "                 ^\r\n",
      "./tensorflow/lite/delegates/gpu/api.h:280:17: error: expected unqualified-id before ‘int’\r\n",
      "   virtual absl::Status SetAllOutputObjectDefsTo(ObjectDef def) {\r\n",
      "                 ^\r\n",
      "./tensorflow/lite/delegates/gpu/api.h:293:17: error: expected unqualified-id before ‘int’\r\n",
      "   virtual absl::Status Build(std::unique_ptr<InferenceRunner>* runner) = 0;\r\n",
      "                 ^\r\n",
      "./tensorflow/lite/delegates/gpu/api.h:310:17: error: expected unqualified-id before ‘int’\r\n",
      "   virtual absl::Status GetInputObject(int index, TensorObject* object) = 0;\r\n",
      "                 ^\r\n",
      "./tensorflow/lite/delegates/gpu/api.h:311:17: error: expected unqualified-id before ‘int’\r\n",
      "   virtual absl::Status GetOutputObject(int index, TensorObject* object) = 0;\r\n",
      "                 ^\r\n",
      "./tensorflow/lite/delegates/gpu/api.h:312:17: error: expected unqualified-id before ‘int’\r\n",
      "   virtual absl::Status SetInputObject(int index, TensorObject object) = 0;\r\n",
      "                 ^\r\n",
      "./tensorflow/lite/delegates/gpu/api.h:313:17: error: expected unqualified-id before ‘int’\r\n",
      "   virtual absl::Status SetOutputObject(int index, TensorObject object) = 0;\r\n",
      "                 ^\r\n",
      "./tensorflow/lite/delegates/gpu/api.h:315:17: error: expected unqualified-id before ‘int’\r\n",
      "   virtual absl::Status Run() = 0;\n",
      "issue labels - \n",
      "TF 2.5\n",
      "comp:lite\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  MGB track\n",
      "issue body -  https://github.com/tensorflow/tensorflow/blob/590d6eef7e91a6a7392c8ffffb7b58f2e0c8bc6b/tensorflow/python/training/tracking/base.py#L498-L990\n",
      "issue labels - \n",
      "stalled\n",
      "stat:awaiting response\n",
      "\n",
      "\n",
      "issue title -  Redo refactoring gather.h from TFLite reference_ops.h\n",
      "issue body -  New PR2 for issue #45196 (the old one was closed due to merging errors)\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -   Micro: port op CAST from Lite\n",
      "issue body -  PR5 (hopefully last PR) for issue #45608\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:L\n",
      "\n",
      "\n",
      "issue title -  Find bogomips on aarch64\n",
      "issue body -  Detect CPU frequency on aarch64.  Fixes #42545 and #38260.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:core\n",
      "prtype:bugfix\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  replace alternative operator to fix msvc build break\n",
      "issue body -  https://docs.microsoft.com/en-us/cpp/cpp/cpp-built-in-operators-precedence-and-associativity#alternative-spellings\r\n",
      "\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:core\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  tf.transpose returns incorrect shape or throws concatenation error when part of keras model\n",
      "issue body -  <em>Please make sure that this is a bug. As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:bug_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n",
      "- TensorFlow installed from (source or binary): pip installed\r\n",
      "- TensorFlow version (use command below): 2.4.1\r\n",
      "- Python version: 3.6\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version: Cuda 11.0/ 8.0.4\r\n",
      "- GPU model and memory: NVIDIA GeForce GTX 1050 / 4.0GB Memory\r\n",
      "\r\n",
      "You can collect some of this information using our environment capture\r\n",
      "[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n",
      "You can also obtain the TensorFlow version with:\r\n",
      "1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n",
      "2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "\r\n",
      "- When run as part of a keras model, tf.tranpose of a rank 2 tensor gives unexpected shape . \r\n",
      "- When run  eagerly tf.transpose gives expected shape. \r\n",
      "- If the first dimension is greater than 32 then tf.transpose either returns an incorrect shape or throws a concatenation error\r\n",
      "- This issue also affects tf.matmul(a, b, transpose_b=True)\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "Given a tensor of shape(i, j) tf.transpose returns a tensor of  shape (j, i) . As it does in eager mode\r\n",
      "Used for calculating similarities, similar in this example https://keras.io/examples/vision/metric_learning/\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "Provide a reproducible test case that is the bare minimum necessary to generate\r\n",
      "the problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n",
      "\r\n",
      "https://colab.research.google.com/drive/1MIXzVp1Jqsu-eypWFeNavW1bk3-B0UTG?usp=sharing\r\n",
      "\r\n",
      "\r\n",
      "**Other info / logs** Include any logs or source code that would be helpful to\r\n",
      "diagnose the problem. If including tracebacks, please include the full\r\n",
      "traceback. Large logs and files should be attached.\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:keras\n",
      "comp:ops\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  AttributeError: 'Tensor' object has no attribute 'numpy'\n",
      "issue body -  Hi, guys, recently I'm trying to built my own metrics class using the sklearn.metrics, but when I running the update_state funcation inherit from the tensorflow.keras.metrics.Metric, I find the argument in update_state function which are y_true and y_pred is Tensor object but I need numpy array, I have tried the method given in the documentation which says using y_true.numpy but I got an error: AttributeError: 'Tensor' object has no attribute 'numpy'. I have tried many methods following the Internet but it still doesn't work. Could you help me solve this? Here are my code, thanks!\r\n",
      "\r\n",
      "class SelfF1(Metric):\r\n",
      "    def __init__(self, average, name=\"self_f1\", **kwargs):\r\n",
      "        super(SelfF1, self).__init__(name=name, **kwargs)\r\n",
      "        self.f1_value = self.add_weight(name=\"sf\", initializer=\"zeros\", dtype=\"float64\")\r\n",
      "        self.average = average\r\n",
      "\r\n",
      "    def update_state(self, y_true, y_pred, sample_weight=None):\r\n",
      "\r\n",
      "        y_pred = tf.reshape(tf.argmax(y_pred, axis=1), shape=(-1,))\r\n",
      "        if len(y_true.shape) == 2 and y_true.shape[1] != 1:\r\n",
      "            y_true = tf.reshape(tf.argmax(y_true, axis=1), shape=(-1,))\r\n",
      "\r\n",
      "        f1_value = f1_score(y_true=y_true.numpy(), y_pred=y_pred.numpy(), average=self.average)\r\n",
      "\r\n",
      "        self.f1_value.assign(f1_value)\r\n",
      "\r\n",
      "    def result(self):\r\n",
      "        return self.f1_value\r\n",
      "\r\n",
      "    def reset_states(self):\r\n",
      "        self.f1_value.assign(0.0)\n",
      "issue labels - \n",
      "TF 2.0\n",
      "comp:keras\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Incomplete ImageProjectiveTransformV3 documentation.\n",
      "issue body -  \"NEAREST\" option is missing in the documentation: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/image/image_ops.cc/#L141\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:core\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Valid device NotFoundError when running BatchNormalization op of Tensorflow 2.0.0\n",
      "issue body -  <em>Please make sure that this is a bug. As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:bug_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 2019\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n",
      "- TensorFlow installed from (source or binary): binary\r\n",
      "- TensorFlow version (use command below): 2.0.0\r\n",
      "- Python version: 3.7\r\n",
      "- Bazel version (if compiling from source):  N/A\r\n",
      "- GCC/Compiler version (if compiling from source): N/A\r\n",
      "- CUDA/cuDNN version: N/A\r\n",
      "- GPU model and memory: N/A\r\n",
      "\r\n",
      "You can collect some of this information using our environment capture\r\n",
      "[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n",
      "You can also obtain the TensorFlow version with:\r\n",
      "1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n",
      "2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n",
      "\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "As declared in the specification, tf.keras.layers.BatchNormalization should support to set the dtype to 'float64' . However, per our experiment, running it as the Norm layer in float64  will trigger some unexpected error: tensorflow.python.framework.errors_impl.NotFoundError: Could not find valid device for node.Node:{{node FusedBatchNormV3}}\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "No exception during execution\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "x = np.random.randn(1, 2, 4, 4)\r\n",
      "x=tf.convert_to_tensor(x, dtype=tf.float16)\r\n",
      "tf__Norm_64=tf.keras.layers.BatchNormalization(\r\n",
      "        axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True,\r\n",
      "        beta_initializer='zeros', gamma_initializer='ones',\r\n",
      "        moving_mean_initializer='zeros', moving_variance_initializer='ones',dtype='float64'\r\n",
      ")\r\n",
      "out_16_64 = tf_Norm_64(x_16)\r\n",
      "\r\n",
      "**Other info / logs** Include any logs or source code that would be helpful to\r\n",
      "diagnose the problem. If including tracebacks, please include the full\r\n",
      "traceback. Large logs and files should be attached.\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.0\n",
      "comp:keras\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  tensorflow.greater returns a wrong bool\n",
      "issue body -  ### System information\r\n",
      "[tf_env.txt](https://github.com/tensorflow/tensorflow/files/5861094/tf_env.txt)\r\n",
      "\r\n",
      "\r\n",
      "-   I used the custom script shown below\r\n",
      "-   Windows 10 Family version 19041.746\r\n",
      "-   TensorFlow is installed from source\r\n",
      "-   v2.4.0-49-g85c8b2a817f 2.4.1\r\n",
      "-   Python 3.8.6 (tags/v3.8.6:db45529, Sep 23 2020, 15:52:53) [MSC v.1927 64 bit (AMD64)] on win32\r\n",
      "-   CUDA v11.1\r\n",
      "-   CPU Intel Core i7-3770 @ 3.40GHz\r\n",
      "-   GPU: GeForce GTX 1060 6GB\r\n",
      "\r\n",
      "### Describe the problem\r\n",
      "Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n",
      "when I use the greater function from the package it gives me **True** when 1 > 1000** and **False when 1 > 10000**. I think the error is obvious enough\r\n",
      "\r\n",
      "EDIT: Issue Fixed, it was obviously the fact that I was using Int8. I guess Python made me unused to check for overflows. Sorry for that dumb issue ^-^\r\n",
      "\r\n",
      "### Source code / logs\r\n",
      ">>> import tensorflow as tf\r\n",
      "initialization is successful\r\n",
      ">>>tf.greater(tf.convert_to_tensor(1, dtype=tf.int8), tf.convert_to_tensor(1000, dtype=tf.int8))\r\n",
      "<tf.Tensor: shape=(), dtype=bool, numpy=True>\r\n",
      "\r\n",
      ">>>tf.greater(tf.convert_to_tensor(1, dtype=tf.int8), tf.convert_to_tensor(10000, dtype=tf.int8))\r\n",
      "<tf.Tensor: shape=(), dtype=bool, numpy=False>\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:core\n",
      "stat:awaiting tensorflower\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  memory leak of tf2.3 with LSTM\n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Red Hat Enterprise Linux Server 7.9 (Maipo)\r\n",
      "- TensorFlow version (use command below): 2.3.1\r\n",
      "- Python version: 3.7.4\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "memory leak with training batch when using LSTM layer (no problem with dense `layer)`\r\n",
      "**Describe the expected behavior**\r\n",
      "no memory leak\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "      \r\n",
      "import numpy as np\r\n",
      "import tensorflow\r\n",
      "from tensorflow import keras\r\n",
      "from tensorflow.keras import layers\r\n",
      "from tensorflow.keras.callbacks import Callback\r\n",
      "\r\n",
      "import resource\r\n",
      "class MemoryCallback(Callback):\r\n",
      "    def on_epoch_end(self, epoch, log={}):\r\n",
      "        print('')\r\n",
      "        print('Memory usage: ', resource.getrusage(resource.RUSAGE_SELF).ru_maxrss)\r\n",
      "        \r\n",
      "if __name__ == \"__main__\":\r\n",
      "        n_sample=1000\r\n",
      "        n_timestep =1000\r\n",
      "        num_channel = 1\r\n",
      "        batch_size=100\r\n",
      "        epochs=10\r\n",
      "        i_train_on_batch = False\r\n",
      "        \r\n",
      "        train = np.random.rand(n_sample, n_timestep, num_channel)\r\n",
      "        label = np.random.rand(n_sample, num_channel)\r\n",
      "        \r\n",
      "        x=layers.Input(shape=(n_timestep,num_channel))\r\n",
      "        z=  layers.LSTM(num_channel)(x)  \r\n",
      "        model = keras.Model(inputs=x, outputs=z)\r\n",
      "        \r\n",
      "        model.compile(optimizer=\"adam\", loss=\"mse\")\r\n",
      "        if i_train_on_batch:\r\n",
      "           for epoch in range(epochs):\r\n",
      "               for step in range(n_sample//batch_size):\r\n",
      "                   model.train_on_batch(train[step*batch_size:(step+1)*batch_size], label[step*batch_size:(step+1)*batch_size],)\r\n",
      "               print('Epoch ', epoch,'/', epochs, '  Memory usage: ', resource.getrusage(resource.RUSAGE_SELF).ru_maxrss)\r\n",
      "        else:\r\n",
      "           model.fit(train, label, batch_size=batch_size, epochs=epochs, callbacks=[MemoryCallback()])\r\n",
      "\r\n",
      "**Other info / logs** Include any logs or source code that would be helpful to\r\n",
      "diagnose the problem. If including tracebacks, please include the full\r\n",
      "traceback. Large logs and files should be attached.\r\n",
      "\r\n",
      "###################################################\r\n",
      "i_train_on_batch = False:\r\n",
      "###################################################\r\n",
      "\r\n",
      "bash-4.2$ python memory_leak_epoch.py \r\n",
      "2021-01-23 16:17:38.709232: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n",
      "2021-01-23 16:17:43.397736: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\r\n",
      "2021-01-23 16:17:43.404166: E tensorflow/stream_executor/cuda/cuda_driver.cc:314] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\r\n",
      "2021-01-23 16:17:43.404195: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (taurusi4181.taurus.hrsk.tu-dresden.de): /proc/driver/nvidia/version does not exist\r\n",
      "2021-01-23 16:17:43.406436: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n",
      "Epoch 1/10\r\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2690\r\n",
      "Memory usage:  424672\r\n",
      "10/10 [==============================] - 4s 364ms/step - loss: 0.2690\r\n",
      "Epoch 2/10\r\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2612\r\n",
      "Memory usage:  475888\r\n",
      "10/10 [==============================] - 4s 370ms/step - loss: 0.2612\r\n",
      "Epoch 3/10\r\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2533\r\n",
      "Memory usage:  527780\r\n",
      "10/10 [==============================] - 4s 374ms/step - loss: 0.2533\r\n",
      "Epoch 4/10\r\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2453\r\n",
      "Memory usage:  578732\r\n",
      "10/10 [==============================] - 4s 376ms/step - loss: 0.2453\r\n",
      "Epoch 5/10\r\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2373\r\n",
      "Memory usage:  627480\r\n",
      "10/10 [==============================] - 4s 381ms/step - loss: 0.2373\r\n",
      "Epoch 6/10\r\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2293\r\n",
      "Memory usage:  679224\r\n",
      "10/10 [==============================] - 4s 386ms/step - loss: 0.2293\r\n",
      "Epoch 7/10\r\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2212\r\n",
      "Memory usage:  728328\r\n",
      "10/10 [==============================] - 4s 385ms/step - loss: 0.2212\r\n",
      "Epoch 8/10\r\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2130\r\n",
      "Memory usage:  777960\r\n",
      "10/10 [==============================] - 4s 385ms/step - loss: 0.2130\r\n",
      "Epoch 9/10\r\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2050\r\n",
      "Memory usage:  827592\r\n",
      "10/10 [==============================] - 4s 390ms/step - loss: 0.2050\r\n",
      "Epoch 10/10\r\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1969\r\n",
      "Memory usage:  877752\r\n",
      "10/10 [==============================] - 4s 388ms/step - loss: 0.1969\r\n",
      "bash-4.2$ \r\n",
      "###################################################\r\n",
      "i_train_on_batch = True:\r\n",
      "###################################################\r\n",
      "\r\n",
      "bash-4.2$ python memory_leak_epoch.py \r\n",
      "2021-01-23 16:15:53.238289: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n",
      "2021-01-23 16:15:58.148876: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\r\n",
      "2021-01-23 16:15:58.155607: E tensorflow/stream_executor/cuda/cuda_driver.cc:314] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\r\n",
      "2021-01-23 16:15:58.155647: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (taurusi4181.taurus.hrsk.tu-dresden.de): /proc/driver/nvidia/version does not exist\r\n",
      "2021-01-23 16:15:58.158445: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n",
      "Epoch  0 / 10   Memory usage:  419684\r\n",
      "Epoch  1 / 10   Memory usage:  472220\r\n",
      "Epoch  2 / 10   Memory usage:  523804\r\n",
      "Epoch  3 / 10   Memory usage:  574228\r\n",
      "Epoch  4 / 10   Memory usage:  623068\r\n",
      "Epoch  5 / 10   Memory usage:  673756\r\n",
      "Epoch  6 / 10   Memory usage:  724180\r\n",
      "Epoch  7 / 10   Memory usage:  774076\r\n",
      "Epoch  8 / 10   Memory usage:  824500\r\n",
      "Epoch  9 / 10   Memory usage:  874660\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.3\n",
      "comp:keras\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:performance\n",
      "\n",
      "\n",
      "issue title -  object detection identifying even there is no image\n",
      "issue body -  hello guys i have been working in tensor flow object detection I have choose mobile ssd net model and trained the model but i am  getting a very false model it is detecting even when there is no image in the video stream.\r\n",
      "![image](https://user-images.githubusercontent.com/53184702/105580005-ca528e00-5daf-11eb-95ed-097ceeded753.png)\r\n",
      "Even it detects the same thing while a white blank wall is before it my accuracy is good.\r\n",
      "\n",
      "issue labels - \n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:performance\n",
      "\n",
      "\n",
      "issue title -  An error maybe about protobuf？\n",
      "issue body -  \r\n",
      "**System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Window 10\r\n",
      "- TensorFlow installed from (source or binary):\r\n",
      "- TensorFlow version: 2.3.0-gpu\r\n",
      "- Python version: 3.6.0\r\n",
      "- Installed using virtualenv? pip? conda?:  pip\r\n",
      "- CUDA/cuDNN version: 10.1 / 7.6.4\r\n",
      "- GPU model and memory: 1050Ti ，16G\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "\r\n",
      "My tensorflow is installed in the new virtual environment of anaconda.And at first my protobuf version is 3.6.0\r\n",
      "\r\n",
      "Today, I tried to run a tensorflow program, but it reported this error.\r\n",
      "\r\n",
      "``` File \"E:\\Anaconda3.5\\envs\\ten2\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 1666, in _init_from_args\r\n",
      "    graph_mode=self._in_graph_mode)\r\n",
      "  File \"E:\\Anaconda3.5\\envs\\ten2\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 243, in eager_safe_variable_handle\r\n",
      "    shape, dtype, shared_name, name, graph_mode, initial_value)\r\n",
      "  File \"E:\\Anaconda3.5\\envs\\ten2\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 179, in _variable_handle_from_shape_and_dtype\r\n",
      "    handle_data.shape_and_type.append(\r\n",
      "AttributeError: 'google.protobuf.pyext._message.RepeatedCompositeCo' object has no attribute 'append'\r\n",
      "\r\n",
      "Process finished with exit code 1\r\n",
      "```\r\n",
      "\r\n",
      "Then,i try to search some informations ,some ways suggest to upgrade the version of protobuf to the latest version, the 3.14.0.\r\n",
      "But,after upgrading,there is new problems.It show import error.\r\n",
      "```\r\n",
      "  File \"E:\\Anaconda3.5\\envs\\ten2\\lib\\site-packages\\tensorflow\\core\\framework\\function_pb2.py\", line 7, in <module>\r\n",
      "    from google.protobuf import descriptor as _descriptor\r\n",
      "  File \"E:\\Anaconda3.5\\envs\\ten2\\lib\\site-packages\\google\\protobuf\\descriptor.py\", line 48, in <module>\r\n",
      "    from google.protobuf.pyext import _message\r\n",
      "ImportError: DLL load failed: 找不到指定的程序。\r\n",
      "```\r\n",
      "\r\n",
      "It work well in tensorflow1.14.0 and cuda 10.0.What should i do if i still want to use tensorflow2.3.0？\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.3\n",
      "subtype:windows\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  [MLIR] compile error\n",
      "issue body -  when I build the latest tensorflow mlir, commit id c30ad4ae0ca1a6240ec8619d5027a272394726a5, \r\n",
      "with \r\n",
      "`bazel build --override_repository=llvm-project=$LLVM_SRC -c opt tensorflow/compiler/mlir:tf-opt` or even \r\n",
      "`bazel build -c opt tensorflow/compiler/mlir:tf-opt`\r\n",
      "I always get the message as below :\r\n",
      "`tensorflow/tensorflow/compiler/mlir/xla/BUILD:176:11: C++ compilation of rule '//tensorflow/compiler/mlir/xla:mhlo_to_lhlo_with_xla' failed (Exit 1): gcc failed: error executing command /path/to/gcc650/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' ... (remaining 320 argument(s) skipped)                                                                                                                                                                                                                                    tensorflow/compiler/mlir/xla/transforms/mhlo_to_lhlo_with_xla.cc: In instantiation of 'mlir::LhloDialectEmitter::EmitGemm(const xla::HloCustomCallInstruction*)::<lambda(auto:10)> [with auto:10 = mlir::lmhlo_gpu::GEMMOp]':         tensorflow/compiler/mlir/xla/transforms/mhlo_to_lhlo_with_xla.cc:670:38:   required from here                                                                                                                                         tensorflow/compiler/mlir/xla/transforms/mhlo_to_lhlo_with_xla.cc:651:32: error: cannot call member function 'mlir::DenseIntElementsAttr mlir::LhloDialectEmitter::GetI64DenseElementsAttr(const T&) [with T = google::protobuf::RepeatedField<long int>]' without object                                                                                                                                                                                                             GetI64DenseElementsAttr(hlo_dims.lhs_batch_dimensions()),                                                                                                                                                                             ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                                                                                                                                                     tensorflow/compiler/mlir/xla/transforms/mhlo_to_lhlo_with_xla.cc:652:32: error: cannot call member function 'mlir::DenseIntElementsAttr mlir::LhloDialectEmitter::GetI64DenseElementsAttr(const T&) [with T = google::protobuf::RepeatedField<long int>]' without object                                                                                                                                                                                                             GetI64DenseElementsAttr(hlo_dims.rhs_batch_dimensions()),                                                                                                                                                                             ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                                                                                                                                                     tensorflow/compiler/mlir/xla/transforms/mhlo_to_lhlo_with_xla.cc:653:32: error: cannot call member function 'mlir::DenseIntElementsAttr mlir::LhloDialectEmitter::GetI64DenseElementsAttr(const T&) [with T = google::protobuf::RepeatedField<long int>]' without object                                                                                                                                                                                                             GetI64DenseElementsAttr(hlo_dims.lhs_contracting_dimensions()),                                                                                                                                                                       ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                                                                                                                                               tensorflow/compiler/mlir/xla/transforms/mhlo_to_lhlo_with_xla.cc:654:32: error: cannot call member function 'mlir::DenseIntElementsAttr mlir::LhloDialectEmitter::GetI64DenseElementsAttr(const T&) [with T = google::protobuf::RepeatedField<long int>]' without object                                                                                                                                                                                                             GetI64DenseElementsAttr(hlo_dims.rhs_contracting_dimensions()),                                                                                                                                                                       ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                                                                                                                                               tensorflow/compiler/mlir/xla/transforms/mhlo_to_lhlo_with_xla.cc: In instantiation of 'mlir::LhloDialectEmitter::EmitGemm(const xla::HloCustomCallInstruction*)::<lambda(auto:10)> [with auto:10 = mlir::lmhlo_gpu::GEMM_BiasOp]':    tensorflow/compiler/mlir/xla/transforms/mhlo_to_lhlo_with_xla.cc:678:43:   required from here                                                                                                                                         tensorflow/compiler/mlir/xla/transforms/mhlo_to_lhlo_with_xla.cc:651:32: error: cannot call member function 'mlir::DenseIntElementsAttr mlir::LhloDialectEmitter::GetI64DenseElementsAttr(const T&) [with T = google::protobuf::RepeatedField<long int>]' without object                                                                                                                                                                                                             GetI64DenseElementsAttr(hlo_dims.lhs_batch_dimensions()),                                                                                                                                                                             ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                                                                                                                                                     tensorflow/compiler/mlir/xla/transforms/mhlo_to_lhlo_with_xla.cc:652:32: error: cannot call member function 'mlir::DenseIntElementsAttr mlir::LhloDialectEmitter::GetI64DenseElementsAttr(const T&) [with T = google::protobuf::RepeatedField<long int>]' without object                                                                                                                                                                                                             GetI64DenseElementsAttr(hlo_dims.rhs_batch_dimensions()),                                                                                                                                                                             ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                                                                                                                                                     tensorflow/compiler/mlir/xla/transforms/mhlo_to_lhlo_with_xla.cc:653:32: error: cannot call member function 'mlir::DenseIntElementsAttr mlir::LhloDialectEmitter::GetI64DenseElementsAttr(const T&) [with T = google::protobuf::RepeatedField<long int>]' without object                                                                                                                                                                                                             GetI64DenseElementsAttr(hlo_dims.lhs_contracting_dimensions()),`\r\n",
      "\r\n",
      "I have remove all the unrelated enviroment variables in the bash, but it still occurs. I have no idea, could anyone give some suggestions?\r\n",
      "\r\n",
      "**System information**\r\n",
      "- OS Platform and Distribution : CentOS Linux release 7.8.2003\r\n",
      "- GCC/Compiler version : 6.5.0\r\n",
      "- CUDA/cuDNN version: V10.0.130\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "subtype: ubuntu/linux\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  Process finished with exit code 132 (interrupted by signal 4: SIGILL)\n",
      "issue body -  ![image](https://user-images.githubusercontent.com/24789356/105575143-48a93300-5da4-11eb-9260-4846a809e2ea.png)\r\n",
      "i use tensorflow 2.4.1  in mac\n",
      "issue labels - \n",
      "TF 2.3\n",
      "TF 2.4\n",
      "stat:awaiting response\n",
      "subtype:macOS\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Function contrastive_loss is not supported when converted\n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:a GNU/Linux system with Linux kernel 4.15.0 on 1 6-core 3.60GHz Intel Core CPU i7-6850K with 64 GB RAM equipped with a NVIDIA Corporation GP102 GPUs\r\n",
      "- TensorFlow installed from (source or binary): source\r\n",
      "- TensorFlow version (use command below):tensorflow2-GPU\r\n",
      "- Python version:3.6\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "When I converted the trained hdf5 model to pb, the following function non-support occurred ：contrastive_loss\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "The hdf5 model should be successfully converted.\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "Provide a reproducible test case that is the bare minimum necessary to generate\r\n",
      "the problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n",
      "```\r\n",
      "from __future__ import absolute_import\r\n",
      "from __future__ import print_function\r\n",
      "import numpy as np\r\n",
      "\r\n",
      "import random\r\n",
      "from keras.datasets import mnist\r\n",
      "from keras.models import Model\r\n",
      "from keras.layers import Input, Flatten, Dense, Dropout, Lambda\r\n",
      "from keras.optimizers import RMSprop\r\n",
      "from keras import backend as K\r\n",
      "import time\r\n",
      "begin_time = time.time()\r\n",
      "num_classes = 10\r\n",
      "epochs = 20\r\n",
      "\r\n",
      "\r\n",
      "def euclidean_distance(vects):\r\n",
      "    x, y = vects\r\n",
      "    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)\r\n",
      "    return K.sqrt(K.maximum(sum_square, K.epsilon()))\r\n",
      "\r\n",
      "\r\n",
      "def eucl_dist_output_shape(shapes):\r\n",
      "    shape1, shape2 = shapes\r\n",
      "    return (shape1[0], 1)\r\n",
      "\r\n",
      "\r\n",
      "def contrastive_loss(y_true, y_pred):\r\n",
      "    '''Contrastive loss from Hadsell-et-al.'06\r\n",
      "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\r\n",
      "    '''\r\n",
      "    margin = 1\r\n",
      "    square_pred = K.square(y_pred)\r\n",
      "    margin_square = K.square(K.maximum(margin - y_pred, 0))\r\n",
      "    return K.mean(y_true * square_pred + (1 - y_true) * margin_square)\r\n",
      "\r\n",
      "\r\n",
      "def create_pairs(x, digit_indices):\r\n",
      "    '''Positive and negative pair creation.\r\n",
      "    Alternates between positive and negative pairs.\r\n",
      "    '''\r\n",
      "    pairs = []\r\n",
      "    labels = []\r\n",
      "    n = min([len(digit_indices[d]) for d in range(num_classes)]) - 1\r\n",
      "    for d in range(num_classes):\r\n",
      "        for i in range(n):\r\n",
      "            z1, z2 = digit_indices[d][i], digit_indices[d][i + 1]\r\n",
      "            pairs += [[x[z1], x[z2]]]\r\n",
      "            inc = random.randrange(1, num_classes)\r\n",
      "            dn = (d + inc) % num_classes\r\n",
      "            z1, z2 = digit_indices[d][i], digit_indices[dn][i]\r\n",
      "            pairs += [[x[z1], x[z2]]]\r\n",
      "            labels += [1, 0]\r\n",
      "    return np.array(pairs), np.array(labels)\r\n",
      "\r\n",
      "\r\n",
      "def create_base_network(input_shape):\r\n",
      "    '''Base network to be shared (eq. to feature extraction).\r\n",
      "    '''\r\n",
      "    input = Input(shape=input_shape)\r\n",
      "    x = Flatten()(input)\r\n",
      "    x = Dense(128, activation='relu')(x)\r\n",
      "    x = Dropout(0.6)(x)\r\n",
      "    x = Dense(128, activation='relu')(x)\r\n",
      "    x = Dropout(0.1)(x)\r\n",
      "    x = Dense(128, activation='relu')(x)\r\n",
      "    return Model(input, x)\r\n",
      "\r\n",
      "\r\n",
      "def compute_accuracy(y_true, y_pred):\r\n",
      "    '''Compute classification accuracy with a fixed threshold on distances.\r\n",
      "    '''\r\n",
      "    pred = y_pred.ravel() < 0.5\r\n",
      "    return np.mean(pred == y_true)\r\n",
      "\r\n",
      "\r\n",
      "def accuracy(y_true, y_pred):\r\n",
      "    '''Compute classification accuracy with a fixed threshold on distances.\r\n",
      "    '''\r\n",
      "    return K.mean(K.equal(y_true, K.cast(y_pred < 0.5, y_true.dtype)))\r\n",
      "\r\n",
      "\r\n",
      "# the data, split between train and test sets\r\n",
      "(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\n",
      "x_train = x_train.astype('float32')\r\n",
      "x_test = x_test.astype('float32')\r\n",
      "x_train /= 255\r\n",
      "x_test /= 255\r\n",
      "input_shape = x_train.shape[1:]\r\n",
      "\r\n",
      "# create training+test positive and negative pairs\r\n",
      "digit_indices = [np.where(y_train == i)[0] for i in range(num_classes)]\r\n",
      "tr_pairs, tr_y = create_pairs(x_train, digit_indices)\r\n",
      "\r\n",
      "digit_indices = [np.where(y_test == i)[0] for i in range(num_classes)]\r\n",
      "te_pairs, te_y = create_pairs(x_test, digit_indices)\r\n",
      "\r\n",
      "# network definition\r\n",
      "base_network = create_base_network(input_shape)\r\n",
      "\r\n",
      "input_a = Input(shape=input_shape)\r\n",
      "input_b = Input(shape=input_shape)\r\n",
      "\r\n",
      "# because we re-use the same instance `base_network`,\r\n",
      "# the weights of the network\r\n",
      "# will be shared across the two branches\r\n",
      "processed_a = base_network(input_a)\r\n",
      "processed_b = base_network(input_b)\r\n",
      "\r\n",
      "distance = Lambda(euclidean_distance,\r\n",
      "                  output_shape=eucl_dist_output_shape)([processed_a, processed_b])\r\n",
      "\r\n",
      "model = Model([input_a, input_b], distance)\r\n",
      "\r\n",
      "# train\r\n",
      "rms = RMSprop()\r\n",
      "model.compile(loss=contrastive_loss, optimizer=rms, metrics=[accuracy])\r\n",
      "model.fit([tr_pairs[:, 0], tr_pairs[:, 1]], tr_y,\r\n",
      "          batch_size=128,\r\n",
      "          epochs=epochs,\r\n",
      "          validation_data=([te_pairs[:, 0], te_pairs[:, 1]], te_y))\r\n",
      "# compute final accuracy on training and test sets\r\n",
      "y_pred = model.predict([tr_pairs[:, 0], tr_pairs[:, 1]])\r\n",
      "tr_acc = compute_accuracy(tr_y, y_pred)\r\n",
      "y_pred = model.predict([te_pairs[:, 0], te_pairs[:, 1]])\r\n",
      "te_acc = compute_accuracy(te_y, y_pred)\r\n",
      "start_time = time.time()\r\n",
      "end_time = time.time()\r\n",
      "time_data = end_time - start_time\r\n",
      "print('Training Time costs', start_time - begin_time)\r\n",
      "print('Tesing Time costs', time_data)\r\n",
      "print('Test accuracy:' ,te_acc)\r\n",
      "model.save('model/151.h5')\r\n",
      "```\r\n",
      "\r\n",
      "**Other info / logs** Include any logs or source code that would be helpful to\r\n",
      "diagnose the problem. If including tracebacks, please include the full\r\n",
      "traceback. Large logs and files should be attached.\r\n",
      "```\r\n",
      "Using TensorFlow backend.\r\n",
      "2020-10-23 10:09:30.768594: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n",
      "2020-10-23 10:09:32.233609: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: \r\n",
      "name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.6325\r\n",
      "pciBusID: 0000:05:00.0\r\n",
      "totalMemory: 10.92GiB freeMemory: 10.77GiB\r\n",
      "2020-10-23 10:09:32.340284: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 1 with properties: \r\n",
      "name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.6325\r\n",
      "pciBusID: 0000:09:00.0\r\n",
      "totalMemory: 10.92GiB freeMemory: 10.77GiB\r\n",
      "2020-10-23 10:09:32.341621: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0, 1\r\n",
      "2020-10-23 10:09:33.016225: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n",
      "2020-10-23 10:09:33.016290: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 1 \r\n",
      "2020-10-23 10:09:33.016303: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N Y \r\n",
      "2020-10-23 10:09:33.016312: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 1:   Y N \r\n",
      "2020-10-23 10:09:33.016476: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10426 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:05:00.0, compute capability: 6.1)\r\n",
      "2020-10-23 10:09:33.199564: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10428 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:09:00.0, compute capability: 6.1)\r\n",
      "E1023 10:09:33.461142 140655137916672 keras_to_tensorflow.py:94] Input file specified only holds the weights, and not the model definition. Save the model using model.save(filename.h5) which will contain the network architecture as well as its weights. If the model is saved using the model.save_weights(filename) function, either input_model_json or input_model_yaml flags should be set to to import the network architecture prior to loading the weights. \r\n",
      "Check the keras documentation for more details (https://keras.io/getting-started/faq/)\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"keras_to_tensorflow.py\", line 181, in <module>\r\n",
      "    app.run(main)\r\n",
      "  File \"/home/dl/anaconda3/envs/dong/lib/python3.6/site-packages/absl/app.py\", line 299, in run\r\n",
      "    _run_main(main, args)\r\n",
      "  File \"/home/dl/anaconda3/envs/dong/lib/python3.6/site-packages/absl/app.py\", line 250, in _run_main\r\n",
      "    sys.exit(main(argv))\r\n",
      "  File \"keras_to_tensorflow.py\", line 127, in main\r\n",
      "    model = load_model(FLAGS.input_model, FLAGS.input_model_json, FLAGS.input_model_yaml)\r\n",
      "  File \"keras_to_tensorflow.py\", line 105, in load_model\r\n",
      "    raise wrong_file_err\r\n",
      "  File \"keras_to_tensorflow.py\", line 62, in load_model\r\n",
      "    model = keras.models.load_model(input_model_path)\r\n",
      "  File \"/home/dl/anaconda3/envs/dong/lib/python3.6/site-packages/keras/engine/saving.py\", line 419, in load_model\r\n",
      "    model = _deserialize_model(f, custom_objects, compile)\r\n",
      "  File \"/home/dl/anaconda3/envs/dong/lib/python3.6/site-packages/keras/engine/saving.py\", line 312, in _deserialize_model\r\n",
      "    sample_weight_mode=sample_weight_mode)\r\n",
      "  File \"/home/dl/anaconda3/envs/dong/lib/python3.6/site-packages/keras/engine/training.py\", line 139, in compile\r\n",
      "    loss_function = losses.get(loss)\r\n",
      "  File \"/home/dl/anaconda3/envs/dong/lib/python3.6/site-packages/keras/losses.py\", line 133, in get\r\n",
      "    return deserialize(identifier)\r\n",
      "  File \"/home/dl/anaconda3/envs/dong/lib/python3.6/site-packages/keras/losses.py\", line 114, in deserialize\r\n",
      "    printable_module_name='loss function')\r\n",
      "  File \"/home/dl/anaconda3/envs/dong/lib/python3.6/site-packages/keras/utils/generic_utils.py\", line 165, in deserialize_keras_object\r\n",
      "    ':' + function_name)\r\n",
      "ValueError: Unknown loss function:contrastive_loss\r\n",
      "\r\n",
      "```\n",
      "issue labels - \n",
      "TF 2.0\n",
      "comp:keras\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Operator Softsign and Softplus are not supported by the standard TensorFlow Lite runtime\n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:a GNU/Linux system with Linux kernel 4.15.0 on 1 6-core 3.60GHz Intel Core CPU i7-6850K with 64 GB RAM equipped with a NVIDIA Corporation GP102 GPUs\r\n",
      "- TensorFlow installed from (source or binary): source\r\n",
      "- TensorFlow version (use command below):tensorflow2-GPU\r\n",
      "- Python version:3.6\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "When I converted the trained hdf5 model to tflite, the following operator non-support occurred ：Softsign\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "The hdf5 model should be successfully converted to the format of tflite.\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "Provide a reproducible test case that is the bare minimum necessary to generate\r\n",
      "the problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n",
      "```\r\n",
      "batch_size = 80\r\n",
      "epochs = 171\r\n",
      "num_classes = 10\r\n",
      "import os\r\n",
      "save_dir = 'model'\r\n",
      "model_name = 'model.h5'\r\n",
      "import keras as keras\r\n",
      "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\r\n",
      "img_rows, img_cols = x_train.shape[1], x_train.shape[2]\r\n",
      "\r\n",
      "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\r\n",
      "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\r\n",
      "input_shape = (img_rows, img_cols, 1)\r\n",
      "x_train = x_train.astype('float32')\r\n",
      "x_test = x_test.astype('float32')\r\n",
      "x_train /= 255\r\n",
      "x_test /= 255\r\n",
      "y_train = keras.utils.to_categorical(y_train, num_classes)\r\n",
      "y_test = keras.utils.to_categorical(y_test, num_classes)\r\n",
      "\r\n",
      "import keras as keras\r\n",
      "model = keras.models.Sequential()\r\n",
      "model.add(keras.layers.ThresholdedReLU(theta=0.3514439122821289))\r\n",
      "model.add(keras.layers.LeakyReLU(alpha=0.4855740853866919))\r\n",
      "model.add(keras.layers.AveragePooling2D(pool_size = (1, 2), padding='same'))\r\n",
      "\r\n",
      "model.add(keras.layers.Flatten())\r\n",
      "model.add(keras.layers.Dense(num_classes, activation='softsign'))\r\n",
      "model.compile(loss=keras.losses.categorical_crossentropy,optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])\r\n",
      "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\r\n",
      "\r\n",
      "model_path = os.path.join(save_dir, model_name)\r\n",
      "model.save(model_path)\r\n",
      "```\r\n",
      "\r\n",
      "**Other info / logs** Include any logs or source code that would be helpful to\r\n",
      "diagnose the problem. If including tracebacks, please include the full\r\n",
      "traceback. Large logs and files should be attached.\r\n",
      "```\r\n",
      "Exception: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n",
      " and pasting the following:\r\n",
      "\r\n",
      "Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: AVERAGE_POOL_2D, CAST, FULLY_CONNECTED, GREATER, MAXIMUM, MUL. Here is a list of operators for which you will need custom implementations: Softsign.\r\n",
      "```\r\n",
      "\n",
      "issue labels - \n",
      "comp:lite\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Operator Softplus is not supported by the standard TensorFlow Lite runtime\n",
      "issue body -  \r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): a GNU/Linux system with Linux kernel 4.15.0 on 1 6-core 3.60GHz Intel Core CPU i7-6850K with 64 GB RAM equipped with a NVIDIA Corporation GP102 GPUs\r\n",
      "- TensorFlow installed from (source or binary): source\r\n",
      "- TensorFlow version (use command below): tensorflow2.1.0-GPU\r\n",
      "- Python version: 3.6\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "When I converted the trained hdf5 model to tflite, the following operator non-support occurred ：Softplus\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "The hdf5 model should be successfully converted to the format of tflite.\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "Provide a reproducible test case that is the bare minimum necessary to generate\r\n",
      "the problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n",
      "```\r\n",
      "batch_size = 122\r\n",
      "epochs = 148\r\n",
      "num_classes = 10\r\n",
      "import os\r\n",
      "save_dir = 'model'\r\n",
      "model_name = 'trained_model.h5'\r\n",
      "import keras as keras\r\n",
      "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\r\n",
      "img_rows, img_cols = x_train.shape[1], x_train.shape[2]\r\n",
      "\r\n",
      "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\r\n",
      "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\r\n",
      "input_shape = (img_rows, img_cols, 1)\r\n",
      "x_train = x_train.astype('float32')\r\n",
      "x_test = x_test.astype('float32')\r\n",
      "x_train /= 255\r\n",
      "x_test /= 255\r\n",
      "y_train = keras.utils.to_categorical(y_train, num_classes)\r\n",
      "y_test = keras.utils.to_categorical(y_test, num_classes)\r\n",
      "\r\n",
      "import keras as keras\r\n",
      "model = keras.models.Sequential()\r\n",
      "model.add(keras.layers.ThresholdedReLU(theta=0.3597445834106594))\r\n",
      "model.add(keras.layers.MaxPooling2D(pool_size = (1, 1), strides = (1, 1), padding = 'valid'))\r\n",
      "\r\n",
      "model.add(keras.layers.Flatten())\r\n",
      "model.add(keras.layers.Dense(num_classes, activation='softplus'))\r\n",
      "model.compile(loss=keras.losses.categorical_crossentropy,optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])\r\n",
      "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\r\n",
      "\r\n",
      "model_path = os.path.join(save_dir, model_name)\r\n",
      "model.save(model_path)\r\n",
      "```\r\n",
      "\r\n",
      "**Other info / logs** Include any logs or source code that would be helpful to\r\n",
      "diagnose the problem. If including tracebacks, please include the full\r\n",
      "traceback. Large logs and files should be attached.\r\n",
      "```\r\n",
      "Exception: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n",
      " and pasting the following:\r\n",
      "\r\n",
      "Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CAST, FULLY_CONNECTED, GREATER, MAX_POOL_2D, MUL. Here is a list of operators for which you will need custom implementations: Softplus.\r\n",
      "```\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.1\n",
      "comp:keras\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  NaN occurs when building with BatchNormalization, PReLU, Flatten and Dense\n",
      "issue body -  <em>Please make sure that this is a bug. As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:bug_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:a GNU/Linux system with Linux kernel 4.15.0 on 1 6-core 3.60GHz Intel Core CPU i7-6850K with 64 GB RAM equipped with a NVIDIA Corporation GP102 GPUs\r\n",
      "- TensorFlow installed from (source or binary): source\r\n",
      "- TensorFlow version (use command below):tensorflow2-GPU\r\n",
      "- Python version:3.6\r\n",
      "\r\n",
      "You can collect some of this information using our environment capture\r\n",
      "[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n",
      "You can also obtain the TensorFlow version with:\r\n",
      "1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n",
      "2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n",
      "\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "I got the NaN loss when I tried to train my model\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "The loss should be a number\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "Provide a reproducible test case that is the bare minimum necessary to generate\r\n",
      "the problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n",
      "```\r\n",
      "batch_size = 112\r\n",
      "epochs = 119\r\n",
      "num_classes = 10\r\n",
      "import os\r\n",
      "save_dir = 'model'\r\n",
      "model_name = 'test971_trained_model.h5'\r\n",
      "import tensorflow.keras as keras\r\n",
      "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\r\n",
      "print('x_train shape:', x_train.shape)\r\n",
      "print(x_train.shape[0], 'train samples')\r\n",
      "print(x_test.shape[0], 'test samples')\r\n",
      "img_rows, img_cols = x_train.shape[1], x_train.shape[2]\r\n",
      "y_train = keras.utils.to_categorical(y_train, num_classes)\r\n",
      "y_test = keras.utils.to_categorical(y_test, num_classes)\r\n",
      "model = keras.models.Sequential()\r\n",
      "model.add(keras.layers.BatchNormalization(momentum = 0.4639004933194679,epsilon=0.6515653837017596))\r\n",
      "model.add(keras.layers.PReLU(alpha_initializer='Zeros'))\r\n",
      "\r\n",
      "model.add(keras.layers.Flatten())\r\n",
      "model.add(keras.layers.Dense(num_classes))\r\n",
      "x_train = x_train.astype('float32')\r\n",
      "x_test = x_test.astype('float32')\r\n",
      "x_train /= 255\r\n",
      "x_test /= 255\r\n",
      "model.compile(loss=keras.losses.categorical_crossentropy,optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])\r\n",
      "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\r\n",
      "model_path = os.path.join(save_dir, model_name)\r\n",
      "model.save(model_path)\r\n",
      "```\r\n",
      "\r\n",
      "**Other info / logs** Include any logs or source code that would be helpful to\r\n",
      "diagnose the problem. If including tracebacks, please include the full\r\n",
      "traceback. Large logs and files should be attached.\r\n",
      "\r\n",
      "```\r\n",
      "Epoch 1/119\r\n",
      "\r\n",
      "  112/50000 [..............................] - ETA: 37:05 - loss: nan - accuracy: 0.0804\r\n",
      "\r\n",
      "  448/50000 [..............................] - ETA: 9:20 - loss: nan - accuracy: 0.0826 \r\n",
      "\r\n",
      "  784/50000 [..............................] - ETA: 5:22 - loss: nan - accuracy: 0.0778\r\n",
      "```\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:keras\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Error MTCNN with tensorflow ROCM \n",
      "issue body -  Traceback (most recent call last):\r\n",
      "  File \"webcam.py\", line 51, in <module>\r\n",
      "    main()\r\n",
      "  File \"webcam.py\", line 32, in main\r\n",
      "    boxes, _ = detect_face.detect_face(img, minsize, pnet, rnet, onet, threshold, factor)\r\n",
      "  File \"/home/taitang/tracking/mtcnn-face-detect/src/detect_face.py\", line 361, in detect_face\r\n",
      "    tmp[dy[k]-1:edy[k],dx[k]-1:edx[k],:] = img[y[k]-1:ey[k],x[k]-1:ex[k],:]\r\n",
      "ValueError: could not broadcast input array from shape (457,628,3) into shape (0,628,3)\r\n",
      "\r\n",
      "i tried to run pretrain model MTCNN with Tensorflow ROCM and I received error message as above. Any one help me? Thanks\n",
      "issue labels - \n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  TF 2.4.1 and tf_nightly-2.5.0.dev20210122 dump core on Intel Xeon ES462 under Ubuntu 20.04.1\n",
      "issue body -  **System information**\r\n",
      "System: Intel Xeon ES462 under Ubuntu 20.04.1\r\n",
      "Python 3.8.5\r\n",
      "TensorFlow 2.4.1 and tf_nightly-2.5.0.dev20210122-cp38-cp38-manylinux2010_x86_64.whl\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "Core dumped with illegal instruction (vpxor)\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "Core not dumped\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "import tensorflow\r\n",
      "\r\n",
      "**Other info / logs** Include any logs or source code that would be helpful to\r\n",
      "Partial backtrace:\r\n",
      "```\r\n",
      "Program terminated with signal SIGILL, Illegal instruction.\r\n",
      "#0  0x00007fee27336930 in nsync::nsync_mu_init(nsync::nsync_mu_s_*) () from /home/REDACTED/virtualenv3/lib/python3.8/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n",
      "[Current thread is 1 (Thread 0x7fee80a8d740 (LWP 68631))]\r\n",
      "(gdb) bt\r\n",
      "#0  0x00007fee27336930 in nsync::nsync_mu_init(nsync::nsync_mu_s_*) () from /home/REDACTED/virtualenv3/lib/python3.8/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n",
      "#1  0x00007fee18748c5b in tensorflow::monitoring::Counter<2>* tensorflow::monitoring::Counter<2>::New<char const (&) [46], char const (&) [58], char const (&) [11], char const (&) [7]>(char const (&) [46], char const (&) [58], char const (&) [11], char const (&) [7]) ()\r\n",
      "   from /home/REDACTED/virtualenv3/lib/python3.8/site-packages/tensorflow/python/../libtensorflow_framework.so.2\r\n",
      "#2  0x00007fee1870d31e in _GLOBAL__sub_I_loader.cc () from /home/REDACTEDr/virtualenv3/lib/python3.8/site-packages/tensorflow/python/../libtensorflow_framework.so.2\r\n",
      "#3  0x00007fee80e6fb8a in ?? () from /lib64/ld-linux-x86-64.so.2\r\n",
      "\r\n",
      "(gdb) disassemble\r\n",
      "Dump of assembler code for function _ZN5nsync13nsync_mu_initEPNS_11nsync_mu_s_E:\r\n",
      "=> 0x00007fee27336930 <+0>:\tvpxor  %xmm0,%xmm0,%xmm0\r\n",
      "   0x00007fee27336934 <+4>:\tpush   %rbp\r\n",
      "   0x00007fee27336935 <+5>:\tmov    %rsp,%rbp\r\n",
      "   0x00007fee27336938 <+8>:\tvmovups %xmm0,(%rdi)\r\n",
      "   0x00007fee2733693c <+12>:\tpop    %rbp\r\n",
      "   0x00007fee2733693d <+13>:\tretq   \r\n",
      "```\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "subtype: ubuntu/linux\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  XLA Convolution Causes Segmentation Fault\n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 20.04\r\n",
      "- TensorFlow installed from (source or binary): From source, commit: 78ba23b6bcd7fe416aad1da4fe47b2b6036e09ad\r\n",
      "- TensorFlow version (use command below): commit: 78ba23b6bcd7fe416aad1da4fe47b2b6036e09ad\r\n",
      "- Python version: 3.8.5\r\n",
      "- Bazel version (if compiling from source): 3.7.2\r\n",
      "- GCC/Compiler version (if compiling from source): 9.3.0\r\n",
      "- CUDA/cuDNN version: 11.0/8.0.4\r\n",
      "- GPU model and memory: GeForce RTX 2070, Driver: 460.32.03\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "\r\n",
      "I'm currently working on writing XLA support for another language. Recently, I upgraded to CUDA 11.0 and cuDNN 8.0.4. Previously, we were using CUDA 10.2 and cuDNN 7. After the switch, all of our convolution tests started producing SegFaults. We also get some warnings during execution:\r\n",
      "\r\n",
      "```\r\n",
      "warning: Linking two modules of different target triples: '/usr/local/cuda-11.0/nvvm/libdevice/libdevice.10.bc' is 'nvptx64-nvidia-gpulibs' whereas '' is 'nvptx64-nvidia-cuda'\r\n",
      "```\r\n",
      "\r\n",
      "Occasionally, it will warn about failing to find an optimal convolution algorithm and fall back to the default before SegFaulting.\r\n",
      "\r\n",
      "I suspect it has something to do with a version mismatch. I was originally running off an older commit (prior to 2.4 getting released), and upgraded to the most recent commit to see if it would fix it, but the issue persists.\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "\r\n",
      "An XLA computation similar to this should reproduce (I'm working in a separate language to build up the computations).\r\n",
      "\r\n",
      "```\r\n",
      "  xla::XlaBuilder* builder = new xla::XlaBuilder(\"conv\");\r\n",
      "  xla::Shape input_shape = xla::ShapeUtil::MakeShape(xla::PrimitiveType::F32, {32, 3, 120, 120});\r\n",
      "  xla::Shape kernel_shape = xla::ShapeUtil::MakeShape(xla::PrimitiveType::F32, {32, 3, 3, 3});\r\n",
      "\r\n",
      "  xla::XlaOp inp = xla::RngUniform(*builder, input_shape);\r\n",
      "  xla::XlaOp kernel = xla::RngUniform(*builder, kernel_shape);\r\n",
      "\r\n",
      "  xla::ConvolutionDimensionNumbers dimension_numbers;\r\n",
      "  dimension_numbers.set_input_batch_dimension(0);\r\n",
      "  dimension_numbers.set_input_feature_dimension(1);\r\n",
      "  dimension_numbers.set_kernel_output_feature_dimension(0);\r\n",
      "  dimension_numbers.set_kernel_input_feature_dimension(1);\r\n",
      "  dimension_numbers.set_output_batch_dimension(0);\r\n",
      "  dimension_numbers.set_output_feature_dimension(1);\r\n",
      "\r\n",
      "  xla::XlaOp result = xla::ConvGeneralDilated(inp,\r\n",
      "                                              kernel,\r\n",
      "                                              /*padding=*/{},\r\n",
      "                                              /*lhs_dilation=*/{},\r\n",
      "                                              /*rhs_dilation=*/{},\r\n",
      "                                              /*conv_dimnos=*/dimension_numbers);\r\n",
      "```\r\n",
      "\r\n",
      "**Other info / logs** Include any logs or source code that would be helpful to\r\n",
      "diagnose the problem. If including tracebacks, please include the full\r\n",
      "traceback. Large logs and files should be attached.\r\n",
      "\r\n",
      "Core Dump backtrace:\r\n",
      "\r\n",
      "```\r\n",
      "#0  0x00007fae997adc84 in ?? () from /lib/x86_64-linux-gnu/libcuda.so.1\r\n",
      "#1  0x00007fae9974144b in ?? () from /lib/x86_64-linux-gnu/libcuda.so.1\r\n",
      "#2  0x00007fae997417b8 in ?? () from /lib/x86_64-linux-gnu/libcuda.so.1\r\n",
      "#3  0x00007fae99998697 in ?? () from /lib/x86_64-linux-gnu/libcuda.so.1\r\n",
      "#4  0x00007fae99998c02 in ?? () from /lib/x86_64-linux-gnu/libcuda.so.1\r\n",
      "#5  0x00007fae99759d8a in ?? () from /lib/x86_64-linux-gnu/libcuda.so.1\r\n",
      "#6  0x00007fae9999e3e0 in ?? () from /lib/x86_64-linux-gnu/libcuda.so.1\r\n",
      "#7  0x00007fae99713243 in ?? () from /lib/x86_64-linux-gnu/libcuda.so.1\r\n",
      "#8  0x00007fae99714555 in ?? () from /lib/x86_64-linux-gnu/libcuda.so.1\r\n",
      "#9  0x00007fae997bab93 in cuLaunchKernel () from /lib/x86_64-linux-gnu/libcuda.so.1\r\n",
      "#10 0x00007faa1d62f8bb in ?? () from /lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8\r\n",
      "#11 0x00007faa1d671686 in ?? () from /lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8\r\n",
      "#12 0x00007faa1a926124 in cudnn::cnn::cudnnIm2Col4d(cudnnContext*, cudnnTensor4dStruct*, void const*, cudnnFilter4dStruct*, cudnnConvolutionStruct*, void*) ()\r\n",
      "   from /lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8\r\n",
      "#13 0x00007faa1a8e05a0 in cudnn::cnn::GemmConvolveEngine<cudnn::cnn::gemm_convolve_launch_pf<double, double, double, double>, false, 9, 2>::execute_internal_impl(cudnn::backend::VariantPack const&, CUstream_st*) () from /lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8\r\n",
      "#14 0x00007faa1a537033 in cudnn::cnn::EngineInterface::execute(cudnn::backend::VariantPack const&, CUstream_st*) () from /lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8\r\n",
      "#15 0x00007faa1a563960 in cudnn::cnn::EngineContainer<(cudnnBackendEngineName_t)2, 4096ul>::execute_internal_impl(cudnn::backend::VariantPack const&, CUstream_st*) ()\r\n",
      "   from /lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8\r\n",
      "#16 0x00007faa1a537033 in cudnn::cnn::EngineInterface::execute(cudnn::backend::VariantPack const&, CUstream_st*) () from /lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8\r\n",
      "#17 0x00007faa1a5be18c in cudnn::cnn::AutoTransformationExecutor::execute_pipeline(cudnn::cnn::ConvolutionEngine&, cudnn::backend::VariantPack const&, CUstream_st*) const ()\r\n",
      "   from /lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8\r\n",
      "#18 0x00007faa1a5d9871 in cudnn::cnn::GeneralizedConvolutionEngine<cudnn::cnn::EngineContainer<(cudnnBackendEngineName_t)2, 4096ul> >::execute_internal_impl(cudnn::backend::VariantPack const&, CUs--Type <RET> for more, q to quit, c to continue without paging--\r\n",
      "tream_st*) () from /lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8\r\n",
      "#19 0x00007faa1a537033 in cudnn::cnn::EngineInterface::execute(cudnn::backend::VariantPack const&, CUstream_st*) () from /lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8\r\n",
      "#20 0x00007faa1a53e730 in cudnn::backend::execute(cudnnContext*, cudnn::backend::ExecutionPlan&, cudnn::backend::VariantPack&) () from /lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8\r\n",
      "#21 0x00007faa1a63f40c in cudnn::backend::EnginesAlgoMap<cudnnConvolutionFwdAlgo_t, 8>::execute_wrapper(cudnnContext*, cudnnConvolutionFwdAlgo_t, cudnn::backend::ExecutionPlan&, cudnn::backend::VariantPack&) () from /lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8\r\n",
      "#22 0x00007faa1a638ad9 in cudnn::backend::convolutionForward(cudnnContext*, void const*, cudnnTensorStruct const*, void const*, cudnnFilterStruct const*, void const*, cudnnConvolutionStruct const*, cudnnConvolutionFwdAlgo_t, void*, unsigned long, bool, void const*, void const*, void const*, cudnnActivationStruct const*, cudnnTensorStruct const*, void*) ()\r\n",
      "   from /lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8\r\n",
      "#23 0x00007faa1a73ae96 in cudnn::cnn::convolutionForward(cudnnContext*, void const*, cudnnTensorStruct*, void const*, cudnnFilterStruct*, void const*, cudnnConvolutionStruct*, cudnnConvolutionFwdAlgo_t, void*, unsigned long, void const*, cudnnTensorStruct*, void*) () from /lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8\r\n",
      "#24 0x00007faa1a73b94c in cudnnConvolutionForward () from /lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8\r\n",
      "#25 0x00007fadf91ab772 in cudnnConvolutionForward () from /home/sean/projects/exla/_build/test/lib/exla/priv/libexla.so\r\n",
      "#26 0x00007fadf9191c6d in stream_executor::gpu::CudnnSupport::DoConvolve(stream_executor::dnn::ConvolutionKind, stream_executor::dnn::DataType, stream_executor::dnn::DataType, stream_executor::Stream*, stream_executor::dnn::BatchDescriptor const&, stream_executor::DeviceMemoryBase, stream_executor::dnn::FilterDescriptor const&, stream_executor::DeviceMemoryBase, stream_executor::dnn::BatchDescriptor const&, stream_executor::DeviceMemoryBase, stream_executor::dnn::ConvolutionDescriptor const&, stream_executor::dnn::AlgorithmDesc, stream_executor::DeviceMemory<unsigned char>, stream_executor::dnn::ProfileResult*) () from /home/sean/projects/exla/_build/test/lib/exla/priv/libexla.so\r\n",
      "#27 0x00007fadf72a1735 in tensorflow::Status xla::gpu::(anonymous namespace)::RunGpuConvImpl<double, double, double>(xla::gpu::GpuConvParams const&, stream_executor::ScratchAllocator*, stream_executor::Stream*, xla::gpu::RunConvOptions) () from /home/sean/projects/exla/_build/test/lib/exla/priv/libexla.so\r\n",
      "#28 0x00007fadf72a4a2e in xla::gpu::RunGpuConv(xla::gpu::GpuConvConfig const&, absl::lts_2020_02_25::Span<stream_executor::DeviceMemoryBase>, stream_executor::DeviceMemoryBase, stream_executor::ScratchAllocator*, stream_executor::Stream*, xla::gpu::RunConvOptions) () from /home/sean/projects/exla/_build/test/lib/exla/priv/libexla.so\r\n",
      "#29 0x00007fadf72a70e4 in xla::gpu::RunGpuConv(xla::gpu::GpuConvConfig const&, absl::lts_2020_02_25::Span<stream_executor::DeviceMemoryBase>, stream_executor::DeviceMemoryBase, stream_executor::DeviceMemoryBase, stream_executor::Stream*, xla::gpu::RunConvOptions) () from /home/sean/projects/exla/_build/test/lib/exla/priv/libexla.so\r\n",
      "#30 0x00007fadf72473fb in xla::gpu::ConvolutionThunk::ExecuteOnStream(xla::gpu::Thunk::ExecuteParams const&) () from /home/sean/projects/exla/_build/test/lib/exla/priv/libexla.so\r\n",
      "--Type <RET> for more, q to quit, c to continue without paging--\r\n",
      "#31 0x00007fadf72573bb in xla::gpu::GpuExecutable::ExecuteThunks(xla::ServiceExecutableRunOptions const*, xla::gpu::BufferAllocations const&, bool, xla::HloExecutionProfile*) ()\r\n",
      "   from /home/sean/projects/exla/_build/test/lib/exla/priv/libexla.so\r\n",
      "#32 0x00007fadf725c2cc in xla::gpu::GpuExecutable::ExecuteAsyncOnStreamImpl(xla::ServiceExecutableRunOptions const*, absl::lts_2020_02_25::variant<absl::lts_2020_02_25::Span<xla::ShapedBuffer const* const>, absl::lts_2020_02_25::Span<xla::ExecutionInput> >, xla::HloExecutionProfile*) () from /home/sean/projects/exla/_build/test/lib/exla/priv/libexla.so\r\n",
      "#33 0x00007fadf725c8f8 in xla::gpu::GpuExecutable::ExecuteAsyncOnStream(xla::ServiceExecutableRunOptions const*, std::vector<xla::ExecutionInput, std::allocator<xla::ExecutionInput> >, xla::HloExecutionProfile*) () from /home/sean/projects/exla/_build/test/lib/exla/priv/libexla.so\r\n",
      "#34 0x00007fadfd69b286 in xla::Executable::ExecuteAsyncOnStreamWrapper(xla::ServiceExecutableRunOptions const*, std::vector<xla::ExecutionInput, std::allocator<xla::ExecutionInput> >) ()\r\n",
      "   from /home/sean/projects/exla/_build/test/lib/exla/priv/libexla.so\r\n",
      "#35 0x00007fadf90c0df7 in xla::LocalExecutable::RunAsync(absl::lts_2020_02_25::Span<xla::Shape const* const>, std::vector<xla::ExecutionInput, std::allocator<xla::ExecutionInput> >, xla::ExecutableRunOptions) () from /home/sean/projects/exla/_build/test/lib/exla/priv/libexla.so\r\n",
      "#36 0x00007fadf90c17c8 in xla::LocalExecutable::RunAsync(std::vector<xla::ExecutionInput, std::allocator<xla::ExecutionInput> >, xla::ExecutableRunOptions) ()\r\n",
      "```\r\n",
      "\r\n",
      "Forgive me if this is a CUDA version issue, or if this is a question better asked in the XLA Dev Group.\n",
      "issue labels - \n",
      "TF 2.5\n",
      "comp:gpu\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Make H-loop-fusion share operands with users.\n",
      "issue body -  \n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:xla\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  ERRORS \n",
      "issue body -  \r\n",
      "\r\n",
      "- Windows 10 64-bit \r\n",
      "- python 3.8.3 and pip 20.3.3\r\n",
      "\r\n",
      "every time I try to install I get the same error messages:\r\n",
      "ERROR: Could not find a version that satisfies the requirement tensorflow\r\n",
      "ERROR: No matching distribution found for tensorflow\r\n",
      "\r\n",
      "im kind of at a loss for what to do, any help would be appreciated \r\n",
      "\n",
      "issue labels - \n",
      "stalled\n",
      "stat:awaiting response\n",
      "subtype:windows\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  Missed \")\"\n",
      "issue body -  Small fix to doc.\n",
      "issue labels - \n",
      "cla: no\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  [ROCm] Fix for a bug in ROCm batchnorm implementation.\n",
      "issue body -  Fixing the bug also makes the unit-test `//tensorflow/python/keras/layers:normalization_test_gpu` pass, so removing the `no_rocm` tag from it as well.\r\n",
      "\r\n",
      "------------------------------------\r\n",
      "\r\n",
      "/cc @cheshire @chsigg @nvining-work \n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:gpu\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Object Detection API to Tensorflow Lite\n",
      "issue body -  **System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n",
      "- TensorFlow installed from (source or binary): pip\r\n",
      "- TensorFlow version (or github SHA if from source): 2.4.0\r\n",
      "- CUDA 11.0\r\n",
      "\r\n",
      "I have successfully trained my own model using the Object Detection API. \r\n",
      "As a pre-trained model I used ssd_resnet50v1_fpn_640x60.\r\n",
      "The detection works fine.\r\n",
      "\r\n",
      "Now I want to convert the trained model to Tensorflow Lite.\r\n",
      "I used export_tflite_graph_tf2.py and after that the TensorFlow Lite-Konverter-API:\r\n",
      "\r\n",
      "```\r\n",
      "import tensorflow as tf\r\n",
      "\r\n",
      "# Convert the model\r\n",
      "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir) # path to the SavedModel directory from export_tflite_graph_tf2.py\r\n",
      "tflite_model = converter.convert()\r\n",
      "\r\n",
      "# Save the model.\r\n",
      "with open('model.tflite', 'wb') as f:\r\n",
      "  f.write(tflite_model)\r\n",
      "```\r\n",
      "\r\n",
      "The model.tflite was succesfully created but unfortunately when I run the detection, nothing is detected.\r\n",
      "tflite-runtime: 2.5.0\r\n",
      "I used this guide https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/blob/master/Raspberry_Pi_Guide.md and with the coco_ssd_mobilenet_v1_1.0_quant_2018_06_29 everything works.\r\n",
      "\r\n",
      "I got no errors, just no detection with my own model at all.\r\n",
      "does anyone have an idea what this can be due to?\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:lite\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Deleting a model and clearing the session if other models are still loaded and in use\n",
      "issue body -  In my Python application I load several TF 2.4 / Keras models in parallel and use their `predict()` functions in separate threads. The single GPU in the machine is locked by a Python `Lock`, so that these threads cannot run \"their\" TF models at the same time. Now at some point a thread can decide to delete its currently loaded TF model (via Python's `del` command) and load another one (via `tensorflow.keras.models.load_model()`). However, in that case the other threads with their loaded TF models must not be affected.\r\n",
      "\r\n",
      "Now I want to free all memory when I delete a model in a thread and load another model. So I thought about calling `tf.keras.backend.clear_session()` after deleting a model. But it seems that is not an option in my case, since other models are still loaded and in use, as described above.\r\n",
      "\r\n",
      "I might put all these models into their own processes (instead of using threads), but then I might run into the problem that I cannot lock and make use of that single GPU from multiple processes in parallel. So is there another, \"official\" method to properly free all memory after deleting a TF/Keras model in such a multi-model setup?\n",
      "issue labels - \n",
      "comp:keras\n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  INFO:absl:using experimental converter:if you encountered a problem please file a bug\n",
      "issue body -  **System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n",
      "- TensorFlow installed from (source or binary):\r\n",
      "- TensorFlow version (or github SHA if from source):\r\n",
      "\r\n",
      "\r\n",
      "**Command used to run the converter or code if you’re using the Python API**\r\n",
      "If possible, please share a link to Colab/Jupyter/any notebook.\r\n",
      "\r\n",
      "```\r\n",
      "# Copy and paste here the exact command\r\n",
      "```\r\n",
      "\r\n",
      "**The output from the converter invocation**\r\n",
      "\r\n",
      "```\r\n",
      "# Copy and paste the output here.\r\n",
      "```\r\n",
      "\r\n",
      "**Also, please include a link to the saved model or GraphDef**\r\n",
      "\r\n",
      "```\r\n",
      "# Put link here or attach to the issue.\r\n",
      "```\r\n",
      "\r\n",
      "**Failure details**\r\n",
      "If the conversion is successful, but the generated model is wrong,\r\n",
      "state what is wrong:\r\n",
      "- Producing wrong results and/or decrease in accuracy\r\n",
      "- Producing correct results, but the model is slower than expected (model generated from old converter)\r\n",
      "\r\n",
      "\r\n",
      "**RNN conversion support**\r\n",
      "If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.\r\n",
      "\r\n",
      "**Any other info / logs**\r\n",
      "\r\n",
      "Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n",
      "\n",
      "issue labels - \n",
      "comp:lite\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  No GPU support for Windows10 + CUDA 11.0 + Tensorflow 2.4.1: Could not load dynamic library 'cudart64_110.dll'\n",
      "issue body -  **System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows10\r\n",
      "- TensorFlow installed from (source or binary): pip / packet manager of PyCharm\r\n",
      "- TensorFlow version: 2.4.1\r\n",
      "- Python version: 3.8\r\n",
      "- Installed using virtualenv? pip? conda?: pip / from PyCharm settings menu but it is pip install tensorflow\r\n",
      "- CUDA/cuDNN version: CUDA: cuda_11.0.2_451.48_win10 and cuDNN: 8.0.4: cudnn-11.0-windows-x64-v8.0.4.30\r\n",
      "- GPU model and memory: GeForce MX330 with 2GB\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "Hi Guys, I've been trying for around 6hours to get Tensorflow 2.4.1 working with GPU support and following tutorial from pyimagesearch:\r\n",
      "https://www.pyimagesearch.com/2018/09/10/keras-tutorial-how-to-get-started-with-keras-deep-learning-and-python/\r\n",
      "Using only CPU it worked fine.\r\n",
      "I have followed this guide here:\r\n",
      "https://www.tensorflow.org/install/gpu\r\n",
      "I installed the newest drivers for my GeforceMX330, as well as VisualStudio, CUDA11.0 and the cuDNN 8.0.4 version. Please see my log output\r\n",
      "\r\n",
      "\r\n",
      "**Any other info / logs**\r\n",
      "`train_vgg.py --dataset animals --model output/smallvggnet.model --label-bin output/smallvggnet_lb.pickle --plot output/smallvggnet_plot.png\r\n",
      "2021-01-22 15:21:23.627735: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\r\n",
      "2021-01-22 15:21:23.628081: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n",
      "[INFO] loading images...\r\n",
      "2021-01-22 15:21:42.850382: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n",
      "2021-01-22 15:21:42.852694: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n",
      "2021-01-22 15:21:43.635517: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\n",
      "pciBusID: 0000:02:00.0 name: GeForce MX330 computeCapability: 6.1\r\n",
      "coreClock: 1.594GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 52.21GiB/s\r\n",
      "2021-01-22 15:21:43.636419: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\r\n",
      "2021-01-22 15:21:43.637390: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cublas64_11.dll'; dlerror: cublas64_11.dll not found\r\n",
      "2021-01-22 15:21:43.638212: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cublasLt64_11.dll'; dlerror: cublasLt64_11.dll not found\r\n",
      "2021-01-22 15:21:43.639402: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cufft64_10.dll'; dlerror: cufft64_10.dll not found\r\n",
      "2021-01-22 15:21:43.640209: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'curand64_10.dll'; dlerror: curand64_10.dll not found\r\n",
      "2021-01-22 15:21:43.641187: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cusolver64_10.dll'; dlerror: cusolver64_10.dll not found\r\n",
      "2021-01-22 15:21:43.642002: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cusparse64_11.dll'; dlerror: cusparse64_11.dll not found\r\n",
      "2021-01-22 15:21:43.642798: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudnn64_8.dll'; dlerror: cudnn64_8.dll not found\r\n",
      "2021-01-22 15:21:43.643052: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\n",
      "Skipping registering GPU devices...\r\n",
      "2021-01-22 15:21:43.645778: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\r\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n",
      "2021-01-22 15:21:43.647564: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n",
      "2021-01-22 15:21:43.647835: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      \r\n",
      "2021-01-22 15:21:43.648038: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n",
      "[INFO] training network...\r\n",
      "2021-01-22 15:21:44.544947: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)`\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:gpu\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Subclassed tf.keras.Model can't access intermediate layer's output\n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n",
      "*Yes*\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n",
      "*Google Colab*\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "*No*\r\n",
      "- TensorFlow installed from (source or binary):\r\n",
      "*Binary, pip install*\r\n",
      "- TensorFlow version (use command below):\r\n",
      "*2.4*\r\n",
      "- Python version:\r\n",
      "*Colab's version: 3.6.9\"\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "*N/A - Colab*\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "*N/A - Colab*\r\n",
      "- CUDA/cuDNN version:\r\n",
      "*N/A - Colab without GPU*\r\n",
      "- GPU model and memory:\r\n",
      "*N/A - Colab without GPU*\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "\r\n",
      "Accessing internal layer's output of a sublcassed `Model` leads to `AttributeError: Layer XXX has no inbound nodes` exception. The code to reproduce the issue given below or avaiable [here](https://colab.research.google.com/drive/1w5p68LvzwPnYKzpDm6HzsRsRXlBp35hq?usp=sharing) \r\n",
      "\r\n",
      "```python\r\n",
      "import tensorflow as tf\r\n",
      "\r\n",
      "class MyModel(tf.keras.Model):\r\n",
      "\r\n",
      "  def __init__(self, name='MyModel', **kwargs):\r\n",
      "    super(MyModel, self).__init__(name=name, **kwargs)\r\n",
      "    self.dense1 = tf.keras.layers.Dense(4, name='layer1')\r\n",
      "    self.dense2 = tf.keras.layers.Dense(2, name='layer2')\r\n",
      "\r\n",
      "  def call(self, inputs, training=None, mask=None):\r\n",
      "    x = self.dense1(inputs)\r\n",
      "    return self.dense2(x)\r\n",
      "\r\n",
      "x_in = tf.keras.layers.Input(shape=(2,), dtype=tf.float32)\r\n",
      "my_model = MyModel()\r\n",
      "y = my_model(x_in, training=False)\r\n",
      "mid_feat = my_model.get_layer('layer1').output\r\n",
      "```\r\n",
      "\r\n",
      "This happens when using tensorflow `2.4`, however it does work with `2.3`. Something must have changed between the two versions. \r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "\r\n",
      "The old behaviour, where we can access internal layer's outputs from a model whether it has been build with `functional` API or sublclassed `Model`.\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:keras\n",
      "regression issue\n",
      "stat:awaiting tensorflower\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  The Example Code in tf.feature_column.categorical_column_with_identity is not Executable/Complete/Self-Sufficient\n",
      "issue body -  ## URL(s) with the issue:\r\n",
      "\r\n",
      "Please provide a link to the documentation entry, for example:\r\n",
      "https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_identity#linear_model\r\n",
      "\r\n",
      "## Description of issue (what needs changing): \r\n",
      "The example code cannot be Executed as it is. We have to add the namespaces by searching in the [tensorflow.org site](https://www.tensorflow.org/).\r\n",
      "\r\n",
      "Complete/Self-Sufficient/Stand-Alone example code will be very helpful, especially for the New Developers.\r\n",
      "\r\n",
      "Similar issue was raised in #46203 (was waiting for it to be resolved) but it is closed now.\n",
      "issue labels - \n",
      "type:docs-bug\n",
      "\n",
      "\n",
      "issue title -  Update broken GitHub policy link in issue templates\n",
      "issue body -  The GitHub policy page has been moved from \r\n",
      "\r\n",
      "https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md\r\n",
      "\r\n",
      "to \r\n",
      "\r\n",
      "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/opensource_only/ISSUES.md\r\n",
      "\r\n",
      "Updated the links in the issue templates. Fixes [#46592](https://github.com/tensorflow/tensorflow/issues/46592).\n",
      "issue labels - \n",
      "cla: yes\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  TensorflowLite 1.15.2  aar build failure in Win10\n",
      "issue body -  <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- OS Platform and Distribution :**win10**\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:**no**\r\n",
      "- TensorFlow installed from (source or binary):**source**\r\n",
      "- TensorFlow version:**1.15.2**\r\n",
      "- Python version:**3.7.9**\r\n",
      "- Installed using virtualenv? pip? conda?: **no,using bazel and the default python in system**\r\n",
      "- Bazel version (if compiling from source):**0.26.1**\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version: **no**\r\n",
      "- GPU model and memory:**no**\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "\r\n",
      "I was trying to build the tensorflow-lite-1.15.2.aar from source on my local WIN10 PC,\r\n",
      "I downloaded the source code and installed bazel following instructions online.\r\n",
      "The version of  ANDROID_NDK is 18.1.5063045  and ANDOIRD_SDK is 23\r\n",
      "After configuring the bazelrc, I executed the build command and got the following error:\r\n",
      "\r\n",
      "```\r\n",
      "[18 / 56] Processing Android resources for //tensorflow/lite/java:tensorflow-lite_dummy_app_for_so; 1s local ... (12 actions, 11 running)\r\n",
      "ERROR: D:/code/tensorflow-1.15.2/tensorflow-1.15.2/tensorflow/lite/java/BUILD:22:1: Processing Android resources for //tensorflow/lite/java:tensorflow-lite_dummy_app_for_so failed (Exit 1): ResourceProcessorBusyBox.exe failed: error executing command\r\n",
      "  cd C:/users/c00/_bazel_c00/7oqu2vjd/execroot/org_tensorflow\r\n",
      "  SET ANDROID_BUILD_TOOLS_VERSION=30.0.0\r\n",
      "    SET ANDROID_NDK_API_LEVEL=18\r\n",
      "    SET ANDROID_NDK_HOME=D:/softwares/AndroidSDK/ndk/18.1.5063045\r\n",
      "    SET ANDROID_SDK_API_LEVEL=23\r\n",
      "    SET ANDROID_SDK_HOME=D:/softwares/AndroidSDK\r\n",
      "    SET PATH=C:\\Program Files\\Git\\usr\\bin;C:\\Program Files\\Git\\bin;C:\\WINDOWS;C:\\WINDOWS\\System32;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0;C:\\Users\\c00\\bin;C:\\Program Files\\Git\\mingw64\\bin;C:\\Program Files\\Git\\usr\\local\\bin;C:\\Program Files\\Git\\usr\\bin;C:\\Program Files\\Git\\usr\\bin;C:\\Program Files\\Git\\mingw64\\bin;C:\\Program Files\\Git\\usr\\bin;C:\\Users\\c00\\bin;%PyCharm%;%GoLand%;%IntelliJ IDEA%;C:\\WINDOWS\\system32\\config\\systemprofile\\go\\bin;C:\\Program Files\\Python37;C:\\Program Files\\Python37\\Scripts;D:\\softwares\\apache-maven-3.6.3\\bin;D:\\softwares\\AndroidSDK\\platform-tools\\adb.exe;C:\\Program Files (x86)\\GnuWin32\\bin;C:\\Windows\\System32;C:\\Program Files\\Git\\usr\\bin\\git.exe;C:\\Program Files\\Git LFS;C:\\Program Files\\Java\\jdk1.8.0_202\\bin;C:\\Program Files\\Java\\jdk1.8.0_202\\jre\\bin;D:\\softwares\\PyCharm 2020.1\\bin;D:\\softwares\\GoLand 2020.1.3\\bin;D:\\softwares\\IntelliJ IDEA 2020.2\\bin;C:\\Users\\c00\\go\\bin;C:\\Program Files\\Git\\usr\\bin\\vendor_perl;C:\\Program Files\\Git\\usr\\bin\\core_perl\r\n",
      "    SET PYTHON_BIN_PATH=C:/Program Files/Python37/python.exe\r\n",
      "    SET PYTHON_LIB_PATH=C:/Program Files/Python37/lib/site-packages\r\n",
      "    SET RUNFILES_MANIFEST_ONLY=1\r\n",
      "    SET TF2_BEHAVIOR=0\r\n",
      "    SET TF_CONFIGURE_IOS=0\r\n",
      "    SET TF_ENABLE_XLA=1\r\n",
      "  bazel-out/x64_windows-opt/bin/external/bazel_tools/src/tools/android/java/com/google/devtools/build/android/ResourceProcessorBusyBox.exe @bazel-out/x64_windows-opt/bin/tensorflow/lite/java/tensorflow-lite_dummy_app_for_so_symbols/R.txt-0.params\r\n",
      "Execution platform: @bazel_tools//platforms:host_platform\r\n",
      "一月 22, 2021 5:18:13 下午 com.google.devtools.build.android.ResourceProcessorBusyBox processRequest\r\n",
      "严重: Error during processing\r\n",
      "java.nio.file.InvalidPathException: Illegal character [:] in path at index 4: ///C:/Users/C00520~1/AppData/Local/Temp/android_resources_tmp5724987117968661034/linked/bin.-pb.apk\r\n",
      "        at sun.nio.fs.WindowsPathParser.nextSlash(WindowsPathParser.java:212)\r\n",
      "        at sun.nio.fs.WindowsPathParser.parse(WindowsPathParser.java:111)\r\n",
      "        at sun.nio.fs.WindowsPathParser.parse(WindowsPathParser.java:77)\r\n",
      "        at sun.nio.fs.WindowsPath.parse(WindowsPath.java:94)\r\n",
      "        at sun.nio.fs.WindowsFileSystem.getPath(WindowsFileSystem.java:255)\r\n",
      "        at java.nio.file.Paths.get(Paths.java:84)\r\n",
      "        at com.google.devtools.build.android.aapt2.ProtoApk.asApkPath(ProtoApk.java:203)\r\n",
      "        at com.google.devtools.build.android.aapt2.ResourceLinker.link(ResourceLinker.java:555)\r\n",
      "        at com.google.devtools.build.android.aapt2.ResourceLinker.link(ResourceLinker.java:536)\r\n",
      "        at com.google.devtools.build.android.Aapt2ResourcePackagingAction.main(Aapt2ResourcePackagingAction.java:185)\r\n",
      "        at com.google.devtools.build.android.ResourceProcessorBusyBox$Tool$14.call(ResourceProcessorBusyBox.java:144)\r\n",
      "        at com.google.devtools.build.android.ResourceProcessorBusyBox.processRequest(ResourceProcessorBusyBox.java:240)\r\n",
      "        at com.google.devtools.build.android.ResourceProcessorBusyBox.main(ResourceProcessorBusyBox.java:203)\r\n",
      "\r\n",
      "Exception in thread \"main\" java.nio.file.InvalidPathException: Illegal character [:] in path at index 4: ///C:/Users/C00520~1/AppData/Local/Temp/android_resources_tmp5724987117968661034/linked/bin.-pb.apk\r\n",
      "        at sun.nio.fs.WindowsPathParser.nextSlash(WindowsPathParser.java:212)\r\n",
      "        at sun.nio.fs.WindowsPathParser.parse(WindowsPathParser.java:111)\r\n",
      "        at sun.nio.fs.WindowsPathParser.parse(WindowsPathParser.java:77)\r\n",
      "        at sun.nio.fs.WindowsPath.parse(WindowsPath.java:94)\r\n",
      "        at sun.nio.fs.WindowsFileSystem.getPath(WindowsFileSystem.java:255)\r\n",
      "        at java.nio.file.Paths.get(Paths.java:84)\r\n",
      "        at com.google.devtools.build.android.aapt2.ProtoApk.asApkPath(ProtoApk.java:203)\r\n",
      "        at com.google.devtools.build.android.aapt2.ResourceLinker.link(ResourceLinker.java:555)\r\n",
      "        at com.google.devtools.build.android.aapt2.ResourceLinker.link(ResourceLinker.java:536)\r\n",
      "        at com.google.devtools.build.android.Aapt2ResourcePackagingAction.main(Aapt2ResourcePackagingAction.java:185)\r\n",
      "        at com.google.devtools.build.android.ResourceProcessorBusyBox$Tool$14.call(ResourceProcessorBusyBox.java:144)\r\n",
      "        at com.google.devtools.build.android.ResourceProcessorBusyBox.processRequest(ResourceProcessorBusyBox.java:240)\r\n",
      "        at com.google.devtools.build.android.ResourceProcessorBusyBox.main(ResourceProcessorBusyBox.java:203)\r\n",
      "Target //tensorflow/lite/java:tensorflow-lite failed to build\r\n",
      "INFO: Elapsed time: 2.462s, Critical Path: 1.37s\r\n",
      "INFO: 14 processes: 14 local.\r\n",
      "FAILED: Build did NOT complete successfully\r\n",
      "FAILED: Build did NOT complete successfully\r\n",
      "\r\n",
      "```\r\n",
      "\r\n",
      "**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n",
      "```\r\n",
      "_bazel build --cxxopt='--std=c++11' -c opt --fat_apk_cpu=x86,x86_64,arm64-v8a,armeabi-v7a //tensorflow/lite/java:tensorflow-lite\r\n",
      "```\r\n",
      "\r\n",
      "**Any other info / logs**\r\n",
      "Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n",
      "\r\n",
      "Here is the tf_configure.bazelrc content I got ,\r\n",
      "- I have JDK1.8 installed before ,so I added the \"java_toolchain\" setting\r\n",
      "- I used the ./configure to interactively configure it,but I couldn't get these \"action_env\" adout android, so I add those action_env manually.\r\n",
      "- I tried to remove some of the \"build:opt\" ，it didn't work，cause I don't really understand all of them\r\n",
      "- I tried to change the SDK and NDK higher ,to like 29/30, it didn't solve the problem either.\r\n",
      "```\r\n",
      "build --action_env PYTHON_BIN_PATH=\"C:/Program Files/Python37/python.exe\"\r\n",
      "build --action_env PYTHON_LIB_PATH=\"C:/Program Files/Python37/lib/site-packages\"\r\n",
      "build --python_path=\"C:/Program Files/Python37/python.exe\"\r\n",
      "build:xla --define with_xla_support=true\r\n",
      "build --config=xla\r\n",
      "build:opt --copt=-march=native\r\n",
      "build:opt --copt=-Wno-sign-compare\r\n",
      "build:opt --host_copt=-march=native\r\n",
      "build:opt --define with_default_optimizations=true\r\n",
      "build --config monolithic\r\n",
      "build --copt=-w --host_copt=-w\r\n",
      "build --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI\r\n",
      "build --verbose_failures\r\n",
      "build --distinct_host_configuration=false\r\n",
      "build --define=override_eigen_strong_inline=true\r\n",
      "build:v2 --define=tf_api_version=2\r\n",
      "build --java_toolchain=@bazel_tools//tools/jdk:toolchain_hostjdk8\r\n",
      "build --action_env ANDROID_NDK_HOME=\"D:/softwares/AndroidSDK/ndk/18.1.5063045\"\r\n",
      "build --action_env ANDROID_NDK_API_LEVEL=\"18\"\r\n",
      "build --action_env ANDROID_BUILD_TOOLS_VERSION=\"30.0.0\"\r\n",
      "build --action_env ANDROID_SDK_API_LEVEL=\"23\"\r\n",
      "build --action_env ANDROID_SDK_HOME=\"D:/softwares/AndroidSDK\"\r\n",
      "test --flaky_test_attempts=3\r\n",
      "test --test_size_filters=small,medium\r\n",
      "test --test_tag_filters=-benchmark-test,-no_oss,-oss_serial\r\n",
      "test --build_tag_filters=-benchmark-test,-no_oss\r\n",
      "test --test_tag_filters=-no_windows,-gpu\r\n",
      "test --build_tag_filters=-no_windows,-gpu\r\n",
      "build --action_env TF_CONFIGURE_IOS=\"0\"\r\n",
      "```\r\n",
      "\r\n",
      "I am new to bazel and tensorflow , I feel there must be something wrong with my bazelrc, but I can't find similar problem online, so I'm trying to seek for help here. Thanks.\n",
      "issue labels - \n",
      "TF 1.15\n",
      "comp:lite\n",
      "subtype:windows\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  Conversion img_to_array and array_to_img does not work in TF 2.4\n",
      "issue body -  **System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n",
      "- TensorFlow installed from (source or binary): pip\r\n",
      "- TensorFlow version (use command below): 2.4\r\n",
      "- Python version: 3.6.1\r\n",
      "- CUDA/cuDNN version: 11\r\n",
      "- GPU model and memory: RTX2060 6GB\r\n",
      "\r\n",
      "I try to convert numpy array ones to img and img to array with preprocessing.image functions.\r\n",
      "\r\n",
      "Code to repoduce issue:\r\n",
      "\r\n",
      "```\r\n",
      "import numpy as np\r\n",
      "from tensorflow.compat.v1.keras.preprocessing.image import array_to_img, img_to_array\r\n",
      "\r\n",
      "input_array_ones = np.ones((4, 8, 1), dtype=np.uint8)\r\n",
      "img_ones = array_to_img(input_array_ones)\r\n",
      "output_array_ones = img_to_array(img_ones, dtype=np.uint8)\r\n",
      "```\r\n",
      "\r\n",
      "```\r\n",
      ">>> (input_array_ones) = [[[1],  [1],  [1],  [1],  [1],  [1],  [1],  [1]],, [[1],  [1],  [1],  [1],  [1],  [1],  [1],  [1]],, [[1],  [1],  [1],  [1],  [1],  [1],  [1],  [1]],, [[1],  [1],  [1],  [1],  [1],  [1],  [1],  [1]]]\r\n",
      ">>> (img_ones) = <PIL.Image.Image image mode=L size=8x4 at 0x20990EBADA0>\r\n",
      ">>> (output_array_ones) = [[[0],  [0],  [0],  [0],  [0],  [0],  [0],  [0]],, [[0],  [0],  [0],  [0],  [0],  [0],  [0],  [0]],, [[0],  [0],  [0],  [0],  [0],  [0],  [0],  [0]],, [[0],  [0],  [0],  [0],  [0],  [0],  [0],  [0]]]\r\n",
      "```\r\n",
      "\r\n",
      "Works fine with the previous version of TensorFlow (1.x).\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:keras\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Loading model from GCS takes longer than copying it to local and then loading the model\n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): x86_64 GNU/Linux\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "- TensorFlow installed from (source or binary): binary\r\n",
      "- TensorFlow version (use command below): 2.3\r\n",
      "- Python version: 3.7\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "```\r\n",
      "new_model = tf.keras.models.load_model(\r\n",
      " 'gs://benchmarking_test/bert_en_uncased_L-12_H-768_A-12_2' \r\n",
      ")\r\n",
      "```\r\n",
      "takes about 30min or so.\r\n",
      "whereas, following works in less than a minute\r\n",
      "```\r\n",
      "!gsutil cp -r gs://benchmarking_test/bert_en_uncased_L-12_H-768_A-12_2 local_path\r\n",
      "new_model = tf.keras.models.load_model(\r\n",
      " 'local_path' \r\n",
      ")\r\n",
      "```\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "Both should take a similar time.\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.3\n",
      "comp:keras\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:performance\n",
      "\n",
      "\n",
      "issue title -  Fix build errors on OSX\n",
      "issue body -  Without explicitly specifying C++ 11 as a required minimal version, on OSX Catalina I was getting these errors:\r\n",
      "\r\n",
      "```\r\n",
      "➜  hello_tflite pwd\r\n",
      "/tmp/hello_tflite\r\n",
      "➜  hello_tflite cmake --build . -j\r\n",
      "[100%] Building CXX object tensorflow-lite/CMakeFiles/tensorflow-lite.dir/tools/optimize/sparsity/format_converter.cc.o\r\n",
      "[100%] Linking CXX static library libtensorflow-lite.a\r\n",
      "[100%] Built target tensorflow-lite\r\n",
      "Scanning dependencies of target minimal\r\n",
      "[100%] Building CXX object CMakeFiles/minimal.dir/minimal.cc.o\r\n",
      "In file included from /Users/tleyden/Development/tensorflow_src/tensorflow/lite/examples/minimal/minimal.cc:16:\r\n",
      "In file included from /Users/tleyden/Development/tensorflow_src/tensorflow/lite/interpreter.h:34:\r\n",
      "/Users/tleyden/Development/tensorflow_src/tensorflow/lite/allocation.h:38:8: warning: scoped enumerations are a\r\n",
      "      C++11 extension [-Wc++11-extensions]\r\n",
      "  enum class Type {\r\n",
      "       ^\r\n",
      "/Users/tleyden/Development/tensorflow_src/tensorflow/lite/allocation.h:66:28: warning: 'override' keyword is a C++11\r\n",
      "      extension [-Wc++11-extensions]\r\n",
      "  const void* base() const override;\r\n",
      "                           ^\r\n",
      "/Users/tleyden/Development/tensorflow_src/tensorflow/lite/allocation.h:67:24: warning: 'override' keyword is a C++11\r\n",
      "      extension [-Wc++11-extensions]\r\n",
      "  size_t bytes() const override;\r\n",
      "etc..\r\n",
      "```\r\n",
      "\r\n",
      "Adding the proposed change fixed the issues.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:lite\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  TFLite from file descriptor on Android 11\n",
      "issue body -  **System information**\r\n",
      "- TensorFlow version (you are using): Likely https://github.com/mozilla/tensorflow/tree/23ad988fcde60fb01f9533e95004bbc4877a9143\r\n",
      "- Are you willing to contribute it (Yes/No): No\r\n",
      "\r\n",
      "**Describe the feature and the current behavior/state.**\r\n",
      "\r\n",
      "This issue is a continuation of mozilla/DeepSpeech#3507. TFLite is used in the app, but the model is provided by the user and usually in `/sdcard/Download`.\r\n",
      "\r\n",
      "Currently, TFLite accepts arrays or file paths. With Android's scoped storage and SAF, it's hard to use file paths to the external storage without either compatibility edge cases or overly invasive permissions. On Android 10, I could use the workaround of converting the file descriptor I received from Android to a path like `\"/proc/self/fd/\" + fd`. On Android 11, that stopped working. It seems to be no longer possible to open a file under `/proc/self/fd/` that points to external storage without requesting the read permission I was hoping to avoid.\r\n",
      "\r\n",
      "I would like TFLite to be compatible with Android 11's external storage. The following solutions come to mind:\r\n",
      "\r\n",
      "- Duplicate `tflite::FlatBufferModel::BuildFromFile` into `tflite::FlatBufferModel::BuildFromFileDescriptor`\r\n",
      "- Add https://stackoverflow.com/a/59004193/10477326\r\n",
      "- On Android builds, manually add a conditional to detect the `/proc/self/fd/` prefix and use a `dup` instead of a `fopen`\r\n",
      "\r\n",
      "**Will this change the current api? How?**\r\n",
      "\r\n",
      "Not unless we choose the `BuildFromFileDescriptor` solution.\r\n",
      "\r\n",
      "**Who will benefit with this feature?**\r\n",
      "\r\n",
      "Developers and users of apps that:\r\n",
      "\r\n",
      "- Explore, educate, or demonstrate machine learning, and\r\n",
      "- Run on up-to-date Android versions, and\r\n",
      "- Load user-supplied models, and\r\n",
      "- Usually take large (~1 GiB) models that reside on external storage, which we cannot afford to read into memory or duplicate on disk, and\r\n",
      "- Would like to respect privacy by obeying scoped storage and SAF by not requesting file permissions to everything but rather allow the user to pick only the files necessary\r\n",
      "\n",
      "issue labels - \n",
      "comp:lite\n",
      "type:feature\n",
      "\n",
      "\n",
      "issue title -  ISSUES.md link broken\n",
      "issue body -  ## URL(s) with the issue:\r\n",
      "\r\n",
      "Most files under https://github.com/tensorflow/tensorflow/tree/f2d8cfe09234329e14c98798540f146bd94558e0/.github/ISSUE_TEMPLATE\r\n",
      "\r\n",
      "## Description of issue (what needs changing):\r\n",
      "\r\n",
      "The links to the [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md) are broken.\r\n",
      "\r\n",
      "### Clear description\r\n",
      "\r\n",
      "For example, why should someone use this method? How is it useful?\r\n",
      "\r\n",
      "I don't know why there is a separate GitHub policy aside from the CONTRIBUTING.md, but it seems important and people are supposed to read it\r\n",
      "\r\n",
      "### Correct links\r\n",
      "\r\n",
      "Is the link to the source code correct? No\r\n",
      "\r\n",
      "### Parameters defined\r\n",
      "\r\n",
      "Are all parameters defined and formatted correctly? N/A\r\n",
      "\r\n",
      "### Returns defined\r\n",
      "\r\n",
      "Are return values defined? N/A\r\n",
      "\r\n",
      "### Raises listed and defined\r\n",
      "\r\n",
      "Are the errors defined? N/A\r\n",
      "\r\n",
      "### Usage example\r\n",
      "\r\n",
      "Is there a usage example? N/A\r\n",
      "\r\n",
      "### Request visuals, if applicable\r\n",
      "\r\n",
      "Are there currently visuals? If not, will it clarify the content? N/A\r\n",
      "\r\n",
      "### Submit a pull request?\r\n",
      "\r\n",
      "Are you planning to also submit a pull request to fix the issue? No\r\n",
      "\n",
      "issue labels - \n",
      "stat:awaiting response\n",
      "type:docs-bug\n",
      "\n",
      "\n",
      "issue title -  Fix the source specialization for the examples (and minor fixes to the documentation).\n",
      "issue body -  With https://github.com/tensorflow/tensorflow/pull/46473, we removed support for TAGS on the makefile command line. An unintended consequence was that we were no longer specializing the sources in the examples.\r\n",
      "\r\n",
      "This change specializes the sources using the TARGET, which appears to be the only command line option that is needed.\r\n",
      "\r\n",
      "Manually confirmed that the generated arduino projects have the correct sources (e.g. micro_speech/arduino/audio_provider.cc is used in the output directory).\r\n",
      "\r\n",
      "Test sequence:\r\n",
      "```\r\n",
      "make -f tensorflow/lite/micro/tools/make/Makefile TARGET=arduino OPTIMIZED_KERNEL_DIR=cmsis_nn generate_arduino_zip\r\n",
      "cd tensorflow/lite/micro/tools/make/gen/arduino_x86_64_default\r\n",
      "unzip prj/tensorflow_lite.zip\r\n",
      "```\r\n",
      "\r\n",
      "And then confirmed that the code in\r\n",
      "```\r\n",
      "tensorflow/lite/micro/tools/make/gen/arduino_x86_64_default/tensorflow_lite/examples/micro_speech/arduino_audio_provider.cc matches\r\n",
      "```\r\n",
      "\r\n",
      "matches the code in:\r\n",
      "```\r\n",
      "tensorflow/lite/micro/examples/micro_speech/arduino/audio_provider.cc\r\n",
      "```\r\n",
      "\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  Mixed Precision - Conv3D - error: No algorithm worked!\n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n",
      "Yes, I wrote a custom code.\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n",
      "Linux (CentOS 7)\r\n",
      "- TensorFlow installed from (source or binary):\r\n",
      "Installed using PIP\r\n",
      "- TensorFlow version (use command below):\r\n",
      "v2.4.0-rc4-71-g582c8d236cb 2.4.0\r\n",
      "- Python version:\r\n",
      "3.8.0\r\n",
      "- CUDA/cuDNN version:\r\n",
      "CUDA/11.1.1\r\n",
      "cuDNN/8.0.4.30-CUDA-11.1.1\r\n",
      "- GPU model and memory:\r\n",
      "2 NVIDIA A100-PCIE-40GB\r\n",
      "2 GPUs on the same computer\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "When running the code with double precision, the code does not generate errors. When running with mixed precision, launches an error:\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "Not having errors when running with mixed precision.\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "I tried to create a google colab for this code. There is no error there, but I could not get a google colab with 2 GPUs, like in my enviroment.\r\n",
      "```\r\n",
      "import tensorflow as tf\r\n",
      "from tensorflow.keras.mixed_precision import experimental as mixed_precision\r\n",
      "from tensorflow.keras import layers, Model, optimizers\r\n",
      "import numpy as np\r\n",
      "\r\n",
      "# If you comment the following 2 lines, the code runs\r\n",
      "policy = mixed_precision.Policy('mixed_float16')\r\n",
      "mixed_precision.set_policy(policy)\r\n",
      "\r\n",
      "\r\n",
      "mirrored_strategy = tf.distribute.MirroredStrategy()\r\n",
      "with mirrored_strategy.scope():\r\n",
      "    x_input = layers.Input(shape=(60, 60, 2, 32))\r\n",
      "\r\n",
      "    x = layers.Conv3D(filters=32, kernel_size=(3, 3, 1), strides=(1, 1, 1), padding = 'same')(x_input)\r\n",
      "    x = layers.BatchNormalization()(x)\r\n",
      "    x_out = layers.ReLU()(x)\r\n",
      "\r\n",
      "    test_model = Model(inputs=x_input, outputs=x_out, name='test_model')\r\n",
      "    adam = optimizers.Adam(learning_rate=1E-4)\r\n",
      "\r\n",
      "test_model.compile(optimizer=adam, loss=\"mse\", metrics=[\"mae\"])\r\n",
      "\r\n",
      "input_data = np.random.random((1000, 60, 60, 2, 32))\r\n",
      "target_data = np.random.random((1000, 60, 60, 2, 32))\r\n",
      "\r\n",
      "ds_tuple = tf.data.Dataset.from_tensor_slices((input_data,target_data))\r\n",
      "ds_tuple = ds_tuple.shuffle(1000).batch(100)\r\n",
      "\r\n",
      "history = test_model.fit(ds_tuple, epochs=10, verbose=1)\r\n",
      "```\r\n",
      "\r\n",
      "**Other info / logs** \r\n",
      "This is the log of the run when I try running with mixed precision:\r\n",
      "```\r\n",
      "2021-01-21 15:37:22.318957: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "2021-01-21 15:37:36.119468: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n",
      "2021-01-21 15:37:36.123358: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n",
      "2021-01-21 15:37:36.201391: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:\r\n",
      "pciBusID: 0000:3b:00.0 name: A100-PCIE-40GB computeCapability: 8.0\r\n",
      "coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.59GiB deviceMemoryBandwidth: 1.41TiB/s\r\n",
      "2021-01-21 15:37:36.204229: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties:\r\n",
      "pciBusID: 0000:d8:00.0 name: A100-PCIE-40GB computeCapability: 8.0\r\n",
      "coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.59GiB deviceMemoryBandwidth: 1.41TiB/s\r\n",
      "2021-01-21 15:37:36.204423: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "2021-01-21 15:37:37.945043: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n",
      "2021-01-21 15:37:37.945235: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n",
      "2021-01-21 15:37:37.998423: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n",
      "2021-01-21 15:37:38.072705: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n",
      "2021-01-21 15:37:38.207114: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n",
      "2021-01-21 15:37:38.574293: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n",
      "2021-01-21 15:37:38.612582: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n",
      "2021-01-21 15:37:38.624584: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1\r\n",
      "2021-01-21 15:37:38.624772: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n",
      "2021-01-21 15:37:38.626778: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n",
      "WARNING:tensorflow:From /scratch/user/eee/conda/myEnvs/tf_2.4/lib/python3.8/site-packages/tensorflow/python/keras/mixed_precision/loss_scale.py:56: DynamicLossScale.__init__ (from tensorflow.python.training.experimental.loss_scale) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use tf.keras.mixed_precision.LossScaleOptimizer instead. LossScaleOptimizer now has all the functionality of DynamicLossScale\r\n",
      "2021-01-21 15:37:38.638220: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512F\r\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n",
      "2021-01-21 15:37:38.638847: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n",
      "2021-01-21 15:37:38.840057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:\r\n",
      "pciBusID: 0000:3b:00.0 name: A100-PCIE-40GB computeCapability: 8.0\r\n",
      "coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.59GiB deviceMemoryBandwidth: 1.41TiB/s\r\n",
      "2021-01-21 15:37:38.842228: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties:\r\n",
      "pciBusID: 0000:d8:00.0 name: A100-PCIE-40GB computeCapability: 8.0\r\n",
      "coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.59GiB deviceMemoryBandwidth: 1.41TiB/s\r\n",
      "2021-01-21 15:37:38.842375: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "2021-01-21 15:37:38.842436: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n",
      "2021-01-21 15:37:38.842486: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n",
      "2021-01-21 15:37:38.842536: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n",
      "2021-01-21 15:37:38.842585: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n",
      "2021-01-21 15:37:38.842635: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n",
      "2021-01-21 15:37:38.842683: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n",
      "2021-01-21 15:37:38.842733: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n",
      "2021-01-21 15:37:38.850268: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1\r\n",
      "2021-01-21 15:37:38.850360: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "2021-01-21 15:37:40.396681: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n",
      "2021-01-21 15:37:40.396829: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 1\r\n",
      "2021-01-21 15:37:40.396881: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N Y\r\n",
      "2021-01-21 15:37:40.396925: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 1:   Y N\r\n",
      "2021-01-21 15:37:40.407031: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 37570 MB memory) -> physical GPU (device: 0, name: A100-PCIE-40GB, pci bus id: 0000:3b:00.0, compute capability: 8.0)\r\n",
      "2021-01-21 15:37:40.414936: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 37570 MB memory) -> physical GPU (device: 1, name: A100-PCIE-40GB, pci bus id: 0000:d8:00.0, compute capability: 8.0)\r\n",
      "     --->     Running Tensorflow in Mixed Precision...\r\n",
      "WARNING:tensorflow:tf.keras.mixed_precision.experimental.LossScaleOptimizer is deprecated. Please use tf.keras.mixed_precision.LossScaleOptimizer instead. Note that the non-experimental LossScaleOptimizer does not take a DynamicLossScale but instead takes the dynamic configuration directly in the constructor. For example:\r\n",
      "  opt = tf.keras.mixed_precision.experimental.LossScaleOptimizer(opt)\r\n",
      "\r\n",
      "2021-01-21 15:37:47.583748: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Found an unshardable source dataset: name: \"TensorSliceDataset/_2\"\r\n",
      "op: \"TensorSliceDataset\"\r\n",
      "input: \"Placeholder/_0\"\r\n",
      "input: \"Placeholder/_1\"\r\n",
      "attr {\r\n",
      "  key: \"Toutput_types\"\r\n",
      "  value {\r\n",
      "    list {\r\n",
      "      type: DT_DOUBLE\r\n",
      "      type: DT_DOUBLE\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "attr {\r\n",
      "  key: \"output_shapes\"\r\n",
      "  value {\r\n",
      "    list {\r\n",
      "      shape {\r\n",
      "        dim {\r\n",
      "          size: 60\r\n",
      "        }\r\n",
      "        dim {\r\n",
      "          size: 60\r\n",
      "        }\r\n",
      "        dim {\r\n",
      "          size: 2\r\n",
      "        }\r\n",
      "        dim {\r\n",
      "          size: 32\r\n",
      "        }\r\n",
      "      }\r\n",
      "      shape {\r\n",
      "        dim {\r\n",
      "          size: 60\r\n",
      "        }\r\n",
      "        dim {\r\n",
      "          size: 60\r\n",
      "        }\r\n",
      "        dim {\r\n",
      "          size: 2\r\n",
      "        }\r\n",
      "        dim {\r\n",
      "          size: 32\r\n",
      "        }\r\n",
      "      }\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "\r\n",
      "2021-01-21 15:37:48.602974: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n",
      "2021-01-21 15:37:48.606716: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3000000000 Hz\r\n",
      "Epoch 1/10\r\n",
      "2021-01-21 15:37:56.331798: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n",
      "2021-01-21 15:39:18.436971: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n",
      "2021-01-21 15:39:28.779314: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n",
      "2021-01-21 15:39:31.933180: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at conv_grad_ops_3d.cc:1994 : Not found: No algorithm worked!\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"main.py\", line 31, in <module>\r\n",
      "    history = test_model.fit(ds_tuple, epochs=10, verbose=1)\r\n",
      "  File \"/scratch/user/eee/conda/myEnvs/tf_2.4/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\", line 1100, in fit\r\n",
      "    tmp_logs = self.train_function(iterator)\r\n",
      "  File \"/scratch/user/eee/conda/myEnvs/tf_2.4/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 828, in __call__\r\n",
      "    result = self._call(*args, **kwds)\r\n",
      "  File \"/scratch/user/eee/conda/myEnvs/tf_2.4/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 888, in _call\r\n",
      "    return self._stateless_fn(*args, **kwds)\r\n",
      "  File \"/scratch/user/eee/conda/myEnvs/tf_2.4/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 2942, in __call__\r\n",
      "    return graph_function._call_flat(\r\n",
      "  File \"/scratch/user/eee/conda/myEnvs/tf_2.4/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 1918, in _call_flat\r\n",
      "    return self._build_call_outputs(self._inference_function.call(\r\n",
      "  File \"/scratch/user/eee/conda/myEnvs/tf_2.4/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 555, in call\r\n",
      "    outputs = execute.execute(\r\n",
      "  File \"/scratch/user/eee/conda/myEnvs/tf_2.4/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\r\n",
      "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n",
      "tensorflow.python.framework.errors_impl.NotFoundError: 3 root error(s) found.\r\n",
      "  (0) Not found:  No algorithm worked!\r\n",
      "         [[node gradient_tape/test_model/conv3d/Conv3D/Conv3DBackpropFilterV2 (defined at /threading.py:932) ]]\r\n",
      "  (1) Not found:  No algorithm worked!\r\n",
      "         [[node gradient_tape/test_model/conv3d/Conv3D/Conv3DBackpropFilterV2 (defined at /threading.py:932) ]]\r\n",
      "         [[cond_4/then/_40/cond_4/cond/pivot_t/_256/_187]]\r\n",
      "  (2) Not found:  No algorithm worked!\r\n",
      "         [[node gradient_tape/test_model/conv3d/Conv3D/Conv3DBackpropFilterV2 (defined at /threading.py:932) ]]\r\n",
      "         [[div_no_nan/ReadVariableOp_3/_108]]\r\n",
      "0 successful operations.\r\n",
      "0 derived errors ignored. [Op:__inference_train_function_2870]\r\n",
      "\r\n",
      "Function call stack:\r\n",
      "train_function -> train_function -> train_function\r\n",
      "\r\n",
      "```\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:keras\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Handle `set_weights()` for dtype string\n",
      "issue body -  For further details see issue: https://github.com/tensorflow/tensorflow/issues/46553\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:keras\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  error: cannot initialize a member subobject of type 'ternaryfunc' (aka '_object *(*)(_object *, _object *, _object *)') with an lvalue of type 'PyObject *(PyObject *)' (aka '_object *(_object *)'): different number of parameters (3 vs 1)\n",
      "issue body -  <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n",
      "- OS X 10.15.7 - Catalina\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "- TensorFlow installed from (source or binary):\r\n",
      "- source\r\n",
      "- TensorFlow version:\r\n",
      "- Master\r\n",
      "- Python version:\r\n",
      "- Anaconda 3.6\r\n",
      "- Installed using virtualenv? pip? conda?:\r\n",
      "- Conda\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "- bazelisk\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- Apple clang version 12.0.0 (clang-1200.0.32.28)\r\n",
      "- CUDA/cuDNN version:\r\n",
      "- GPU model and memory:\r\n",
      "- CPU\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "\r\n",
      "ERROR: /Users/davidlaxer/tensorflow/tensorflow/python/lib/core/BUILD:49:11: C++ compilation of rule '//tensorflow/python/lib/core:bfloat16_lib' failed (Exit 1): wrapped_clang failed: error executing command external/local_config_cc/wrapped_clang '-D_FORTIFY_SOURCE=1' -fstack-protector -fcolor-diagnostics -Wall -Wthread-safety -Wself-assign -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG ... (remaining 63 argument(s) skipped)\r\n",
      "tensorflow/python/lib/core/bfloat16.cc:219:5: error: cannot initialize a member subobject of type 'ternaryfunc' (aka '_object *(*)(_object *, _object *, _object *)') with an lvalue of type 'PyObject *(PyObject *)' (aka '_object *(_object *)'): different number of parameters (3 vs 1)\r\n",
      "    PyBfloat16_Negative,  // nb_negative\r\n",
      "    ^~~~~~~~~~~~~~~~~~~\r\n",
      "tensorflow/python/lib/core/bfloat16.cc:229:5: error: cannot initialize a member subobject of type 'binaryfunc' (aka '_object *(*)(_object *, _object *)') with an lvalue of type 'PyObject *(PyObject *)' (aka '_object *(_object *)'): different number of parameters (2 vs 1)\r\n",
      "    PyBfloat16_Int,       // nb_int\r\n",
      "    ^~~~~~~~~~~~~~\r\n",
      "tensorflow/python/lib/core/bfloat16.cc:333:1: error: unknown type name 'Py_hash_t'; did you mean 'npy_hash_t'?\r\n",
      "Py_hash_t PyBfloat16_Hash(PyObject* self) {\r\n",
      "^~~~~~~~~\r\n",
      "npy_hash_t\r\n",
      "bazel-out/darwin-opt/bin/external/local_config_python/numpy_include/numpy/npy_common.h:357:14: note: 'npy_hash_t' declared here\r\n",
      "typedef long npy_hash_t;\r\n",
      "             ^\r\n",
      "3 errors generated.\r\n",
      "**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n",
      "bazel build --config=opt  //tensorflow/tools/pip_package:build_pip_package\r\n",
      "\r\n",
      "\r\n",
      "**Any other info / logs**\r\n",
      "Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n",
      "\n",
      "issue labels - \n",
      "subtype:macOS\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  What is mac osx tensorflow=2.0.0 equivalent to in windows 64bit?\n",
      "issue body -  I have a mac book pro and I've been using tensorflow 2.0.0, but my lab computer is a windows (likely 64bit). I want to make sure that this lab computer has the equivalent package as my mac. Can anyone tell me which tensorflow version for windows is equivalent to mac osx 2.0.0?\n",
      "issue labels - \n",
      "TF 2.0\n",
      "stat:awaiting response\n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  Added tf_doctest_lib.py to COMMON_PIP_DEPS\n",
      "issue body -  Fixes #41279\r\n",
      "\r\n",
      "@mihaimaruseac could you kindly review this?\n",
      "issue labels - \n",
      "cla: yes\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Rename directories to have underscores instead of dashes (cmsis-nn -> cmsis_nn and ethos-u -> ethos_u) \n",
      "issue body -  \r\n",
      "Fixes http://b/168824958\r\n",
      "\r\n",
      "Some additional background discussion is in https://github.com/tensorflow/tensorflow/pull/46352 (which has the same PR title).\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  Jupyter kernel restarts/crashes when importing Tensorflow, 'ModuleNotFoundError: No module named 'tensorflow' in Python CLI\n",
      "issue body -  **System information**\r\n",
      "Using an M1 MacBook\r\n",
      "\r\n",
      "I cannot use environment capture because the package is not being detected. I have tried the following: restarting computer, reinstalling Conda, reinstalling individual packages, installing using different package managers, setting up new environments, using different versions of python in different environments. It was working before and then suddenly stopped. I have seen reports of it being a memory issue, but it never occurred before, and I have 16GB of RAM, it should be more than enough. \r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "\r\n",
      "Jupyter kernel restarts/crashes when importing Tensorflow, 'ModuleNotFoundError: No module named 'tensorflow' in Python CLI\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "\r\n",
      "For it to import normally\r\n",
      "\r\n",
      "Below is a video of the behaviour. \r\n",
      "\r\n",
      "https://drive.google.com/file/d/11hCwbneKVdx0srZN9YxCyyIwDE8BKKQN/view?usp=sharing\r\n",
      "\r\n",
      "<img width=\"724\" alt=\"Screen Shot 2021-01-21 at 10 02 34 AM\" src=\"https://user-images.githubusercontent.com/51058259/105384701-c9630600-5bcf-11eb-9f2c-53cb693ddf88.png\">\r\n",
      "<img width=\"723\" alt=\"Screen Shot 2021-01-21 at 10 02 57 AM\" src=\"https://user-images.githubusercontent.com/51058259/105384741-d7b12200-5bcf-11eb-850a-9a47cfb3c8ea.png\">\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "subtype:macOS\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  Hi, I actually received this issue when I am running a code which is : WARNING:tensorflow:AutoGraph could not transform <function read_image at 0x0000026A371D8700> and will run it as-is. Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Index' To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "issue body -  <em>Please make sure that this is a bug. As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:bug_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "- TensorFlow installed from (source or binary):\r\n",
      "- TensorFlow version (use command below):\r\n",
      "- Python version:\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version:\r\n",
      "- GPU model and memory:\r\n",
      "\r\n",
      "You can collect some of this information using our environment capture\r\n",
      "[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n",
      "You can also obtain the TensorFlow version with:\r\n",
      "1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n",
      "2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n",
      "\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "Provide a reproducible test case that is the bare minimum necessary to generate\r\n",
      "the problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n",
      "\r\n",
      "**Other info / logs** Include any logs or source code that would be helpful to\r\n",
      "diagnose the problem. If including tracebacks, please include the full\r\n",
      "traceback. Large logs and files should be attached.\r\n",
      "\n",
      "issue labels - \n",
      "comp:autograph\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  tf.config.run_functions_eagerly(run_eagerly=True) cashes on model load\n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "- TensorFlow installed from (source or binary): binary\r\n",
      "- TensorFlow version (use command below): 2.3 and 2.4 verified\r\n",
      "- Python version: 3.8\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version: Not required\r\n",
      "- GPU model and memory: Not required\r\n",
      "\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "Exception when loading saved model when a custom layer having `training=None` as argument is implemented.\r\n",
      "```\r\n",
      "ValueError: Could not find matching function to call loaded from the SavedModel. Got:\r\n",
      "  Positional arguments (1 total):\r\n",
      "    * Tensor(\"Placeholder:0\", shape=(None, 28, 28), dtype=float32)\r\n",
      "  Keyword arguments: {'training': False}\r\n",
      "\r\n",
      "Expected these arguments to match one of the following 2 option(s):\r\n",
      "\r\n",
      "Option 1:\r\n",
      "  Positional arguments (2 total):\r\n",
      "    * TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name='inputs')\r\n",
      "    * False\r\n",
      "  Keyword arguments: {}\r\n",
      "\r\n",
      "Option 2:\r\n",
      "  Positional arguments (2 total):\r\n",
      "    * TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name='inputs')\r\n",
      "    * True\r\n",
      "  Keyword arguments: {}\r\n",
      "```\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "No exception\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "Provide a reproducible test case that is the bare minimum necessary to generate\r\n",
      "the problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n",
      "[minimal example](https://colab.research.google.com/drive/1NBb6neAWOLhtBfORWviZB-Cy2BRrScz3?usp=sharing)\r\n",
      "\r\n",
      "**Other info / logs**\r\n",
      "See Colab error.\r\n",
      "Removing the `tf.config.run_functions_eagerly` does not result in an exception and training the model works flawlessly.\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:apis\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  remove duplicated lines in documentation\n",
      "issue body -  Remove duplicated lines in documentation. See: https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/RandomRotation\r\n",
      "\r\n",
      "![Captura de tela de 2021-01-21 09-44-28](https://user-images.githubusercontent.com/12160840/105353478-0c23de80-5bce-11eb-9589-890722f33d31.png)\r\n",
      "\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:keras\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Request to add tf.Cholesky and tf.MatrixTriangularSolve to tensorflow lite operations \n",
      "issue body -  **System information**\r\n",
      "- OS Platform and Distribution: Linux Ubuntu 18.04.5\r\n",
      "- Python version: 3.7.4\r\n",
      "- Tensorflow : 2.4.0\r\n",
      "\r\n",
      "I'm trying to use the function tf.linalg.lstsq inside my module and I'm getting this error for the converter: \r\n",
      "\r\n",
      "**Provide the text output from tflite_convert**\r\n",
      "\r\n",
      "    ```\r\n",
      "    Traceback (most recent call last):\r\n",
      "      File \"/home/aia/anaconda3/envs/notebooks/lib/python3.7/site-packages/tensorflow/lite/python/convert.py\", line 213, in toco_convert_protos\r\n",
      "        enable_mlir_converter)\r\n",
      "      File \"/home/aia/anaconda3/envs/notebooks/lib/python3.7/site-packages/tensorflow/lite/python/wrap_toco.py\", line 38, in wrapped_toco_convert\r\n",
      "        enable_mlir_converter)\r\n",
      "    Exception: <unknown>:0: error: loc(callsite(callsite(callsite(\"matrix_solve_ls_1/cond/Cholesky@matrix_solve_ls_1_cond_false_28664\" at \"matrix_solve_ls_1/cond@__inference_calcute_hbr_31510\") at \"PartitionedCall@__inference_signature_wrapper_31523\") at \"PartitionedCall\")): 'tf.Cholesky' op is neither a custom op nor a flex op\r\n",
      "    <unknown>:0: note: loc(\"PartitionedCall\"): called from\r\n",
      "    <unknown>:0: error: loc(callsite(callsite(callsite(\"matrix_solve_ls_1/cond/cholesky_solve/MatrixTriangularSolve@matrix_solve_ls_1_cond_false_28664\" at \"matrix_solve_ls_1/cond@__inference_calcute_hbr_31510\") at \"PartitionedCall@__inference_signature_wrapper_31523\") at \"PartitionedCall\")): 'tf.MatrixTriangularSolve' op is neither a custom op nor a flex op\r\n",
      "    <unknown>:0: note: loc(\"PartitionedCall\"): called from\r\n",
      "    <unknown>:0: error: loc(callsite(callsite(callsite(\"matrix_solve_ls_1/cond/cholesky_solve/MatrixTriangularSolve_1@matrix_solve_ls_1_cond_false_28664\" at \"matrix_solve_ls_1/cond@__inference_calcute_hbr_31510\") at \"PartitionedCall@__inference_signature_wrapper_31523\") at \"PartitionedCall\")): 'tf.MatrixTriangularSolve' op is neither a custom op nor a flex op\r\n",
      "    <unknown>:0: note: loc(\"PartitionedCall\"): called from\r\n",
      "    <unknown>:0: error: loc(callsite(callsite(callsite(\"matrix_solve_ls_1/cond/Cholesky@matrix_solve_ls_1_cond_true_28663\" at \"matrix_solve_ls_1/cond@__inference_calcute_hbr_31510\") at \"PartitionedCall@__inference_signature_wrapper_31523\") at \"PartitionedCall\")): 'tf.Cholesky' op is neither a custom op nor a flex op\r\n",
      "    <unknown>:0: note: loc(\"PartitionedCall\"): called from\r\n",
      "    <unknown>:0: error: loc(callsite(callsite(callsite(\"matrix_solve_ls_1/cond/cholesky_solve/MatrixTriangularSolve@matrix_solve_ls_1_cond_true_28663\" at \"matrix_solve_ls_1/cond@__inference_calcute_hbr_31510\") at \"PartitionedCall@__inference_signature_wrapper_31523\") at \"PartitionedCall\")): 'tf.MatrixTriangularSolve' op is neither a custom op nor a flex op\r\n",
      "    <unknown>:0: note: loc(\"PartitionedCall\"): called from\r\n",
      "    <unknown>:0: error: loc(callsite(callsite(callsite(\"matrix_solve_ls_1/cond/cholesky_solve/MatrixTriangularSolve_1@matrix_solve_ls_1_cond_true_28663\" at \"matrix_solve_ls_1/cond@__inference_calcute_hbr_31510\") at \"PartitionedCall@__inference_signature_wrapper_31523\") at \"PartitionedCall\")): 'tf.MatrixTriangularSolve' op is neither a custom op nor a flex op\r\n",
      "    <unknown>:0: note: loc(\"PartitionedCall\"): called from\r\n",
      "    <unknown>:0: error: loc(callsite(callsite(callsite(\"matrix_solve_ls/cond/Cholesky@matrix_solve_ls_cond_false_25991\" at \"matrix_solve_ls/cond@__inference_calcute_hbr_31510\") at \"PartitionedCall@__inference_signature_wrapper_31523\") at \"PartitionedCall\")): 'tf.Cholesky' op is neither a custom op nor a flex op\r\n",
      "    <unknown>:0: note: loc(\"PartitionedCall\"): called from\r\n",
      "    <unknown>:0: error: loc(callsite(callsite(callsite(\"matrix_solve_ls/cond/cholesky_solve/MatrixTriangularSolve@matrix_solve_ls_cond_false_25991\" at \"matrix_solve_ls/cond@__inference_calcute_hbr_31510\") at \"PartitionedCall@__inference_signature_wrapper_31523\") at \"PartitionedCall\")): 'tf.MatrixTriangularSolve' op is neither a custom op nor a flex op\r\n",
      "    <unknown>:0: note: loc(\"PartitionedCall\"): called from\r\n",
      "    <unknown>:0: error: loc(callsite(callsite(callsite(\"matrix_solve_ls/cond/cholesky_solve/MatrixTriangularSolve_1@matrix_solve_ls_cond_false_25991\" at \"matrix_solve_ls/cond@__inference_calcute_hbr_31510\") at \"PartitionedCall@__inference_signature_wrapper_31523\") at \"PartitionedCall\")): 'tf.MatrixTriangularSolve' op is neither a custom op nor a flex op\r\n",
      "    <unknown>:0: note: loc(\"PartitionedCall\"): called from\r\n",
      "    <unknown>:0: error: loc(callsite(callsite(callsite(\"matrix_solve_ls/cond/Cholesky@matrix_solve_ls_cond_true_25990\" at \"matrix_solve_ls/cond@__inference_calcute_hbr_31510\") at \"PartitionedCall@__inference_signature_wrapper_31523\") at \"PartitionedCall\")): 'tf.Cholesky' op is neither a custom op nor a flex op\r\n",
      "    <unknown>:0: note: loc(\"PartitionedCall\"): called from\r\n",
      "    <unknown>:0: error: loc(callsite(callsite(callsite(\"matrix_solve_ls/cond/cholesky_solve/MatrixTriangularSolve@matrix_solve_ls_cond_true_25990\" at \"matrix_solve_ls/cond@__inference_calcute_hbr_31510\") at \"PartitionedCall@__inference_signature_wrapper_31523\") at \"PartitionedCall\")): 'tf.MatrixTriangularSolve' op is neither a custom op nor a flex op\r\n",
      "    <unknown>:0: note: loc(\"PartitionedCall\"): called from\r\n",
      "    <unknown>:0: error: loc(callsite(callsite(callsite(\"matrix_solve_ls/cond/cholesky_solve/MatrixTriangularSolve_1@matrix_solve_ls_cond_true_25990\" at \"matrix_solve_ls/cond@__inference_calcute_hbr_31510\") at \"PartitionedCall@__inference_signature_wrapper_31523\") at \"PartitionedCall\")): 'tf.MatrixTriangularSolve' op is neither a custom op nor a flex op\r\n",
      "    <unknown>:0: note: loc(\"PartitionedCall\"): called from\r\n",
      "    <unknown>:0: error: loc(callsite(callsite(\"matrix_solve_ls_1/cond@__inference_calcute_hbr_31510\" at \"PartitionedCall@__inference_signature_wrapper_31523\") at \"PartitionedCall\")): failed while converting: 'tf.IfRegion306_else': Ops that need custom implementation (enabled via setting the -emit-custom-ops flag):\r\n",
      "\t    tf.Cholesky {device = \"\"}\r\n",
      "\t    tf.MatrixTriangularSolve {adjoint = false, device = \"\", lower = true}\r\n",
      "\t    tf.MatrixTriangularSolve {adjoint = true, device = \"\", lower = true}\r\n",
      "    <unknown>:0: note: loc(\"PartitionedCall\"): called from\r\n",
      "    \r\n",
      "    \r\n",
      "    During handling of the above exception, another exception occurred:\r\n",
      "    \r\n",
      "    Traceback (most recent call last):\r\n",
      "      File \"/home/aia/repos/algo-pipeline/TF_HBR.py\", line 144, in <module>\r\n",
      "        tflite_model = converter.convert()\r\n",
      "      File \"/home/aia/anaconda3/envs/notebooks/lib/python3.7/site-packages/tensorflow/lite/python/lite.py\", line 739, in convert\r\n",
      "        result = _convert_saved_model(**converter_kwargs)\r\n",
      "      File \"/home/aia/anaconda3/envs/notebooks/lib/python3.7/site-packages/tensorflow/lite/python/convert.py\", line 637, in convert_saved_model\r\n",
      "        enable_mlir_converter=True)\r\n",
      "      File \"/home/aia/anaconda3/envs/notebooks/lib/python3.7/site-packages/tensorflow/lite/python/convert.py\", line 216, in toco_convert_protos\r\n",
      "        raise ConverterError(str(e))\r\n",
      "    tensorflow.lite.python.convert.ConverterError: <unknown>:0: error: loc(callsite(callsite(callsite(\"matrix_solve_ls_1/cond/Cholesky@matrix_solve_ls_1_cond_false_28664\" at \"matrix_solve_ls_1/cond@__inference_calcute_hbr_31510\") at \"PartitionedCall@__inference_signature_wrapper_31523\") at \"PartitionedCall\")): 'tf.Cholesky' op is neither a custom op nor a flex op\r\n",
      "    <unknown>:0: note: loc(\"PartitionedCall\"): called from\r\n",
      "    <unknown>:0: error: loc(callsite(callsite(callsite(\"matrix_solve_ls_1/cond/cholesky_solve/MatrixTriangularSolve@matrix_solve_ls_1_cond_false_28664\" at \"matrix_solve_ls_1/cond@__inference_calcute_hbr_31510\") at \"PartitionedCall@__inference_signature_wrapper_31523\") at \"PartitionedCall\")): 'tf.MatrixTriangularSolve' op is neither a custom op nor a flex op\r\n",
      "    <unknown>:0: note: loc(\"PartitionedCall\"): called from\r\n",
      "    <unknown>:0: error: loc(callsite(callsite(callsite(\"matrix_solve_ls_1/cond/cholesky_solve/MatrixTriangularSolve_1@matrix_solve_ls_1_cond_false_28664\" at \"matrix_solve_ls_1/cond@__inference_calcute_hbr_31510\") at \"PartitionedCall@__inference_signature_wrapper_31523\") at \"PartitionedCall\")): 'tf.MatrixTriangularSolve' op is neither a custom op nor a flex op\r\n",
      "    <unknown>:0: note: loc(\"PartitionedCall\"): called from\r\n",
      "    <unknown>:0: error: loc(callsite(callsite(callsite(\"matrix_solve_ls_1/cond/Cholesky@matrix_solve_ls_1_cond_true_28663\" at \"matrix_solve_ls_1/cond@__inference_calcute_hbr_31510\") at \"PartitionedCall@__inference_signature_wrapper_31523\") at \"PartitionedCall\")): 'tf.Cholesky' op is neither a custom op nor a flex op\r\n",
      "    <unknown>:0: note: loc(\"PartitionedCall\"): called from\r\n",
      "    <unknown>:0: error: loc(callsite(callsite(callsite(\"matrix_solve_ls_1/cond/cholesky_solve/MatrixTriangularSolve@matrix_solve_ls_1_cond_true_28663\" at \"matrix_solve_ls_1/cond@__inference_calcute_hbr_31510\") at \"PartitionedCall@__inference_signature_wrapper_31523\") at \"PartitionedCall\")): 'tf.MatrixTriangularSolve' op is neither a custom op nor a flex op\r\n",
      "    <unknown>:0: note: loc(\"PartitionedCall\"): called from\r\n",
      "    <unknown>:0: error: loc(callsite(callsite(callsite(\"matrix_solve_ls_1/cond/cholesky_solve/MatrixTriangularSolve_1@matrix_solve_ls_1_cond_true_28663\" at \"matrix_solve_ls_1/cond@__inference_calcute_hbr_31510\") at \"PartitionedCall@__inference_signature_wrapper_31523\") at \"PartitionedCall\")): 'tf.MatrixTriangularSolve' op is neither a custom op nor a flex op\r\n",
      "    <unknown>:0: note: loc(\"PartitionedCall\"): called from\r\n",
      "    <unknown>:0: error: loc(callsite(callsite(callsite(\"matrix_solve_ls/cond/Cholesky@matrix_solve_ls_cond_false_25991\" at \"matrix_solve_ls/cond@__inference_calcute_hbr_31510\") at \"PartitionedCall@__inference_signature_wrapper_31523\") at \"PartitionedCall\")): 'tf.Cholesky' op is neither a custom op nor a flex op\r\n",
      "    <unknown>:0: note: loc(\"PartitionedCall\"): called from\r\n",
      "    <unknown>:0: error: loc(callsite(callsite(callsite(\"matrix_solve_ls/cond/cholesky_solve/MatrixTriangularSolve@matrix_solve_ls_cond_false_25991\" at \"matrix_solve_ls/cond@__inference_calcute_hbr_31510\") at \"PartitionedCall@__inference_signature_wrapper_31523\") at \"PartitionedCall\")): 'tf.MatrixTriangularSolve' op is neither a custom op nor a flex op\r\n",
      "    <unknown>:0: note: loc(\"PartitionedCall\"): called from\r\n",
      "    <unknown>:0: error: loc(callsite(callsite(callsite(\"matrix_solve_ls/cond/cholesky_solve/MatrixTriangularSolve_1@matrix_solve_ls_cond_false_25991\" at \"matrix_solve_ls/cond@__inference_calcute_hbr_31510\") at \"PartitionedCall@__inference_signature_wrapper_31523\") at \"PartitionedCall\")): 'tf.MatrixTriangularSolve' op is neither a custom op nor a flex op\r\n",
      "    <unknown>:0: note: loc(\"PartitionedCall\"): called from\r\n",
      "    <unknown>:0: error: loc(callsite(callsite(callsite(\"matrix_solve_ls/cond/Cholesky@matrix_solve_ls_cond_true_25990\" at \"matrix_solve_ls/cond@__inference_calcute_hbr_31510\") at \"PartitionedCall@__inference_signature_wrapper_31523\") at \"PartitionedCall\")): 'tf.Cholesky' op is neither a custom op nor a flex op\r\n",
      "    <unknown>:0: note: loc(\"PartitionedCall\"): called from\r\n",
      "    <unknown>:0: error: loc(callsite(callsite(callsite(\"matrix_solve_ls/cond/cholesky_solve/MatrixTriangularSolve@matrix_solve_ls_cond_true_25990\" at \"matrix_solve_ls/cond@__inference_calcute_hbr_31510\") at \"PartitionedCall@__inference_signature_wrapper_31523\") at \"PartitionedCall\")): 'tf.MatrixTriangularSolve' op is neither a custom op nor a flex op\r\n",
      "    <unknown>:0: note: loc(\"PartitionedCall\"): called from\r\n",
      "    <unknown>:0: error: loc(callsite(callsite(callsite(\"matrix_solve_ls/cond/cholesky_solve/MatrixTriangularSolve_1@matrix_solve_ls_cond_true_25990\" at \"matrix_solve_ls/cond@__inference_calcute_hbr_31510\") at \"PartitionedCall@__inference_signature_wrapper_31523\") at \"PartitionedCall\")): 'tf.MatrixTriangularSolve' op is neither a custom op nor a flex op\r\n",
      "    <unknown>:0: note: loc(\"PartitionedCall\"): called from\r\n",
      "    <unknown>:0: error: loc(callsite(callsite(\"matrix_solve_ls_1/cond@__inference_calcute_hbr_31510\" at \"PartitionedCall@__inference_signature_wrapper_31523\") at \"PartitionedCall\")): failed while converting: 'tf.IfRegion306_else': Ops that need custom implementation (enabled via setting the -emit-custom-ops flag):\r\n",
      "\t    tf.Cholesky {device = \"\"}\r\n",
      "\t    tf.MatrixTriangularSolve {adjoint = false, device = \"\", lower = true}\r\n",
      "\t    tf.MatrixTriangularSolve {adjoint = true, device = \"\", lower = true}\r\n",
      "    <unknown>:0: note: loc(\"PartitionedCall\"): called from```\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue** \r\n",
      "\r\n",
      "https://colab.research.google.com/gist/AiaHaruv/396c2473b4601e9d35332884dbd02fa9/tflie_gist.ipynb\r\n",
      "\r\n",
      "**Any other info / logs**\r\n",
      "In the colab, the traceback is: \r\n",
      "          4 frames\r\n",
      "          Exception: <unknown>:0: error: \r\n",
      "  loc(callsite(callsite(\"matrix_solve_ls/cholesky_solve/MatrixTriangularSolve@__inference_calculate_403\" at \r\n",
      "  \"PartitionedCall@__inference_signature_wrapper_411\") at \"PartitionedCall\")): 'tf.MatrixTriangularSolve' op is neither a custom \r\n",
      "  op nor a flex op\r\n",
      "          <unknown>:0: note: loc(\"PartitionedCall\"): called from\r\n",
      "          <unknown>:0: error: \r\n",
      "  loc(callsite(callsite(\"matrix_solve_ls/cholesky_solve/MatrixTriangularSolve_1@__inference_calculate_403\" at \r\n",
      "  \"PartitionedCall@__inference_signature_wrapper_411\") at \"PartitionedCall\")): 'tf.MatrixTriangularSolve' op is neither a custom \r\n",
      "  op nor a flex op\r\n",
      "          <unknown>:0: note: loc(\"PartitionedCall\"): called from\r\n",
      "          <unknown>:0: error: failed while converting: 'main': Ops that need custom implementation (enabled via setting the -emit- \r\n",
      "  custom-ops flag):\r\n",
      "\t          tf.MatrixTriangularSolve {adjoint = false, device = \"\", lower = true}\r\n",
      "\t          tf.MatrixTriangularSolve {adjoint = true, device = \"\", lower = true}\r\n",
      "          \r\n",
      "        \r\n",
      "    During handling of the above exception, another exception occurred:\r\n",
      "    \r\n",
      "    ConverterError                            Traceback (most recent call last)\r\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n",
      "        214       return model_str\r\n",
      "        215     except Exception as e:\r\n",
      "    --> 216       raise ConverterError(str(e))\r\n",
      "        217 \r\n",
      "        218   if distutils.spawn.find_executable(_toco_from_proto_bin) is None:\r\n",
      "    \r\n",
      "    ConverterError: <unknown>:0: error: loc(callsite(callsite(\"matrix_solve_ls/cholesky_solve/MatrixTriangularSolve@__inference_calculate_403\" at \"PartitionedCall@__inference_signature_wrapper_411\") at \"PartitionedCall\")): 'tf.MatrixTriangularSolve' op is neither a custom op nor a flex op\r\n",
      "    <unknown>:0: note: loc(\"PartitionedCall\"): called from\r\n",
      "    <unknown>:0: error: loc(callsite(callsite(\"matrix_solve_ls/cholesky_solve/MatrixTriangularSolve_1@__inference_calculate_403\" at \"PartitionedCall@__inference_signature_wrapper_411\") at \"PartitionedCall\")): 'tf.MatrixTriangularSolve' op is neither a custom op nor a flex op\r\n",
      "    <unknown>:0: note: loc(\"PartitionedCall\"): called from\r\n",
      "    <unknown>:0: error: failed while converting: 'main': Ops that need custom implementation (enabled via setting the -emit-custom-ops flag):\r\n",
      "\t    tf.MatrixTriangularSolve {adjoint = false, device = \"\", lower = true}\r\n",
      "\t    tf.MatrixTriangularSolve {adjoint = true, device = \"\", lower = true}\r\n",
      "\r\n",
      "Thanks you \r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:lite\n",
      "stat:awaiting tensorflower\n",
      "type:feature\n",
      "\n",
      "\n",
      "issue title -  Illegal instruction (core dumped) while importing TensorFlow\n",
      "issue body -  System:\r\n",
      " - HP EliteBook 8470p\r\n",
      " - Freshly installed Ubuntu 20.04 (5.8.0-38-generic)\r\n",
      " - Python 3.8.5\r\n",
      " - TensorFlow 2.4\r\n",
      "\r\n",
      "I installed TensorFlow with \r\n",
      "``` \r\n",
      "pip install --upgrade pip\r\n",
      "pip install tensorflow\r\n",
      "```\r\n",
      "as recommended on [https://www.tensorflow.org/install](https://www.tensorflow.org/install).\r\n",
      "\r\n",
      "When I try to import TensorFlow : \r\n",
      "```\r\n",
      ">>> import tensorflow as tf\r\n",
      "```\r\n",
      "I've got the following error:\r\n",
      "```\r\n",
      "Illegal instruction (core dumped)\r\n",
      "```\r\n",
      "\r\n",
      "I already read the following issues or SO questions:\r\n",
      "- [https://github.com/tensorflow/tensorflow/issues/17411](https://github.com/tensorflow/tensorflow/issues/17411)\r\n",
      "- [https://github.com/tensorflow/tensorflow/issues/45744](https://github.com/tensorflow/tensorflow/issues/45744)\r\n",
      "- [https://tech.amikelive.com/node-887/how-to-resolve-error-illegal-instruction-core-dumped-when-running-import-tensorflow-in-a-python-program/](https://tech.amikelive.com/node-887/how-to-resolve-error-illegal-instruction-core-dumped-when-running-import-tensorflow-in-a-python-program/)\r\n",
      "- [https://stackoverflow.com/questions/49092527/illegal-instructioncore-dumped-tensorflow](https://stackoverflow.com/questions/49092527/illegal-instructioncore-dumped-tensorflow)\r\n",
      "\r\n",
      "Most are referring to old TensorFlow versions (1.5) or related to CPU that does not support AVX. \r\n",
      "\r\n",
      "My CPU does support AVX:\r\n",
      "```\r\n",
      "$ more /proc/cpuinfo | grep flags\r\n",
      "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl smx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm cpuid_fault epb pti ssbd ibrs ibpb stibp fsgsbase smep erms xsaveopt dtherm ida arat pln pts md_clear flush_l1d\r\n",
      "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl smx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm cpuid_fault epb pti ssbd ibrs ibpb stibp fsgsbase smep erms xsaveopt dtherm ida arat pln pts md_clear flush_l1d\r\n",
      "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl smx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm cpuid_fault epb pti ssbd ibrs ibpb stibp fsgsbase smep erms xsaveopt dtherm ida arat pln pts md_clear flush_l1d\r\n",
      "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl smx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm cpuid_fault epb pti ssbd ibrs ibpb stibp fsgsbase smep erms xsaveopt dtherm ida arat pln pts md_clear flush_l1d\r\n",
      "```\r\n",
      "\r\n",
      "Since my laptop is quite recent and my OS is a fresh insall, I think something sounds wrong with the TensorFlow installer. \n",
      "issue labels - \n",
      "\n",
      "\n",
      "issue title -  I got this error while trying to run the webcam_demo.py example in Posenet library from tensorflow. how to resolve this?\n",
      "issue body -  I got this error/warning while trying to run the webcam_demo.py example in Posenet library from Tensorflow. how to resolve this?\r\n",
      "\r\n",
      "This is the **Git Repo** from where I forked this code : [posenet-python](https://github.com/rwightman/posenet-python)\r\n",
      "\r\n",
      "and This is my **Output Screen** :\r\n",
      "```\r\n",
      ">>> \r\n",
      " RESTART: A:\\Python\\Scripts\\Posenet-Forked -- OGCode\\posenet-python-master\\webcam_demo.py \r\n",
      "Cannot find model file ./_models\\model-mobilenet_v1_101.pb, converting from tfjs...\r\n",
      "WARNING:tensorflow:From A:\\Python\\lib\\site-packages\\tensorflow\\python\\tools\\freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use standard file APIs to check for files with this prefix.\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"A:\\Python\\Scripts\\Posenet-Forked -- OGCode\\posenet-python-master\\webcam_demo.py\", line 66, in <module>\r\n",
      "    main()\r\n",
      "  File \"A:\\Python\\Scripts\\Posenet-Forked -- OGCode\\posenet-python-master\\webcam_demo.py\", line 20, in main\r\n",
      "    model_cfg, model_outputs = posenet.load_model(args.model, sess)\r\n",
      "  File \"A:\\Python\\Scripts\\Posenet-Forked -- OGCode\\posenet-python-master\\posenet\\model.py\", line 42, in load_model\r\n",
      "    convert(model_ord, model_dir, check=False)\r\n",
      "  File \"A:\\Python\\Scripts\\Posenet-Forked -- OGCode\\posenet-python-master\\posenet\\converter\\tfjs2python.py\", line 198, in convert\r\n",
      "    initializer_nodes=\"\")\r\n",
      "  File \"A:\\Python\\lib\\site-packages\\tensorflow\\python\\tools\\freeze_graph.py\", line 361, in freeze_graph\r\n",
      "    checkpoint_version=checkpoint_version)\r\n",
      "  File \"A:\\Python\\lib\\site-packages\\tensorflow\\python\\tools\\freeze_graph.py\", line 190, in freeze_graph_with_def_protos\r\n",
      "    var_list=var_list, write_version=checkpoint_version)\r\n",
      "  File \"A:\\Python\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 835, in __init__\r\n",
      "    self.build()\r\n",
      "  File \"A:\\Python\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 847, in build\r\n",
      "    self._build(self._filename, build_save=True, build_restore=True)\r\n",
      "  File \"A:\\Python\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 885, in _build\r\n",
      "    build_restore=build_restore)\r\n",
      "  File \"A:\\Python\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 489, in _build_internal\r\n",
      "    names_to_saveables)\r\n",
      "  File \"A:\\Python\\lib\\site-packages\\tensorflow\\python\\training\\saving\\saveable_object_util.py\", line 362, in validate_and_slice_inputs\r\n",
      "    for converted_saveable_object in saveable_objects_for_op(op, name):\r\n",
      "  File \"A:\\Python\\lib\\site-packages\\tensorflow\\python\\training\\saving\\saveable_object_util.py\", line 223, in saveable_objects_for_op\r\n",
      "    yield ResourceVariableSaveable(variable, \"\", name)\r\n",
      "  File \"A:\\Python\\lib\\site-packages\\tensorflow\\python\\training\\saving\\saveable_object_util.py\", line 95, in __init__\r\n",
      "    self.handle_op = var.op.inputs[0]\r\n",
      "IndexError: tuple index out of range\r\n",
      ">>> \r\n",
      "```\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:apis\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  How to avoid downloading when using many times of the tff.simulation.datasets.stackoverflow.load_data()\n",
      "issue body -  As I see in this function, there is a parameter called cache_dir, which can be used to define the ```__pycache__```. However when I set this each time, every time when I run the code, it still tries to download the data. Is there any way that can avoid this? Or should I change the type of HDF5ClientData file to other format then I can save the data?\r\n",
      "\r\n",
      "Thanks a lot for your help!\n",
      "issue labels - \n",
      "comp:data\n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  cannot convert tf savedmodel to onnx\n",
      "issue body -  **System information**\r\n",
      "OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n",
      "Linux Ubuntu 18.04\r\n",
      "TensorFlow installed from (source or binary):\r\n",
      "binary\r\n",
      "TensorFlow version (use command below):\r\n",
      "tf-nightly-gpu         2.5.0.dev20210119\r\n",
      "Python version:\r\n",
      "3.6 (Anaconda)\r\n",
      "Tensorflow-onnx version:\r\n",
      "1.8.0. build from source\r\n",
      "\r\n",
      "my command line :\r\n",
      "```shell\r\n",
      "python -m tf2onnx.convert --saved-model ./model.savedmodel --output fea.onnx --custom-ops Bucketize,AsString,StringToHashBucketFast --signature_def serving_default --tag serve --opset 12 \r\n",
      "```\r\n",
      "But I got the following error：\r\n",
      "```shell\r\n",
      "......\r\n",
      "2021-01-21 11:29:41,413 - ERROR - Could not find table resource to replace placeholder unknown_172\r\n",
      "2021-01-21 11:29:41,415 - ERROR - Could not find table resource to replace placeholder unknown_174\r\n",
      "2021-01-21 11:29:41,416 - ERROR - Could not find table resource to replace placeholder unknown_176\r\n",
      "2021-01-21 11:29:41,417 - ERROR - Could not find table resource to replace placeholder unknown_178\r\n",
      "2021-01-21 11:29:41,418 - ERROR - Could not find table resource to replace placeholder unknown_180\r\n",
      "2021-01-21 11:29:41,418 - ERROR - Could not find table resource to replace placeholder unknown_183\r\n",
      "2021-01-21 11:29:41,418 - ERROR - Could not find table resource to replace placeholder unknown_185\r\n",
      "2021-01-21 11:29:41,418 - ERROR - Could not find table resource to replace placeholder unknown_187\r\n",
      "2021-01-21 11:29:41,418 - ERROR - Could not find table resource to replace placeholder unknown_189\r\n",
      "2021-01-21 11:29:41,418 - ERROR - Could not find table resource to replace placeholder unknown_193\r\n",
      "2021-01-21 11:29:41,418 - ERROR - Could not find table resource to replace placeholder unknown_195\r\n",
      "2021-01-21 11:29:41,419 - ERROR - Could not find table resource to replace placeholder unknown_197\r\n",
      "......\r\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: 'func' argument to TF_GraphCopyFunction cannot be null\r\n",
      "Exception ignored in: <bound method CapturableResourceDeleter.__del__ of <tensorflow.python.training.tracking.tracking.CapturableResourceDeleter object at 0x7f70486cbcf8>>\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/usr/local/anaconda3/envs/tf2.2-n/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py\", line 208, in __del__\r\n",
      "    self._destroy_resource()\r\n",
      "  File \"/usr/local/anaconda3/envs/tf2.2-n/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 797, in __call__\r\n",
      "    result = self._call(*args, **kwds)\r\n",
      "  File \"/usr/local/anaconda3/envs/tf2.2-n/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 841, in _call\r\n",
      "    self._initialize(args, kwds, add_initializers_to=initializers)\r\n",
      "  File \"/usr/local/anaconda3/envs/tf2.2-n/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 695, in _initialize\r\n",
      "    *args, **kwds))\r\n",
      "  File \"/usr/local/anaconda3/envs/tf2.2-n/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 2981, in _get_concrete_function_internal_garbage_collected\r\n",
      "    graph_function, _ = self._maybe_define_function(args, kwargs)\r\n",
      "  File \"/usr/local/anaconda3/envs/tf2.2-n/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 3373, in _maybe_define_function\r\n",
      "    graph_function = self._create_graph_function(args, kwargs)\r\n",
      "  File \"/usr/local/anaconda3/envs/tf2.2-n/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 3218, in _create_graph_function\r\n",
      "    capture_by_value=self._capture_by_value),\r\n",
      "  File \"/usr/local/anaconda3/envs/tf2.2-n/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\", line 998, in func_graph_from_py_func\r\n",
      "    func_outputs = python_func(*func_args, **func_kwargs)\r\n",
      "  File \"/usr/local/anaconda3/envs/tf2.2-n/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 603, in wrapped_fn\r\n",
      "    out = weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n",
      "  File \"/usr/local/anaconda3/envs/tf2.2-n/lib/python3.6/site-packages/tensorflow/python/saved_model/function_deserialization.py\", line 257, in restored_function_body\r\n",
      "    return _call_concrete_function(function, inputs)\r\n",
      "  File \"/usr/local/anaconda3/envs/tf2.2-n/lib/python3.6/site-packages/tensorflow/python/saved_model/function_deserialization.py\", line 75, in _call_concrete_function\r\n",
      "    result = function._call_flat(tensor_inputs, function._captured_inputs)  # pylint: disable=protected-access\r\n",
      "  File \"/usr/local/anaconda3/envs/tf2.2-n/lib/python3.6/site-packages/tensorflow/python/saved_model/load.py\", line 116, in _call_flat\r\n",
      "    cancellation_manager)\r\n",
      "  File \"/usr/local/anaconda3/envs/tf2.2-n/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 1944, in _call_flat\r\n",
      "    flat_outputs = forward_function.call(ctx, args_with_tangents)\r\n",
      "  File \"/usr/local/anaconda3/envs/tf2.2-n/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 590, in call\r\n",
      "    executor_type=executor_type)\r\n",
      "  File \"/usr/local/anaconda3/envs/tf2.2-n/lib/python3.6/site-packages/tensorflow/python/ops/functional_ops.py\", line 1206, in partitioned_call\r\n",
      "    f.add_to_graph(graph)\r\n",
      "  File \"/usr/local/anaconda3/envs/tf2.2-n/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 506, in add_to_graph\r\n",
      "    g._add_function(self)\r\n",
      "  File \"/usr/local/anaconda3/envs/tf2.2-n/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3403, in _add_function\r\n",
      "    gradient)\r\n",
      "\r\n",
      "```\r\n",
      "I want to get the ONNX model, desperate for some advice！\r\n",
      "\r\n",
      "thank you very much\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.5\n",
      "comp:apis\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Virtual Batch Normalization using Tensorflow\n",
      "issue body -  What is the recommended way to use [VBN](https://arxiv.org/pdf/1606.03498v1.pdf) using Tensorflow? \r\n",
      "Would it be an idea to add this to the documentation?\n",
      "issue labels - \n",
      "comp:keras\n",
      "stat:awaiting tensorflower\n",
      "type:feature\n",
      "\n",
      "\n",
      "issue title -  constant learning rate logging added to TensorBoard callback\n",
      "issue body -  Allowing TensorBoard callback to log leraning rate (lr) even if there is no scheduler used. Tensorboard will log every lr even when it is a constant. This is especially important when using ReduceLROnPlateau callback as the lr is treated as constatnt in every iteration and changed by assigning a new constant. \n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:keras\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Update merge.py\n",
      "issue body -  Updated typo as per issue #46568\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:keras\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -   A function to get number of TFLite model parameters\n",
      "issue body -  **System information**\r\n",
      "- TensorFlow version (you are using): 2.3 \r\n",
      "- Are you willing to contribute it (Yes/No): No\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**Describe the feature and the current behavior/state.** A function to get number of TFLite model parameters\r\n",
      "\r\n",
      "**Will this change the current api? How?** It will add a new function to Interpreter\r\n",
      "\r\n",
      "**Who will benefit with this feature?** People who need to optimize the model. They will be able to see the change in parameter number and estimate benefits \r\n",
      "\r\n",
      "**Any Other info.** If there is a way to do it in a current version, please, let me know.\r\n",
      "\n",
      "issue labels - \n",
      "comp:lite\n",
      "type:feature\n",
      "\n",
      "\n",
      "issue title -  typos\n",
      "issue body -  There are multiple typos in the tensorflow/python/keras/layers/merge.py\r\n",
      "Used in a **_functiona_** model\r\n",
      "should be **_functional_**\n",
      "issue labels - \n",
      "comp:keras\n",
      "type:docs-bug\n",
      "\n",
      "\n",
      "issue title -  LSTM layer not Using cuDNN even with default arguments\n",
      "issue body -  **Problem description**\r\n",
      "I am using a TF model in RLlib in my project. The model contains a ConvLSTM2D layer and a LSTM layer. However, even with default arguments, there is a warning:\r\n",
      "`WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU`\r\n",
      "Note that there is no complaint about my ConvLSTM2D layer.\r\n",
      "I also tried to manually filled in all relevant arguments with values required in TF document, but the warning still persists. Can anyone help me figure out where the problem is?\r\n",
      "Full code for constructing the model and the system information are provided below.\r\n",
      "\r\n",
      "**Code for model construction**\r\n",
      "Note that obs_space.shape=46800, activation=tanh, and normc_initializer is a built-in kernel initializer in RLlib.\r\n",
      "```\r\n",
      "        inputs = tf.keras.layers.Input(obs_space.shape, name=\"observations\")\r\n",
      "\r\n",
      "        input_depth = inputs[..., :45000]\r\n",
      "        input_sensor = inputs[..., 45000:]\r\n",
      "\r\n",
      "        input_depth = tf.keras.layers.Reshape(target_shape=(50, 30, 30, 1))(input_depth)\r\n",
      "        input_sensor = tf.keras.layers.Reshape(target_shape=(50, 36))(input_sensor)\r\n",
      "\r\n",
      "        conv_lstm = tf.keras.layers.ConvLSTM2D(filters=10, kernel_size=(3, 3))(input_depth)\r\n",
      "        pool = tf.keras.layers.MaxPool2D()(conv_lstm)\r\n",
      "        flat = tf.keras.layers.Flatten()(pool)\r\n",
      "\r\n",
      "        fc1 = tf.keras.layers.Dense(512, activation=activation)(flat)\r\n",
      "        fc2 = tf.keras.layers.Dense(128, activation=activation)(fc1)\r\n",
      "        conv_out = tf.keras.layers.Dense(16, activation=activation)(fc2)\r\n",
      "\r\n",
      "        sensor_lstm = tf.keras.layers.LSTM(units=64)(input_sensor)\r\n",
      "\r\n",
      "        action_out = tf.keras.layers.Concatenate()([sensor_lstm, conv_out])\r\n",
      "        action_out = tf.keras.layers.Dense(128, activation=activation,\r\n",
      "                                           kernel_initializer=normc_initializer(1.0))(action_out)\r\n",
      "        action_out = tf.keras.layers.Dense(64, activation=activation,\r\n",
      "                                           kernel_initializer=normc_initializer(1.0))(action_out)\r\n",
      "        final_action_out = tf.keras.layers.Dense(num_outputs,\r\n",
      "                                                 activation=activation,\r\n",
      "                                                 kernel_initializer=normc_initializer(1.0))(action_out)\r\n",
      "\r\n",
      "        value_out = tf.keras.layers.Dense(\r\n",
      "            1,\r\n",
      "            name=\"value_out\",\r\n",
      "            activation=None,\r\n",
      "            kernel_initializer=normc_initializer(0.01))(action_out)\r\n",
      "\r\n",
      "        self.base_model = tf.keras.Model(\r\n",
      "            inputs, [final_action_out, value_out])\r\n",
      "        self.register_variables(self.base_model.variables)\r\n",
      "```\r\n",
      "\r\n",
      "**System information**\r\n",
      "- OS Platform and Distribution: Ubuntu 18.04.5 LTS\r\n",
      "- TensorFlow installed from (source or binary): source\r\n",
      "- TensorFlow version: 2.4.0\r\n",
      "- Python version: 3.6.9\r\n",
      "- Bazel version (if compiling from source): 3.7.2\r\n",
      "- GCC/Compiler version (if compiling from source): 7.5.0\r\n",
      "- CUDA version: 11.2\r\n",
      "- cuDNN version: 8.0.5\r\n",
      "- GPU model and memory: GeForce RTX 3080, 10009MiB\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:keras\n",
      "stat:awaiting response\n",
      "type:performance\n",
      "\n",
      "\n",
      "issue title -  Regarding tensorflow error in ubuntu 18.04\n",
      "issue body -   F tensorflow/core/platform/cpu_feature_guard.cc:38] The TensorFlow library was compiled to use FMA instructions, but these aren't available on your machine.\r\n",
      "Aborted (core dumped)\r\n",
      "\r\n",
      "\r\n",
      "can anyone please help me with this error.\n",
      "issue labels - \n",
      "stalled\n",
      "stat:awaiting response\n",
      "subtype: ubuntu/linux\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  [Intel MKL] Fixing threadpool bug\n",
      "issue body -  This change fixes a potential bug by managing the lifetime of MKL threadpool object in a better way.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:mkl\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  Add sanitizer builds to the TFLM CI.\n",
      "issue body -  Also, updated the docs with the new way of using the sanitizers with bazel.\r\n",
      "    \r\n",
      "With https://github.com/tensorflow/tensorflow/commit/38f8ad1795b392f0f6bdf86a5a9c4f227bac7b76 we now have a more concise way of using asan/msan/ubsan with bazel.\r\n",
      "    \r\n",
      "Fixes http://b/177076500\r\n",
      "\r\n",
      "Still open issue: http://b/178621680\r\n",
      "\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  Update version numbers for TensorFlow 2.4.1\n",
      "issue body -  Before merging this PR, please double check that it has correctly updated\n",
      "`core/public/version.h`, `tools/pip_package/setup.py`, and\n",
      "`tensorflow/tensorflow.bzl`. Also review the execution notes below:\n",
      "\n",
      "```\n",
      "Major: 2 -> 2\n",
      "Minor: 4 -> 4\n",
      "Patch: 0 -> 1\n",
      "\n",
      "WARNING: Below are potentially instances of lingering old version string \n",
      "\"2.4.0\" in source directory \"tensorflow/\" that are not updated by this script. \n",
      "Please check them manually!\n",
      "Binary file \n",
      "tensorflow/cc/saved_model/testdata/StaticHashTableModule/saved_model.pb matches\n",
      "Binary file tensorflow/cc/saved_model/testdata/AssetModule/saved_model.pb \n",
      "matches\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:188:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:211:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:212:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:217:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:267:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:268:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:269:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:270:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:271:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:272:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:296:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:297:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:298:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:301:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:302:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:321:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:321:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:321:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:129:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:178:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:179:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:184:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:215:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:216:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:217:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:218:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:222:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:223:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:224:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:225:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:244:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:244:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:244:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:128:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:176:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:177:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:181:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:217:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:218:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:219:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:220:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:227:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:228:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:229:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:230:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:249:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:249:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:249:2.4.0\n",
      "tensorflow/lite/tools/versioning/runtime_version.cc:63:2.4.0\n",
      "tensorflow/lite/tools/versioning/runtime_version.cc:68:2.4.0\n",
      "tensorflow/lite/tools/versioning/runtime_version.cc:77:2.4.0\n",
      "tensorflow/lite/tools/versioning/runtime_version.cc:78:2.4.0\n",
      "tensorflow/lite/tools/versioning/runtime_version.cc:86:2.4.0\n",
      "tensorflow/lite/tools/versioning/runtime_version.cc:87:2.4.0\n",
      "tensorflow/lite/tools/versioning/runtime_version.cc:117:2.4.0\n",
      "tensorflow/lite/tools/versioning/runtime_version.cc:148:2.4.0\n",
      "tensorflow/lite/tools/versioning/runtime_version.cc:153:2.4.0\n",
      "tensorflow/lite/tools/versioning/runtime_version.cc:164:2.4.0\n",
      "tensorflow/lite/tools/versioning/runtime_version.cc:180:2.4.0\n",
      "tensorflow/lite/tools/versioning/runtime_version.cc:197:2.4.0\n",
      "tensorflow/lite/tools/versioning/runtime_version.cc:233:2.4.0\n",
      "tensorflow/lite/tools/versioning/runtime_version.cc:313:2.4.0\n",
      "tensorflow/lite/tools/versioning/runtime_version.cc:321:2.4.0\n",
      "tensorflow/security/advisory/tfsa-2020-028.md:16:2.4.0\n",
      "tensorflow/security/advisory/tfsa-2020-027.md:41:2.4.0\n",
      "tensorflow/tools/pip_package/setup.py:100:2.4.0\n",
      "tensorflow/tools/pip_package/setup.py:112:2.4.0\n",
      "tensorflow/tools/pip_package/setup.py:114:2.4.0\n",
      "tensorflow/tools/ci_build/release/common_win.bat:49:2.4.0\n",
      "tensorflow/tools/ci_build/release/common.sh:143:2.4.0\n",
      "tensorflow/tools/ci_build/release/common.sh:199:2.4.0\n",
      "tensorflow/python/keras/__init__.py:35:2.4.0\n",
      "Binary file \n",
      "tensorflow/c/experimental/saved_model/internal/testdata/UninitializedVariable/sa\n",
      "ved_model.pb matches\n",
      "\n",
      "WARNING: Below are potentially instances of lingering old version string \n",
      "\"2.4.0\" in source directory \"tensorflow/\" that are not updated by this script. \n",
      "Please check them manually!\n",
      "Binary file \n",
      "tensorflow/cc/saved_model/testdata/StaticHashTableModule/saved_model.pb matches\n",
      "Binary file tensorflow/cc/saved_model/testdata/AssetModule/saved_model.pb \n",
      "matches\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:188:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:211:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:212:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:217:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:267:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:268:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:269:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:270:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:271:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:272:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:296:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:297:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:298:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:301:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:302:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:321:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:321:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:321:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:129:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:178:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:179:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:184:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:215:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:216:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:217:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:218:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:222:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:223:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:224:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:225:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:244:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:244:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:244:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:128:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:176:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:177:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:181:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:217:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:218:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:219:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:220:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:227:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:228:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:229:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:230:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:249:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:249:2.4.0\n",
      "tensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:249:2.4.0\n",
      "tensorflow/lite/tools/versioning/runtime_version.cc:63:2.4.0\n",
      "tensorflow/lite/tools/versioning/runtime_version.cc:68:2.4.0\n",
      "tensorflow/lite/tools/versioning/runtime_version.cc:77:2.4.0\n",
      "tensorflow/lite/tools/versioning/runtime_version.cc:78:2.4.0\n",
      "tensorflow/lite/tools/versioning/runtime_version.cc:86:2.4.0\n",
      "tensorflow/lite/tools/versioning/runtime_version.cc:87:2.4.0\n",
      "tensorflow/lite/tools/versioning/runtime_version.cc:117:2.4.0\n",
      "tensorflow/lite/tools/versioning/runtime_version.cc:148:2.4.0\n",
      "tensorflow/lite/tools/versioning/runtime_version.cc:153:2.4.0\n",
      "tensorflow/lite/tools/versioning/runtime_version.cc:164:2.4.0\n",
      "tensorflow/lite/tools/versioning/runtime_version.cc:180:2.4.0\n",
      "tensorflow/lite/tools/versioning/runtime_version.cc:197:2.4.0\n",
      "tensorflow/lite/tools/versioning/runtime_version.cc:233:2.4.0\n",
      "tensorflow/lite/tools/versioning/runtime_version.cc:313:2.4.0\n",
      "tensorflow/lite/tools/versioning/runtime_version.cc:321:2.4.0\n",
      "tensorflow/security/advisory/tfsa-2020-028.md:16:2.4.0\n",
      "tensorflow/security/advisory/tfsa-2020-027.md:41:2.4.0\n",
      "tensorflow/tools/pip_package/setup.py:100:2.4.0\n",
      "tensorflow/tools/pip_package/setup.py:112:2.4.0\n",
      "tensorflow/tools/pip_package/setup.py:114:2.4.0\n",
      "tensorflow/tools/ci_build/release/common_win.bat:49:2.4.0\n",
      "tensorflow/tools/ci_build/release/common.sh:143:2.4.0\n",
      "tensorflow/tools/ci_build/release/common.sh:199:2.4.0\n",
      "tensorflow/python/keras/__init__.py:35:2.4.0\n",
      "Binary file \n",
      "tensorflow/c/experimental/saved_model/internal/testdata/UninitializedVariable/sa\n",
      "ved_model.pb matches\n",
      "```\n",
      "issue labels - \n",
      "cla: yes\n",
      "\n",
      "\n",
      "issue title -  Issue on using tf.vectorized_map on a function with a tf.while_loop\n",
      "issue body -  <em>Please make sure that this is a bug. As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:bug_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n",
      "- TensorFlow installed from (source or binary): ```conda install tensorflow-gpu``` in a conda env\r\n",
      "- TensorFlow version (use command below): 2.2\r\n",
      "- Python version: 3.8.3\r\n",
      "- Bazel version (if compiling from source): No\r\n",
      "- GCC/Compiler version (if compiling from source): No\r\n",
      "- CUDA/cuDNN version: 10.2\r\n",
      "- GPU model and memory:  GeForce RTX 2060 SUPER computeCapability: 7.5, Memory  8 GB\r\n",
      "\r\n",
      "You can collect some of this information using our environment capture\r\n",
      "[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n",
      "You can also obtain the TensorFlow version with:\r\n",
      "1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n",
      "2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n",
      "\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "The included code tries to vectorize a function which adds 10.0 to the input and does so through a while loop which adds 1.0 each time (for 10 times). The function runs perfectly when using tf.map_fn and fails when using tf.vectorized_map\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "The function would not run when using vectorized map and the error points towards  `` Either add a converter or set --op_conversion_fallback_to_while_loop=True, which may run slower``\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "```\r\n",
      "if __name__ == \"__main__\":\r\n",
      "    import tensorflow as tf\r\n",
      "    @tf.function()\r\n",
      "    def add(a):       \r\n",
      "        i = tf.constant(0, dtype = tf.int32)\r\n",
      "        c = tf.constant(1., dtype = tf.float32)\r\n",
      "        loop_index = lambda  i, c, a: i < 10\r\n",
      "        def body(i, c, a):\r\n",
      "            a = c + a\r\n",
      "            i = i + 1\r\n",
      "            return i,c, a\r\n",
      "        i,c, a = tf.while_loop(loop_index, body, [i,c, a],\\\r\n",
      "             shape_invariants=[tf.TensorShape(()), tf.TensorShape(()),tf.TensorShape([1])], back_prop= False, parallel_iterations=1)\r\n",
      "\r\n",
      "        return a\r\n",
      "\r\n",
      "    counter =  tf.reshape(tf.range(0, 40, delta = 1, dtype = tf.float32), shape = [40,1])\r\n",
      "\r\n",
      "    all_ = tf.vectorized_map(add, counter) # does not work\r\n",
      "    # all_ = tf.map_fn(add, counter)\r\n",
      "    print(all_, '<-- should be [40,1] float32 tensor with elements [10., 11., ...49.]')\r\n",
      "```\r\n",
      "\r\n",
      "**Other info / logs** Include any logs or source code that would be helpful to\r\n",
      "diagnose the problem. If including tracebacks, please include the full\r\n",
      "traceback. Large logs and files should be attached.\r\n",
      "```\r\n",
      "2021-01-20 17:26:53.074085: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n",
      "2021-01-20 17:26:53.101801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n",
      "2021-01-20 17:26:53.102114: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\n",
      "pciBusID: 0000:01:00.0 name: GeForce RTX 2060 SUPER computeCapability: 7.5\r\n",
      "coreClock: 1.71GHz coreCount: 34 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s\r\n",
      "2021-01-20 17:26:53.102265: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n",
      "2021-01-20 17:26:53.103295: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n",
      "2021-01-20 17:26:53.104241: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n",
      "2021-01-20 17:26:53.104386: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n",
      "2021-01-20 17:26:53.105247: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n",
      "2021-01-20 17:26:53.105734: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n",
      "2021-01-20 17:26:53.107658: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n",
      "2021-01-20 17:26:53.107742: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n",
      "2021-01-20 17:26:53.108011: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n",
      "2021-01-20 17:26:53.108212: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n",
      "2021-01-20 17:26:53.108397: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n",
      "2021-01-20 17:26:53.112400: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 3600000000 Hz\r\n",
      "2021-01-20 17:26:53.112694: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55853e36c7b0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n",
      "2021-01-20 17:26:53.112704: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n",
      "2021-01-20 17:26:53.112820: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n",
      "2021-01-20 17:26:53.113110: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\n",
      "pciBusID: 0000:01:00.0 name: GeForce RTX 2060 SUPER computeCapability: 7.5\r\n",
      "coreClock: 1.71GHz coreCount: 34 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s\r\n",
      "2021-01-20 17:26:53.113143: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n",
      "2021-01-20 17:26:53.113154: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n",
      "2021-01-20 17:26:53.113163: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n",
      "2021-01-20 17:26:53.113171: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n",
      "2021-01-20 17:26:53.113180: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n",
      "2021-01-20 17:26:53.113188: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n",
      "2021-01-20 17:26:53.113197: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n",
      "2021-01-20 17:26:53.113232: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n",
      "2021-01-20 17:26:53.113508: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n",
      "2021-01-20 17:26:53.113760: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n",
      "2021-01-20 17:26:53.113779: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n",
      "2021-01-20 17:26:53.182683: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n",
      "2021-01-20 17:26:53.182703: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \r\n",
      "2021-01-20 17:26:53.182707: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \r\n",
      "2021-01-20 17:26:53.182837: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n",
      "2021-01-20 17:26:53.183090: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n",
      "2021-01-20 17:26:53.183343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n",
      "2021-01-20 17:26:53.183550: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6620 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2060 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n",
      "2021-01-20 17:26:53.184629: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55853ec63ff0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n",
      "2021-01-20 17:26:53.184638: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2060 SUPER, Compute Capability 7.5\r\n",
      "WARNING:tensorflow:From distransTest.py:266: calling while_loop_v2 (from tensorflow.python.ops.control_flow_ops) with back_prop=False is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\r\n",
      "Instead of:\r\n",
      "results = tf.while_loop(c, b, vars, back_prop=False)\r\n",
      "Use:\r\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.while_loop(c, b, vars))\r\n",
      "ERROR:tensorflow:Got error while pfor was converting op name: \"loop_body/PartitionedCall\"\r\n",
      "op: \"PartitionedCall\"\r\n",
      "input: \"loop_body/GatherV2\"\r\n",
      "attr {\r\n",
      "  key: \"Tin\"\r\n",
      "  value {\r\n",
      "    list {\r\n",
      "      type: DT_FLOAT\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "attr {\r\n",
      "  key: \"Tout\"\r\n",
      "  value {\r\n",
      "    list {\r\n",
      "      type: DT_FLOAT\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "attr {\r\n",
      "  key: \"_read_only_resource_inputs\"\r\n",
      "  value {\r\n",
      "    list {\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "attr {\r\n",
      "  key: \"config\"\r\n",
      "  value {\r\n",
      "    s: \"\"\r\n",
      "  }\r\n",
      "}\r\n",
      "attr {\r\n",
      "  key: \"config_proto\"\r\n",
      "  value {\r\n",
      "    s: \"\\n\\007\\n\\003CPU\\020\\001\\n\\007\\n\\003GPU\\020\\0012\\005*\\0010J\\0008\\001\"\r\n",
      "  }\r\n",
      "}\r\n",
      "attr {\r\n",
      "  key: \"executor_type\"\r\n",
      "  value {\r\n",
      "    s: \"\"\r\n",
      "  }\r\n",
      "}\r\n",
      "attr {\r\n",
      "  key: \"f\"\r\n",
      "  value {\r\n",
      "    func {\r\n",
      "      name: \"__inference_add_57\"\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "with inputs (<tf.Tensor 'loop_body/GatherV2:0' shape=(1,) dtype=float32>,)\r\n",
      ", converted inputs [WrappedTensor(t=<tf.Tensor 'loop_body/GatherV2/params:0' shape=(40, 1) dtype=float32>, is_stacked=True, is_sparse_stacked=False)]\r\n",
      "in user code:\r\n",
      "\r\n",
      "    /home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/ops/parallel_for/pfor.py:3600 f  *\r\n",
      "        [converter._convert_helper(x).t for x in func._func_graph_outputs])\r\n",
      "    /home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/ops/parallel_for/pfor.py:1460 _convert_helper  **\r\n",
      "        raise ValueError(\"No converter defined for %s\\n%s\\ninputs: %s. \"\r\n",
      "\r\n",
      "    ValueError: No converter defined for StatelessWhile\r\n",
      "    name: \"while\"\r\n",
      "    op: \"StatelessWhile\"\r\n",
      "    input: \"while/loop_counter\"\r\n",
      "    input: \"while/maximum_iterations\"\r\n",
      "    input: \"Const\"\r\n",
      "    input: \"Const_1\"\r\n",
      "    input: \"a\"\r\n",
      "    attr {\r\n",
      "      key: \"T\"\r\n",
      "      value {\r\n",
      "        list {\r\n",
      "          type: DT_INT32\r\n",
      "          type: DT_INT32\r\n",
      "          type: DT_INT32\r\n",
      "          type: DT_FLOAT\r\n",
      "          type: DT_FLOAT\r\n",
      "        }\r\n",
      "      }\r\n",
      "    }\r\n",
      "    attr {\r\n",
      "      key: \"_lower_using_switch_merge\"\r\n",
      "      value {\r\n",
      "        b: true\r\n",
      "      }\r\n",
      "    }\r\n",
      "    attr {\r\n",
      "      key: \"_num_original_outputs\"\r\n",
      "      value {\r\n",
      "        i: 5\r\n",
      "      }\r\n",
      "    }\r\n",
      "    attr {\r\n",
      "      key: \"_read_only_resource_inputs\"\r\n",
      "      value {\r\n",
      "        list {\r\n",
      "        }\r\n",
      "      }\r\n",
      "    }\r\n",
      "    attr {\r\n",
      "      key: \"body\"\r\n",
      "      value {\r\n",
      "        func {\r\n",
      "          name: \"while_body_20\"\r\n",
      "        }\r\n",
      "      }\r\n",
      "    }\r\n",
      "    attr {\r\n",
      "      key: \"cond\"\r\n",
      "      value {\r\n",
      "        func {\r\n",
      "          name: \"while_cond_19\"\r\n",
      "        }\r\n",
      "      }\r\n",
      "    }\r\n",
      "    attr {\r\n",
      "      key: \"output_shapes\"\r\n",
      "      value {\r\n",
      "        list {\r\n",
      "          shape {\r\n",
      "          }\r\n",
      "          shape {\r\n",
      "          }\r\n",
      "          shape {\r\n",
      "          }\r\n",
      "          shape {\r\n",
      "          }\r\n",
      "          shape {\r\n",
      "            dim {\r\n",
      "              size: 1\r\n",
      "            }\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "    }\r\n",
      "    attr {\r\n",
      "      key: \"parallel_iterations\"\r\n",
      "      value {\r\n",
      "        i: 1\r\n",
      "      }\r\n",
      "    }\r\n",
      "    \r\n",
      "    inputs: [WrappedTensor(t=<tf.Tensor 'while/loop_counter/pfor/Const:0' shape=() dtype=int32>, is_stacked=False, is_sparse_stacked=False), WrappedTensor(t=<tf.Tensor 'while/maximum_iterations/pfor/Const:0' shape=() dtype=int32>, is_stacked=False, is_sparse_stacked=False), WrappedTensor(t=<tf.Tensor 'Const/pfor/Const:0' shape=() dtype=int32>, is_stacked=False, is_sparse_stacked=False), WrappedTensor(t=<tf.Tensor 'Const_1/pfor/Const:0' shape=() dtype=float32>, is_stacked=False, is_sparse_stacked=False), WrappedTensor(t=<tf.Tensor 'args_0:0' shape=(40, 1) dtype=float32>, is_stacked=True, is_sparse_stacked=False)]. \r\n",
      "    Either add a converter or set --op_conversion_fallback_to_while_loop=True, which may run slower\r\n",
      "\r\n",
      "Here are the pfor conversion stack traces:\r\n",
      "ERROR:tensorflow:name: \"loop_body/PartitionedCall\"\r\n",
      "op: \"PartitionedCall\"\r\n",
      "input: \"loop_body/GatherV2\"\r\n",
      "attr {\r\n",
      "  key: \"Tin\"\r\n",
      "  value {\r\n",
      "    list {\r\n",
      "      type: DT_FLOAT\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "attr {\r\n",
      "  key: \"Tout\"\r\n",
      "  value {\r\n",
      "    list {\r\n",
      "      type: DT_FLOAT\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "attr {\r\n",
      "  key: \"_read_only_resource_inputs\"\r\n",
      "  value {\r\n",
      "    list {\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "attr {\r\n",
      "  key: \"config\"\r\n",
      "  value {\r\n",
      "    s: \"\"\r\n",
      "  }\r\n",
      "}\r\n",
      "attr {\r\n",
      "  key: \"config_proto\"\r\n",
      "  value {\r\n",
      "    s: \"\\n\\007\\n\\003CPU\\020\\001\\n\\007\\n\\003GPU\\020\\0012\\005*\\0010J\\0008\\001\"\r\n",
      "  }\r\n",
      "}\r\n",
      "attr {\r\n",
      "  key: \"executor_type\"\r\n",
      "  value {\r\n",
      "    s: \"\"\r\n",
      "  }\r\n",
      "}\r\n",
      "attr {\r\n",
      "  key: \"f\"\r\n",
      "  value {\r\n",
      "    func {\r\n",
      "      name: \"__inference_add_57\"\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "\r\n",
      "created at:\r\n",
      "    File \"distransTest.py\", line 273, in <module>\r\n",
      "    all_ = tf.vectorized_map(add, counter) # does not work\r\n",
      "    File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py\", line 407, in vectorized_map\r\n",
      "    return pfor(loop_fn, batch_size)\r\n",
      "    File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py\", line 198, in pfor\r\n",
      "    outputs = f()\r\n",
      "    File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 580, in __call__\r\n",
      "    result = self._call(*args, **kwds)\r\n",
      "    File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 627, in _call\r\n",
      "    self._initialize(args, kwds, add_initializers_to=initializers)\r\n",
      "    File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 505, in _initialize\r\n",
      "    self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n",
      "    File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 2446, in _get_concrete_function_internal_garbage_collected\r\n",
      "    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n",
      "    File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 2777, in _maybe_define_function\r\n",
      "    graph_function = self._create_graph_function(args, kwargs)\r\n",
      "    File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 2657, in _create_graph_function\r\n",
      "    func_graph_module.func_graph_from_py_func(\r\n",
      "    File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\", line 981, in func_graph_from_py_func\r\n",
      "    func_outputs = python_func(*func_args, **func_kwargs)\r\n",
      "    File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 441, in wrapped_fn\r\n",
      "    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n",
      "    File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\", line 957, in wrapper\r\n",
      "    return autograph.converted_call(\r\n",
      "    File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py\", line 183, in f\r\n",
      "    return _pfor_impl(loop_fn, iters, parallel_iterations=parallel_iterations)\r\n",
      "    File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py\", line 237, in _pfor_impl\r\n",
      "    loop_fn_outputs = loop_fn(loop_var)\r\n",
      "    File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py\", line 400, in loop_fn\r\n",
      "    return fn(gathered_elems)\r\n",
      "    File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 580, in __call__\r\n",
      "    result = self._call(*args, **kwds)\r\n",
      "    File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 650, in _call\r\n",
      "    return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access\r\n",
      "    File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 1661, in _filtered_call\r\n",
      "    return self._call_flat(\r\n",
      "    File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 1760, in _call_flat\r\n",
      "    flat_outputs = forward_function.call(ctx, args_with_tangents)\r\n",
      "    File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 621, in call\r\n",
      "    outputs = functional_ops.partitioned_call(\r\n",
      "    File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/ops/functional_ops.py\", line 1180, in partitioned_call\r\n",
      "    op = graph.create_op(op_name, args, tout, name=op_name, attrs=op_attrs)\r\n",
      "    File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n",
      "    return func(*args, **kwargs)\r\n",
      "    File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 3257, in create_op\r\n",
      "    return self._create_op_internal(op_type, inputs, dtypes, input_types, name,\r\n",
      "    File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\", line 593, in _create_op_internal\r\n",
      "    return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\r\n",
      "    File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 3319, in _create_op_internal\r\n",
      "    ret = Operation(\r\n",
      "    File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 1791, in __init__\r\n",
      "    self._traceback = tf_stack.extract_stack()\r\n",
      "\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"distransTest.py\", line 273, in <module>\r\n",
      "    all_ = tf.vectorized_map(add, counter) # does not work\r\n",
      "  File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py\", line 407, in vectorized_map\r\n",
      "    return pfor(loop_fn, batch_size)\r\n",
      "  File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py\", line 198, in pfor\r\n",
      "    outputs = f()\r\n",
      "  File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 580, in __call__\r\n",
      "    result = self._call(*args, **kwds)\r\n",
      "  File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 627, in _call\r\n",
      "    self._initialize(args, kwds, add_initializers_to=initializers)\r\n",
      "  File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 505, in _initialize\r\n",
      "    self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n",
      "  File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 2446, in _get_concrete_function_internal_garbage_collected\r\n",
      "    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n",
      "  File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 2777, in _maybe_define_function\r\n",
      "    graph_function = self._create_graph_function(args, kwargs)\r\n",
      "  File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 2657, in _create_graph_function\r\n",
      "    func_graph_module.func_graph_from_py_func(\r\n",
      "  File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\", line 981, in func_graph_from_py_func\r\n",
      "    func_outputs = python_func(*func_args, **func_kwargs)\r\n",
      "  File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 441, in wrapped_fn\r\n",
      "    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n",
      "  File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\", line 968, in wrapper\r\n",
      "    raise e.ag_error_metadata.to_exception(e)\r\n",
      "ValueError: in user code:\r\n",
      "\r\n",
      "    /home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py:183 f  *\r\n",
      "        return _pfor_impl(loop_fn, iters, parallel_iterations=parallel_iterations)\r\n",
      "    /home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/ops/parallel_for/pfor.py:3600 f  *\r\n",
      "        [converter._convert_helper(x).t for x in func._func_graph_outputs])\r\n",
      "    /home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/ops/parallel_for/pfor.py:1460 _convert_helper  **\r\n",
      "        raise ValueError(\"No converter defined for %s\\n%s\\ninputs: %s. \"\r\n",
      "\r\n",
      "    ValueError: No converter defined for StatelessWhile\r\n",
      "    name: \"while\"\r\n",
      "    op: \"StatelessWhile\"\r\n",
      "    input: \"while/loop_counter\"\r\n",
      "    input: \"while/maximum_iterations\"\r\n",
      "    input: \"Const\"\r\n",
      "    input: \"Const_1\"\r\n",
      "    input: \"a\"\r\n",
      "    attr {\r\n",
      "      key: \"T\"\r\n",
      "      value {\r\n",
      "        list {\r\n",
      "          type: DT_INT32\r\n",
      "          type: DT_INT32\r\n",
      "          type: DT_INT32\r\n",
      "          type: DT_FLOAT\r\n",
      "          type: DT_FLOAT\r\n",
      "        }\r\n",
      "      }\r\n",
      "    }\r\n",
      "    attr {\r\n",
      "      key: \"_lower_using_switch_merge\"\r\n",
      "      value {\r\n",
      "        b: true\r\n",
      "      }\r\n",
      "    }\r\n",
      "    attr {\r\n",
      "      key: \"_num_original_outputs\"\r\n",
      "      value {\r\n",
      "        i: 5\r\n",
      "      }\r\n",
      "    }\r\n",
      "    attr {\r\n",
      "      key: \"_read_only_resource_inputs\"\r\n",
      "      value {\r\n",
      "        list {\r\n",
      "        }\r\n",
      "      }\r\n",
      "    }\r\n",
      "    attr {\r\n",
      "      key: \"body\"\r\n",
      "      value {\r\n",
      "        func {\r\n",
      "          name: \"while_body_20\"\r\n",
      "        }\r\n",
      "      }\r\n",
      "    }\r\n",
      "    attr {\r\n",
      "      key: \"cond\"\r\n",
      "      value {\r\n",
      "        func {\r\n",
      "          name: \"while_cond_19\"\r\n",
      "        }\r\n",
      "      }\r\n",
      "    }\r\n",
      "    attr {\r\n",
      "      key: \"output_shapes\"\r\n",
      "      value {\r\n",
      "        list {\r\n",
      "          shape {\r\n",
      "          }\r\n",
      "          shape {\r\n",
      "          }\r\n",
      "          shape {\r\n",
      "          }\r\n",
      "          shape {\r\n",
      "          }\r\n",
      "          shape {\r\n",
      "            dim {\r\n",
      "              size: 1\r\n",
      "            }\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "    }\r\n",
      "    attr {\r\n",
      "      key: \"parallel_iterations\"\r\n",
      "      value {\r\n",
      "        i: 1\r\n",
      "      }\r\n",
      "    }\r\n",
      "    \r\n",
      "    inputs: [WrappedTensor(t=<tf.Tensor 'while/loop_counter/pfor/Const:0' shape=() dtype=int32>, is_stacked=False, is_sparse_stacked=False), WrappedTensor(t=<tf.Tensor 'while/maximum_iterations/pfor/Const:0' shape=() dtype=int32>, is_stacked=False, is_sparse_stacked=False), WrappedTensor(t=<tf.Tensor 'Const/pfor/Const:0' shape=() dtype=int32>, is_stacked=False, is_sparse_stacked=False), WrappedTensor(t=<tf.Tensor 'Const_1/pfor/Const:0' shape=() dtype=float32>, is_stacked=False, is_sparse_stacked=False), WrappedTensor(t=<tf.Tensor 'args_0:0' shape=(40, 1) dtype=float32>, is_stacked=True, is_sparse_stacked=False)]. \r\n",
      "    Either add a converter or set --op_conversion_fallback_to_while_loop=True, which may run slower\r\n",
      "\r\n",
      "```\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.2\n",
      "comp:ops\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  [Intel MKL] Re-enable Conv2D+Squeeze+BiasAdd fusion\n",
      "issue body -  This PR enables Conv2D+Squeeze+BiasAdd fusion in grappler which is already supported by MKL.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:grappler\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  Update release notes for TensorFlow 2.4.1\n",
      "issue body -  This PR is intentionally incomplete. One of the Release Owners for 2.4.1\n",
      "needs to fill in the internal release notes for this version before the PR gets\n",
      "submitted. Click on the :pencil2: icon in the header for `RELEASE.md` under\n",
      "\"Files Changed\" above.\n",
      "issue labels - \n",
      "cla: yes\n",
      "\n",
      "\n",
      "issue title -  TypeError: cannot pickle '_thread.lock' object in TensorFlow 2.4\n",
      "issue body -  <em>Please make sure that this is a bug. As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:bug_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 10\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "- TensorFlow installed from (source or binary): Binary\r\n",
      "- TensorFlow version (use command below): v2.4.0-0-g582c8d236cb 2.4.0\r\n",
      "- Python version: 3.7.9\r\n",
      "- Bazel version (if compiling from source): n/a\r\n",
      "- GCC/Compiler version (if compiling from source): n/a\r\n",
      "- CUDA/cuDNN version: n/a\r\n",
      "- GPU model and memory: n/a\r\n",
      "\r\n",
      "You can collect some of this information using our environment capture\r\n",
      "[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n",
      "You can also obtain the TensorFlow version with:\r\n",
      "1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n",
      "2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n",
      "\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "Running a simple training process with MultiWorkerMirroredStrategy fails with `TypeError: can't pickle _thread.lock objects`.\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "The training should proceed without errors.\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "Provide a reproducible test case that is the bare minimum necessary to generate\r\n",
      "the problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n",
      "\r\n",
      "The example needs to run in a distributed environment to reproduce the issue, so save the script in a file and run it in 3 different terminals.\r\n",
      "\r\n",
      "```\r\n",
      "TF_CONFIG='{\"cluster\": {\"chief\": [\"localhost:2222\"], \"worker\": [\"localhost:2223\", \"localhost:2224\"]}, \"task\": {\"type\": \"chief\", \"index\": 0}}' python script.py \r\n",
      "TF_CONFIG='{\"cluster\": {\"chief\": [\"localhost:2222\"], \"worker\": [\"localhost:2223\", \"localhost:2224\"]}, \"task\": {\"type\": \"worker\", \"index\": 0}}' python script.py\r\n",
      "TF_CONFIG='{\"cluster\": {\"chief\": [\"localhost:2222\"], \"worker\": [\"localhost:2223\", \"localhost:2224\"]}, \"task\": {\"type\": \"worker\", \"index\": 1}}' python script.py\r\n",
      "```\r\n",
      "\r\n",
      "```\r\n",
      "import tensorflow as tf\r\n",
      "import tensorflow_datasets as tfds\r\n",
      " \r\n",
      "buffer_size = 10000\r\n",
      "batch_size = 64\r\n",
      "learning_rate = 1e-4\r\n",
      " \r\n",
      "def input_fn(mode, input_context=None):\r\n",
      "  tfds.disable_progress_bar()\r\n",
      "  datasets, _ = tfds.load(name='mnist', with_info=True, as_supervised=True)\r\n",
      "  mnist_dataset = ( \r\n",
      "      datasets['train']\r\n",
      "      if mode == tf.estimator.ModeKeys.TRAIN else datasets['test'])\r\n",
      " \r\n",
      "  def scale(image, label):\r\n",
      "    image = tf.cast(image, tf.float32)\r\n",
      "    image /= 255 \r\n",
      "    return image, label\r\n",
      " \r\n",
      "  if input_context:\r\n",
      "    mnist_dataset = mnist_dataset.shard(input_context.num_input_pipelines,\r\n",
      "                                        input_context.input_pipeline_id)\r\n",
      "  return mnist_dataset.map(scale).cache().shuffle(buffer_size).batch(batch_size)\r\n",
      " \r\n",
      "def model_fn(features, labels, mode):\r\n",
      "  model = tf.keras.Sequential([\r\n",
      "      tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\r\n",
      "      tf.keras.layers.MaxPooling2D(),\r\n",
      "      tf.keras.layers.Flatten(),\r\n",
      "      tf.keras.layers.Dense(64, activation='relu'),\r\n",
      "      tf.keras.layers.Dense(10)\r\n",
      "  ])\r\n",
      "  logits = model(features, training=False)\r\n",
      " \r\n",
      "  if mode == tf.estimator.ModeKeys.PREDICT:\r\n",
      "    predictions = {'logits': logits}\r\n",
      "    return tf.estimator.EstimatorSpec(labels=labels, predictions=predictions)\r\n",
      " \r\n",
      "  optimizer = tf.compat.v1.train.GradientDescentOptimizer(\r\n",
      "      learning_rate=learning_rate)\r\n",
      "  loss = tf.keras.losses.SparseCategoricalCrossentropy(\r\n",
      "      from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(labels,\r\n",
      "                                                                  logits)\r\n",
      "  loss = tf.reduce_sum(loss) * (1. / batch_size)\r\n",
      "  if mode == tf.estimator.ModeKeys.EVAL:\r\n",
      "    return tf.estimator.EstimatorSpec(mode, loss=loss)\r\n",
      " \r\n",
      "  logging_hook = tf.estimator.LoggingTensorHook({'loss': loss}, every_n_iter=10)\r\n",
      " \r\n",
      "  return tf.estimator.EstimatorSpec(\r\n",
      "      mode=mode,\r\n",
      "      loss=loss,\r\n",
      "      training_hooks=[logging_hook],\r\n",
      "      train_op=optimizer.minimize(\r\n",
      "          loss, tf.compat.v1.train.get_or_create_global_step()))\r\n",
      " \r\n",
      " \r\n",
      "strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\n",
      " \r\n",
      "config = tf.estimator.RunConfig(train_distribute=strategy)\r\n",
      " \r\n",
      "classifier = tf.estimator.Estimator(\r\n",
      "    model_fn=model_fn, model_dir='/tmp/multiworker', config=config)\r\n",
      " \r\n",
      "tf.estimator.train_and_evaluate(\r\n",
      "    classifier,\r\n",
      "    train_spec=tf.estimator.TrainSpec(input_fn=input_fn),\r\n",
      "    eval_spec=tf.estimator.EvalSpec(input_fn=input_fn))\r\n",
      "```\r\n",
      "\r\n",
      "**Other info / logs** Include any logs or source code that would be helpful to\r\n",
      "diagnose the problem. If including tracebacks, please include the full\r\n",
      "traceback. Large logs and files should be attached.\r\n",
      "\r\n",
      "Full logs:\r\n",
      "```\r\n",
      "TF_CONFIG='{\"cluster\": {\"chief\": [\"localhost:2222\"], \"worker\": [\"localhost:2223\", \"localhost:2224\"]}, \"task\": {\"type\": \"worker\", \"index\": 1}}' python script.py\r\n",
      "WARNING:tensorflow:From script.py:68: _CollectiveAllReduceStrategyExperimental.__init__ (from tensorflow.python.distribute.collective_all_reduce_strategy) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:                                                                \r\n",
      "use distribute.MultiWorkerMirroredStrategy instead                                                       \r\n",
      "2021-01-20 18:24:44.477611: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n",
      "2021-01-20 18:24:44.479538: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n",
      "2021-01-20 18:24:44.491607: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job chief -> {0 -> localhost:2222}\r\n",
      "2021-01-20 18:24:44.491654: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2223, 1 -> localhost:2224}\r\n",
      "2021-01-20 18:24:44.492211: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:2224\r\n",
      "Traceback (most recent call last):                                                                                           \r\n",
      "  File \"script.py\", line 73, in <module>                                                  \r\n",
      "    model_fn=model_fn, model_dir='/tmp/multiworker', config=config)                                \r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 183, in __init__\r\n",
      "    config, model_dir)                                                                          \r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1832, in maybe_overwrite_model_dir_and_session_config\r\n",
      "    config = run_config.RunConfig.replace(config, session_config=session_config)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/run_config.py\", line 923, in replace\r\n",
      "    copy.deepcopy(self),                                                             \r\n",
      "  File \"/opt/conda/lib/python3.7/copy.py\", line 180, in deepcopy                    \r\n",
      "    y = _reconstruct(x, memo, *rv)                                                                 \r\n",
      "  File \"/opt/conda/lib/python3.7/copy.py\", line 281, in _reconstruct         \r\n",
      "    state = deepcopy(state, memo)                                                              \r\n",
      "  File \"/opt/conda/lib/python3.7/copy.py\", line 150, in deepcopy            \r\n",
      "    y = copier(x, memo)                                                                            \r\n",
      "  File \"/opt/conda/lib/python3.7/copy.py\", line 241, in _deepcopy_dict       \r\n",
      "    y[deepcopy(key, memo)] = deepcopy(value, memo)          \r\n",
      "  File \"/opt/conda/lib/python3.7/copy.py\", line 161, in deepcopy                      \r\n",
      "    y = copier(memo)                                                                                                       \r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py\", line 1542, in __deepcopy__\r\n",
      "    setattr(result, k, copy.deepcopy(v, memo))                                                    \r\n",
      "  File \"/opt/conda/lib/python3.7/copy.py\", line 180, in deepcopy                                             \r\n",
      "    y = _reconstruct(x, memo, *rv)                                                \r\n",
      "  File \"/opt/conda/lib/python3.7/copy.py\", line 281, in _reconstruct                                             \r\n",
      "    state = deepcopy(state, memo)                                                   \r\n",
      "  File \"/opt/conda/lib/python3.7/copy.py\", line 150, in deepcopy                             \r\n",
      "    y = copier(x, memo)                                                                    \r\n",
      "  File \"/opt/conda/lib/python3.7/copy.py\", line 241, in _deepcopy_dict              \r\n",
      "    y[deepcopy(key, memo)] = deepcopy(value, memo)                                                                   \r\n",
      "  File \"/opt/conda/lib/python3.7/copy.py\", line 180, in deepcopy                      \r\n",
      "    y = _reconstruct(x, memo, *rv)                                                             \r\n",
      "  File \"/opt/conda/lib/python3.7/copy.py\", line 281, in _reconstruct                                             \r\n",
      "    state = deepcopy(state, memo)                                                   \r\n",
      "  File \"/opt/conda/lib/python3.7/copy.py\", line 150, in deepcopy            \r\n",
      "    y = copier(x, memo)                                                                      \r\n",
      "  File \"/opt/conda/lib/python3.7/copy.py\", line 241, in _deepcopy_dict                               \r\n",
      "    y[deepcopy(key, memo)] = deepcopy(value, memo)                            \r\n",
      "  File \"/opt/conda/lib/python3.7/copy.py\", line 180, in deepcopy                       \r\n",
      "    y = _reconstruct(x, memo, *rv)                                                                                             \r\n",
      "  File \"/opt/conda/lib/python3.7/copy.py\", line 281, in _reconstruct                       \r\n",
      "    state = deepcopy(state, memo)                                                                          \r\n",
      "  File \"/opt/conda/lib/python3.7/copy.py\", line 150, in deepcopy                 \r\n",
      "    y = copier(x, memo)                                                                   \r\n",
      "  File \"/opt/conda/lib/python3.7/copy.py\", line 241, in _deepcopy_dict                     \r\n",
      "    y[deepcopy(key, memo)] = deepcopy(value, memo)                                                                     \r\n",
      "  File \"/opt/conda/lib/python3.7/copy.py\", line 169, in deepcopy                       \r\n",
      "    rv = reductor(4)                                                                                                 \r\n",
      "TypeError: can't pickle _thread.lock objects  \r\n",
      "```\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:dist-strat\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  [INTEL MKL] Remove unnecessary oneDNN dependency.\n",
      "issue body -  Removes the redundant inclusion of oneDNN in libtensorflow_framework.so.2.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:mkl\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  set_weights() for tf.keras.layers.Layer not working for type string.\n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Cataline 10.15.7\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "- TensorFlow installed from (source or binary): Source\r\n",
      "- TensorFlow version (use command below): 2.4.0\r\n",
      "- Python version: 3.7.5\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "- GCC/Compiler version (if compiling from source): clang 11.0.3\r\n",
      "- CUDA/cuDNN version:\r\n",
      "- GPU model and memory:\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "When trying to `set_weights()` for a layer where the weights are strings the data type `string` is not understood. See the standalone code below.  \r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "The weights should be updated with no error.\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "```\r\n",
      "import tensorflow as tf\r\n",
      "\r\n",
      "\r\n",
      "def initializer(shape, dtype=None):\r\n",
      "    return tf.reshape(tf.constant([str(x) for x in range(shape[0])]), shape)\r\n",
      "\r\n",
      "\r\n",
      "class CustomLayer(tf.keras.layers.Layer):\r\n",
      "\r\n",
      "    def __init__(self, length=10, **kwargs):\r\n",
      "        super(CustomLayer, self).__init__(**kwargs)\r\n",
      "        self.length = length\r\n",
      "\r\n",
      "    def build(self, input_shape=None):\r\n",
      "        self.keys = self.add_weight(shape=(self.length,),\r\n",
      "                                    name='keys',\r\n",
      "                                    initializer=initializer,\r\n",
      "                                    dtype=tf.string,\r\n",
      "                                    trainable=False)\r\n",
      "\r\n",
      "        self.built = True\r\n",
      "\r\n",
      "    def call(self, x):\r\n",
      "        return self.keys\r\n",
      "\r\n",
      "    def get_config(self):\r\n",
      "        config = super(CustomLayer, self).get_config()\r\n",
      "        config.update({'length': self.length})\r\n",
      "        return config\r\n",
      "\r\n",
      "\r\n",
      "inputs = tf.keras.layers.Input(shape=())\r\n",
      "outputs = CustomLayer(name='lookup')(inputs)\r\n",
      "model = tf.keras.Model(inputs, outputs)\r\n",
      "model.compile()\r\n",
      "\r\n",
      "model.get_layer('lookup').set_weights(model.get_layer('lookup').get_weights())\r\n",
      "```\r\n",
      "\r\n",
      "**Other info / logs** \r\n",
      "```\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"example.py\", line 37, in <module>\r\n",
      "    model.get_layer('lookup').set_weights(model.get_layer('lookup').get_weights())\r\n",
      "  File \"/Users/mans.bermell/.pyenv/versions/tensorflow2.4/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 1877, in set_weights\r\n",
      "    backend.batch_set_value(weight_value_tuples)\r\n",
      "  File \"/Users/mans.bermell/.pyenv/versions/tensorflow2.4/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\r\n",
      "    return target(*args, **kwargs)\r\n",
      "  File \"/Users/mans.bermell/.pyenv/versions/tensorflow2.4/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\", line 3706, in batch_set_value\r\n",
      "    x.assign(np.asarray(value, dtype=dtype(x)))\r\n",
      "  File \"/Users/mans.bermell/.pyenv/versions/tensorflow2.4/lib/python3.7/site-packages/numpy/core/_asarray.py\", line 83, in asarray\r\n",
      "    return array(a, dtype, copy=False, order=order)\r\n",
      "TypeError: data type 'string' not understood\r\n",
      "```\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:keras\n",
      "stat:awaiting tensorflower\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Source code for tf.signal.rfft\n",
      "issue body -  Hi,\r\n",
      "\r\n",
      "I am trying to convert tf.signal.fft into c code for my project. I have gone through the below link to understand the functionality\r\n",
      "\r\n",
      "https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/signal/fft_ops.py#L115-L141\r\n",
      "\r\n",
      "I could not find the complete source to understand the  tf.signal.fft  API.\r\n",
      "\r\n",
      "The source code found is returning  fft_fn(input_tensor, fft_length, Tcomplex=complex_dtype, name=name) but no complete steps.\r\n",
      "\r\n",
      "Can someone please help to find the definition of the above function  fft_fn(..)\r\n",
      "\r\n",
      "or\r\n",
      "\r\n",
      "if there is any c code for this function, please provide the link or attachment.\r\n",
      "\r\n",
      "Regards,\r\n",
      "Yugesh.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:signal\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  Add cudaMallocAsync as an option\n",
      "issue body -  @sanjoy \r\n",
      "\r\n",
      "This PR add cudaMallocAsync as an option when CUDA 11.2 is used.\n",
      "issue labels - \n",
      "awaiting review\n",
      "cla: yes\n",
      "size:L\n",
      "\n",
      "\n",
      "issue title -  Allow environment variables to execution of flatc command\n",
      "issue body -  Allow setting of LD_LIBRARY_PATH to reach execution environment of flatc so it can load correct version of libstdc++ and so avoid build failure when built with non-system gcc\r\n",
      "Fixes #46549 \n",
      "issue labels - \n",
      "cla: yes\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Build failure when using non-system gcc with later glibc\n",
      "issue body -  **System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 10\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "- TensorFlow installed from (source or binary): Source\r\n",
      "- TensorFlow version: HEAD\r\n",
      "- Python version: 3.7.3\r\n",
      "- Installed using virtualenv? pip? conda?: n/a\r\n",
      "- Bazel version (if compiling from source): 3.7.2\r\n",
      "- GCC/Compiler version (if compiling from source): 10.2.0\r\n",
      "- CUDA/cuDNN version: n/a\r\n",
      "- GPU model and memory: n/a\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "\r\n",
      "Build using gcc 10.2.0 compiled on system with a later version of glibc than installed on system.\r\n",
      "The tool flatc which is compiled during the build links with the glibc from gcc 10.2.0.\r\n",
      "When flatc is executed later in the build, the environment is cleared out and it is not possible to set LD_LIBRARY_PATH so that it would load the correct version of glibc.\r\n",
      "\r\n",
      "**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n",
      "\r\n",
      "System installed libstdc++.so.6 is 3.4.25\r\n",
      "gcc 10.2.0 built with glibc 2.28 and libstdc++.so.6 3.4.28 installed to /usr/local\r\n",
      "Build command line:\r\n",
      "$ TF_MKL_ROOT=~builder/1/built_tarballs/onednn/ bazel build --config=noaws --config=nogcp --config=nonccl --config=mkl_aarch64 --action_env=TF_MKL_ROOT=/home/builder/1/built_tarballs/onednn/ --action_env=LD_LIBRARY_PATH=/usr/local/lib64 --host_action_env=LD_LIBRARY_PATH=/usr/local/lib64 //tensorflow/tools/pip_package:build_pip_package --verbose_failures\r\n",
      "\r\n",
      "**Any other info / logs**\r\n",
      "Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n",
      "\r\n",
      "Build fails with the below messages:\r\n",
      "ERROR: /home/debian/src/tensorflow_1dnn-git/tensorflow/lite/python/BUILD:11:22: Generating flatbuffer files for <source file tensorflow/lite/schema/schema.fbs>: failed (Exit 1): flatc failed: error executing command \r\n",
      "  (cd /home/debian/.cache/bazel/_bazel_debian/0988af24c3a1bb0051e8f606aa777b9e/execroot/org_tensorflow && \\\r\n",
      "  exec env - \\\r\n",
      "  bazel-out/host/bin/external/flatbuffers/flatc --python -o bazel-out/aarch64-opt/bin/tensorflow/lite/python/schema_py_srcs_no_include_all -I ./ -I bazel-out/aarch64-opt/bin -I bazel-out/aarch64-opt/bin --no-includes --no-union-value-namespacing --gen-object-api tensorflow/lite/schema/schema.fbs)\r\n",
      "Execution platform: @local_execution_config_platform//:platform\r\n",
      "bazel-out/host/bin/external/flatbuffers/flatc: /lib/aarch64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.26' not found (required by bazel-out/host/bin/external/flatbuffers/flatc)\r\n",
      "Target //tensorflow/tools/pip_package:build_pip_package failed to build\r\n",
      "\n",
      "issue labels - \n",
      "subtype: ubuntu/linux\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  Compared to TF1.15, do TF2 have some speedup  in CPU inference?\n",
      "issue body -  I find this is not a good Stackoverflow question.\r\n",
      "\r\n",
      "So I post it here. Thank you for your time.\n",
      "issue labels - \n",
      "TF 2.0\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:performance\n",
      "\n",
      "\n",
      "issue title -  TensorFlow Lite  support multi scale input?\n",
      "issue body -  Did TensorFlow Lite  support multi scale input?\r\n",
      "In same case,i need dynamic input shapes;\r\n",
      "\r\n",
      "for examples:\r\n",
      "the tensorflow lite mode input shape is  1x3x360x640\r\n",
      "\r\n",
      "but I can not input 1x3x720x1080 .\r\n",
      "(without fixed size, FC,all ops are conv).\r\n",
      "caffe ,ncnn,TNN model, i can change the protext  to support multi scale input.\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TFLiteConverter\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  [ROCm] Enabling/Disabling some unit tests (via no_rocm tag) on the ROCm platform\n",
      "issue body -  all commits either add or remove the `no_rocm` tag from unit-tests to disable / enable them...nothing else.\r\n",
      "\r\n",
      "\r\n",
      "-----------------------------------------------\r\n",
      "\r\n",
      "\r\n",
      "/cc @cheshire @chsigg @nvining-work \r\n",
      "\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:gpu\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  Upgrade to CUDNN RNN v8 APIs\n",
      "issue body -  This PR updates the code to use cudnnRNNForward/cudnnRNNBackwardxxx_v8 when the CUDNN Version >= 8.0 and CudnnRNNV3 operation is used.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "fyi. @nluehr \n",
      "issue labels - \n",
      "cla: yes\n",
      "ready to pull\n",
      "size:L\n",
      "\n",
      "\n",
      "issue title -  [CherryPick:r2.4] Pass cc_opt_flags to host_copt.\n",
      "issue body -  \n",
      "issue labels - \n",
      "cla: yes\n",
      "\n",
      "\n",
      "issue title -  tf.keras.applications.densenet.preprocess_input does not work for data_format=\"channels_first\" for symbolic tensors\n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): colab\r\n",
      "- TensorFlow installed from (source or binary): colab\r\n",
      "- TensorFlow version (use command below): 2.4.0, v2.4.0-0-g582c8d236cb\r\n",
      "- Python version: 3.6.9\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "Running `tf.keras.applications.densenet.preprocess_input` on a `data_format=\"channels_first\"` symbolic tensor raises an Exception. This is caused by the input transposition not being applied if the `mode` parameter to [_preprocess_symbolic_input](https://github.com/tensorflow/tensorflow/blob/7a49c87f9f56a7fc169669cfe97728859798967c/tensorflow/python/keras/applications/imagenet_utils.py#L242) is set to \"torch\" (as is the case for densenet preprocessing).\r\n",
      "\r\n",
      "See the relevant lines here:\r\n",
      "https://github.com/tensorflow/tensorflow/blob/7a49c87f9f56a7fc169669cfe97728859798967c/tensorflow/python/keras/applications/imagenet_utils.py#L262-L281\r\n",
      "\r\n",
      "This is not caught by the unit tests as only the default `mode=\"caffe\"` is tested.\r\n",
      "\r\n",
      "The equivalent numpy function `_preprocess_numpy_input` handles this correctly (treating the input differently depending on `data_format` for all modes\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "Preprocessing should work for `data_format=\"channels_first\"`\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "Colab notebook with minimum example:\r\n",
      "https://colab.research.google.com/drive/1THrNYTAAzPxw9135h-sFt-LxMz_7SEeQ?usp=sharing\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:keras\n",
      "stat:awaiting tensorflower\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  libtensorflow-gpu-windows-x86_64-2.3.1 does not support GPUs with compute capability 5.0\n",
      "issue body -  TensorFlow 2.3.1 crashes on my GPU with compute capability (CC) 5.0 with the error \"no kernel image is available for execution on the device\". I used [cuobjdump](https://docs.nvidia.com/cuda/cuda-binary-utilities/index.html#cuobjdump) to inspect tensorflow.dll to see what binary and PTX code was included. I found binary code for CC 3.5, 3.7, 5.2, 6.0, 6.1, and 7.0 as well as PTX code for CC 7.0.\r\n",
      "\r\n",
      "PTX code is forward-compatible, but 7.0 PTX cannot run on a 5.0 GPU. Binary code is forward-compatible but only for minor version updates. This means that neither the 3.7 nor the 5.2 binary can run on a 5.0 GPU. In fact, the current configuration means that TensorFlow 2.3.1 can run on any GPU with CC 3.5 and up *except* for CC 5.0.\r\n",
      "\r\n",
      "I suggest building for CC 5.0 instead of CC 5.2 so that TensorFlow 2.3.X can run on any GPU with CC 3.5 and up.\n",
      "issue labels - \n",
      "TF 2.3\n",
      "comp:gpu\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Example code bug in documentation related to tf.data.Dataset.interleave\n",
      "issue body -  ## URL with the issue:\r\n",
      "https://www.tensorflow.org/guide/data_performance\r\n",
      "\r\n",
      "## Description of issue:\r\n",
      "The following has been tested with TF version v2.4.0-0-g582c8d236cb.\r\n",
      "\r\n",
      "On the above-mentioned documentation page, it is defined:\r\n",
      "\r\n",
      "`def __new__(cls, num_samples=3):`\r\n",
      "\r\n",
      "which is ok up to some point, but results in a bug for the variant using `interleave`, since then internally a tensor is provided as second argument, so that `num_samples` won't be 3. In fact, it alternates between 0 and 1 in `_generator`. One can easily check this out by using the `print` function - both in `__new__` and in `_generator`.\r\n",
      "\r\n",
      "Assuming the person writing this part of the documentation wasn't aware of this and this problem is not version specific, it is very likely that the timing results are wrong accordingly when using `interleave`.\r\n",
      "\r\n",
      "It is also worth mentioning that when using `tf.data.Dataset.range(2).interleave`, the generated data in total is doubled in size (**if fixing the above-mentioned bug**), so that the timing results cannot be compared 1:1. When using `tf.data.Dataset.range(2).interleave`, the timing results should be divided by two to allow for a fair comparison, which isn't mentioned in the documentation.\r\n",
      "\r\n",
      "See my gist [here](https://gist.github.com/padoremu/de948c6133c365bb3249c77b172aa4fd).\n",
      "issue labels - \n",
      "comp:data\n",
      "stat:awaiting tensorflower\n",
      "type:docs-bug\n",
      "\n",
      "\n",
      "issue title -  AssertionError: Nesting violated for default stack of <class 'tensorflow.python.framework.ops.Graph'> objects\n",
      "issue body -  I want to make a prediction(do beam search) after every save of ckpt using Estimator for saving best ckpt. So I add a saving_listeners(CheckpointSaverListener) to estimator.train. But every time I raise a mistake. \r\n",
      "\r\n",
      "**System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macos\r\n",
      "- TensorFlow installed from (source or binary): source\r\n",
      "- TensorFlow version (use command below): 1.15\r\n",
      "- Python version: 3.6\r\n",
      "\r\n",
      "Here is my CheckpointSaverListener code.\r\n",
      "```\r\n",
      "class SaveBestCheckpointSaverListener(tf.train.CheckpointSaverListener):\r\n",
      "    def __init__(self, save_checkpoints_steps, keep_checkpoint_max, output_dir):\r\n",
      "        self.best_ckpt = None\r\n",
      "        self.best_path_match = None\r\n",
      "        self.best_global_step_value = None\r\n",
      "        self.save_checkpoints_steps = save_checkpoints_steps\r\n",
      "        self.keep_checkpoint_max = keep_checkpoint_max\r\n",
      "        self.output_dir = output_dir\r\n",
      "\r\n",
      "    def begin(self):\r\n",
      "        # You can add ops to the graph here.\r\n",
      "        print('Starting the session.')\r\n",
      "        # self.your_tensor = ...\r\n",
      "\r\n",
      "    def before_save(self, session, global_step_value):\r\n",
      "        print('About to write a checkpoint')\r\n",
      "\r\n",
      "    def after_save(self, session, global_step_value):\r\n",
      "        print('Done writing checkpoint at {}.'.format(global_step_value))\r\n",
      "        global_step_value = int(global_step_value)\r\n",
      "        if global_step_value == 0:\r\n",
      "            return\r\n",
      "        current_ckpt = 'model.ckpt-{}'.format(global_step_value)\r\n",
      "       # do beam search prediction\r\n",
      "        current_path_match = do_predict()\r\n",
      "        print('current result: {} : {}'.format(current_ckpt, current_path_match))\r\n",
      "        if not self.best_ckpt:\r\n",
      "            self.best_ckpt = current_ckpt\r\n",
      "            self.best_path_match = current_path_match\r\n",
      "            self.best_global_step_value = str(global_step_value)\r\n",
      "        else:\r\n",
      "            if current_path_match > self.best_path_match:\r\n",
      "                self.best_ckpt = current_ckpt\r\n",
      "                self.best_path_match = current_path_match\r\n",
      "                self.best_global_step_value = str(global_step_value)\r\n",
      "                print('Saved best ckpt with path_match {}'.format(current_path_match))\r\n",
      "\r\n",
      "    def end(self, session, global_step_value):\r\n",
      "        print('best model {}, remove useless models.'.format(self.best_ckpt))\r\n",
      "        # for f in os.listdir(self.output_dir):\r\n",
      "        #     file = os.path.join(self.output_dir, f)\r\n",
      "        #     if os.path.isfile(file) and f.startswith('model'):\r\n",
      "        #         model_global_step = f[f.index('-') + 1:f.rindex('.')]\r\n",
      "        #         if model_global_step != self.best_global_step_value:\r\n",
      "        #             os.remove(file)\r\n",
      "        #             print('remove {}'.format(file))\r\n",
      "\r\n",
      "```\r\n",
      "After all ckpts were saved, training process will report errors. Here are errors.\r\n",
      "```\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"run_classifier_nsp_multitask_typeEmb.py\", line 797, in <module>\r\n",
      "    tf.app.run()\r\n",
      "  File \"/Users/bytedance/opt/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/platform/app.py\", line 40, in run\r\n",
      "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n",
      "  File \"/Users/bytedance/opt/anaconda3/lib/python3.6/site-packages/absl/app.py\", line 303, in run\r\n",
      "    _run_main(main, args)\r\n",
      "  File \"/Users/bytedance/opt/anaconda3/lib/python3.6/site-packages/absl/app.py\", line 251, in _run_main\r\n",
      "    sys.exit(main(argv))\r\n",
      "  File \"run_classifier_nsp_multitask_typeEmb.py\", line 767, in main\r\n",
      "    estimator.train(input_fn=train_input_fn, max_steps=FLAGS.num_train_steps, hooks=hooks, saving_listeners=[listener])\r\n",
      "  File \"/Users/bytedance/opt/anaconda3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 370, in train\r\n",
      "    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n",
      "  File \"/Users/bytedance/opt/anaconda3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1161, in _train_model\r\n",
      "    return self._train_model_default(input_fn, hooks, saving_listeners)\r\n",
      "  File \"/Users/bytedance/opt/anaconda3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1195, in _train_model_default\r\n",
      "    saving_listeners)\r\n",
      "  File \"/Users/bytedance/opt/anaconda3/lib/python3.6/contextlib.py\", line 88, in __exit__\r\n",
      "    next(self.gen)\r\n",
      "  File \"/Users/bytedance/opt/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 5480, in get_controller\r\n",
      "    yield g\r\n",
      "  File \"/Users/bytedance/opt/anaconda3/lib/python3.6/contextlib.py\", line 88, in __exit__\r\n",
      "    next(self.gen)\r\n",
      "  File \"/Users/bytedance/opt/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 5295, in get_controller\r\n",
      "    type(default))\r\n",
      "AssertionError: Nesting violated for default stack of <class 'tensorflow.python.framework.ops.Graph'> objects\r\n",
      "```\r\n",
      "\n",
      "issue labels - \n",
      "TF 1.15\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Segfault upon importing tensorflow\n",
      "issue body -  Linux Mint 20\r\n",
      "Python 3.8.5\r\n",
      "No virtual environment\r\n",
      "i7 870 (old processor)\r\n",
      "pip3 install tensorflow, also fails with tf-nightly\r\n",
      "TensorFlow version 2.4.0\r\n",
      "GTX 1060 3GB\r\n",
      "\r\n",
      "\"import tensorflow\" caues a segfault\n",
      "issue labels - \n",
      "TF 2.4\n",
      "stat:awaiting response\n",
      "subtype: ubuntu/linux\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  TFLite model runs slowly in TV devices when set use NNAPI\n",
      "issue body -  <em>Please make sure that this is an issue related to performance of TensorFlow.\r\n",
      "As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:performance_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n",
      "- Yes, all codes were written by myself. \r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n",
      "- Win10、Android\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "- TV devices (Hisense U7F) with mediatek 9652 SOC\r\n",
      "- TensorFlow installed from (source or binary):\r\n",
      "- Offical\r\n",
      "- TensorFlow version (use command below):\r\n",
      "- TF2.3、TF1.14、TF1.13.1 were tried\r\n",
      "- Python version:\r\n",
      "- 3.7\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version:\r\n",
      "- GPU model and memory:\r\n",
      "\r\n",
      "You can collect some of this information using our environment capture\r\n",
      "[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n",
      "You can also obtain the TensorFlow version with:\r\n",
      "1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n",
      "2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "Aim: Run a MobileNet TFLite model in TV devices with NNAPI in 100ms.\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "I write a MobileNet-v2 custom code and convert it to tflite model. We set the tflite file to Hisense U7F TV devices with mediatek 9652 SOC (containing NNAPI). TV capture the image through camera (android.camera2) and inference with Interpreter but the reference time up to 300ms. It should be mentioned that the mediatek profiler reported that all OPs were run in NNAPI successfully and time consuming in NNAPI was 10ms.\r\n",
      "Besides, we tried different TF version and its java denpendencies including 2.3, 1.14, 1.13.1. \r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "Provide a reproducible test case that is the bare minimum necessary to generate\r\n",
      "the problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n",
      "The tflite model as :\r\n",
      "[TFModel-short.zip](https://github.com/tensorflow/tensorflow/files/5834848/TFModel-short.zip)\r\n",
      "\r\n",
      "The core java code as following:\r\n",
      "```\r\n",
      "                try {\r\n",
      "                    Interpreter.Options options = (new Interpreter.Options()).setAllowFp16PrecisionForFp32(true);\r\n",
      "                    mTFLite = new Interpreter(loadModelFile(mModelname), options);\r\n",
      "                    mTFLite.setUseNNAPI(true);\r\n",
      "                } catch (IOException e) {\r\n",
      "                    e.printStackTrace();\r\n",
      "                }\r\n",
      "\r\n",
      "                float[][][][] firstOutput = new float[1][7][7][34];\r\n",
      "\r\n",
      "                long startTime = System.currentTimeMillis();\r\n",
      "                mTFLite.run(byteBuffer,firstOutput);\r\n",
      "                long endTime = System.currentTimeMillis();\r\n",
      "                inferenceTime = endTime - startTime;\r\n",
      "\r\n",
      "                System.out.println(\"Inference Time: \"+inferenceTime+\"ms\");\r\n",
      "                mTFLite.close();\r\n",
      "```\r\n",
      "\r\n",
      "**Other info / logs** Include any logs or source code that would be helpful to\r\n",
      "diagnose the problem. If including tracebacks, please include the full\r\n",
      "traceback. Large logs and files should be attached.\r\n",
      "The android Studio log as following:\r\n",
      "\r\n",
      "> 2021-01-19 15:48:50.041 15661-16292/com.example.android_camera2 I/MtkExecutionBuilder: ****************** Profiler Start ******************\r\n",
      "> 2021-01-19 15:48:50.041 15661-16292/com.example.android_camera2 I/MtkExecutionBuilder: Execution Step   : 0\r\n",
      "> 2021-01-19 15:48:50.041 15661-16292/com.example.android_camera2 I/MtkExecutionBuilder: Execution Result : 1\r\n",
      "> 2021-01-19 15:48:50.041 15661-16292/com.example.android_camera2 I/MtkExecutionBuilder: Device Name      : neuron-ann\r\n",
      "> 2021-01-19 15:48:50.041 15661-16292/com.example.android_camera2 I/MtkExecutionBuilder: Operations       : CONV:CONV:DEPTHWISE_CONV:CONV:CONV:DEPTHWISE_CONV:CONV:ADD:CONV:DEPTHWISE_CONV:CONV:CONV:DEPTHWISE_CONV:CONV:ADD:CONV:DEPTHWISE_CONV:CONV:ADD:CONV:DEPTHWISE_CONV:CONV:CONV:DEPTHWISE_CONV:CONV:ADD:CONV:DEPTHWISE_CONV:CONV:ADD:CONV:DEPTHWISE_CONV:CONV:ADD:CONV:DEPTHWISE_CONV:CONV:CONV:DEPTHWISE_CONV:CONV:ADD:CONV:DEPTHWISE_CONV:CONV:ADD:CONV:DEPTHWISE_CONV:CONV:CONV:DEPTHWISE_CONV:CONV:ADD:CONV:DEPTHWISE_CONV:CONV:ADD:CONV:DEPTHWISE_CONV:CONV:CONV:DEPTHWISE_CONV:CONV:CONV\r\n",
      "> 2021-01-19 15:48:50.041 15661-16292/com.example.android_camera2 I/MtkExecutionBuilder: Spent Time       : 13747 us\r\n",
      "> 2021-01-19 15:48:50.041 15661-16292/com.example.android_camera2 I/MtkExecutionBuilder: ****************** Profiler End   ******************\r\n",
      "> 2021-01-19 15:48:50.043 15661-15699/com.example.android_camera2 I/System.out: Inference Time: 319ms\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.3\n",
      "TFLiteNNAPIDelegate\n",
      "comp:lite\n",
      "type:performance\n",
      "\n",
      "\n",
      "issue title -  Should pb file match CUDA version?\n",
      "issue body -  \r\n",
      "I used tensorflow.git and bazel to build a model. The detail process is\r\n",
      "\r\n",
      "1. git clone tensorflow.git\r\n",
      "2. ./configure << tensorflow configure\r\n",
      "3. I answered N in every question of configure process including 'Use CUDA'.\r\n",
      "4. bazel-build my model to pb file\r\n",
      "\r\n",
      "Then, can I use my model in any device that set CPU or GPU(any CUDA, Cudnn version)?\r\n",
      "\r\n",
      "When I run my pb file, it occurs to fail to make cuFFT batched plan.\r\n",
      "\r\n",
      "As I searched, it was because CUDA version doesn't match, is it right?\n",
      "issue labels - \n",
      "stalled\n",
      "stat:awaiting response\n",
      "subtype: ubuntu/linux\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  tflite.Interpreter version 2.5.0 not run on widows 10  , please get simple python code example\n",
      "issue body -  <em>Please make sure that this is a bug. As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:bug_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "- TensorFlow installed from (source or binary):\r\n",
      "- TensorFlow version (use command below):\r\n",
      "- Python version:\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version:\r\n",
      "- GPU model and memory:\r\n",
      "\r\n",
      "You can collect some of this information using our environment capture\r\n",
      "[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n",
      "You can also obtain the TensorFlow version with:\r\n",
      "1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n",
      "2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n",
      "\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "Provide a reproducible test case that is the bare minimum necessary to generate\r\n",
      "the problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n",
      "\r\n",
      "**Other info / logs** Include any logs or source code that would be helpful to\r\n",
      "diagnose the problem. If including tracebacks, please include the full\r\n",
      "traceback. Large logs and files should be attached.\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.5\n",
      "comp:lite\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  RecvAsync is cancelled error on Ryzen High Performance mode\n",
      "issue body -  <em>Please make sure that this is a bug. As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:bug_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): \r\n",
      ">No\r\n",
      "\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n",
      "> Windows 10\r\n",
      "\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      ">No\r\n",
      "\r\n",
      "- TensorFlow installed from (source or binary):\r\n",
      ">binary\r\n",
      "\r\n",
      "- TensorFlow version (use command below):\r\n",
      ">2.4.0\r\n",
      "\r\n",
      "- Python version:\r\n",
      ">3.8.7\r\n",
      "\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "N/A\r\n",
      "\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "N/A\r\n",
      "\r\n",
      "- CUDA/cuDNN version:\r\n",
      ">V11.0.221\r\n",
      "\r\n",
      "- GPU model and memory:\r\n",
      ">RTX 3070 FE (VRAM - 8GB, system memory- 32GB)\r\n",
      "\r\n",
      "You can collect some of this information using our environment capture\r\n",
      "[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n",
      "You can also obtain the TensorFlow version with:\r\n",
      "1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n",
      "2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n",
      ">v2.4.0-rc4-71-g582c8d236cb 2.4.0\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "\r\n",
      ">I am trying to run the sample code [here](https://www.tensorflow.org/tutorials/text/text_classification_rnn). \r\n",
      "1. If the computer is on Ryzen High Performance mode, while executing the statement \"model.fit()\", the code breaks with an error \"RecvAsync is cancelled\" after 2 epochs.  (attached is the screenshot of the error)\r\n",
      "error- \r\n",
      "CancelledError:  [_Derived_]RecvAsync is cancelled.\r\n",
      "\t [[{{node gradient_tape/sequential/embedding/embedding_lookup/Reshape/_54}}]] [Op:__inference_train_function_22374]\r\n",
      "\r\n",
      "Function call stack:\r\n",
      "train_function\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "2. If the computer is on Power saving mode, the model trains successfully without an error. \r\n",
      "In both, the above cases the CPU, RAM and GPU utilization was below 20%\r\n",
      " \r\n",
      "System specifications - \r\n",
      "Ryzen 3700X | 32GB 3000MHz CL15 DDR4 ram | RTX 3070 | Windows 10\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "The code should run successfully regardless of the Power option selected. Especially in Ryzen High Performance mode, the code should run faster and without any issues. \r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "Provide a reproducible test case that is the bare minimum necessary to generate\r\n",
      "the problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n",
      "\r\n",
      "I am trying to run the code from the tensorflow website - \r\n",
      "https://www.tensorflow.org/tutorials/text/text_classification_rnn\r\n",
      "\r\n",
      "\r\n",
      "**Other info / logs** Include any logs or source code that would be helpful to\r\n",
      "diagnose the problem. If including tracebacks, please include the full\r\n",
      "traceback. Large logs and files should be attached.\r\n",
      "\r\n",
      "\r\n",
      "![system_sw_info](https://user-images.githubusercontent.com/25884862/104998987-20db6780-5a52-11eb-835e-5f25b8a97cdc.png)\r\n",
      "![tensorflow_error](https://user-images.githubusercontent.com/25884862/105003986-5a63a100-5a59-11eb-93b6-fc6203bdc681.png)\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:keras\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Create config.yml\n",
      "issue body -  Created this file for the New Template to be reflected\n",
      "issue labels - \n",
      "cla: yes\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Tflite own trained model is not running\n",
      "issue body -   I am using tflite object detection android sample the project is running perfectly with default model but when i change my custom object trained model and build on phone then application crushed. Is there are some steps to change model ? I just imported my tflite model in asset folder and changed name in DetectorActivity.java and DetectorTest.java. Is there any more step i need to do please let me know.\r\n",
      " \r\n",
      " Thank you\n",
      "issue labels - \n",
      "comp:lite\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Flow\n",
      "issue body -  https://github.com/tensorflow/tensorflow/blob/582c8d236cb079023657287c318ff26adb239002/tensorflow/python/data/experimental/ops/data_service_ops.py#L301-L483\n",
      "issue labels - \n",
      "invalid\n",
      "stat:awaiting response\n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  BatchNormalization inference equation in doc is incorrect\n",
      "issue body -  \r\n",
      "## URL(s) with the issue:\r\n",
      "\r\n",
      "https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization\r\n",
      "\r\n",
      "## Description of issue (what needs changing):\r\n",
      "\r\n",
      "This documentation states the following:\r\n",
      "\r\n",
      "> During inference (i.e. when using evaluate() or predict() or when calling the layer/model with the argument training=False (which is the default), the layer normalizes its output using a moving average of the mean and standard deviation of the batches it has seen during training. That is to say, it returns (batch - self.moving_mean) / (self.moving_var + epsilon) * gamma + beta.\r\n",
      "\r\n",
      "This equation is incorrect, testing with the source code shows it gives the wrong output.  The correct equation that gives the right output (and matches the correct equation from literature) is:\r\n",
      "\r\n",
      "`(batch - self.moving_mean) / sqrt(self.moving_var + epsilon) * gamma + beta.`\r\n",
      "\r\n",
      "I.e., it is missing square-root.\r\n",
      "\r\n",
      "Further, for more clarity (to avoid confusion for some) it would be better to write it as:\r\n",
      "\r\n",
      "`gamma*(batch - self.moving_mean) / sqrt(self.moving_var + epsilon) + beta.`\r\n",
      "\n",
      "issue labels - \n",
      "comp:keras\n",
      "type:docs-bug\n",
      "\n",
      "\n",
      "issue title -  partially convert_variables_to_constants use variable_names_whitelist/variable_names_blacklist will make the save/restore op disappeared and can't restore after the conversion\n",
      "issue body -  <em>Please make sure that this is a bug. As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:bug_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux CentOS 7\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not use Mobile device\r\n",
      "- TensorFlow installed from (source or binary): binary\r\n",
      "- TensorFlow version (use command below): 2.3.0\r\n",
      "- Python version: 3.7\r\n",
      "- Bazel version (if compiling from source): Not use Bazel\r\n",
      "- GCC/Compiler version (if compiling from source): GCC7.4\r\n",
      "- CUDA/cuDNN version: Not use\r\n",
      "- GPU model and memory: Not use GPU\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "code is \r\n",
      "'''\r\n",
      "\r\n",
      "builder = tf.compat.v1.saved_model.Builder('converted')\r\n",
      "\r\n",
      "with tf.compat.v1.Session(graph=tf.Graph()) as sess:\r\n",
      "\r\n",
      "    loaded = tf.compat.v1.saved_model.loader.load(sess, [\"serve\"], model_dir)\r\n",
      "\r\n",
      "    builder.add_meta_graph_and_variables(sess,[\"serve\"], signature_def_map=loaded.signature_def)\r\n",
      "\r\n",
      "    graph_def = convert_variables_to_constants(\r\n",
      "\r\n",
      "        sess,\r\n",
      "\r\n",
      "        graph_def,\r\n",
      "\r\n",
      "        output_node_names=output_node_list,\r\n",
      "\r\n",
      "        variable_names_blacklist=embedding_var_list,\r\n",
      "\r\n",
      "       #variable_names_whitelist=freeze_var_list)\r\n",
      "\r\n",
      "with tf.compat.v1.Session(graph=tf.Graph()) as sess:\r\n",
      "\r\n",
      "    tf.import_graph_def(graph_def, name=\"\")\r\n",
      "\r\n",
      "    output_tensors = prepare_output_tensors(sess)\r\n",
      "\r\n",
      "    input_tensors = prepare_input_tensors(sess)\r\n",
      "\r\n",
      "    sigs = {}\r\n",
      "\r\n",
      "    sigs['eval'] = tf.compat.v1.saved_model.signature_def_utils.predict_signature_def(output_tensors, input_tensors)\r\n",
      "\r\n",
      "    builder.add_meta_graph(['eval'], signature_def_map=sigs)\r\n",
      "'''\r\n",
      "the reason to use variable_name_blacklist is because the embedding variables is larger than the limit 2GB to freeze into the inference proto buffer, so i want to partially keep them as variables and freeze other nodes, then optimize for performance improvement. \r\n",
      "but after i get the graph_def and add the meta graph to builder, i got a new saved model, when i loading it:\r\n",
      "'''\r\n",
      "with tf.compat.v1.Session(graph=tf.Graph()) as sess:\r\n",
      "\r\n",
      "    model = tf.compat.v1.saved_model.loader.load(sess, [\"eval\"], model_dir)\r\n",
      "\r\n",
      "'''\r\n",
      "it comes out :\r\n",
      "INFO: tensorflow : Saver not created because there are no variables in the graph to restore.\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "i also check the graph def after convert_variables_to_constants , i found the save/resore nodes all disappeared including the kept variables.\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.3\n",
      "comp:apis\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  dso_loader.cc Could not load dynamic library 'cupti64_110.dll'; dlerror: cupti64_110.dll not found\n",
      "issue body -  **System information**\r\n",
      "- Windows 10 - 20H2 Build 19042.746\r\n",
      "- TensorFlow installed from (source or binary): Nightly\r\n",
      "- TensorFlow version: \r\n",
      "- tensorboard==2.4.1\r\n",
      "tensorboard-plugin-wit==1.7.0\r\n",
      "tensorflow-estimator==2.4.0\r\n",
      "tensorflow-gpu==2.4.0\r\n",
      "tensorflow-hub==0.11.0\r\n",
      "tf-estimator-nightly==2.5.0.dev2021011401\r\n",
      "tf-nightly-gpu==2.5.0.dev20210114\r\n",
      "\r\n",
      "- Python version: 3.8.5\r\n",
      "- Installed using virtualenv? conda - with pip installs on top\r\n",
      "- Bazel version (if compiling from source): NA\r\n",
      "- GCC/Compiler version (if compiling from source): NA\r\n",
      "- CUDA/cuDNN version: 11.1/8.0.5.39 for CUDA 11.1\r\n",
      "- GPU model and memory: RTX 3090 - 24GB\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "Running through a search for the best hyper params. This means I am looping through these actions in the same process - creating ANN(LSTM), compiling , training, validating an ANN, then changing the hypers and running again etc. This used to work on TF2.1 with my Super 2080, now I have changed to TF 2.5 as it supports the 3090. The system crashes on this line with no traceback, and no exception thrown.\r\n",
      "\r\n",
      "training_history = self.model.fit (ann_training_input_batched_scaled_3d, training_output,\r\n",
      "                                               epochs = regressor_number_epochs, batch_size = regressor_batch_size,\r\n",
      "                                               callbacks = callbks,\r\n",
      "                                               verbose=1,\r\n",
      "                                               validation_data = (ann_validation_input_batched_scaled_3d, valoutput))\r\n",
      "\r\n",
      "This is the second iteration, so the second time this gets executed (but its with a new ANN so its the the first time with a new ANN, second time in the process). Note that if I use a CPU instead of a GPU I get the same errors about CUPTI, but it doesnt crash so quickly, it dos a few more iterations then crashes with no exception/logging etc.\r\n",
      "\r\n",
      "Before it dies, I see these warnings and finally errors\r\n",
      "\r\n",
      "ensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cupti64_110.dll'; dlerror: cupti64_110.dll not found\r\n",
      "2021-01-18 17:40:51.410137: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cupti.dll'; dlerror: cupti.dll not found\r\n",
      "2021-01-18 17:40:51.410283: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1644] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.\r\n",
      "2021-01-18 17:40:51.410657: I tensorflow/core/profiler/lib/profiler_session.cc:158] Profiler session tear down.\r\n",
      "2021-01-18 17:40:51.410812: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1735] function cupti_interface_->Finalize()failed with error CUPTI could not be loaded or symbol could not be found.\r\n",
      "\r\n",
      "\r\n",
      "**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n",
      "\r\n",
      "\r\n",
      "**Any other info / logs**\r\n",
      "I dont understand how the process is dying, and I have the offending code wrapped in a try block, but no exception is being thrown. \r\n",
      "\r\n",
      "Also how do I resolve this dll CUPTI warning ? It should be using cupti64_2020.2.1.dll. It works for the first iteration.\r\n",
      "\r\n",
      "Marcus\n",
      "issue labels - \n",
      "TF 2.5\n",
      "comp:gpu\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Use flattened tflite namespace for lite/micro/kernels/cast.cc; clean-up\n",
      "issue body -  PR4 for issue #45608: Use flattened tflite namespace for CAST; clean-up the test code.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:lite\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  TensorFlow 2.4.0 core dumped issue on Ubuntu 16.04\n",
      "issue body -  **System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04 LTS\r\n",
      "- TensorFlow installed from (source or binary): installed from pip, with this version: tensorflow-2.4.0-cp36-cp36m-manylinux2010_x86_64.whl (394.7 MB)\r\n",
      "- TensorFlow version: 2.4.0\r\n",
      "- Python version: 3.6.8\r\n",
      "- Installed using virtualenv? pip? conda?: virtualenv\r\n",
      "- CUDA/cuDNN version:\r\n",
      "- GPU model and memory: No GPU\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "After installation exactly following the official guideline at: https://www.tensorflow.org/install/pip\r\n",
      "\r\n",
      "(venv) $ python -c \"import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"\r\n",
      "Illegal instruction (core dumped)\r\n",
      "\r\n",
      "**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n",
      "Exactly the same as the guideline listed above, namely\r\n",
      "$ python3 -m venv --system-site-packages ./venv\r\n",
      "$ source ./venv/bin/activate  # sh, bash, or zsh\r\n",
      "(venv) $ pip install --upgrade pip\r\n",
      "(venv) $ pip install --upgrade tensorflow\r\n",
      "\r\n",
      "**Any other info / logs**\r\n",
      "I also did the installations with tensorflow==1.15 and tf-nightly. Both are working fine without the \"core dumped\" issue, however, I would really like to use the stable TensorFlow version 2.4.0.\r\n",
      "\r\n",
      "Thank you very much for your time!\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "stat:awaiting response\n",
      "subtype: ubuntu/linux\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  Apple M1 `MLC` ops not supported (e.g., tf.MLCConv2D)\n",
      "issue body -  **System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n",
      "\r\n",
      "macOS 11.1 (Big Sur) running on a MacBook Pro (13-inch, M1, 2020)\r\n",
      "\r\n",
      "- TensorFlow installed from (source or binary):\r\n",
      "\r\n",
      "Binary, from https://github.com/apple/tensorflow_macos\r\n",
      "\r\n",
      "- TensorFlow version (or github SHA if from source):\r\n",
      "\r\n",
      "'2.4.0-rc0'\r\n",
      "\r\n",
      "**Command used to run the converter or code if you’re using the Python API**\r\n",
      "If possible, please share a link to Colab/Jupyter/any notebook.\r\n",
      "\r\n",
      "```\r\n",
      "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n",
      "    tflite_model = converter.convert()\r\n",
      "```\r\n",
      "\r\n",
      "**The output from the converter invocation**\r\n",
      "\r\n",
      "```\r\n",
      "/python/framework/func_graph.py:590:0: error: 'tf.MLCConv2D' op is neither a custom op nor a flex op\r\n",
      "```\r\n",
      "\r\n",
      "**Also, please include a link to the saved model or GraphDef**\r\n",
      "\r\n",
      "```\r\n",
      "There's a similar issue here:\r\n",
      "https://github.com/apple/tensorflow_macos/issues/133\r\n",
      "```\r\n",
      "\r\n",
      "**Failure details**\r\n",
      "Conversion fails, since tf.MLCConv2D is not the same as tf.Conv2D.  The same is true for other operations like tf.MLCMatMul.\r\n",
      "\r\n",
      "**Any other info / logs**\r\n",
      "What's the right path forward here for using Lite, and then Lite Micro on an M1 Mac?  Should conversion from MLC be supported?  I attempted to work around this using `TF_DISABLE_MLC_EAGER` as mentioned at the very bottom of https://github.com/apple/tensorflow_macos but did not understand what syntax I should be using.\n",
      "issue labels - \n",
      "TF 2.4\n",
      "TFLiteConverter\n",
      "comp:lite\n",
      "stat:awaiting tensorflower\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Unable to import tensorflow in python3 after wheels install / Python3.7 / RPi 3B+ Raspbian aarch64\n",
      "issue body -  \r\n",
      "### System information\r\n",
      "\r\n",
      "-   **Have I written custom code (as opposed to using a stock example script\r\n",
      "    provided in TensorFlow)**: NO\r\n",
      "-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux raspberrypi 5.10.5-v8+ #1392 SMP PREEMPT Sat Jan 9 18:56:30 GMT 2021 aarch64 GNU/Linux\r\n",
      "-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n",
      "    happens on a mobile device**: Raspberry Pi 3 B+\r\n",
      "-   **TensorFlow installed from (source or binary)**: source\r\n",
      "-   **TensorFlow version (use command below)**: 2.4.0\r\n",
      "-   **Python version**: 3.7\r\n",
      "-   **Bazel version (if compiling from source)**:\r\n",
      "-   **GCC/Compiler version (if compiling from source)**: GCC 8.3.0\r\n",
      "-   **CUDA/cuDNN version**:\r\n",
      "-   **GPU model and memory**:\r\n",
      "-   **Exact command to reproduce**: \r\n",
      "        sudo -H pip3 install tensorflow-2.4.0-cp37-cp37m-linux_aarch64.whl\r\n",
      "        python3 -c \"import tensorflow as tf\"\r\n",
      "\r\n",
      "\r\n",
      "### Describe the problem\r\n",
      "unable to import tensorflow module from python3 (3.7) :\r\n",
      "pi@raspberrypi:~ $ python3 -c \"import tensorflow as tf\"\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"<string>\", line 1, in <module>\r\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/__init__.py\", line 41, in <module>\r\n",
      "    from tensorflow.python.tools import module_util as _module_util\r\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/__init__.py\", line 41, in <module>\r\n",
      "    from tensorflow.python.eager import context\r\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/context.py\", line 32, in <module>\r\n",
      "    from tensorflow.core.framework import function_pb2\r\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/core/framework/function_pb2.py\", line 16, in <module>\r\n",
      "    from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2\r\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/core/framework/attr_value_pb2.py\", line 16, in <module>\r\n",
      "    from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2\r\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/core/framework/tensor_pb2.py\", line 16, in <module>\r\n",
      "    from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2\r\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/core/framework/resource_handle_pb2.py\", line 16, in <module>\r\n",
      "    from tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\r\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/core/framework/tensor_shape_pb2.py\", line 23, in <module>\r\n",
      "    serialized_pb=_b('\\n,tensorflow/core/framework/tensor_shape.proto\\x12\\ntensorflow\\\"z\\n\\x10TensorShapeProto\\x12-\\n\\x03\\x64im\\x18\\x02 \\x03(\\x0b\\x32 .tensorflow.TensorShapeProto.Dim\\x12\\x14\\n\\x0cunknown_rank\\x18\\x03 \\x01(\\x08\\x1a!\\n\\x03\\x44im\\x12\\x0c\\n\\x04size\\x18\\x01 \\x01(\\x03\\x12\\x0c\\n\\x04name\\x18\\x02 \\x01(\\tB\\x87\\x01\\n\\x18org.tensorflow.frameworkB\\x11TensorShapeProtosP\\x01ZSgithub.com/tensorflow/tensorflow/tensorflow/go/core/framework/tensor_shape_go_proto\\xf8\\x01\\x01\\x62\\x06proto3')\r\n",
      "TypeError: __init__() got an unexpected keyword argument 'serialized_options'\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "### Source code / logs\r\n",
      "\r\n",
      "Here is the output tf_env.txt generated by tf_env_collect.sh :\r\n",
      "\r\n",
      "== check python ===================================================\r\n",
      "python version: 3.7.3\r\n",
      "python branch: \r\n",
      "python build version: ('default', 'Jul 25 2020 13:03:44')\r\n",
      "python compiler version: GCC 8.3.0\r\n",
      "python implementation: CPython\r\n",
      "\r\n",
      "\r\n",
      "== check os platform ===============================================\r\n",
      "os: Linux\r\n",
      "os kernel version: #1392 SMP PREEMPT Sat Jan 9 18:56:30 GMT 2021\r\n",
      "os release version: 5.10.5-v8+\r\n",
      "os platform: Linux-5.10.5-v8+-aarch64-with-debian-10.7\r\n",
      "linux distribution: ('debian', '10.7', '')\r\n",
      "linux os distribution: ('debian', '10.7', '')\r\n",
      "mac version: ('', ('', '', ''), '')\r\n",
      "uname: uname_result(system='Linux', node='raspberrypi', release='5.10.5-v8+', version='#1392 SMP PREEMPT Sat Jan 9 18:56:30 GMT 2021', machine='aarch64', processor='')\r\n",
      "architecture: ('64bit', 'ELF')\r\n",
      "machine: aarch64\r\n",
      "\r\n",
      "\r\n",
      "== are we in docker =============================================\r\n",
      "No\r\n",
      "\r\n",
      "== compiler =====================================================\r\n",
      "c++ (Debian 8.3.0-6) 8.3.0\r\n",
      "Copyright (C) 2018 Free Software Foundation, Inc.\r\n",
      "This is free software; see the source for copying conditions.  There is NO\r\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n",
      "\r\n",
      "\r\n",
      "== check pips ===================================================\r\n",
      "numpy                  1.19.5\r\n",
      "protobuf               3.0.0\r\n",
      "tensorflow             2.4.0\r\n",
      "tensorflow-estimator   2.4.0\r\n",
      "\r\n",
      "== check for virtualenv =========================================\r\n",
      "False\r\n",
      "\r\n",
      "== tensorflow import ============================================\r\n",
      "      7074:\tfind library=libcrypt.so.1 [0]; searching\r\n",
      "      7074:\t search cache=/etc/ld.so.cache\r\n",
      "      7074:\t  trying file=/lib/aarch64-linux-gnu/libcrypt.so.1\r\n",
      "      7074:\t\r\n",
      "      7074:\tfind library=libpthread.so.0 [0]; searching\r\n",
      "      7074:\t search cache=/etc/ld.so.cache\r\n",
      "      7074:\t  trying file=/lib/aarch64-linux-gnu/libpthread.so.0\r\n",
      "      7074:\t\r\n",
      "      7074:\tfind library=libdl.so.2 [0]; searching\r\n",
      "      7074:\t search cache=/etc/ld.so.cache\r\n",
      "      7074:\t  trying file=/lib/aarch64-linux-gnu/libdl.so.2\r\n",
      "      7074:\t\r\n",
      "      7074:\tfind library=libutil.so.1 [0]; searching\r\n",
      "      7074:\t search cache=/etc/ld.so.cache\r\n",
      "      7074:\t  trying file=/lib/aarch64-linux-gnu/libutil.so.1\r\n",
      "      7074:\t\r\n",
      "      7074:\tfind library=libexpat.so.1 [0]; searching\r\n",
      "      7074:\t search cache=/etc/ld.so.cache\r\n",
      "      7074:\t  trying file=/lib/aarch64-linux-gnu/libexpat.so.1\r\n",
      "      7074:\t\r\n",
      "      7074:\tfind library=libz.so.1 [0]; searching\r\n",
      "      7074:\t search cache=/etc/ld.so.cache\r\n",
      "      7074:\t  trying file=/lib/aarch64-linux-gnu/libz.so.1\r\n",
      "      7074:\t\r\n",
      "      7074:\tfind library=libm.so.6 [0]; searching\r\n",
      "      7074:\t search cache=/etc/ld.so.cache\r\n",
      "      7074:\t  trying file=/lib/aarch64-linux-gnu/libm.so.6\r\n",
      "      7074:\t\r\n",
      "      7074:\tfind library=libc.so.6 [0]; searching\r\n",
      "      7074:\t search cache=/etc/ld.so.cache\r\n",
      "      7074:\t  trying file=/lib/aarch64-linux-gnu/libc.so.6\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling init: /lib/aarch64-linux-gnu/libpthread.so.0\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling init: /lib/aarch64-linux-gnu/libc.so.6\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling init: /lib/aarch64-linux-gnu/libm.so.6\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling init: /lib/aarch64-linux-gnu/libz.so.1\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling init: /lib/aarch64-linux-gnu/libexpat.so.1\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling init: /lib/aarch64-linux-gnu/libutil.so.1\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling init: /lib/aarch64-linux-gnu/libdl.so.2\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling init: /lib/aarch64-linux-gnu/libcrypt.so.1\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tinitialize program: python3\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\ttransferring control: python3\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling init: /usr/lib/python3.7/lib-dynload/_opcode.cpython-37m-aarch64-linux-gnu.so\r\n",
      "      7074:\t\r\n",
      "      7074:\tfind library=libffi.so.6 [0]; searching\r\n",
      "      7074:\t search cache=/etc/ld.so.cache\r\n",
      "      7074:\t  trying file=/usr/lib/aarch64-linux-gnu/libffi.so.6\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling init: /usr/lib/aarch64-linux-gnu/libffi.so.6\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling init: /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-aarch64-linux-gnu.so\r\n",
      "      7074:\t\r\n",
      "      7074:\tfind library=libstdc++.so.6 [0]; searching\r\n",
      "      7074:\t search path=/usr/local/lib/python3.7/dist-packages/tensorflow/python/tls/aarch64:/usr/local/lib/python3.7/dist-packages/tensorflow/python/tls:/usr/local/lib/python3.7/dist-packages/tensorflow/python/aarch64:/usr/local/lib/python3.7/dist-packages/tensorflow/python:/usr/local/lib/python3.7/dist-packages/tensorflow/python/../tls/aarch64:/usr/local/lib/python3.7/dist-packages/tensorflow/python/../tls:/usr/local/lib/python3.7/dist-packages/tensorflow/python/../aarch64:/usr/local/lib/python3.7/dist-packages/tensorflow/python/..\t\t(RUNPATH from file /usr/local/lib/python3.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\r\n",
      "      7074:\t  trying file=/usr/local/lib/python3.7/dist-packages/tensorflow/python/tls/aarch64/libstdc++.so.6\r\n",
      "      7074:\t  trying file=/usr/local/lib/python3.7/dist-packages/tensorflow/python/tls/libstdc++.so.6\r\n",
      "      7074:\t  trying file=/usr/local/lib/python3.7/dist-packages/tensorflow/python/aarch64/libstdc++.so.6\r\n",
      "      7074:\t  trying file=/usr/local/lib/python3.7/dist-packages/tensorflow/python/libstdc++.so.6\r\n",
      "      7074:\t  trying file=/usr/local/lib/python3.7/dist-packages/tensorflow/python/../tls/aarch64/libstdc++.so.6\r\n",
      "      7074:\t  trying file=/usr/local/lib/python3.7/dist-packages/tensorflow/python/../tls/libstdc++.so.6\r\n",
      "      7074:\t  trying file=/usr/local/lib/python3.7/dist-packages/tensorflow/python/../aarch64/libstdc++.so.6\r\n",
      "      7074:\t  trying file=/usr/local/lib/python3.7/dist-packages/tensorflow/python/../libstdc++.so.6\r\n",
      "      7074:\t search cache=/etc/ld.so.cache\r\n",
      "      7074:\t  trying file=/usr/lib/aarch64-linux-gnu/libstdc++.so.6\r\n",
      "      7074:\t\r\n",
      "      7074:\tfind library=librt.so.1 [0]; searching\r\n",
      "      7074:\t search path=/usr/local/lib/python3.7/dist-packages/tensorflow/python:/usr/local/lib/python3.7/dist-packages/tensorflow/python/..\t\t(RUNPATH from file /usr/local/lib/python3.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\r\n",
      "      7074:\t  trying file=/usr/local/lib/python3.7/dist-packages/tensorflow/python/librt.so.1\r\n",
      "      7074:\t  trying file=/usr/local/lib/python3.7/dist-packages/tensorflow/python/../librt.so.1\r\n",
      "      7074:\t search cache=/etc/ld.so.cache\r\n",
      "      7074:\t  trying file=/lib/aarch64-linux-gnu/librt.so.1\r\n",
      "      7074:\t\r\n",
      "      7074:\tfind library=libgcc_s.so.1 [0]; searching\r\n",
      "      7074:\t search path=/usr/local/lib/python3.7/dist-packages/tensorflow/python:/usr/local/lib/python3.7/dist-packages/tensorflow/python/..\t\t(RUNPATH from file /usr/local/lib/python3.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\r\n",
      "      7074:\t  trying file=/usr/local/lib/python3.7/dist-packages/tensorflow/python/libgcc_s.so.1\r\n",
      "      7074:\t  trying file=/usr/local/lib/python3.7/dist-packages/tensorflow/python/../libgcc_s.so.1\r\n",
      "      7074:\t search cache=/etc/ld.so.cache\r\n",
      "      7074:\t  trying file=/lib/aarch64-linux-gnu/libgcc_s.so.1\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling init: /lib/aarch64-linux-gnu/libgcc_s.so.1\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling init: /lib/aarch64-linux-gnu/librt.so.1\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling init: /usr/lib/aarch64-linux-gnu/libstdc++.so.6\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling init: /usr/local/lib/python3.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n",
      "      7074:\t\r\n",
      "      7074:\tfind library=libssl.so.1.1 [0]; searching\r\n",
      "      7074:\t search cache=/etc/ld.so.cache\r\n",
      "      7074:\t  trying file=/usr/lib/aarch64-linux-gnu/libssl.so.1.1\r\n",
      "      7074:\t\r\n",
      "      7074:\tfind library=libcrypto.so.1.1 [0]; searching\r\n",
      "      7074:\t search cache=/etc/ld.so.cache\r\n",
      "      7074:\t  trying file=/usr/lib/aarch64-linux-gnu/libcrypto.so.1.1\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling init: /usr/lib/aarch64-linux-gnu/libcrypto.so.1.1\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling init: /usr/lib/aarch64-linux-gnu/libssl.so.1.1\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling init: /usr/lib/python3.7/lib-dynload/_hashlib.cpython-37m-aarch64-linux-gnu.so\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling init: /usr/lib/python3.7/lib-dynload/termios.cpython-37m-aarch64-linux-gnu.so\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling init: /usr/lib/python3.7/lib-dynload/_csv.cpython-37m-aarch64-linux-gnu.so\r\n",
      "      7074:\t\r\n",
      "      7074:\tfind library=libopenblas.so.0 [0]; searching\r\n",
      "      7074:\t search cache=/etc/ld.so.cache\r\n",
      "      7074:\t  trying file=/usr/lib/aarch64-linux-gnu/libopenblas.so.0\r\n",
      "      7074:\t\r\n",
      "      7074:\tfind library=libgfortran.so.5 [0]; searching\r\n",
      "      7074:\t search cache=/etc/ld.so.cache\r\n",
      "      7074:\t  trying file=/usr/lib/aarch64-linux-gnu/libgfortran.so.5\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling init: /usr/lib/aarch64-linux-gnu/libgfortran.so.5\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling init: /usr/lib/aarch64-linux-gnu/libopenblas.so.0\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling init: /usr/local/lib/python3.7/dist-packages/numpy/core/_multiarray_umath.cpython-37m-aarch64-linux-gnu.so\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling init: /usr/local/lib/python3.7/dist-packages/numpy/core/_multiarray_tests.cpython-37m-aarch64-linux-gnu.so\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling init: /usr/local/lib/python3.7/dist-packages/numpy/linalg/lapack_lite.cpython-37m-aarch64-linux-gnu.so\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling init: /usr/local/lib/python3.7/dist-packages/numpy/linalg/_umath_linalg.cpython-37m-aarch64-linux-gnu.so\r\n",
      "      7074:\t\r\n",
      "      7074:\tfind library=libbz2.so.1.0 [0]; searching\r\n",
      "      7074:\t search cache=/etc/ld.so.cache\r\n",
      "      7074:\t  trying file=/lib/aarch64-linux-gnu/libbz2.so.1.0\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling init: /lib/aarch64-linux-gnu/libbz2.so.1.0\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling init: /usr/lib/python3.7/lib-dynload/_bz2.cpython-37m-aarch64-linux-gnu.so\r\n",
      "      7074:\t\r\n",
      "      7074:\tfind library=liblzma.so.5 [0]; searching\r\n",
      "      7074:\t search cache=/etc/ld.so.cache\r\n",
      "      7074:\t  trying file=/lib/aarch64-linux-gnu/liblzma.so.5\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling init: /lib/aarch64-linux-gnu/liblzma.so.5\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling init: /usr/lib/python3.7/lib-dynload/_lzma.cpython-37m-aarch64-linux-gnu.so\r\n",
      "      7074:\t\r\n",
      "      7074:\tfind library=libmpdec.so.2 [0]; searching\r\n",
      "      7074:\t search cache=/etc/ld.so.cache\r\n",
      "      7074:\t  trying file=/usr/lib/aarch64-linux-gnu/libmpdec.so.2\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling init: /usr/lib/aarch64-linux-gnu/libmpdec.so.2\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling init: /usr/lib/python3.7/lib-dynload/_decimal.cpython-37m-aarch64-linux-gnu.so\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling init: /usr/local/lib/python3.7/dist-packages/numpy/fft/_pocketfft_internal.cpython-37m-aarch64-linux-gnu.so\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling init: /usr/local/lib/python3.7/dist-packages/numpy/random/mtrand.cpython-37m-aarch64-linux-gnu.so\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling init: /usr/local/lib/python3.7/dist-packages/numpy/random/bit_generator.cpython-37m-aarch64-linux-gnu.so\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling init: /usr/local/lib/python3.7/dist-packages/numpy/random/_common.cpython-37m-aarch64-linux-gnu.so\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling init: /usr/local/lib/python3.7/dist-packages/numpy/random/_bounded_integers.cpython-37m-aarch64-linux-gnu.so\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling init: /usr/local/lib/python3.7/dist-packages/numpy/random/_mt19937.cpython-37m-aarch64-linux-gnu.so\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling init: /usr/local/lib/python3.7/dist-packages/numpy/random/_philox.cpython-37m-aarch64-linux-gnu.so\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling init: /usr/local/lib/python3.7/dist-packages/numpy/random/_pcg64.cpython-37m-aarch64-linux-gnu.so\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling init: /usr/local/lib/python3.7/dist-packages/numpy/random/_sfc64.cpython-37m-aarch64-linux-gnu.so\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling init: /usr/local/lib/python3.7/dist-packages/numpy/random/_generator.cpython-37m-aarch64-linux-gnu.so\r\n",
      "      7074:\t\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"<string>\", line 1, in <module>\r\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/__init__.py\", line 41, in <module>\r\n",
      "    from tensorflow.python.tools import module_util as _module_util\r\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/__init__.py\", line 41, in <module>\r\n",
      "    from tensorflow.python.eager import context\r\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/context.py\", line 32, in <module>\r\n",
      "    from tensorflow.core.framework import function_pb2\r\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/core/framework/function_pb2.py\", line 16, in <module>\r\n",
      "    from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2\r\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/core/framework/attr_value_pb2.py\", line 16, in <module>\r\n",
      "    from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2\r\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/core/framework/tensor_pb2.py\", line 16, in <module>\r\n",
      "    from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2\r\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/core/framework/resource_handle_pb2.py\", line 16, in <module>\r\n",
      "    from tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\r\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/core/framework/tensor_shape_pb2.py\", line 23, in <module>\r\n",
      "    serialized_pb=_b('\\n,tensorflow/core/framework/tensor_shape.proto\\x12\\ntensorflow\\\"z\\n\\x10TensorShapeProto\\x12-\\n\\x03\\x64im\\x18\\x02 \\x03(\\x0b\\x32 .tensorflow.TensorShapeProto.Dim\\x12\\x14\\n\\x0cunknown_rank\\x18\\x03 \\x01(\\x08\\x1a!\\n\\x03\\x44im\\x12\\x0c\\n\\x04size\\x18\\x01 \\x01(\\x03\\x12\\x0c\\n\\x04name\\x18\\x02 \\x01(\\tB\\x87\\x01\\n\\x18org.tensorflow.frameworkB\\x11TensorShapeProtosP\\x01ZSgithub.com/tensorflow/tensorflow/tensorflow/go/core/framework/tensor_shape_go_proto\\xf8\\x01\\x01\\x62\\x06proto3')\r\n",
      "TypeError: __init__() got an unexpected keyword argument 'serialized_options'\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling fini: python3 [0]\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling fini: /lib/aarch64-linux-gnu/libcrypt.so.1 [0]\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling fini: /lib/aarch64-linux-gnu/libutil.so.1 [0]\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling fini: /lib/aarch64-linux-gnu/libexpat.so.1 [0]\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling fini: /usr/lib/python3.7/lib-dynload/_opcode.cpython-37m-aarch64-linux-gnu.so [0]\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling fini: /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-aarch64-linux-gnu.so [0]\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling fini: /usr/lib/aarch64-linux-gnu/libffi.so.6 [0]\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling fini: /usr/local/lib/python3.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so [0]\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling fini: /usr/lib/aarch64-linux-gnu/libstdc++.so.6 [0]\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling fini: /lib/aarch64-linux-gnu/librt.so.1 [0]\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling fini: /usr/lib/python3.7/lib-dynload/_hashlib.cpython-37m-aarch64-linux-gnu.so [0]\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling fini: /usr/lib/aarch64-linux-gnu/libssl.so.1.1 [0]\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling fini: /usr/lib/aarch64-linux-gnu/libcrypto.so.1.1 [0]\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling fini: /lib/aarch64-linux-gnu/libdl.so.2 [0]\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling fini: /usr/lib/python3.7/lib-dynload/termios.cpython-37m-aarch64-linux-gnu.so [0]\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling fini: /usr/lib/python3.7/lib-dynload/_csv.cpython-37m-aarch64-linux-gnu.so [0]\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling fini: /usr/local/lib/python3.7/dist-packages/numpy/core/_multiarray_umath.cpython-37m-aarch64-linux-gnu.so [0]\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling fini: /usr/local/lib/python3.7/dist-packages/numpy/core/_multiarray_tests.cpython-37m-aarch64-linux-gnu.so [0]\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling fini: /usr/local/lib/python3.7/dist-packages/numpy/linalg/lapack_lite.cpython-37m-aarch64-linux-gnu.so [0]\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling fini: /usr/local/lib/python3.7/dist-packages/numpy/linalg/_umath_linalg.cpython-37m-aarch64-linux-gnu.so [0]\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling fini: /usr/lib/aarch64-linux-gnu/libopenblas.so.0 [0]\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling fini: /usr/lib/aarch64-linux-gnu/libgfortran.so.5 [0]\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling fini: /lib/aarch64-linux-gnu/libgcc_s.so.1 [0]\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling fini: /lib/aarch64-linux-gnu/libz.so.1 [0]\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling fini: /usr/lib/python3.7/lib-dynload/_bz2.cpython-37m-aarch64-linux-gnu.so [0]\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling fini: /lib/aarch64-linux-gnu/libbz2.so.1.0 [0]\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling fini: /usr/lib/python3.7/lib-dynload/_lzma.cpython-37m-aarch64-linux-gnu.so [0]\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling fini: /lib/aarch64-linux-gnu/liblzma.so.5 [0]\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling fini: /usr/lib/python3.7/lib-dynload/_decimal.cpython-37m-aarch64-linux-gnu.so [0]\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling fini: /usr/lib/aarch64-linux-gnu/libmpdec.so.2 [0]\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling fini: /usr/local/lib/python3.7/dist-packages/numpy/fft/_pocketfft_internal.cpython-37m-aarch64-linux-gnu.so [0]\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling fini: /usr/local/lib/python3.7/dist-packages/numpy/random/mtrand.cpython-37m-aarch64-linux-gnu.so [0]\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling fini: /usr/local/lib/python3.7/dist-packages/numpy/random/bit_generator.cpython-37m-aarch64-linux-gnu.so [0]\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling fini: /usr/local/lib/python3.7/dist-packages/numpy/random/_common.cpython-37m-aarch64-linux-gnu.so [0]\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling fini: /usr/local/lib/python3.7/dist-packages/numpy/random/_bounded_integers.cpython-37m-aarch64-linux-gnu.so [0]\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling fini: /usr/local/lib/python3.7/dist-packages/numpy/random/_mt19937.cpython-37m-aarch64-linux-gnu.so [0]\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling fini: /usr/local/lib/python3.7/dist-packages/numpy/random/_philox.cpython-37m-aarch64-linux-gnu.so [0]\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling fini: /usr/local/lib/python3.7/dist-packages/numpy/random/_pcg64.cpython-37m-aarch64-linux-gnu.so [0]\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling fini: /usr/local/lib/python3.7/dist-packages/numpy/random/_sfc64.cpython-37m-aarch64-linux-gnu.so [0]\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling fini: /usr/local/lib/python3.7/dist-packages/numpy/random/_generator.cpython-37m-aarch64-linux-gnu.so [0]\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling fini: /lib/aarch64-linux-gnu/libm.so.6 [0]\r\n",
      "      7074:\t\r\n",
      "      7074:\t\r\n",
      "      7074:\tcalling fini: /lib/aarch64-linux-gnu/libpthread.so.0 [0]\r\n",
      "      7074:\t\r\n",
      "\r\n",
      "== env ==========================================================\r\n",
      "LD_LIBRARY_PATH is unset\r\n",
      "DYLD_LIBRARY_PATH is unset\r\n",
      "\r\n",
      "== nvidia-smi ===================================================\r\n",
      "./tf_env_collect.sh: line 146: nvidia-smi: command not found\r\n",
      "\r\n",
      "== cuda libs  ===================================================\r\n",
      "\r\n",
      "== tensorflow installed from info ==================\r\n",
      "Name: tensorflow\r\n",
      "Version: 2.4.0\r\n",
      "Summary: TensorFlow is an open source machine learning framework for everyone.\r\n",
      "Home-page: https://www.tensorflow.org/\r\n",
      "Author-email: packages@tensorflow.org\r\n",
      "License: Apache 2.0\r\n",
      "Location: /usr/local/lib/python3.7/dist-packages\r\n",
      "Required-by: \r\n",
      "\r\n",
      "== python version  ==============================================\r\n",
      "(major, minor, micro, releaselevel, serial)\r\n",
      "(2, 7, 16, 'final', 0)\r\n",
      "\r\n",
      "== bazel version  ===============================================\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "stalled\n",
      "stat:awaiting response\n",
      "subtype: raspberry pi\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  TFLM: Fix GCC unsafe pointer conversions\n",
      "issue body -  Cast first to uintptr_t in Ethos-U kernel.\r\n",
      "\r\n",
      "This is fixing: https://github.com/tensorflow/tensorflow/issues/46508\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "prtype:bugfix\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Unsafe conversion from pointer to uint64_t in Ethos-U kernel\n",
      "issue body -  @tensorflow/micro\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n",
      "- TensorFlow installed from (source or binary):\r\n",
      "- Tensorflow version (commit SHA if source):\r\n",
      "- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Ethos-U\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "Example:\r\n",
      "reinterpret_cast<uint64_t>((void*)0x78000000)=0000000078000000\r\n",
      "reinterpret_cast<uint64_t>((void*)0x80000000)=ffffffff80000000\r\n",
      "reinterpret_cast<uint64_t>((void*)0x88000000)=ffffffff88000000\r\n",
      "\r\n",
      "This happens specifically for GCC and prevents using addresses at 0x80000000 or above.\r\n",
      "\r\n",
      "**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "comp:lite\n",
      "comp:micro\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Instead of x2 variable name should be x0\n",
      "issue body -  https://www.tensorflow.org/guide/autodiff\r\n",
      "Instead of x2 variable name should be x0 in last line\r\n",
      "\r\n",
      "x0 = tf.Variable(3.0)\r\n",
      "x1 = tf.Variable(0.0)\r\n",
      "\r\n",
      "with tf.GradientTape() as tape:\r\n",
      "  // Update x1 = x1 + x0.\r\n",
      "  x1.assign_add(x0)\r\n",
      "  // The tape starts recording from x1.\r\n",
      "  y = x1**2   # y = (x1 + x0)**2\r\n",
      "\r\n",
      "// This doesn't work.\r\n",
      "print(tape.gradient(y, x0))   // dy/dx0 = 2*(x1 + x2)[](url)\n",
      "issue labels - \n",
      "stat:awaiting response\n",
      "type:docs-bug\n",
      "\n",
      "\n",
      "issue title -  python3 pip Install Error: No matching distribution found for tensorflow==2.2.0\n",
      "issue body -  I have macOS Big Sur on a Apple Silicon M1 and I'm unable to install Tensorflow in python3.\r\n",
      "I removed xcode python3 and installed brew arm64 python3 (x86 python3 doesn't work as well)\r\n",
      "\r\n",
      "(This is  a follow up of closed https://github.com/tensorflow/tensorflow/issues/39130)\r\n",
      "\r\n",
      "I checked successful 64 bis version \r\n",
      "`python3 -c \"import sys; print(sys.version)\" or python -c \"import struct; print(struct.calcsize('P')*8)\"`\r\n",
      "\r\n",
      "> 3.8.7 (default, Dec 30 2020, 02:09:32) \r\n",
      "> [Clang 12.0.0 (clang-1200.0.32.28)]\r\n",
      "\r\n",
      "\r\n",
      "<img width=\"606\" alt=\"image\" src=\"https://user-images.githubusercontent.com/3314607/104881650-f6aa7c80-5961-11eb-8cbb-6128190a0a3a.png\">\r\n",
      "\r\n",
      "Can this work anyhow, or who knows, how to make this work ?\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "stalled\n",
      "stat:awaiting response\n",
      "subtype:macOS\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  micro: copy operator FLOOR_DIV kernel from lite\n",
      "issue body -  This is a copy with minimal modification of the kernel and test for\r\n",
      "operator FLOOR_DIV from tensorflow/lite/kernels.\r\n",
      "Adaptations to micro and addition to the micro build to follow.\r\n",
      "\r\n",
      "PR step 3 for issue #45657\r\n",
      "\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "prtype:bugfix\n",
      "ready to pull\n",
      "size:L\n",
      "\n",
      "\n",
      "issue title -  Can I use cmdline option `--allow_nudging_weights_to_use_fast_gemm_kernel` via python API?\n",
      "issue body -  **System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n",
      "- TensorFlow installed from (source or binary): source from git\r\n",
      "- TensorFlow version (or github SHA if from source): 2.3.0\r\n",
      "\r\n",
      "\r\n",
      "**Command used to run the converter or code if you’re using the Python API**\r\n",
      "\r\n",
      "```\r\n",
      "// input, output and weight settings above..\r\n",
      "converter = tf.lite.TFLiteConverter.from_session(sess, input_tensors, output_tensors)\r\n",
      "...\r\n",
      "tflite_model = converter.convert()\r\n",
      "```\r\n",
      "\r\n",
      "\r\n",
      "**The output from the converter invocation**\r\n",
      "\r\n",
      "```\r\n",
      "2021-01-05 08:02:27.757106: F tensorflow/lite/toco/graph_transformations/ensure_uint8_weights_safe_for_fast_int8_kernels.cc:190] Bad value for Conv2D_4/filter at index 134, previous bad value at index 132, distance=2, kMinDistanceBetweenBadValues=16. Consider passing --allow_nudging_weights_to_use_fast_gemm_kernel if you don't care about accuracy.\r\n",
      "Fatal Python error: Aborted\r\n",
      "```\r\n",
      "\r\n",
      "**Failure details**\r\n",
      "- We randomly generate Fully Connected weights and convert the tf session to tflite using `toco` - python API. It seems that `toco` doesn't allow a certain type of weights with some zero values in the vicinity to other zero values.\r\n",
      "- The console output says I need to pass `--allow_nudging_weights_to_use_fast_gemm_kernel`, which is only useful for the command line. I cannot find a way to pass it via Python API.\r\n",
      "\r\n",
      "Is there any way that I can pass the `--allow_nudging_weights_to_use_fast_gemm_kernel` option via Python API?\n",
      "issue labels - \n",
      "TF 2.3\n",
      "TFLiteConverter\n",
      "comp:lite\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Sagemaker endpoint serving doesnt work for multiple timestep `and` multiple inputs (mulit input-output LSTM)\n",
      "issue body -  Update: Minimum Reproducible code\r\n",
      "https://colab.research.google.com/drive/1RJ2ou705xfrqjeFGImvMk02lTxfFPZek?usp=sharing\r\n",
      "\r\n",
      "I have a LSTM network that has 3 inputs and 3 outputs(built with [functional api][1] in Tf.keras) , that I am trying to deploy as sagemaker endpoint. I have input shape of (None,10,1) for each input/feature, which means 10 timesteps.(I later concatenate the embeddings, but its irrelevant here) \r\n",
      "\r\n",
      "Everything works fine on training time on sagemaker training jobs as well and training completes and artifacts are made successfully. But at time of invocation, endpoint is not working to predict `1 example, having 10 timesteps with 3 inputs` , I have tried multiple things but cant provide three inputs for prediction(input_1,input_2,input_1).\r\n",
      "\r\n",
      "As I said that each input has 10 timesteps, so have shape (10,1). Endpoint only returns the output if I format my payload as below, but by doing so, it treats each time-step as separate example/instance and return 10 predictions for each output\r\n",
      "\r\n",
      "    {'inputs':{\r\n",
      "               'input_1': [[0], [0], [0], [0], [2], [12], [11], [7], [7], [2]],\r\n",
      "               'input_2': [[0], [0], [0], [0], [30], [21], [2], [15], [27], [30]],\r\n",
      "               'input_3': [[0], [0], [0], [0], [6], [2], [3], [13], [15], [6]]}\r\n",
      "              } # gives len(pred['output_1\"])) == 10\r\n",
      "\r\n",
      "This is expected as it consider this request as 10 examples, but in my case it is one example with 10-timesteps for each feature (1,10,1). So I tried different things from the [documentation][2]. Like using instances.\r\n",
      "\r\n",
      "    {'instances': [\r\n",
      "                    {\r\n",
      "                          'input_1': [[0],[0], [0],[0],[2],[12],[11], [7], [7], [2]],\r\n",
      "                          'input_2': [[0], [0], [0], [0], [30], [21], [2], [15], [27], [30]],\r\n",
      "                          'input_3': [[0], [0], [0], [0], [6], [2], [3], [13], [15], [6]]\r\n",
      "                    }\r\n",
      "                  ]\r\n",
      "    }\r\n",
      "\r\n",
      "But it gives this error.\r\n",
      "\r\n",
      ">  transpose expects a vector of size 4. But input(1) is a vector of\r\n",
      "> size 3\\\\n\\\\t [[{{node transpose_1}}]]\\\\n\\\\t\r\n",
      "> [[functional_1/lstm/PartitionedCall]]\\\\n\\\\t\r\n",
      "> [[StatefulPartitionedCall/StatefulPartitionedCall]]\\\"\\n}\"}\"\r\n",
      "\r\n",
      "Document also gives example and says\r\n",
      "\r\n",
      "> for models with multiple named inputs, just include all the keys in the input dict\r\n",
      "\r\n",
      "but when I use this, I get error saying `Missing 'inputs' or 'instances' key\\\"\\n}\"`\r\n",
      "\r\n",
      "    {'input_1': [[0], [0], [0], [0], [2], [12], [11], [7], [7], [2]],\r\n",
      "     'input_2': [[0], [0], [0], [0], [30], [21], [2], [15], [27], [30]],\r\n",
      "     'input_3': [[0], [0], [0], [0], [6], [2], [3], [13], [15], [6]]}\r\n",
      "\r\n",
      "\r\n",
      "My invocation code is below.\r\n",
      "\r\n",
      "    import boto3\r\n",
      "    import json\r\n",
      "    \r\n",
      "    sm = boto3.client('sagemaker-runtime')    \r\n",
      "    endpoint_name = \"tensorflow--------------------4\"\r\n",
      "    response = sm.invoke_endpoint(EndpointName=endpoint_name, \r\n",
      "                                  Body=json.dumps(payload),\r\n",
      "                                  ContentType='application/json')\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "I am not sure how to solve this issue, looking forward for help\r\n",
      "\r\n",
      "\r\n",
      "  [1]: https://keras.io/guides/functional_api/\r\n",
      "  [2]: https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/deploying_tensorflow_serving.html\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Sagemaker and Ubuntu 20\r\n",
      "- TensorFlow installed from (source or binary): pip\r\n",
      "- TensorFlow version (use command below): 2.3.1\r\n",
      "- Python version: 3.7\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "\r\n",
      "I am not sure about the standalone code as sagemaker endpoint is confidential, and what should I share for this, i would be thankful for the suggestion on this too.\n",
      "issue labels - \n",
      "TF 2.3\n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  [TFLM] Added support for optimized fully connected op for CEVA-DSP BX1 and SP500\n",
      "issue body -  As described in the github issue: https://github.com/tensorflow/tensorflow/issues/45607\r\n",
      "\r\n",
      "We will be porting and adding a considerable number of operations to CEVA-DSP cores.\r\n",
      "Started with Quantize (https://github.com/tensorflow/tensorflow/pull/46226) and this is fully connected, both int8 and float32 are supported.\r\n",
      "\r\n",
      "@advaitjain \n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "comp:micro:ceva\n",
      "ready to pull\n",
      "size:XL\n",
      "stat:awaiting response\n",
      "type:feature\n",
      "\n",
      "\n",
      "issue title -  Illegal instruction (core dumped) with tf <= that 1.15 and cpu with no AVX instructions\n",
      "issue body -  On a Intel Q9659 cpu that does not support AVX instructions, I have a `Illegal instruction (core dumped)`. So I have installed a previous version, but I'm still getting the error.\r\n",
      "\r\n",
      "This Quad core cpu has the following flags\r\n",
      "\r\n",
      "```\r\n",
      "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ht tm pbe syscall nx lm constant_tsc arch_perfmon pebs bts rep_good nopl cpuid aperfmperf pni dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm sse4_1 lahf_lm pti tpr_shadow vnmi flexpriority dtherm\r\n",
      "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ht tm pbe syscall nx lm constant_tsc arch_perfmon pebs bts rep_good nopl cpuid aperfmperf pni dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm sse4_1 lahf_lm pti tpr_shadow vnmi flexpriority dtherm\r\n",
      "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ht tm pbe syscall nx lm constant_tsc arch_perfmon pebs bts rep_good nopl cpuid aperfmperf pni dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm sse4_1 lahf_lm pti tpr_shadow vnmi flexpriority dtherm\r\n",
      "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ht tm pbe syscall nx lm constant_tsc arch_perfmon pebs bts rep_good nopl cpuid aperfmperf pni dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm sse4_1 lahf_lm pti tpr_shadow vnmi flexpriority dtherm\r\n",
      "```\r\n",
      "\r\n",
      "and\r\n",
      "\r\n",
      "```\r\n",
      "loreto@ombromanto:~$ grep flags -m1 /proc/cpuinfo | cut -d \":\" -f 2 | tr '[:upper:]' '[:lower:]' | { read FLAGS; OPT=\"-march=native\"; for flag in $FLAGS; do case \"$flag\" in \"sse4_1\" | \"sse4_2\" | \"ssse3\" | \"fma\" | \"cx16\" | \"popcnt\" | \"avx\" | \"avx2\") OPT+=\" -m$flag\";; esac; done; MODOPT=${OPT//_/\\.}; echo \"$MODOPT\"; }\r\n",
      "-march=native -mssse3 -mcx16 -msse4.1\r\n",
      "```\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "Here are detailed information about this system: \r\n",
      "\r\n",
      "`bash <(curl -s https://raw.githubusercontent.com/tensorflow/tensorflow/master/tools/tf_env_collect.sh) && cat tf_env.txt`\r\n",
      "\r\n",
      "```\r\n",
      "== check python ===================================================\r\n",
      "python version: 3.7.5\r\n",
      "python branch: \r\n",
      "python build version: ('default', 'Nov  7 2019 10:50:52')\r\n",
      "python compiler version: GCC 8.3.0\r\n",
      "python implementation: CPython\r\n",
      "\r\n",
      "\r\n",
      "== check os platform ===============================================\r\n",
      "os: Linux\r\n",
      "os kernel version: #46~18.04.1-Ubuntu SMP Fri Jul 10 07:21:24 UTC 2020\r\n",
      "os release version: 5.4.0-42-generic\r\n",
      "os platform: Linux-5.4.0-42-generic-x86_64-with-Ubuntu-18.04-bionic\r\n",
      "linux distribution: ('Ubuntu', '18.04', 'bionic')\r\n",
      "linux os distribution: ('Ubuntu', '18.04', 'bionic')\r\n",
      "mac version: ('', ('', '', ''), '')\r\n",
      "uname: uname_result(system='Linux', node='ombromanto', release='5.4.0-42-generic', version='#46~18.04.1-Ubuntu SMP Fri Jul 10 07:21:24 UTC 2020', machine='x86_64', processor='x86_64')\r\n",
      "architecture: ('64bit', 'ELF')\r\n",
      "machine: x86_64\r\n",
      "\r\n",
      "\r\n",
      "== are we in docker =============================================\r\n",
      "No\r\n",
      "\r\n",
      "== compiler =====================================================\r\n",
      "c++ (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\r\n",
      "Copyright (C) 2017 Free Software Foundation, Inc.\r\n",
      "This is free software; see the source for copying conditions.  There is NO\r\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n",
      "\r\n",
      "\r\n",
      "== check pips ===================================================\r\n",
      "numpy                1.19.5\r\n",
      "protobuf             3.14.0\r\n",
      "tensorflow           1.14.0\r\n",
      "tensorflow-estimator 1.14.0\r\n",
      "\r\n",
      "== check for virtualenv =========================================\r\n",
      "False\r\n",
      "\r\n",
      "== tensorflow import ============================================\r\n",
      "...\r\n",
      "== env ==========================================================\r\n",
      "LD_LIBRARY_PATH is unset\r\n",
      "DYLD_LIBRARY_PATH is unset\r\n",
      "\r\n",
      "== nvidia-smi ===================================================\r\n",
      "Sun Jan 17 17:33:54 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce GTX 105...  Off  | 00000000:01:00.0  On |                  N/A |\r\n",
      "| 45%   23C    P8    N/A /  75W |    241MiB /  4033MiB |      1%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      1053      G   /usr/lib/xorg/Xorg                 16MiB |\r\n",
      "|    0   N/A  N/A      1132      G   /usr/bin/gnome-shell               56MiB |\r\n",
      "|    0   N/A  N/A      1934      G   /usr/lib/xorg/Xorg                106MiB |\r\n",
      "|    0   N/A  N/A      2080      G   /usr/bin/gnome-shell               27MiB |\r\n",
      "|    0   N/A  N/A      3447      G   ...AAAAAAAAA= --shared-files       21MiB |\r\n",
      "|    0   N/A  N/A      6932      G   /usr/lib/firefox/firefox            1MiB |\r\n",
      "|    0   N/A  N/A      7286      G   /usr/lib/firefox/firefox            1MiB |\r\n",
      "|    0   N/A  N/A     10042      G   /usr/lib/firefox/firefox            1MiB |\r\n",
      "|    0   N/A  N/A     10159      G   /usr/lib/firefox/firefox            1MiB |\r\n",
      "|    0   N/A  N/A     10256      G   /usr/lib/firefox/firefox            1MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "\r\n",
      "== cuda libs  ===================================================\r\n",
      "\r\n",
      "== tensorflow installed from info ==================\r\n",
      "Name: tensorflow\r\n",
      "Version: 1.14.0\r\n",
      "Summary: TensorFlow is an open source machine learning framework for everyone.\r\n",
      "Home-page: https://www.tensorflow.org/\r\n",
      "Author-email: packages@tensorflow.org\r\n",
      "License: Apache 2.0\r\n",
      "Location: /home/loreto/.venv/lib/python3.7/site-packages\r\n",
      "Required-by: \r\n",
      "\r\n",
      "== python version  ==============================================\r\n",
      "(major, minor, micro, releaselevel, serial)\r\n",
      "(3, 7, 5, 'final', 0)\r\n",
      "\r\n",
      "== bazel version  ===============================================\r\n",
      "\r\n",
      "```\n",
      "issue labels - \n",
      "TF 1.15\n",
      "stat:awaiting response\n",
      "subtype: ubuntu/linux\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  tools:summarize_graph fails to open TFHub models due to parsing errors\n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n",
      "- TensorFlow installed from (source or binary): source\r\n",
      "- TensorFlow version (use command below): v2.4.0-rc4-71-g582c8d236cb 2.4.0\r\n",
      "- Python version: Python 3.8.6 (default, Oct  9 2020, 11:59:16) \r\n",
      "- Bazel version (if compiling from source): Bazelisk version: v1.6.1\r\n",
      "- GCC/Compiler version (if compiling from source): [GCC 9.3.0] on linux\r\n",
      "- CUDA/cuDNN version: not relevant\r\n",
      "- GPU model and memory: not relevant\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "\r\n",
      "`summarize_graph` is not able to load downloaded and extracted models from TFHub e.g. https://tfhub.dev/deepmind/biggan-128/2 due to parsing errors.\r\n",
      "\r\n",
      "```\r\n",
      "$ bazel-bin/tensorflow/tools/graph_transforms/summarize_graph --in_graph='/media/paulgavrikov/DATA/noCNN/model_conversion/biggan/compare_gan_ssgan_128x128_1/saved_model.pb'  --print_structure=true\r\n",
      "[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/text_format.cc:324] Error parsing text-format tensorflow.GraphDef: 1:1: Invalid control characters encountered in text.\r\n",
      "[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/text_format.cc:324] Error parsing text-format tensorflow.GraphDef: 1:2: Interpreting non ascii codepoint 154.\r\n",
      "[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/text_format.cc:324] Error parsing text-format tensorflow.GraphDef: 1:2: Expected identifier, got: �\r\n",
      "2021-01-17 12:13:33.563477: E tensorflow/tools/graph_transforms/summarize_graph_main.cc:320] Loading graph '/media/paulgavrikov/DATA/noCNN/model_conversion/biggan/compare_gan_ssgan_128x128_1/saved_model.pb' failed with Can't parse /media/paulgavrikov/DATA/noCNN/model_conversion/biggan/compare_gan_ssgan_128x128_1/saved_model.pb as binary proto\r\n",
      "\t (both text and binary parsing failed for file /media/paulgavrikov/DATA/noCNN/model_conversion/biggan/compare_gan_ssgan_128x128_1/saved_model.pb)\r\n",
      "2021-01-17 12:13:33.563504: E tensorflow/tools/graph_transforms/summarize_graph_main.cc:322] usage: bazel-bin/tensorflow/tools/graph_transforms/summarize_graph\r\n",
      "```\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "\r\n",
      "The tool should load the model\n",
      "issue labels - \n",
      "TF 2.4\n",
      "stat:awaiting response\n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  documentation missing, tf.keras.Model.fit shuffle argument is being ignored when passing a tf.data.dataset\n",
      "issue body -  ## URL(s) with the issue:\r\n",
      "\r\n",
      "https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit\r\n",
      "\r\n",
      "## Description of issue (what needs changing):\r\n",
      "\r\n",
      "There's an issue with the shuffle argument description, it doesn't state that the \"shuffle\" argument is being ignored\r\n",
      "when the argument \"x\" is a tf.data.dataset in tf.keras.model.fit.\r\n",
      "\r\n",
      "### Clear description\r\n",
      "\r\n",
      "I was inspecting the source code in tf.keras.model.fit and found that when the input arg x to the [data handler](https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/keras/engine/training.py#L1050) is a tf.data.dataset, it ends up using the [dataset adapter](https://github.com/tensorflow/tensorflow/blob/582c8d236cb079023657287c318ff26adb239002/tensorflow/python/keras/engine/data_adapter.py#L671) which silently ignores the shuffle argument. \r\n",
      "\r\n",
      "This issue is not clearly explained in the description of the shuffle argument below:\r\n",
      "\r\n",
      "_\"\"\"Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). This argument is ignored when x is a generator. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when steps_per_epoch is not None.\"\"\"_\r\n",
      "\r\n",
      "I would recommend to write something like this instead \"_This argument is ignored when x is a generator or a tf.data.Dataset._\" \r\n",
      "\r\n",
      "I can contribute by fixing the docs if you agree that this should be done.\r\n",
      "\r\n",
      "Thank you in advance for your review!\r\n",
      "\r\n",
      "\r\n",
      " \r\n",
      "\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "comp:data\n",
      "comp:keras\n",
      "type:docs-bug\n",
      "\n",
      "\n",
      "issue title -  help me to solve this i am running a windows machine 8 gb ram python 3.8 cudnn 6.5 cuda 11 geforce 210\n",
      "issue body -  \r\n",
      "(base) C:\\Users\\ADMIN>python\r\n",
      "Python 3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32\r\n",
      "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n",
      ">>> import tensorflow\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n",
      "    from tensorflow.python._pywrap_tensorflow_internal import *\r\n",
      "ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\r\n",
      "\r\n",
      "During handling of the above exception, another exception occurred:\r\n",
      "\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"<stdin>\", line 1, in <module>\r\n",
      "  File \"C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n",
      "    from tensorflow.python.tools import module_util as _module_util\r\n",
      "  File \"C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\__init__.py\", line 39, in <module>\r\n",
      "    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\r\n",
      "  File \"C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 83, in <module>\r\n",
      "    raise ImportError(msg)\r\n",
      "ImportError: Traceback (most recent call last):\r\n",
      "  File \"C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n",
      "    from tensorflow.python._pywrap_tensorflow_internal import *\r\n",
      "ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\r\n",
      "\r\n",
      "\r\n",
      "Failed to load the native TensorFlow runtime.\r\n",
      "\r\n",
      "See https://www.tensorflow.org/install/errors\r\n",
      "\r\n",
      "for some common reasons and solutions.  Include the entire stack trace\r\n",
      "above this error message when asking for help.\r\n",
      ">>>                                                                                                                                                                     \n",
      "issue labels - \n",
      "stalled\n",
      "stat:awaiting response\n",
      "subtype:windows\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  Please Help Me To Sort This Out \n",
      "issue body -  <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- windows\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "- TensorFlow installed from (source or binary):\r\n",
      "- TensorFlow version:2\r\n",
      "- Python version:3.8\r\n",
      "- Installed using virtualenv? pip? conda?:\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version:6.5\r\n",
      "- GPU model and memory:geforce 210 \r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "\r\n",
      "**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n",
      "\r\n",
      "\r\n",
      "**Any other info / logs**\r\n",
      "Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n",
      "\n",
      "issue labels - \n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  Keras saved model returns different result than original model using Batch Normalization on multiple GPUs (Distributed training)\n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "- TensorFlow installed from (source or binary): Binary via pip3\r\n",
      "- TensorFlow version (use command below): 2.3.1\r\n",
      "- Python version: 3.6.9\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version: 10.1\r\n",
      "- GPU model and memory: Geforce GTX 1080 Ti 4x11GB\r\n",
      "\r\n",
      "I got a different result when using evaluate function on a saved model when compare with the original model. This only happens when Batch Normalization is included in the model and when training on multiple GPUs with MirroredStrategy.\r\n",
      "\r\n",
      "Here is my model\r\n",
      "```\r\n",
      "with strategy.scope():\r\n",
      "  model = tf.keras.Sequential([\r\n",
      "      tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\r\n",
      "      tf.keras.layers.MaxPooling2D(),\r\n",
      "      tf.keras.layers.BatchNormalization(),\r\n",
      "      tf.keras.layers.Flatten(),\r\n",
      "      tf.keras.layers.Dense(64, activation='relu'),\r\n",
      "      tf.keras.layers.Dense(10)\r\n",
      "  ])\r\n",
      "\r\n",
      "  model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n",
      "                optimizer=tf.keras.optimizers.Adam(),\r\n",
      "                metrics=['accuracy'])\r\n",
      "```\r\n",
      "\r\n",
      "Multiple GPUs with MirroredStrategy\r\n",
      "```\r\n",
      "strategy = tf.distribute.MirroredStrategy()\r\n",
      "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\r\n",
      "```\r\n",
      "Output\r\n",
      "```\r\n",
      "Number of devices: 2\r\n",
      "```\r\n",
      "\r\n",
      "Evaluate after training\r\n",
      "```\r\n",
      "eval_loss, eval_acc = model.evaluate(eval_dataset)\r\n",
      "\r\n",
      "print('Eval loss: {}, Eval Accuracy: {}'.format(eval_loss, eval_acc))\r\n",
      "```\r\n",
      "\r\n",
      "Output\r\n",
      "```\r\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.0424 - accuracy: 0.9884\r\n",
      "Eval loss: 0.04239395260810852, Eval Accuracy: **0.9883999824523926**\r\n",
      "```\r\n",
      "\r\n",
      "Save model and evaluate again\r\n",
      "```\r\n",
      "path = 'saved_model/'\r\n",
      "model.save(path, save_format='tf')\r\n",
      "with strategy.scope():\r\n",
      "  replicated_model = tf.keras.models.load_model(path)\r\n",
      "  replicated_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n",
      "                           optimizer=tf.keras.optimizers.Adam(),\r\n",
      "                           metrics=['accuracy'])\r\n",
      "\r\n",
      "  eval_loss, eval_acc = replicated_model.evaluate(eval_dataset)\r\n",
      "  print ('Eval loss: {}, Eval Accuracy: {}'.format(eval_loss, eval_acc))\r\n",
      "```\r\n",
      "Output\r\n",
      "```\r\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.0424 - accuracy: 0.9883\r\n",
      "Eval loss: 0.04239019751548767, Eval Accuracy: **0.9883000254631042**\r\n",
      "```\r\n",
      "\r\n",
      " - Without BN\r\n",
      "\r\n",
      "Output from evaluate\r\n",
      "```\r\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.0450 - accuracy: 0.9837\r\n",
      "Eval loss: 0.04498908668756485, Eval Accuracy: **0.9836999773979187**\r\n",
      "```\r\n",
      "\r\n",
      "Save model and repeat evaluate\r\n",
      "```\r\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.0450 - accuracy: 0.9837\r\n",
      "Eval loss: 0.04498908668756485, Eval Accuracy: **0.9836999773979187**\r\n",
      "```\r\n",
      "\r\n",
      "I have searched for similar issues and thought that this is because [BN computes differently during training and interference ](https://keras.io/api/layers/normalization_layers/batch_normalization/) but when I tried with one GPU, this issue didn't occur.\r\n",
      "\r\n",
      "[Code for reproduce results](https://colab.research.google.com/drive/14iv88UJwFSv1SaVzmEl9RMOa1EH8IiRG?usp=sharing)\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.3\n",
      "comp:dist-strat\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  typo / missing space in RuntimeError\n",
      "issue body -  I just signed up here to report this, sorry if I'm doing everything wrong or something.\r\n",
      "I just got the follwing:\r\n",
      "```\r\n",
      "RuntimeError: in user code:\r\n",
      "\r\n",
      "    <ipython-input-74-5015738e55b1>:22 fitting  *\r\n",
      "        grads = tape.gradient(value, model.trainable_weights)\r\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/backprop.py:1027 gradient  **\r\n",
      "        raise RuntimeError(\"A non-persistent GradientTape can only be used to\"\r\n",
      "\r\n",
      "    RuntimeError: A non-persistent GradientTape can only be used tocompute one set of gradients (or jacobians)\r\n",
      "```\r\n",
      "And I think there is a space missing in the last line's \"tocompute\".\n",
      "issue labels - \n",
      "comp:ops\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Tensorflow DOESNOT WORKS WITH GPU\n",
      "issue body -  <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "- TensorFlow installed from (source or binary):\r\n",
      "- TensorFlow version:\r\n",
      "- Python version:\r\n",
      "- Installed using virtualenv? pip? conda?:\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version:\r\n",
      "- GPU model and memory:\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "\r\n",
      "**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n",
      "\r\n",
      "\r\n",
      "**Any other info / logs**\r\n",
      "Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n",
      "\n",
      "issue labels - \n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  TypeError: '<' not supported between instances of 'function' and 'str'\n",
      "issue body -  <em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- I'm training the model on google collaboratory\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "I've trained and saved my model, on importing the model it gives this error on model evaluation, but model.predict() works perfectly\r\n",
      "\r\n",
      "**Testing Code**\r\n",
      "`model_path = \"/content/drive/MyDrive/Train Data/Models/text_block_model_new_batch.h5\"`\r\n",
      "`model = tf.keras.models.load_model(model1_path,custom_objects={'dice_coef':dice_coef,'dice_coef_loss':dice_coef_loss})`\r\n",
      "`results = model.evaluate(X_test,Y_test)`\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "![Screenshot (40)](https://user-images.githubusercontent.com/54077406/104814641-16448680-5836-11eb-8057-9bcd29159d9f.png)\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "comp:keras\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Update 20-documentation-issue.md\n",
      "issue body -  Minor update to the template to have better clarity over the content.\n",
      "issue labels - \n",
      "cla: yes\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  get_variable  missing reuse parameter\n",
      "issue body -  Please go to Stack Overflow for help and support:\r\n",
      "\r\n",
      "https://stackoverflow.com/questions/tagged/tensorflow\r\n",
      "\r\n",
      "If you open a GitHub issue, here is our policy:\r\n",
      "\r\n",
      "1.  It must be a bug, a feature request, or a significant problem with the\r\n",
      "    documentation (for small docs fixes please send a PR instead).\r\n",
      "2.  The form below must be filled out.\r\n",
      "3.  It shouldn't be a TensorBoard issue. Those go\r\n",
      "    [here](https://github.com/tensorflow/tensorboard/issues).\r\n",
      "\r\n",
      "**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n",
      "\r\n",
      "------------------------\r\n",
      "\r\n",
      "### System information\r\n",
      "\r\n",
      "-   **Have I written custom code (as opposed to using a stock example script\r\n",
      "    provided in TensorFlow)**:\r\n",
      "     Yes\r\n",
      "-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n",
      "    MacOSX 10.14.5 Mojave\r\n",
      "-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n",
      "    happens on a mobile device**:\r\n",
      "-   **TensorFlow installed from (source or binary)**: pip\r\n",
      "-   **TensorFlow version (use command below)**:2.4.0\r\n",
      "-   **Python version**: \r\n",
      "python --version\r\n",
      "Python 3.6.6 :: Anaconda, Inc.\r\n",
      "-   **Bazel version (if compiling from source)**:\r\n",
      "-   **GCC/Compiler version (if compiling from source)**:\r\n",
      "-   **CUDA/cuDNN version**:\r\n",
      "-   **GPU model and memory**:\r\n",
      "-   **Exact command to reproduce**:\r\n",
      "\r\n",
      "You can collect some of this information using our environment capture script:\r\n",
      "\r\n",
      "https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n",
      "\r\n",
      "You can obtain the TensorFlow version with:\r\n",
      "\r\n",
      "```bash\r\n",
      "python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n",
      "```\r\n",
      "v2.4.0-rc4-71-g582c8d236cb 2.4.0\r\n",
      "\r\n",
      "\r\n",
      "### Describe the problem\r\n",
      "Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n",
      "\r\n",
      "tg.get_variable cannot reuse  existing variable.\r\n",
      "\r\n",
      "### Source code / logs\r\n",
      "Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n",
      "\r\n",
      "tf.get_variable cannot pass in the reuse parameter, see  logs below:\r\n",
      "\r\n",
      "(TensorFlow-LiveLessons) bash-3.2$ python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n",
      "v2.4.0-rc4-71-g582c8d236cb 2.4.0\r\n",
      "\r\n",
      "\r\n",
      "(TensorFlow-LiveLessons) bash-3.2$ python\r\n",
      "Python 3.6.6 |Anaconda, Inc.| (default, Jun 28 2018, 11:07:29) \r\n",
      "[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin\r\n",
      "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n",
      ">>> import tensorflow.compat.v1 as tf\r\n",
      ">>> tf.disable_v2_behavior()\r\n",
      "WARNING:tensorflow:From /Users/philren/.local/share/virtualenvs/TensorFlow-LiveLessons-G6MnyUB7/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "non-resource variables are not supported in the long term\r\n",
      ">>> n_input = 784\r\n",
      ">>> n_dense = 128\r\n",
      ">>> W = tf.Variable(tf.random_normal([n_input, n_dense]))\r\n",
      ">>> tf.get_variable('W', [n_input, n_dense], \\\r\n",
      "...                       initializer=tf.keras.initializers.VarianceScaling(\r\n",
      "...                       scale=1.0, mode=\"fan_avg\",\r\n",
      "...                       distribution=\"truncated_normal\"\r\n",
      "...                         ))\r\n",
      "<tf.Variable 'W:0' shape=(784, 128) dtype=float32_ref>\r\n",
      ">>> tf.get_variable('W', [n_input, n_dense], \\\r\n",
      "...                       initializer=tf.keras.initializers.VarianceScaling(\r\n",
      "...                       scale=1.0, mode=\"fan_avg\",\r\n",
      "...                       distribution=\"truncated_normal\"\r\n",
      "...                         ))\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"<stdin>\", line 4, in <module>\r\n",
      "  File \"/Users/philren/.local/share/virtualenvs/TensorFlow-LiveLessons-G6MnyUB7/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 1593, in get_variable\r\n",
      "    aggregation=aggregation)\r\n",
      "  File \"/Users/philren/.local/share/virtualenvs/TensorFlow-LiveLessons-G6MnyUB7/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 1336, in get_variable\r\n",
      "    aggregation=aggregation)\r\n",
      "  File \"/Users/philren/.local/share/virtualenvs/TensorFlow-LiveLessons-G6MnyUB7/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 591, in get_variable\r\n",
      "    aggregation=aggregation)\r\n",
      "  File \"/Users/philren/.local/share/virtualenvs/TensorFlow-LiveLessons-G6MnyUB7/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 543, in _true_getter\r\n",
      "    aggregation=aggregation)\r\n",
      "  File \"/Users/philren/.local/share/virtualenvs/TensorFlow-LiveLessons-G6MnyUB7/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 893, in _get_single_variable\r\n",
      "    (err_msg, \"\".join(traceback.format_list(tb))))\r\n",
      "ValueError: Variable W already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\r\n",
      "\r\n",
      "  File \"<stdin>\", line 4, in <module>\r\n",
      "\r\n",
      ">>> tf.get_variable('W', [n_input, n_dense], \\\r\n",
      "...                       initializer=tf.keras.initializers.VarianceScaling(\r\n",
      "...                       scale=1.0, mode=\"fan_avg\",\r\n",
      "...                       distribution=\"truncated_normal\",\r\n",
      "...                               reuse=tf.AUTO_REUSE))\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"<stdin>\", line 5, in <module>\r\n",
      "  File \"/Users/philren/.local/share/virtualenvs/TensorFlow-LiveLessons-G6MnyUB7/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 538, in new_func\r\n",
      "    return func(*args, **kwargs)\r\n",
      "  File \"/Users/philren/.local/share/virtualenvs/TensorFlow-LiveLessons-G6MnyUB7/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 605, in new_func\r\n",
      "    return func(*args, **kwargs)\r\n",
      "TypeError: __init__() got an unexpected keyword argument 'reuse'\r\n",
      ">>> exit()\r\n",
      "\r\n",
      "\r\n",
      "Digging into the  source code tensorflow/python/ops/variable_scope.py, missing parameter  reuse\r\n",
      "\r\n",
      "@tf_export(v1=[\"get_variable\"])\r\n",
      "def get_variable(name,\r\n",
      "                 shape=None,\r\n",
      "                 dtype=None,\r\n",
      "                 initializer=None,\r\n",
      "                 regularizer=None,\r\n",
      "                 trainable=None,\r\n",
      "                 collections=None,\r\n",
      "                 caching_device=None,\r\n",
      "                 partitioner=None,\r\n",
      "                 validate_shape=True,\r\n",
      "                 use_resource=None,\r\n",
      "                 custom_getter=None,\r\n",
      "                 constraint=None,\r\n",
      "                 synchronization=VariableSynchronization.AUTO,\r\n",
      "                 aggregation=VariableAggregation.NONE):\r\n",
      "  return get_variable_scope().get_variable(\r\n",
      "      _get_default_variable_store(),\r\n",
      "      name,\r\n",
      "      shape=shape,\r\n",
      "      dtype=dtype,\r\n",
      "      initializer=initializer,\r\n",
      "      regularizer=regularizer,\r\n",
      "      trainable=trainable,\r\n",
      "      collections=collections,\r\n",
      "      caching_device=caching_device,\r\n",
      "      partitioner=partitioner,\r\n",
      "      validate_shape=validate_shape,\r\n",
      "      use_resource=use_resource,\r\n",
      "      custom_getter=custom_getter,\r\n",
      "      constraint=constraint,\r\n",
      "      synchronization=synchronization,\r\n",
      "      aggregation=aggregation)\r\n",
      "\r\n",
      "if we can add  this  parameter to  the parameter list the above problem could be fixed.\r\n",
      "Thanks\r\n",
      "\r\n",
      "# The argument list for get_variable must match arguments to get_local_variable.\r\n",
      "# So, if you are updating the arguments, also update arguments to\r\n",
      "# get_local_variable below.\r\n",
      "@tf_export(v1=[\"get_variable\"])\r\n",
      "def get_variable(name,\r\n",
      "                 shape=None,\r\n",
      "                 dtype=None,\r\n",
      "                 initializer=None,\r\n",
      "                 regularizer=None,\r\n",
      "                 trainable=None,\r\n",
      "                 collections=None,\r\n",
      "                 caching_device=None,\r\n",
      "                 partitioner=None,\r\n",
      "                 validate_shape=True,\r\n",
      "                 use_resource=None,\r\n",
      "                 custom_getter=None,\r\n",
      "                 constraint=None,\r\n",
      "                 synchronization=VariableSynchronization.AUTO,\r\n",
      "                 aggregation=VariableAggregation.NONE,\r\n",
      "                 reuse=None):\r\n",
      "  return get_variable_scope().get_variable(\r\n",
      "      _get_default_variable_store(),\r\n",
      "      name,\r\n",
      "      shape=shape,\r\n",
      "      dtype=dtype,\r\n",
      "      initializer=initializer,\r\n",
      "      regularizer=regularizer,\r\n",
      "      trainable=trainable,\r\n",
      "      collections=collections,\r\n",
      "      caching_device=caching_device,\r\n",
      "      partitioner=partitioner,\r\n",
      "      validate_shape=validate_shape,\r\n",
      "      use_resource=use_resource,\r\n",
      "      custom_getter=custom_getter,\r\n",
      "      constraint=constraint,\r\n",
      "      synchronization=synchronization,\r\n",
      "      aggregation=aggregation,\r\n",
      "      reuse=reuse)\r\n",
      "\r\n",
      "Xiquan Ren\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:ops\n",
      "stat:awaiting tensorflower\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Tensorflow issues: Not creating XLA devices, tf_xla_enable_xla_devices not set, This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "issue body -  Hi, I've just started to install TensorFlow, and I'm facing some issues. \r\n",
      "\r\n",
      "I'm trying to use PyCharm, so I installed anaconda and created a virtual environment with the versions being:\r\n",
      "Python 3.8.5\r\n",
      "pip 20.3.3\r\n",
      "tensorflow 2.4.0.\r\n",
      "\r\n",
      "But, I see errors when trying to run a simple tf.constant(\"hello\").\r\n",
      "The two errors are:\r\n",
      "\r\n",
      "1. I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n",
      "2. I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n",
      "\r\n",
      "I've been struggling for a long time...Thanks in advance.\r\n",
      "\n",
      "issue labels - \n",
      "comp:xla\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  Input 0 of layer dense is incompatible with the layer: expected axis -1 of input shape to have value 448 but received input with shape (None, 14944)\n",
      "issue body -  **Sytem Information:**\r\n",
      "\r\n",
      "- Platform : google colab\r\n",
      "- language -python\r\n",
      "- library:tensorflow\r\n",
      "\r\n",
      "**Implementation**\r\n",
      "\r\n",
      "- CNN\r\n",
      "- sentiment analysis\r\n",
      "- text classification\r\n",
      "\r\n",
      "**Code**\r\n",
      "```python\r\n",
      "from keras.utils import to_categorical\r\n",
      "X_train, X_test, Y_train, y_test = train_test_split(X,y, test_size = 0.15, random_state = 42)\r\n",
      "Y_train = to_categorical(Y_train.astype(int))\r\n",
      "y_test = to_categorical(y_test.astype(int))\r\n",
      "model.fit(X_train, Y_train, epochs=10, batch_size=32,verbose = 1,callbacks = callbacks_list,validation_data=(X_test,y_test))\r\n",
      "\r\n",
      "**Error information**\r\n",
      "Epoch 1/10\r\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 28) for input KerasTensor(type_spec=TensorSpec(shape=(None, 28), dtype=tf.float32, name='embedding_input'), name='embedding_input', description=\"created by layer 'embedding_input'\"), but it was called on an input with incompatible shape (None, 934).\r\n",
      "---------------------------------------------------------------------------\r\n",
      "ValueError                                Traceback (most recent call last)\r\n",
      "<ipython-input-19-3aeda4411868> in <module>()\r\n",
      "      6 Y_train = to_categorical(Y_train.astype(int))\r\n",
      "      7 y_test = to_categorical(y_test.astype(int))\r\n",
      "----> 8 model.fit(X_train, Y_train, epochs=10, batch_size=32,verbose = 1,callbacks = callbacks_list,validation_data=(X_test,y_test))\r\n",
      "\r\n",
      "9 frames\r\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)\r\n",
      "    975           except Exception as e:  # pylint:disable=broad-except\r\n",
      "    976             if hasattr(e, \"ag_error_metadata\"):\r\n",
      "--> 977               raise e.ag_error_metadata.to_exception(e)\r\n",
      "    978             else:\r\n",
      "    979               raise\r\n",
      "\r\n",
      "ValueError: in user code:\r\n",
      "\r\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\r\n",
      "        return step_function(self, iterator)\r\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:795 step_function  **\r\n",
      "        outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\r\n",
      "        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\r\n",
      "        return self._call_for_each_replica(fn, args, kwargs)\r\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\r\n",
      "        return fn(*args, **kwargs)\r\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:788 run_step  **\r\n",
      "        outputs = model.train_step(data)\r\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:754 train_step\r\n",
      "        y_pred = self(x, training=True)\r\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:1012 __call__\r\n",
      "        outputs = call_fn(inputs, *args, **kwargs)\r\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py:375 call\r\n",
      "        return super(Sequential, self).call(inputs, training=training, mask=mask)\r\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py:425 call\r\n",
      "        inputs, training=training, mask=mask)\r\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py:560 _run_internal_graph\r\n",
      "        outputs = node.layer(*args, **kwargs)\r\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:998 __call__\r\n",
      "        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\r\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/input_spec.py:259 assert_input_compatibility\r\n",
      "        ' but received input with shape ' + display_shape(x.shape))\r\n",
      "\r\n",
      "    ValueError: Input 0 of layer dense is incompatible with the layer: expected axis -1 of input shape to have value 448 but received input with shape (None, 14944)\r\n",
      "```\n",
      "issue labels - \n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  bazel coverage only generates empty coverage report\n",
      "issue body -  **System information**\r\n",
      "- OS Platform and Distribution: Linux Ubuntu 18.04\r\n",
      "- TensorFlow installed from: source\r\n",
      "- TensorFlow version: 2.1.0\r\n",
      "- Python version: 3.6.9\r\n",
      "- Installed using : source\r\n",
      "- Bazel version (if compiling from source): 0.29.0\r\n",
      "- GCC/Compiler version (if compiling from source): gcc 7.5.0\r\n",
      "- CUDA/cuDNN version: N\r\n",
      "- GPU model and memory: RTX 2080 super\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "The following command is used to test and get test coverage.\r\n",
      "```\r\n",
      "bazel coverage -s --instrument_test_targets --coverage_report_generator=@bazel_tools//tools/test:coverage_report_generator --coverage_support=@bazel_tools//tools/test:coverage_support --collect_code_coverage --jobs 5 //tensorflow\r\n",
      "```\r\n",
      "\r\n",
      "But I only get `~/tensorflow-2.1.0/bazel-testlogs/tensorflow/tensorflow/baseline_coverage.dat` and the data is only filenames and online `end_of_record`.\r\n",
      "\r\n",
      "I am not sure if it is caused by the wrong command.\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "stalled\n",
      "stat:awaiting response\n",
      "subtype:bazel\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  [INTEL_MKL] Add `is_weight_const` Attr for QMatMulDequantize Op\n",
      "issue body -  To add the missing `is_weight_const` attr in MklQuantizedMatMulAndDequantize registration.\r\n",
      "\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:mkl\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  Switch cmsis-nn and ethos-u to using OPTIMIZED_KERNEL_DIR.\n",
      "issue body -  This change started as a switch cmsis-nn and ethos-u to using OPTIMIZED_KERNEL_DIR. However, supporting both OPTIMIZED_KERNEL_DIR=cmsis-nn and TAGS=cmsis-nn required a lot of workarounds.\r\n",
      "\r\n",
      "As a result, it was decided to also remove the deprecated TAGS functionality.\r\n",
      "\r\n",
      "This change also adds a new CO_PROCESSOR command line parameter to the TFLM makefile.\r\n",
      "\r\n",
      "Specifying `CO_PROCESSOR=<c>` and `OPTIMIZED_KERNEL_DIR=<o>` will result in the following:\r\n",
      " * `tools/make/ext_libs/<o>.inc` and `tools/make/ext_libs/<c>.inc` will be included from the Makefile\r\n",
      " * kernel sources will be specialized from both `kernels/<o>.inc` and `kernels/<c>.inc` (in that order).\r\n",
      " * the specialization order is important because it means that if the same kernel in implemented in both optimized_kernel_dir and the co_processor, then the co_processor implementation will be used.\r\n",
      "\r\n",
      "Tested the following commands locally:\r\n",
      "\r\n",
      "Build for cortex-m55 + reference kernels + ethos-u kernel:\r\n",
      "```\r\n",
      "make -f tensorflow/lite/micro/tools/make/Makefile -j8 TARGET=cortex_m_generic TARGET_ARCH=cortex-m55 CO_PROCESSOR=ethos-u microlite\r\n",
      "```\r\n",
      "\r\n",
      "Build for cortex-m55 + cmsis-nn kernels (as available) + ethos-u kernel:\r\n",
      "```\r\n",
      "make -f tensorflow/lite/micro/tools/make/Makefile -j8 TARGET=cortex_m_generic TARGET_ARCH=cortex-m55 OPTIMIZED_KERNEL_DIR=cmsis-nn CO_PROCESSOR=ethos-u microlite\r\n",
      "```\r\n",
      "\r\n",
      "See the discussion on https://github.com/tensorflow/tensorflow/pull/46352 for some more context.\r\n",
      "\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:L\n",
      "\n",
      "\n",
      "issue title -  Fixing eigen tp crash\n",
      "issue body -  \n",
      "issue labels - \n",
      "cla: yes\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  Use TensorFlow with GPU RTX 3090 support on Windows (not WSL2) \n",
      "issue body -  Is there a set of instructions for using a RTX 3090 with CUDA and tensorflow python API on windows ? I dont mind what versions I need, or what I have to build etc, I would just like a definitive set of instructions on how to get this working. \r\n",
      "\r\n",
      "CUDA version\r\n",
      "CUDNN version\r\n",
      "Tensorflow version\r\n",
      "Python version\r\n",
      "\r\n",
      "Thanks. \n",
      "issue labels - \n",
      "comp:gpu\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Update the TFLM CI docs based on the new way to build a docker image.\n",
      "issue body -  The instructions need to be updated after the change from https://github.com/tensorflow/tensorflow/commit/4491d6ee3a13956f3e455f9506459c82de9ed268\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  Update arm_gcc_download.sh\n",
      "issue body -  This fixes #46393, tested on macOS Catalina\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  [CherryPick:r2.4] Don't use `$HOME`, use `~`.\n",
      "issue body -  Should fix `gpu_on_cpu` Docker build.\n",
      "\n",
      "PiperOrigin-RevId: 352055331\n",
      "Change-Id: I964f2cbca39d126b5043bd621b38fa288dddc416\n",
      "issue labels - \n",
      "cla: yes\n",
      "\n",
      "\n",
      "issue title -  Enable clang+bazel as part of the TFLM github CI.\n",
      "issue body -  We patch some of the TF workspace and BUILD files to reduce the number of downloads as part of a bazel build.\r\n",
      "    \r\n",
      "We can revisit this decision if the maintenance overhead from this patching becomes too high.\r\n",
      "    \r\n",
      "Also, there may be an opportunity to further reduce the downloads, but this change has made a decision to leave the TfLite build files unchanged and incur the cost of the additional downloads that are needed for TfLite but not for TfLite Micro.\r\n",
      "\r\n",
      "Fixes #46465\r\n",
      "Fixes http://b/177672856\r\n",
      "\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:L\n",
      "\n",
      "\n",
      "issue title -  Add cuda 11.2 driver and runtime inc files\n",
      "issue body -  CC @sanjoy \n",
      "issue labels - \n",
      "cla: yes\n",
      "ready to pull\n",
      "size:XL\n",
      "\n",
      "\n",
      "issue title -  Switch TFLM CI to use clang+bazel\n",
      "issue body -  @tensorflow/micro\r\n",
      "\r\n",
      "Using clang instead of gcc with bazel will mirror the internal checks and reduce the likelihood of internal changes breaking the github TFLM CI (e.g. https://github.com/tensorflow/tensorflow/issues/46415).\r\n",
      "\r\n",
      "It will also allow showing the results of the sanitizer builds externally as well.\r\n",
      "\n",
      "issue labels - \n",
      "comp:micro\n",
      "\n",
      "\n",
      "issue title -  Query: Convert Tensorflow NMT with attention model to tfjs\n",
      "issue body -  I'm following the TensorFlow NMT with attention (link) guide to build a character level transliteration model. I'm able to train the model and make inference calls in an .ipynb notebook. I'm able to save training checkpoints and load them later. However, my front end application is built on electron js. Therefore, I need to convert the encoder-decoder model to tfjs. Would anybody provide some guidance to convert my model to tfjs? I'd be happy to provide source code if needed.\n",
      "issue labels - \n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  Can't get Tensorflow 2.0 to properly distribute training across multiple GPUs\n",
      "issue body -  <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Redhat Linux 7.5\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "- TensorFlow installed from (source or binary): binary\r\n",
      "- TensorFlow version: 2.0\r\n",
      "- Python version: 3.7.7\r\n",
      "- Installed using virtualenv? pip? conda?: pip\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version: Cuda 10.1/cuDNN 7.6.5\r\n",
      "- GPU model and memory: 2x Titan RTX, 24gb each\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "I have two GPUs, both of which are visible to Tensorflow (and Tensorflow confirms this). However, when I try to train a model using a mirrored strategy it only trains a tiny bit on the second GPU. If I make the first GPU invisible with `export CUDA_VISIBLE_DEVICES=1`, then it will actually train on the second GPU. So I know it is capable of training on both. Furthermore, if I start a model training on the first GPU and then do `export CUDA_VISIBLE_DEVICES=1` and try to train another model, it will train on the second GPU. So I know that Tensorflow can find both GPUs and can train on both GPUs and can have both GPUs training at the same time, but I cannot get it to properly train a single model on both GPUs. \r\n",
      "\r\n",
      "Running `export CUDA_VISIBLE_DEVICES=0,1` does nothing; Tensorflow will still only train a tiny bit on the second GPU. \r\n",
      "\r\n",
      "\r\n",
      "**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n",
      "As stated above, I first ran `export CUDA_VISIBLE_DEVICES=0,1`. I have also tried training without running this command first. The exact code I am using to build my model is below. I have a convolutional neural network called FullModel():\r\n",
      "```python\r\n",
      "strategy = tf.distribute.MirroredStrategy()\r\n",
      "with strategy.scope():\r\n",
      "    net = FullModel()\r\n",
      "\r\n",
      "for epoch in range(100):\r\n",
      "    # Custom training loop\r\n",
      "```\r\n",
      "I have also tried the following:\r\n",
      "```python\r\n",
      "strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"])\r\n",
      "with strategy.scope():\r\n",
      "    net = FullModel()\r\n",
      "```\r\n",
      "\r\n",
      "\r\n",
      "**Any other info / logs**\r\n",
      "The output of `nvidia-smi` right after beginning training:\r\n",
      "```\r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 440.82       Driver Version: 440.82       CUDA Version: 10.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  TITAN RTX           Off  | 00000000:1B:00.0 Off |                  N/A |\r\n",
      "| 52%   70C    P2   199W / 280W |  23537MiB / 24220MiB |     51%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  TITAN RTX           Off  | 00000000:68:00.0 Off |                  N/A |\r\n",
      "| 41%   47C    P8     4W / 280W |    262MiB / 24219MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "\r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0     14382      C   python                                     23525MiB |\r\n",
      "|    1      3337      G   /usr/bin/X                                    27MiB |\r\n",
      "|    1      4783      G   /usr/bin/gnome-shell                          58MiB |\r\n",
      "|    1     14382      C   python                                       163MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "```\r\n",
      "Notice that job 14382 IS doing something on GPU 1. But it is only taking up 163mb on that GPU. I thought that maybe I just could train in larger batches or something to maximize my available GPU memory but that doesn't seem to change the situation. For some reason it just isn't using all of that GPU.\r\n",
      "\r\n",
      "The output of `nvidia-smi` after I run `export CUDA_VISIBLE_DEVICES=1` and start another job training:\r\n",
      "```\r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 440.82       Driver Version: 440.82       CUDA Version: 10.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  TITAN RTX           Off  | 00000000:1B:00.0 Off |                  N/A |\r\n",
      "| 53%   71C    P2   136W / 280W |  23537MiB / 24220MiB |     43%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  TITAN RTX           Off  | 00000000:68:00.0 Off |                  N/A |\r\n",
      "| 41%   62C    P2   246W / 280W |  23499MiB / 24219MiB |     75%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "\r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0     14382      C   python                                     23525MiB |\r\n",
      "|    1      3337      G   /usr/bin/X                                    27MiB |\r\n",
      "|    1      4783      G   /usr/bin/gnome-shell                          58MiB |\r\n",
      "|    1     14382      C   python                                       163MiB |\r\n",
      "|    1     19376      C   python                                     23237MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "```\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.0\n",
      "comp:dist-strat\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Doc : keras.utils.plot_model prints the shape as (None, n) but outdated (?, n) is given in doc  \n",
      "issue body -  ## URL(s) with the issue:\r\n",
      "\r\n",
      "https://www.tensorflow.org/guide/keras/functional\r\n",
      "\r\n",
      "## Description of issue (what needs changing):\r\n",
      "\r\n",
      "keras.utils.plot_model prints the shape as (None, n) but given in doc as (?, n)\r\n",
      "\r\n",
      "### Clear description\r\n",
      "\r\n",
      "Watch the line `keras.utils.plot_model(model, \"my_first_model_with_shape_info.png\", show_shapes=True)` in the doc link provided above.The output shown there is outdated\r\n",
      "\r\n",
      "### Correct links\r\n",
      "\r\n",
      "[Gist](https://colab.research.google.com/drive/19dlb-Qcoo2EBDigpZ8E75cb63woYI_EI?usp=sharing) here to show the changes.\r\n",
      "Tensorflow version: 2.4\r\n",
      "\r\n",
      "### Submit a pull request?\r\n",
      " Yes\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "comp:keras\n",
      "type:docs-bug\n",
      "\n",
      "\n",
      "issue title -  TypeError: An op outside of the function building code is being passed\n",
      "issue body -  Following the [documentation](https://www.tensorflow.org/tutorials/reinforcement_learning/actor_critic) I re-arranged functions into a class, I changed the model and made a few other modifications, however the code fails to work if `tf.function` is enabled, otherwise it works perfectly fine. By commenting out line 97, the error is gone:\r\n",
      "\r\n",
      "    self.model.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\r\n",
      "\r\n",
      "**Error**\r\n",
      "\r\n",
      "    Traceback (most recent call last):\r\n",
      "      File \"/Users/emadboctor/Desktop/code/drl-algos/a2c.py\", line 109, in <module>\r\n",
      "        agn.fit()\r\n",
      "      File \"/Users/emadboctor/Desktop/code/drl-algos/a2c.py\", line 103, in fit\r\n",
      "        self.train_step()\r\n",
      "      File \"/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 828, in __call__\r\n",
      "        result = self._call(*args, **kwds)\r\n",
      "      File \"/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 888, in _call\r\n",
      "        return self._stateless_fn(*args, **kwds)\r\n",
      "      File \"/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 2942, in __call__\r\n",
      "        return graph_function._call_flat(\r\n",
      "      File \"/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 1918, in _call_flat\r\n",
      "        return self._build_call_outputs(self._inference_function.call(\r\n",
      "      File \"/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 555, in call\r\n",
      "        outputs = execute.execute(\r\n",
      "      File \"/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\", line 75, in quick_execute\r\n",
      "        raise e\r\n",
      "      File \"/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\r\n",
      "        tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n",
      "    TypeError: An op outside of the function building code is being passed\r\n",
      "    a \"Graph\" tensor. It is possible to have Graph tensors\r\n",
      "    leak out of the function building context by including a\r\n",
      "    tf.init_scope in your function building code.\r\n",
      "    For example, the following function will fail:\r\n",
      "      @tf.function\r\n",
      "      def has_init_scope():\r\n",
      "        my_constant = tf.constant(1.)\r\n",
      "        with tf.init_scope():\r\n",
      "          added = my_constant * 2\r\n",
      "    The graph tensor has name: while:4\r\n",
      "\r\n",
      "**Code**\r\n",
      "\r\n",
      "    import gym\r\n",
      "    import numpy as np\r\n",
      "    import tensorflow as tf\r\n",
      "    from tensorflow.keras.layers import Conv2D, Dense, Flatten, Input\r\n",
      "    from tensorflow.keras.losses import Huber\r\n",
      "    from tensorflow.keras.models import Model\r\n",
      "    from tensorflow.keras.optimizers import Adam\r\n",
      "\r\n",
      "\r\n",
      "    class A2C:\r\n",
      "        def __init__(self, env, gamma=0.99, fc_units=512):\r\n",
      "            self.env = env\r\n",
      "            self.available_actions = env.action_space.n\r\n",
      "            self.model = self.create_model(fc_units)\r\n",
      "            self.state = tf.cast(self.env.reset(), tf.float32)\r\n",
      "            self.gamma = gamma\r\n",
      "            self.division_eps = np.finfo(np.float32).eps.item()\r\n",
      "            self.loss = Huber(reduction=tf.keras.losses.Reduction.SUM)\r\n",
      "    \r\n",
      "        def create_model(self, fc_units):\r\n",
      "            x0 = Input(self.env.observation_space.shape)\r\n",
      "            x = Conv2D(32, 8, 4, activation='relu')(x0)\r\n",
      "            x = Conv2D(64, 4, 2, activation='relu')(x)\r\n",
      "            x = Conv2D(32, 3, 1, activation='relu')(x)\r\n",
      "            x = Flatten()(x)\r\n",
      "            x = Dense(fc_units, activation='relu')(x)\r\n",
      "            actor = Dense(self.available_actions)(x)\r\n",
      "            critic = Dense(1)(actor)\r\n",
      "            model = Model(x0, [actor, critic])\r\n",
      "            model.call = tf.function(model.call)\r\n",
      "            return model\r\n",
      "    \r\n",
      "        def env_step(self, action):\r\n",
      "            state, reward, done, _ = self.env.step(action)\r\n",
      "            return (\r\n",
      "                state.astype(np.float32),\r\n",
      "                np.array(reward, np.int32),\r\n",
      "                np.array(done, np.int32),\r\n",
      "            )\r\n",
      "    \r\n",
      "        def tf_env_step(self, action):\r\n",
      "            return tf.numpy_function(\r\n",
      "                self.env_step, [action], [tf.float32, tf.int32, tf.int32]\r\n",
      "            )\r\n",
      "    \r\n",
      "        def get_returns(self, rewards, standardize=True):\r\n",
      "            n = tf.shape(rewards)[0]\r\n",
      "            returns = tf.TensorArray(dtype=tf.float32, size=n)\r\n",
      "            rewards = tf.cast(rewards[::-1], dtype=tf.float32)\r\n",
      "            discounted_sum = tf.constant(0.0)\r\n",
      "            discounted_sum_shape = discounted_sum.shape\r\n",
      "            for i in tf.range(n):\r\n",
      "                reward = rewards[i]\r\n",
      "                discounted_sum = reward + self.gamma * discounted_sum\r\n",
      "                discounted_sum.set_shape(discounted_sum_shape)\r\n",
      "                returns = returns.write(i, discounted_sum)\r\n",
      "            returns = returns.stack()[::-1]\r\n",
      "            if standardize:\r\n",
      "                returns = (returns - tf.math.reduce_mean(returns)) / (\r\n",
      "                    tf.math.reduce_std(returns) + self.division_eps\r\n",
      "                )\r\n",
      "            return returns\r\n",
      "    \r\n",
      "        def play_episode(self, max_steps=10000):\r\n",
      "            action_probs = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\r\n",
      "            values = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\r\n",
      "            rewards = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\r\n",
      "            initial_shape = self.state.shape\r\n",
      "            for i in tf.range(max_steps):\r\n",
      "                actor_out, value = self.model(tf.expand_dims(self.state, 0))\r\n",
      "                action = tf.random.categorical(actor_out, 1)[0, 0]\r\n",
      "                action_prob = tf.nn.softmax(actor_out)\r\n",
      "                self.state, reward, done = self.tf_env_step(action)\r\n",
      "                self.state.set_shape(initial_shape)\r\n",
      "                action_probs = action_probs.write(i, action_prob[0, action])\r\n",
      "                values = values.write(i, tf.squeeze(value))\r\n",
      "                rewards = rewards.write(i, reward)\r\n",
      "                if tf.cast(done, tf.bool):\r\n",
      "                    self.state = tf.cast(self.env.reset(), tf.float32)\r\n",
      "                    break\r\n",
      "            return [item.stack() for item in [action_probs, values, rewards]]\r\n",
      "    \r\n",
      "        def compute_loss(self, returns, values, action_probs):\r\n",
      "            advantage = returns - values\r\n",
      "            action_log_probs = tf.math.log(action_probs)\r\n",
      "            actor_loss = -tf.math.reduce_sum(action_log_probs * advantage)\r\n",
      "            critic_loss = self.loss(values, returns)\r\n",
      "            return actor_loss + critic_loss\r\n",
      "    \r\n",
      "        @tf.function\r\n",
      "        def train_step(self):\r\n",
      "            with tf.GradientTape() as tape:\r\n",
      "                action_probs, values, rewards = self.play_episode()\r\n",
      "                returns = self.get_returns(rewards)\r\n",
      "                loss = self.compute_loss(returns, values, action_probs)\r\n",
      "            grads = tape.gradient(loss, self.model.trainable_variables)\r\n",
      "            self.model.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\r\n",
      "            episode_reward = tf.math.reduce_sum(rewards)\r\n",
      "            return episode_reward\r\n",
      "    \r\n",
      "        def fit(self, learning_rate=7e-4):\r\n",
      "            self.model.compile(optimizer=Adam(learning_rate))\r\n",
      "            self.train_step()\r\n",
      "    \r\n",
      "    \r\n",
      "    if __name__ == '__main__':\r\n",
      "        gym_env = gym.make('PongNoFrameskip-v4')\r\n",
      "        agn = A2C(gym_env)\r\n",
      "        agn.fit()\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:autograph\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  No estimated time using model.fit() with TFRecord Dataset.\n",
      "issue body -  There is no estimated time when I call model.fit() with TFRecord Dataset.\r\n",
      "\r\n",
      "```\r\n",
      "   2448/Unknown - 894s 364ms/step - loss: 0.0703\r\n",
      "```\n",
      "issue labels - \n",
      "comp:keras\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  [BugFix] Fix bug when reduce dimension becomes empty in reduction_degenerate_dim_remover pass\n",
      "issue body -  I have encountered the following bug triggered by reduction_degenerate_dim_remover pass for HLO. Suppose we have the following two instructions.\r\n",
      "```\r\n",
      "%add.127 = f32[1,512,1024]{2,1,0} .......\r\n",
      "%reduce = f32[512,1024]{1,0} reduce(f32[1,512,1024]{2,1,0} %add.127, f32[] %constant_3), dimensions={0},\r\n",
      "```\r\n",
      "\r\n",
      "Then the reduction_degenerate_dim_remover pass helps to remove the only reduction dimension for ```%reduce```.\r\n",
      "```\r\n",
      "%add.127 = f32[1,512,1024]{2,1,0} .......\r\n",
      "%bitcast.292 = f32[512,1024]{1,0} bitcast(f32[1,512,1024]{2,1,0} %add.127)\r\n",
      "%reduce = f32[512,1024]{1,0} reduce(f32[512,1024]{1,0} %bitcast.292, f32[] %constant_3), dimensions={}, \r\n",
      "```\r\n",
      "\r\n",
      "This leads to produce an illegal reduce instruction which causes core dump in llvm code generation phase.\r\n",
      "\r\n",
      "So, I submit my bug fix for this scenario. The ```%reduce``` instruction is unnecessary and can be replaced by its operations (```%bitcast```) directly. \r\n",
      "\r\n",
      "Please start a review for this PR, thanks!\r\n",
      " @cheshire \n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:xla\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  [BugFix] Fix bug when reduce dimension becomes emptry in reduction_degenerate_dim_remover pass\n",
      "issue body -  I have encountered the following bug triggered by reduction_degenerate_dim_remover pass for HLO. Suppose we have the following two instructions.\r\n",
      "```\r\n",
      "%add.127 = f32[1,512,1024]{2,1,0} .......\r\n",
      "%reduce = f32[512,1024]{1,0} reduce(f32[1,512,1024]{2,1,0} %add.127, f32[] %constant_3), dimensions={0},\r\n",
      "```\r\n",
      "\r\n",
      "Then the reduction_degenerate_dim_remover pass helps to remove the only reduction dimension for ```%reduce```.\r\n",
      "```\r\n",
      "%add.127 = f32[1,512,1024]{2,1,0} .......\r\n",
      "%bitcast.292 = f32[512,1024]{1,0} bitcast(f32[1,512,1024]{2,1,0} %add.127)\r\n",
      "%reduce = f32[512,1024]{1,0} reduce(f32[512,1024]{1,0} %bitcast.292, f32[] %constant_3), dimensions={}, \r\n",
      "```\r\n",
      "\r\n",
      "This leads to produce an illegal reduce instruction which causes core dump in llvm code generation phase.\r\n",
      "\r\n",
      "So, I submit my bug fix for this scenario. The ```%reduce``` instruction is unnecessary and can be replaced by its operations (```%bitcast```) directly. \r\n",
      "\n",
      "issue labels - \n",
      "cla: no\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  unable to load weights from directories other than the working directory\n",
      "issue body -  <em>Please make sure that this is a bug. As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:bug_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): linux ubuntu 18.04\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "- TensorFlow installed from (source or binary):  binary\r\n",
      "- TensorFlow version (use command below): tf-nightly-gpu v2.5.0.dev20201214\r\n",
      "- Python version: 3.8.5\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version: CUDA v11.0\r\n",
      "- GPU model and memory: Tesla T4 15109MiB\r\n",
      "\r\n",
      "You can collect some of this information using our environment capture\r\n",
      "[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n",
      "You can also obtain the TensorFlow version with:\r\n",
      "1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n",
      "2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n",
      "\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "If i use model.load_weights(dir) and specify a directory similarly to how it's done in the docs:\r\n",
      "\r\n",
      "checkpoint_filepath = \"./checkpoints/training_checkpoints_tests/cp.ckpt\"\r\n",
      "checkpoint_dir = os.path.dirname(checkpoint_filepath)\r\n",
      "\r\n",
      " if os.path.exists(checkpoint_dir):\r\n",
      "            model.load_weights(checkpoint_filepath)\r\n",
      "            print('Checkpoints loaded')\r\n",
      "        else:\r\n",
      "            !mkdir -p './checkpoints/training_checkpoints_tests'\r\n",
      "            print('No checkpoints found')\r\n",
      "\r\n",
      "however the script never actually reads into the directory i'm specifying. If i instead load checkpoints directly from the working directory:\r\n",
      "\r\n",
      "checkpoint_filepath = \"/training_checkpoints_tests/cp.ckpt\"\r\n",
      "checkpoint_dir = os.path.dirname(checkpoint_filepath)\r\n",
      "\r\n",
      " if os.path.exists(checkpoint_dir):\r\n",
      "            model.load_weights(checkpoint_filepath)\r\n",
      "            print('Checkpoints loaded')\r\n",
      "        else:\r\n",
      "            !mkdir -p '/training_checkpoints_tests'\r\n",
      "            print('No checkpoints found')\r\n",
      "\r\n",
      "it works no problem. This second approach is fine if you have few models or checkpoints, but the direcotry quickly fills with folders and folders of checkpoints of different models and runs and it'd be better to keep all checkpoint in a folder\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "load checkpoints from sub-directories\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "See above\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.5\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  no kernel image is available for execution on the device\n",
      "issue body -  \r\n",
      "**System information**\r\n",
      "- I have  written custom code to YOLO V4:\r\n",
      "- In Windows 10:\r\n",
      "- TensorFlow version (use command below): 2.3.1\r\n",
      "- Python version: 3.7.9\r\n",
      "\r\n",
      "- CUDA/cuDNN version: CUDA 10.1 with cuDNN == cudnn-10.1-windows10-x64-v7.6.4.38\r\n",
      "- GPU model and memory: NVIDIA GEFORCE 940MX [In acer E5 laptop]\r\n",
      "\r\n",
      "I have got error like this \r\n",
      "`2021-01-15 13:27:54.527065: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n",
      "2021-01-15 13:28:41.927373: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll\r\n",
      "2021-01-15 13:28:42.898934: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\n",
      "pciBusID: 0000:01:00.0 name: GeForce 940MX computeCapability: 5.0\r\n",
      "coreClock: 1.189GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 29.80GiB/s\r\n",
      "2021-01-15 13:28:42.899439: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n",
      "2021-01-15 13:28:44.140267: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n",
      "2021-01-15 13:28:45.049662: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n",
      "2021-01-15 13:28:45.267345: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n",
      "2021-01-15 13:28:48.207716: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n",
      "2021-01-15 13:28:49.165815: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n",
      "2021-01-15 13:28:52.588923: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n",
      "2021-01-15 13:28:52.878248: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n",
      "GPUs [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\r\n",
      "Loading Darknet_weights from: ../model_data/yolov3.weights\r\n",
      "2021-01-15 13:28:53.793044: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2\r\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n",
      "2021-01-15 13:28:53.950440: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x11f09ecb740 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n",
      "2021-01-15 13:28:53.950804: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n",
      "2021-01-15 13:28:54.109305: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\n",
      "pciBusID: 0000:01:00.0 name: GeForce 940MX computeCapability: 5.0\r\n",
      "coreClock: 1.189GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 29.80GiB/s\r\n",
      "2021-01-15 13:28:54.109697: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n",
      "2021-01-15 13:28:54.109873: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n",
      "2021-01-15 13:28:54.110285: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n",
      "2021-01-15 13:28:54.110996: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n",
      "2021-01-15 13:28:54.111835: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n",
      "2021-01-15 13:28:54.112742: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n",
      "2021-01-15 13:28:54.113099: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n",
      "2021-01-15 13:28:54.113452: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n",
      "2021-01-15 13:28:54.927364: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n",
      "2021-01-15 13:28:54.927667: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n",
      "2021-01-15 13:28:54.927788: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n",
      "2021-01-15 13:28:54.928260: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1464 MB memory) -> physical GPU (device: 0, name: GeForce 940MX, pci bus id: 0000:01:00.0, compute capability: 5.0)\r\n",
      "2021-01-15 13:28:54.953337: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x11f1afa1690 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n",
      "2021-01-15 13:28:54.953588: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce 940MX, Compute Capability 5.0\r\n",
      "2021-01-15 13:28:54.979368: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op RandomStandardNormal in device /job:localhost/replica:0/task:0/device:GPU:0\r\n",
      "2021-01-15 13:28:55.014682: F .\\tensorflow/core/kernels/random_op_gpu.h:232] Non-OK-status: GpuLaunchKernel(FillPhiloxRandomKernelLaunch<Distribution>, num_blocks, block_size, 0, d.stream(), gen, data, size, dist) status: Internal: no kernel image is available for execution on the device\r\n",
      "\r\n",
      "Process finished with exit code -1073740791 (0xC0000409)`\r\n",
      "\r\n",
      "I have mention about my hardware in above.I have states I have mention about my hardware in above. Any help would be greatly appreciated\n",
      "issue labels - \n",
      "TF 2.3\n",
      "comp:gpu\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Revert \"Change definition of tensorflow::int64 to std::int64_t.\"\n",
      "issue body -  This reverts commit 50c714cde2c67b4c326cbec21d9383620ffbd32d.\r\n",
      "\r\n",
      "It breaks tf serving build.\n",
      "issue labels - \n",
      "cla: yes\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  An issue on cross-building Tensorflow Lite for Python\n",
      "issue body -  @tensorflow/micro\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 x86_64 amd64 PC\r\n",
      "- TensorFlow installed from (source or binary): v2.4\r\n",
      "- Tensorflow version (commit SHA if source): 582c8d2\r\n",
      "- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): RK3399 Ubuntu 18.04 aarch64\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "I am going to get tflite whl for Python3 on aarch64.\r\n",
      "So I tried to cross-build tflite whl on Ubuntu 18.04 x86_64 host PC.\r\n",
      "But while compiling the source I met the following issue.\r\n",
      "\r\n",
      "![image](https://user-images.githubusercontent.com/47862419/104678949-87494880-5727-11eb-9b5d-850b3e81bf9d.png)\r\n",
      "\r\n",
      "Host PC environment:\r\n",
      "OS: Ubuntu 18.04 x86_64 AMD64\r\n",
      "native gcc(g++): 7.5.0\r\n",
      "cross-compiler aarch64-linux-gnu-gcc(aarch64-linux-gnu-g++): 7.5.0\r\n",
      "bazel: 3.1.0\r\n",
      "python: virtual env python 3.7\r\n",
      "\r\n",
      "**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n",
      "\r\n",
      "I followed the following steps on my host PC.\r\n",
      "\r\n",
      "sudo apt update\r\n",
      "sudo apt-get install software-properties-common\r\n",
      "sudo apt update\r\n",
      "sudo apt install git curl\r\n",
      "sudo apt install python3.7 python3.7-dev python3.7-venv python3.7-distutils\r\n",
      "sudo apt install mesa-common-dev libegl1-mesa-dev libgles2-mesa-dev\r\n",
      "\r\n",
      "cd ~\r\n",
      "python3.7 -m venv py37\r\n",
      "source ~/py37/bin/activate\r\n",
      "pip install cython\r\n",
      "pip install wheel\r\n",
      "pip install numpy\r\n",
      "\r\n",
      "git clone -b r2.4 https://github.com/tensorflow/tensorflow.git tensorflow_r2.4\r\n",
      "cd tensorflow_r2.4\r\n",
      "python configure.py\r\n",
      "![image](https://user-images.githubusercontent.com/47862419/104679377-81079c00-5728-11eb-9f23-a9bd0a3f8ff9.png)\r\n",
      "\r\n",
      "./tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh aarch64\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:lite\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Can't link against TensorFlowLiteC.framework (iOS) when built on recent TF versions (regression)\n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OSX 11.1\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n",
      "- TensorFlow installed from (source or binary): Source\r\n",
      "- TensorFlow version (use command below): master\r\n",
      "- Python version: 3.8.2\r\n",
      "- Bazel version (if compiling from source): 3.7.2 and 3.1.0\r\n",
      "- GCC/Compiler version (if compiling from source): clang 12.0.0\r\n",
      "- CUDA/cuDNN version: N/A\r\n",
      "- GPU model and memory: N/A\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "\r\n",
      "It looks like there might be a regression in the build system responsible for generating the `TensorFlowLiteC.framework` iOS library. I've bisected it to 3e9fccb0cb9a, which is a little odd since that's just a bazel version bump... I'm using [am15h/tflite_flutter_plugin](https://github.com/am15h/tflite_flutter_plugin) (which binds directly to the TFLite C API) and can build my own `TensorFlowLiteC.framework` successfully and use it with that plugin up until the aforementioned commit.\r\n",
      "\r\n",
      "From 3e9fccb0cb9a forward (bisection script provided below) I get the following link errors:\r\n",
      "\r\n",
      "<details>\r\n",
      "  <summary>Build output</summary>\r\n",
      "\r\n",
      "```\r\n",
      "$ flutter build ios\r\n",
      "Warning: You are using these overridden dependencies:\r\n",
      "! tflite_flutter 0.5.0 from path external/tflite_flutter_plugin\r\n",
      "Running \"flutter pub get\" in RoutespotterApp...                     0.7s\r\n",
      "Building com.mgalgs.routespotter for device (ios-release)...\r\n",
      "Automatically signing iOS for device deployment using specified development team in Xcode project: PH2D8HJC83\r\n",
      "Running pod install...                                              2.3s\r\n",
      "Running Xcode build...\r\n",
      "Xcode build done.                                            4.3s\r\n",
      "Failed to build iOS app\r\n",
      "Error output from Xcode build:\r\n",
      "↳\r\n",
      "    ** BUILD FAILED **\r\n",
      "\r\n",
      "\r\n",
      "Xcode's output:\r\n",
      "↳\r\n",
      "    Undefined symbols for architecture arm64:\r\n",
      "      \"tflite::tensor_utils::MatrixBatchVectorMultiplyAccumulate(signed char const*, int, int, signed char const*, float const*, int, int*, float*,\r\n",
      "      tflite::CpuBackendContext*)\", referenced from:\r\n",
      "          l4840 in TensorFlowLiteC\r\n",
      "      \"tflite::ops::builtin::Register_FLOOR()\", referenced from:\r\n",
      "          l4485 in TensorFlowLiteC\r\n",
      "      \"tflite::ops::custom::Register_MFCC()\", referenced from:\r\n",
      "          l4485 in TensorFlowLiteC\r\n",
      "      \"_xnn_reallocate\", referenced from:\r\n",
      "          l4440 in TensorFlowLiteC\r\n",
      "      \"_xnn_aligned_allocate\", referenced from:\r\n",
      "          l4440 in TensorFlowLiteC\r\n",
      "      \"_xnn_aligned_deallocate\", referenced from:\r\n",
      "          l4440 in TensorFlowLiteC\r\n",
      "      \"flatbuffers::EnsureDirExists(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)\", referenced from:\r\n",
      "          l3532 in TensorFlowLiteC\r\n",
      "      \"flatbuffers::FileExists(char const*)\", referenced from:\r\n",
      "          l2367 in TensorFlowLiteC\r\n",
      "      \"flatbuffers::LoadFile(char const*, bool, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >*)\", referenced from:\r\n",
      "          l2367 in TensorFlowLiteC\r\n",
      "      \"flatbuffers::AbsolutePath(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)\", referenced from:\r\n",
      "          l2254 in TensorFlowLiteC\r\n",
      "      \"tflite::tensor_utils::VectorBatchVectorCwiseProductAccumulate(short const*, int, short const*, int, int, int, short*)\", referenced from:\r\n",
      "          l1730 in TensorFlowLiteC\r\n",
      "      \"tflite::tensor_utils::VectorScalarMultiply(signed char const*, int, float, float*)\", referenced from:\r\n",
      "          l1729 in TensorFlowLiteC\r\n",
      "      \"tflite::tensor_utils::VectorBatchVectorAdd(float const*, int, int, float*)\", referenced from:\r\n",
      "          l1728 in TensorFlowLiteC\r\n",
      "          l1729 in TensorFlowLiteC\r\n",
      "      \"tflite::tensor_utils::CwiseClipping(short*, int, short)\", referenced from:\r\n",
      "          l1727 in TensorFlowLiteC\r\n",
      "          l1731 in TensorFlowLiteC\r\n",
      "      \"tflite::tensor_utils::MatrixBatchVectorMultiply(signed char const*, int, signed char const*, int, int, int, int, int, signed char*, signed char)\", referenced from:\r\n",
      "          l1727 in TensorFlowLiteC\r\n",
      "      \"tflite::tensor_utils::TwoGateSaturatingAdd(signed char const*, signed char, signed char const*, signed char, int, int, int, int, int, int, short*)\", referenced from:\r\n",
      "          l1727 in TensorFlowLiteC\r\n",
      "      \"tflite::tensor_utils::ApplySigmoidFloat(short const*, int, int, short*)\", referenced from:\r\n",
      "          l1727 in TensorFlowLiteC\r\n",
      "      \"tflite::tensor_utils::CwiseMul(short const*, short const*, int, int, int, short*)\", referenced from:\r\n",
      "          l1727 in TensorFlowLiteC\r\n",
      "          l1731 in TensorFlowLiteC\r\n",
      "      \"tflite::tensor_utils::ApplyTanhFloat(short const*, int, int, int, short*)\", referenced from:\r\n",
      "          l1727 in TensorFlowLiteC\r\n",
      "      \"tflite::tensor_utils::ApplyTanh(int, short const*, int, int, short*)\", referenced from:\r\n",
      "          l1726 in TensorFlowLiteC\r\n",
      "          l1730 in TensorFlowLiteC\r\n",
      "      \"tflite::tensor_utils::CwiseMul(short const*, short const*, int, int, int, int, int, signed char*)\", referenced from:\r\n",
      "          l1726 in TensorFlowLiteC\r\n",
      "      \"flatbuffers::StripExtension(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)\", referenced from:\r\n",
      "          l814 in TensorFlowLiteC\r\n",
      "          l1889 in TensorFlowLiteC\r\n",
      "          l3564 in TensorFlowLiteC\r\n",
      "      \"tflite::tensor_utils::MatrixBatchVectorMultiplyAccumulate(signed char const*, int const*, signed char const*, int, int, int, int, int, int, int*, signed char*,\r\n",
      "      tflite::CpuBackendContext*)\", referenced from:\r\n",
      "          l1726 in TensorFlowLiteC\r\n",
      "      \"tflite::tensor_utils::CwiseClipping(float*, int, float)\", referenced from:\r\n",
      "          l1722 in TensorFlowLiteC\r\n",
      "          l1724 in TensorFlowLiteC\r\n",
      "      \"tflite::Subgraph::HasDelegates()\", referenced from:\r\n",
      "          l1472 in TensorFlowLiteC\r\n",
      "      \"tflite::Subgraph::IsCancelled()\", referenced from:\r\n",
      "          l1469 in TensorFlowLiteC\r\n",
      "      \"tflite::tensor_utils::AsymmetricQuantizeFloats(float const*, int, signed char*, float*, int*)\", referenced from:\r\n",
      "          l278 in TensorFlowLiteC\r\n",
      "          l290 in TensorFlowLiteC\r\n",
      "          l414 in TensorFlowLiteC\r\n",
      "          l1254 in TensorFlowLiteC\r\n",
      "          l1417 in TensorFlowLiteC\r\n",
      "          l1724 in TensorFlowLiteC\r\n",
      "          l4108 in TensorFlowLiteC\r\n",
      "          ...\r\n",
      "      \"tflite::Subgraph::SetExecutionPlan(std::__1::vector<int, std::__1::allocator<int> > const&)\", referenced from:\r\n",
      "          l1465 in TensorFlowLiteC\r\n",
      "      \"tflite::tensor_utils::ApplyLayerNorm(short const*, short const*, int const*, int, int, int, int, int, short*)\", referenced from:\r\n",
      "          l1726 in TensorFlowLiteC\r\n",
      "          l1730 in TensorFlowLiteC\r\n",
      "      \"_xnn_deallocate\", referenced from:\r\n",
      "          l4440 in TensorFlowLiteC\r\n",
      "      \"tflite::Subgraph::SetTensorParametersReadWrite(int, TfLiteType, char const*, unsigned long, int const*, TfLiteQuantization, bool, unsigned long, int const*)\",\r\n",
      "      referenced from:\r\n",
      "          l1462 in TensorFlowLiteC\r\n",
      "          l1464 in TensorFlowLiteC\r\n",
      "          l3451 in TensorFlowLiteC\r\n",
      "      \"tflite::Subgraph::SetTensorParametersReadOnly(int, TfLiteType, char const*, unsigned long, int const*, TfLiteQuantization, char const*, unsigned long,\r\n",
      "      tflite::Allocation const*, TfLiteSparsity*)\", referenced from:\r\n",
      "          l1461 in TensorFlowLiteC\r\n",
      "          l1463 in TensorFlowLiteC\r\n",
      "          l3451 in TensorFlowLiteC\r\n",
      "      \"tflite::Subgraph::AddTensors(int, int*)\", referenced from:\r\n",
      "          l1459 in TensorFlowLiteC\r\n",
      "          l3454 in TensorFlowLiteC\r\n",
      "      \"flatbuffers::SaveFile(char const*, char const*, unsigned long, bool)\", referenced from:\r\n",
      "          l812 in TensorFlowLiteC\r\n",
      "          l1892 in TensorFlowLiteC\r\n",
      "          l3563 in TensorFlowLiteC\r\n",
      "      \"tflite::Subgraph::AddNodeWithParameters(std::__1::vector<int, std::__1::allocator<int> > const&, std::__1::vector<int, std::__1::allocator<int> > const&,\r\n",
      "      std::__1::vector<int, std::__1::allocator<int> > const&, char const*, unsigned long, void*, TfLiteRegistration const*, int*)\", referenced from:\r\n",
      "          l1454 in TensorFlowLiteC\r\n",
      "          l3443 in TensorFlowLiteC\r\n",
      "      \"tflite::Subgraph::Invoke()\", referenced from:\r\n",
      "          l1458 in TensorFlowLiteC\r\n",
      "          l3008 in TensorFlowLiteC\r\n",
      "          l4742 in TensorFlowLiteC\r\n",
      "          l5015 in TensorFlowLiteC\r\n",
      "      \"tflite::tensor_utils::Sub1Vector(short const*, int, short*)\", referenced from:\r\n",
      "          l1727 in TensorFlowLiteC\r\n",
      "          l1731 in TensorFlowLiteC\r\n",
      "      \"tflite::Subgraph::ResetVariableTensors()\", referenced from:\r\n",
      "          l1460 in TensorFlowLiteC\r\n",
      "      \"tflite::tensor_utils::ApplySigmoid(short const*, int, int, short*)\", referenced from:\r\n",
      "          l1730 in TensorFlowLiteC\r\n",
      "      \"flatbuffers::StripPath(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)\", referenced from:\r\n",
      "          l814 in TensorFlowLiteC\r\n",
      "          l1889 in TensorFlowLiteC\r\n",
      "          l3564 in TensorFlowLiteC\r\n",
      "      \"tflite::Subgraph::ReleaseNonPersistentMemory()\", referenced from:\r\n",
      "          l1457 in TensorFlowLiteC\r\n",
      "          l5015 in TensorFlowLiteC\r\n",
      "      \"tflite::tensor_utils::SparseMatrixBatchVectorMultiplyAccumulate1x4(float const*, int const*, int const*, int, int, float const*, int, float*)\", referenced from:\r\n",
      "          l1282 in TensorFlowLiteC\r\n",
      "          l1286 in TensorFlowLiteC\r\n",
      "      \"tflite::Subgraph::ResizeInputTensorStrict(int, std::__1::vector<int, std::__1::allocator<int> > const&)\", referenced from:\r\n",
      "          l1456 in TensorFlowLiteC\r\n",
      "      \"tflite::tensor_utils::SparseMatrixBatchVectorMultiplyAccumulate(signed char const*, unsigned char const*, int, int, signed char const*, float const*, int, float*)\",\r\n",
      "      referenced from:\r\n",
      "          l1724 in TensorFlowLiteC\r\n",
      "          l1729 in TensorFlowLiteC\r\n",
      "      \"tflite::Subgraph::ResizeInputTensor(int, std::__1::vector<int, std::__1::allocator<int> > const&)\", referenced from:\r\n",
      "          l1455 in TensorFlowLiteC\r\n",
      "          l3007 in TensorFlowLiteC\r\n",
      "          l3009 in TensorFlowLiteC\r\n",
      "          l4741 in TensorFlowLiteC\r\n",
      "      \"tflite::tensor_utils::MatrixBatchVectorMultiply(short const*, signed char const*, int, int, int const*, int, int, int, int, signed char*)\", referenced from:\r\n",
      "          l1727 in TensorFlowLiteC\r\n",
      "      \"tflite::Subgraph::ReserveNodes(int)\", referenced from:\r\n",
      "          l1452 in TensorFlowLiteC\r\n",
      "          l3443 in TensorFlowLiteC\r\n",
      "      \"tflite::Subgraph::SetExternalContext(TfLiteExternalContextType, TfLiteExternalContext*)\", referenced from:\r\n",
      "          l1445 in TensorFlowLiteC\r\n",
      "      \"tflite::Subgraph::SetCustomAllocationForTensor(int, TfLiteCustomAllocation const&)\", referenced from:\r\n",
      "          l1446 in TensorFlowLiteC\r\n",
      "      \"tflite::tensor_utils::MatrixBatchVectorMultiplyAccumulate(float const*, int, int, float const*, int, float*)\", referenced from:\r\n",
      "          l1252 in TensorFlowLiteC\r\n",
      "          l1416 in TensorFlowLiteC\r\n",
      "          l1722 in TensorFlowLiteC\r\n",
      "          l1728 in TensorFlowLiteC\r\n",
      "          l4106 in TensorFlowLiteC\r\n",
      "      \"tflite::Subgraph::RemoveAllDelegates()\", referenced from:\r\n",
      "          l1451 in TensorFlowLiteC\r\n",
      "          l1470 in TensorFlowLiteC\r\n",
      "          l1471 in TensorFlowLiteC\r\n",
      "      \"tflite::Subgraph::SetVariables(std::__1::vector<int, std::__1::allocator<int> >)\", referenced from:\r\n",
      "          l1449 in TensorFlowLiteC\r\n",
      "          l3454 in TensorFlowLiteC\r\n",
      "      \"flatbuffers::ConCatPathFileName(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char,\r\n",
      "      std::__1::char_traits<char>, std::__1::allocator<char> > const&)\", referenced from:\r\n",
      "          l2367 in TensorFlowLiteC\r\n",
      "      \"tflite::tensor_utils::ReductionSumVector(signed char const*, int*, int, int)\", referenced from:\r\n",
      "          l1724 in TensorFlowLiteC\r\n",
      "          l4108 in TensorFlowLiteC\r\n",
      "          l4109 in TensorFlowLiteC\r\n",
      "          l4554 in TensorFlowLiteC\r\n",
      "          l4864 in TensorFlowLiteC\r\n",
      "      \"tflite::tensor_utils::CwiseAdd(short const*, short const*, int, int, short*)\", referenced from:\r\n",
      "          l1727 in TensorFlowLiteC\r\n",
      "          l1731 in TensorFlowLiteC\r\n",
      "      \"tflite::Subgraph::SetOutputs(std::__1::vector<int, std::__1::allocator<int> >)\", referenced from:\r\n",
      "          l1448 in TensorFlowLiteC\r\n",
      "          l3454 in TensorFlowLiteC\r\n",
      "      \"tflite::Subgraph::Subgraph(tflite::ErrorReporter*, TfLiteExternalContext**, std::__1::vector<std::__1::unique_ptr<tflite::Subgraph,\r\n",
      "      std::__1::default_delete<tflite::Subgraph> >, std::__1::allocator<std::__1::unique_ptr<tflite::Subgraph, std::__1::default_delete<tflite::Subgraph> > > >*,\r\n",
      "      std::__1::unordered_map<int, std::__1::unique_ptr<tflite::resource::ResourceBase, std::__1::default_delete<tflite::resource::ResourceBase> >, std::__1::hash<int>,\r\n",
      "      std::__1::equal_to<int>, std::__1::allocator<std::__1::pair<int const, std::__1::unique_ptr<tflite::resource::ResourceBase,\r\n",
      "      std::__1::default_delete<tflite::resource::ResourceBase> > > > >*)\", referenced from:\r\n",
      "          l1441 in TensorFlowLiteC\r\n",
      "      \"flatbuffers::PosixPath(char const*)\", referenced from:\r\n",
      "          l2367 in TensorFlowLiteC\r\n",
      "      \"tflite::tensor_utils::CwiseClipping(signed char*, int, signed char)\", referenced from:\r\n",
      "          l1726 in TensorFlowLiteC\r\n",
      "          l1727 in TensorFlowLiteC\r\n",
      "      \"tflite::tensor_utils::Sub1Vector(float const*, int, float*)\", referenced from:\r\n",
      "          l1722 in TensorFlowLiteC\r\n",
      "          l1724 in TensorFlowLiteC\r\n",
      "      \"tflite::tensor_utils::BatchVectorBatchVectorDotProduct(short const*, short const*, int, int, int*)\", referenced from:\r\n",
      "          l1418 in TensorFlowLiteC\r\n",
      "      \"tflite::tensor_utils::MatrixBatchVectorMultiplyAccumulate(signed char const*, int, int, signed char const*, float const*, int, float*, float const*, int const*,\r\n",
      "      int*, int*, bool*, tflite::CpuBackendContext*)\", referenced from:\r\n",
      "          l1254 in TensorFlowLiteC\r\n",
      "          l1417 in TensorFlowLiteC\r\n",
      "          l1724 in TensorFlowLiteC\r\n",
      "          l1729 in TensorFlowLiteC\r\n",
      "          l4108 in TensorFlowLiteC\r\n",
      "      \"tflite::Subgraph::SetCancellationFunction(void*, bool (*)(void*))\", referenced from:\r\n",
      "          l1468 in TensorFlowLiteC\r\n",
      "      \"tflite::tensor_utils::ReductionSumVector(int const*, int*, int, int)\", referenced from:\r\n",
      "          l1418 in TensorFlowLiteC\r\n",
      "      \"_xnn_allocate\", referenced from:\r\n",
      "          l4440 in TensorFlowLiteC\r\n",
      "      \"tflite::tensor_utils::MeanStddevNormalization(float const*, float*, int, int)\", referenced from:\r\n",
      "          l1728 in TensorFlowLiteC\r\n",
      "          l1729 in TensorFlowLiteC\r\n",
      "      \"tflite::tensor_utils::VectorVectorDotProduct(float const*, float const*, int)\", referenced from:\r\n",
      "          l1416 in TensorFlowLiteC\r\n",
      "          l1417 in TensorFlowLiteC\r\n",
      "      \"tflite::tensor_utils::IsZeroVector(float const*, int)\", referenced from:\r\n",
      "          l1254 in TensorFlowLiteC\r\n",
      "          l1417 in TensorFlowLiteC\r\n",
      "          l1722 in TensorFlowLiteC\r\n",
      "          l1724 in TensorFlowLiteC\r\n",
      "          l4108 in TensorFlowLiteC\r\n",
      "      \"tflite::tensor_utils::ApplyLayerNormFloat(short const*, short const*, int, int, int const*, int, int, short*)\", referenced from:\r\n",
      "          l1727 in TensorFlowLiteC\r\n",
      "      \"tflite::Subgraph::ModifyGraphWithDelegate(TfLiteDelegate*)\", referenced from:\r\n",
      "          l1451 in TensorFlowLiteC\r\n",
      "          l1470 in TensorFlowLiteC\r\n",
      "      \"tflite::tensor_utils::MatrixScalarMultiplyAccumulate(signed char const*, int, int, int, int*)\", referenced from:\r\n",
      "          l764 in TensorFlowLiteC\r\n",
      "          l1981 in TensorFlowLiteC\r\n",
      "      \"tflite::Subgraph::SetInputs(std::__1::vector<int, std::__1::allocator<int> >)\", referenced from:\r\n",
      "          l1447 in TensorFlowLiteC\r\n",
      "          l3454 in TensorFlowLiteC\r\n",
      "      \"tflite::tensor_utils::ReductionSumVector(float const*, float*, int, int)\", referenced from:\r\n",
      "          l1416 in TensorFlowLiteC\r\n",
      "          l1417 in TensorFlowLiteC\r\n",
      "      \"tflite::tensor_utils::MatrixBatchVectorMultiplyAccumulate(signed char const*, int const*, signed char const*, int, int, int, int, int, int, int*, short*,\r\n",
      "      tflite::CpuBackendContext*)\", referenced from:\r\n",
      "          l1726 in TensorFlowLiteC\r\n",
      "          l1730 in TensorFlowLiteC\r\n",
      "      \"tflite::tensor_utils::SymmetricQuantizeFloats(float const*, int, signed char*, float*, float*, float*)\", referenced from:\r\n",
      "          l1254 in TensorFlowLiteC\r\n",
      "          l1417 in TensorFlowLiteC\r\n",
      "          l1724 in TensorFlowLiteC\r\n",
      "          l4108 in TensorFlowLiteC\r\n",
      "          l4542 in TensorFlowLiteC\r\n",
      "          l4551 in TensorFlowLiteC\r\n",
      "          l4837 in TensorFlowLiteC\r\n",
      "          ...\r\n",
      "      \"tflite::Subgraph::AllocateTensors()\", referenced from:\r\n",
      "          l1450 in TensorFlowLiteC\r\n",
      "          l3006 in TensorFlowLiteC\r\n",
      "          l3008 in TensorFlowLiteC\r\n",
      "          l4741 in TensorFlowLiteC\r\n",
      "          l5015 in TensorFlowLiteC\r\n",
      "    ld: symbol(s) not found for architecture arm64\r\n",
      "    clang: error: linker command failed with exit code 1 (use -v to see invocation)\r\n",
      "    note: Using new build system\r\n",
      "    note: Building targets in parallel\r\n",
      "    note: Planning build\r\n",
      "    note: Constructing build description\r\n",
      "    warning: The iOS deployment target 'IPHONEOS_DEPLOYMENT_TARGET' is set to 8.0, but the range of supported deployment target versions is 9.0 to 14.3.99. (in target\r\n",
      "    'url_launcher' from project 'Pods')\r\n",
      "    warning: The iOS deployment target 'IPHONEOS_DEPLOYMENT_TARGET' is set to 8.0, but the range of supported deployment target versions is 9.0 to 14.3.99. (in target\r\n",
      "    'video_player' from project 'Pods')\r\n",
      "    warning: The iOS deployment target 'IPHONEOS_DEPLOYMENT_TARGET' is set to 8.0, but the range of supported deployment target versions is 9.0 to 14.3.99. (in target\r\n",
      "    'shared_preferences' from project 'Pods')\r\n",
      "    warning: The iOS deployment target 'IPHONEOS_DEPLOYMENT_TARGET' is set to 8.0, but the range of supported deployment target versions is 9.0 to 14.3.99. (in target\r\n",
      "    'path_provider' from project 'Pods')\r\n",
      "    warning: The iOS deployment target 'IPHONEOS_DEPLOYMENT_TARGET' is set to 8.0, but the range of supported deployment target versions is 9.0 to 14.3.99. (in target\r\n",
      "    'package_info' from project 'Pods')\r\n",
      "    warning: The iOS deployment target 'IPHONEOS_DEPLOYMENT_TARGET' is set to 8.0, but the range of supported deployment target versions is 9.0 to 14.3.99. (in target\r\n",
      "    'image_picker' from project 'Pods')\r\n",
      "    warning: The iOS deployment target 'IPHONEOS_DEPLOYMENT_TARGET' is set to 8.0, but the range of supported deployment target versions is 9.0 to 14.3.99. (in target\r\n",
      "    'flutter_isolate' from project 'Pods')\r\n",
      "    warning: The iOS deployment target 'IPHONEOS_DEPLOYMENT_TARGET' is set to 8.0, but the range of supported deployment target versions is 9.0 to 14.3.99. (in target\r\n",
      "    'camera' from project 'Pods')\r\n",
      "    warning: The iOS deployment target 'IPHONEOS_DEPLOYMENT_TARGET' is set to 8.0, but the range of supported deployment target versions is 9.0 to 14.3.99. (in target\r\n",
      "    'Flutter' from project 'Pods')\r\n",
      "\r\n",
      "Encountered error while building for device.\r\n",
      "```\r\n",
      "</details>\r\n",
      "\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "\r\n",
      "I expect to be able to build a `TensorFlowLiteC.framework` and link against it from a Flutter application.\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "\r\n",
      "These steps must be run on an OSX machine.\r\n",
      "\r\n",
      "If needed, install bazel versions 3.1.0 *and* 3.7.2:\r\n",
      "\r\n",
      "```\r\n",
      "(cd \"~/.bazel/bin\" && curl -fLO https://releases.bazel.build/3.1.0/release/bazel-3.1.0-darwin-x86_64 && chmod +x bazel-3.1.0-darwin-x86_64)\r\n",
      "(cd \"~/.bazel/bin\" && curl -fLO https://releases.bazel.build/3.7.2/release/bazel-3.7.2-darwin-x86_64 && chmod +x bazel-3.7.2-darwin-x86_64)\r\n",
      "```\r\n",
      "\r\n",
      "Download [this gist](https://gist.github.com/mgalgs/3a79a6aa4b9ca237e0bea047d2326550) to a `$WORKSPACE` of your choosing, then:\r\n",
      "\r\n",
      "```\r\n",
      "cd $WORKSPACE\r\n",
      "git clone --recurse-submodules https://github.com/mgalgs/object_detection_flutter.git\r\n",
      "cd <tensorflow_dir>\r\n",
      "./configure  # answer yes when it asks about iOS support\r\n",
      "\r\n",
      "# Checkout the breaker and you should see a build failure in our test app.\r\n",
      "git checkout 3e9fccb0cb9a5f55a4e67e3011c20ff31c3cce67\r\n",
      "APPDIR=$WORKSPACE/object_detection_flutter $WORKSPACE/tflite_ios_bisect.sh\r\n",
      "\r\n",
      "# Now checkout the parent of the breaking commit and build again. Should succeed.\r\n",
      "git checkout 3e9fccb0cb9a5f55a4e67e3011c20ff31c3cce67~\r\n",
      "APPDIR=$WORKSPACE/object_detection_flutter $WORKSPACE/tflite_ios_bisect.sh\r\n",
      "```\r\n",
      "\r\n",
      "You can also run `tflite_ios_bisect.sh` with `git bisect` as documented at the top of the file. Theoretically you should land on the same breaking commit that I've indicated above...\r\n",
      "\r\n",
      "**Other info / logs**\r\n",
      "\r\n",
      "Relevant discussion at `tflite_flutter_plugin`: https://github.com/am15h/tflite_flutter_plugin/issues/64\n",
      "issue labels - \n",
      "TF 2.5\n",
      "comp:lite\n",
      "regression issue\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Remove 'Z' because it's a local time\n",
      "issue body -  Unless I'm mistaken, the 'Z' indicates 0-offset UTC time. Since this is using the local time, the Z is incorrect/misleading.\n",
      "issue labels - \n",
      "cla: yes\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  [Cherrypick:r2.4] Install bazelisk in `docker_cpu_pip.sh`\n",
      "issue body -  Since caller scripts use Bazelisk, the docker job is broken if we want to use a bazel that is no longer the default one.\n",
      "\n",
      "PiperOrigin-RevId: 351891433\n",
      "Change-Id: I6caf0b5940934a3a90737f7fae288c42953a23f1\n",
      "issue labels - \n",
      "cla: yes\n",
      "\n",
      "\n",
      "issue title -  Move schema version into micro_interpreter.h\n",
      "issue body -  This is a first (somewhat non-intuitive) step towards being able to use clang as part of the github CI system.\r\n",
      "\r\n",
      "The benefits of this refactor are that we avoid a dependency into tensorflow/core which reduces the number of files that need to be downloaded as part of a bazel build from a TFLM CI docker image.\r\n",
      "\r\n",
      "The tflite schema version has been unchanged since at-least Oct 2018 (when tflite was moved out of tensorflow/contrib).\r\n",
      "\r\n",
      "Progress towards #46465\r\n",
      "Progress towards http://b/177672856\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  cudart64_101.dll not found\n",
      "issue body -  <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "- TensorFlow installed from (source or binary): source\r\n",
      "- TensorFlow version: 2.3\r\n",
      "- Python version: 3.8.5\r\n",
      "- Installed using virtualenv? pip? conda?: Conda\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "- GCC/Compiler version (if compiling from source): \r\n",
      "- CUDA/cuDNN version: 10.2 and 7.6.5\r\n",
      "- GPU model and memory: RTX 2070 Super\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "I am trying to install TensorFlow and be able to use it on my computer. I am only writing \"import tensorflow\" in my python file and getting several errors. I am using conda and created a new virtual environment and downloaded tensorflow and tensorboard. I also downloaded the 10.1 file and put it in my bin folder but still getting the same error.\r\n",
      "\r\n",
      "**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n",
      "python\r\n",
      "import tensorflow\r\n",
      "\r\n",
      "**Any other info / logs**\r\n",
      "Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n",
      "Here is the error that I am currently getting:\r\n",
      " tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\r\n",
      "2021-01-14 14:04:25.725481: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.3\n",
      "stalled\n",
      "stat:awaiting response\n",
      "subtype:windows\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  fix offset datatype for philox random kernel\n",
      "issue body -  The offset parameter in of one of \"FillPhiloxRandomKernel\" template specialization as well as the \"params_i\" in ScatterOpCustomKernel, data types have been set to int32. This is troublesome for tensors of size bigger than 2^32. The issue is significant since initialization of many layers by default end up in this particular implementation and as a result, we cannot have a Dense layer larger than 8GB of data. Simplest replication of the issue:\r\n",
      "`import tensorflow as tf;\r\n",
      "tf.random.uniform(shape=[256, 8493466], maxval=3, dtype=tf.float32, seed=10)`\r\n",
      "\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:core\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  enable bazel builds on CI.\n",
      "issue body -  \n",
      "issue labels - \n",
      "awaiting review\n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  [Cherrypick:r2.4] Pass cc_opt_flags to host_copt\n",
      "issue body -  \n",
      "issue labels - \n",
      "cla: yes\n",
      "\n",
      "\n",
      "issue title -  Blas SGEMM launch failed : m=25600, n=64, k=64 [Op:Conv2D] when building model and restoring weights\n",
      "issue body -  \r\n",
      "### System information\r\n",
      "\r\n",
      "-   **Have I written custom code (as opposed to using a stock example script\r\n",
      "    provided in TensorFlow)**:\r\n",
      "-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 20.04\r\n",
      "-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n",
      "    happens on a mobile device**:\r\n",
      "-   **TensorFlow installed from (source or binary)**: binary\r\n",
      "-   **TensorFlow version (use command below)**: tf-nightly-gpu 2.4.0-dev20201016\r\n",
      "-   **Python version**: 3.8.5\r\n",
      "-   **Bazel version (if compiling from source)**:\r\n",
      "-   **GCC/Compiler version (if compiling from source)**:\r\n",
      "-   **CUDA/cuDNN version**:  Cuda = 11.2 \r\n",
      "-   **GPU model and memory**: GeForce RTX 2060 6000MB\r\n",
      "-   **Exact command to reproduce**: \r\n",
      "\r\n",
      "num_classes = 1\r\n",
      "pipeline_config = 'object_detection/configs/tf2/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.config'\r\n",
      "checkpoint_path = 'object_detection/test_data/checkpoint/ckpt-0'\r\n",
      "\r\n",
      "\r\n",
      "configs = config_util.get_configs_from_pipeline_file(pipeline_config)\r\n",
      "model_config = configs['model']\r\n",
      "model_config.ssd.num_classes = num_classes\r\n",
      "model_config.ssd.freeze_batchnorm = True\r\n",
      "detection_model = model_builder.build(\r\n",
      "      model_config=model_config, is_training=True)\r\n",
      "\r\n",
      "fake_box_predictor = tf.compat.v2.train.Checkpoint(\r\n",
      "    _base_tower_layers_for_heads=detection_model._box_predictor._base_tower_layers_for_heads,\r\n",
      "    # _prediction_heads=detection_model._box_predictor._prediction_heads,\r\n",
      "    #    (i.e., the classification head that we *will not* restore)\r\n",
      "    _box_prediction_head=detection_model._box_predictor._box_prediction_head,\r\n",
      "    )\r\n",
      "fake_model = tf.compat.v2.train.Checkpoint(\r\n",
      "          _feature_extractor=detection_model._feature_extractor,\r\n",
      "          _box_predictor=fake_box_predictor)\r\n",
      "ckpt = tf.compat.v2.train.Checkpoint(model=fake_model)\r\n",
      "ckpt.restore(checkpoint_path).expect_partial()\r\n",
      "\r\n",
      "image, shapes = detection_model.preprocess(tf.zeros([1, 640, 640, 3]))\r\n",
      "prediction_dict = detection_model.predict(image, shapes)\r\n",
      "_ = detection_model.postprocess(prediction_dict, shapes)\r\n",
      "\r\n",
      "\r\n",
      "It fails ath the second last line\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "stat:awaiting response\n",
      "subtype: ubuntu/linux\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  Error when trying to save transformer model to saved model format.\n",
      "issue body -  Hello,\r\n",
      "\r\n",
      "I am following transformer example below\r\n",
      "\r\n",
      "https://www.tensorflow.org/tutorials/text/transformer\r\n",
      "\r\n",
      "Everything is working as expected and saving checkpoint as well, and I want to now save this to SavedModel format using \"transformer.save()\"  but it is throwing the below error.\r\n",
      "\r\n",
      "**TypeError: call() missing 4 required positional arguments: 'tar', 'enc_padding_mask', 'look_ahead_mask', and 'dec_padding_mask'**\r\n",
      "\r\n",
      "TypeError                                 Traceback (most recent call last)\r\n",
      "<ipython-input-107-92d91001b9bf> in <module>\r\n",
      "----> 1 transformer.save('./saved_model/')\r\n",
      "\r\n",
      "~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py in save(self, filepath, overwrite, include_optimizer, save_format, signatures, options)\r\n",
      "   1976     ```\r\n",
      "   1977     \"\"\"\r\n",
      "-> 1978     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\r\n",
      "   1979                     signatures, options)\r\n",
      "   1980 \r\n",
      "\r\n",
      "~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/saving/save.py in save_model(model, filepath, overwrite, include_optimizer, save_format, signatures, options)\r\n",
      "    131         model, filepath, overwrite, include_optimizer)\r\n",
      "    132   else:\r\n",
      "--> 133     saved_model_save.save(model, filepath, overwrite, include_optimizer,\r\n",
      "    134                           signatures, options)\r\n",
      "    135 \r\n",
      "\r\n",
      "~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/save.py in save(model, filepath, overwrite, include_optimizer, signatures, options)\r\n",
      "     78     # we use the default replica context here.\r\n",
      "     79     with distribution_strategy_context._get_default_replica_context():  # pylint: disable=protected-access\r\n",
      "---> 80       save_lib.save(model, filepath, signatures, options)\r\n",
      "     81 \r\n",
      "     82   if not include_optimizer:\r\n",
      "\r\n",
      "~/anaconda3/lib/python3.8/site-packages/tensorflow/python/saved_model/save.py in save(obj, export_dir, signatures, options)\r\n",
      "    973   meta_graph_def = saved_model.meta_graphs.add()\r\n",
      "    974 \r\n",
      "--> 975   _, exported_graph, object_saver, asset_info = _build_meta_graph(\r\n",
      "    976       obj, export_dir, signatures, options, meta_graph_def)\r\n",
      "    977   saved_model.saved_model_schema_version = constants.SAVED_MODEL_SCHEMA_VERSION\r\n",
      "\r\n",
      "~/anaconda3/lib/python3.8/site-packages/tensorflow/python/saved_model/save.py in _build_meta_graph(obj, export_dir, signatures, options, meta_graph_def)\r\n",
      "   1044   checkpoint_graph_view = _AugmentedGraphView(obj)\r\n",
      "   1045   if signatures is None:\r\n",
      "-> 1046     signatures = signature_serialization.find_function_to_export(\r\n",
      "   1047         checkpoint_graph_view)\r\n",
      "   1048 \r\n",
      "\r\n",
      "~/anaconda3/lib/python3.8/site-packages/tensorflow/python/saved_model/signature_serialization.py in find_function_to_export(saveable_view)\r\n",
      "     73   # If the user did not specify signatures, check the root object for a function\r\n",
      "     74   # that can be made into a signature.\r\n",
      "---> 75   functions = saveable_view.list_functions(saveable_view.root)\r\n",
      "     76   signature = functions.get(DEFAULT_SIGNATURE_ATTR, None)\r\n",
      "     77   if signature is not None:\r\n",
      "\r\n",
      "~/anaconda3/lib/python3.8/site-packages/tensorflow/python/saved_model/save.py in list_functions(self, obj, extra_functions)\r\n",
      "    142     obj_functions = self._functions.get(obj, None)\r\n",
      "    143     if obj_functions is None:\r\n",
      "--> 144       obj_functions = obj._list_functions_for_serialization(  # pylint: disable=protected-access\r\n",
      "    145           self._serialization_cache)\r\n",
      "    146       self._functions[obj] = obj_functions\r\n",
      "\r\n",
      "~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py in _list_functions_for_serialization(self, serialization_cache)\r\n",
      "   2587     self.test_function = None\r\n",
      "   2588     self.predict_function = None\r\n",
      "-> 2589     functions = super(\r\n",
      "   2590         Model, self)._list_functions_for_serialization(serialization_cache)\r\n",
      "   2591     self.train_function = train_function\r\n",
      "\r\n",
      "~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py in _list_functions_for_serialization(self, serialization_cache)\r\n",
      "   3016 \r\n",
      "   3017   def _list_functions_for_serialization(self, serialization_cache):\r\n",
      "-> 3018     return (self._trackable_saved_model_saver\r\n",
      "   3019             .list_functions_for_serialization(serialization_cache))\r\n",
      "   3020 \r\n",
      "\r\n",
      "~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/base_serialization.py in list_functions_for_serialization(self, serialization_cache)\r\n",
      "     85         `ConcreteFunction`.\r\n",
      "     86     \"\"\"\r\n",
      "---> 87     fns = self.functions_to_serialize(serialization_cache)\r\n",
      "     88 \r\n",
      "     89     # The parent AutoTrackable class saves all user-defined tf.functions, and\r\n",
      "\r\n",
      "~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py in functions_to_serialize(self, serialization_cache)\r\n",
      "     76 \r\n",
      "     77   def functions_to_serialize(self, serialization_cache):\r\n",
      "---> 78     return (self._get_serialized_attributes(\r\n",
      "     79         serialization_cache).functions_to_serialize)\r\n",
      "     80 \r\n",
      "\r\n",
      "~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py in _get_serialized_attributes(self, serialization_cache)\r\n",
      "     92       return serialized_attr\r\n",
      "     93 \r\n",
      "---> 94     object_dict, function_dict = self._get_serialized_attributes_internal(\r\n",
      "     95         serialization_cache)\r\n",
      "     96 \r\n",
      "\r\n",
      "~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/model_serialization.py in _get_serialized_attributes_internal(self, serialization_cache)\r\n",
      "     49     # cache (i.e. this is the root level object).\r\n",
      "     50     if len(serialization_cache[constants.KERAS_CACHE_KEY]) == 1:\r\n",
      "---> 51       default_signature = save_impl.default_save_signature(self.obj)\r\n",
      "     52 \r\n",
      "     53     # Other than the default signature function, all other attributes match with\r\n",
      "\r\n",
      "~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/save_impl.py in default_save_signature(layer)\r\n",
      "    203   original_losses = _reset_layer_losses(layer)\r\n",
      "    204   fn = saving_utils.trace_model_call(layer)\r\n",
      "--> 205   fn.get_concrete_function()\r\n",
      "    206   _restore_layer_losses(original_losses)\r\n",
      "    207   return fn\r\n",
      "\r\n",
      "~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in get_concrete_function(self, *args, **kwargs)\r\n",
      "   1165       ValueError: if this object has not yet been called on concrete values.\r\n",
      "   1166     \"\"\"\r\n",
      "-> 1167     concrete = self._get_concrete_function_garbage_collected(*args, **kwargs)\r\n",
      "   1168     concrete._garbage_collector.release()  # pylint: disable=protected-access\r\n",
      "   1169     return concrete\r\n",
      "\r\n",
      "~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in _get_concrete_function_garbage_collected(self, *args, **kwargs)\r\n",
      "   1071       if self._stateful_fn is None:\r\n",
      "   1072         initializers = []\r\n",
      "-> 1073         self._initialize(args, kwargs, add_initializers_to=initializers)\r\n",
      "   1074         self._initialize_uninitialized_variables(initializers)\r\n",
      "   1075 \r\n",
      "\r\n",
      "~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)\r\n",
      "    694     self._graph_deleter = FunctionDeleter(self._lifted_initializer_graph)\r\n",
      "    695     self._concrete_stateful_fn = (\r\n",
      "--> 696         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n",
      "    697             *args, **kwds))\r\n",
      "    698 \r\n",
      "\r\n",
      "~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\r\n",
      "   2853       args, kwargs = None, None\r\n",
      "   2854     with self._lock:\r\n",
      "-> 2855       graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n",
      "   2856     return graph_function\r\n",
      "   2857 \r\n",
      "\r\n",
      "~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)\r\n",
      "   3211 \r\n",
      "   3212       self._function_cache.missed.add(call_context_key)\r\n",
      "-> 3213       graph_function = self._create_graph_function(args, kwargs)\r\n",
      "   3214       self._function_cache.primary[cache_key] = graph_function\r\n",
      "   3215       return graph_function, args, kwargs\r\n",
      "\r\n",
      "~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n",
      "   3063     arg_names = base_arg_names + missing_arg_names\r\n",
      "   3064     graph_function = ConcreteFunction(\r\n",
      "-> 3065         func_graph_module.func_graph_from_py_func(\r\n",
      "   3066             self._name,\r\n",
      "   3067             self._python_function,\r\n",
      "\r\n",
      "~/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n",
      "    984         _, original_func = tf_decorator.unwrap(python_func)\r\n",
      "    985 \r\n",
      "--> 986       func_outputs = python_func(*func_args, **func_kwargs)\r\n",
      "    987 \r\n",
      "    988       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n",
      "\r\n",
      "~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)\r\n",
      "    598         # __wrapped__ allows AutoGraph to swap in a converted function. We give\r\n",
      "    599         # the function a weak reference to itself to avoid a reference cycle.\r\n",
      "--> 600         return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n",
      "    601     weak_wrapped_fn = weakref.ref(wrapped_fn)\r\n",
      "    602 \r\n",
      "\r\n",
      "~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/saving/saving_utils.py in _wrapped_model(*args)\r\n",
      "    132     with base_layer_utils.call_context().enter(\r\n",
      "    133         model, inputs=inputs, build_graph=False, training=False, saving=True):\r\n",
      "--> 134       outputs = model(inputs, training=False)\r\n",
      "    135 \r\n",
      "    136     # Outputs always has to be a flat dict.\r\n",
      "\r\n",
      "~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)\r\n",
      "    983 \r\n",
      "    984         with ops.enable_auto_cast_variables(self._compute_dtype_object):\r\n",
      "--> 985           outputs = call_fn(inputs, *args, **kwargs)\r\n",
      "    986 \r\n",
      "    987         if self._activity_regularizer:\r\n",
      "\r\n",
      "~/anaconda3/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py in wrapper(*args, **kwargs)\r\n",
      "    300   def wrapper(*args, **kwargs):\r\n",
      "    301     with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):\r\n",
      "--> 302       return func(*args, **kwargs)\r\n",
      "    303 \r\n",
      "    304   if inspect.isfunction(func) or inspect.ismethod(func):\r\n",
      "\r\n",
      "TypeError: call() missing 4 required positional arguments: 'tar', 'enc_padding_mask', 'look_ahead_mask', and 'dec_padding_mask'\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "comp:model\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Error when load model with layer tensorflow.keras.layers.experimental.preprocessing.Normalization()\n",
      "issue body -  Hello,\r\n",
      "I have an issue with tensorflow.keras.layers.experimental.preprocessing.Normalization(). \r\n",
      "I can't load my model when I use it. \r\n",
      "\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I custom un example script provided TensorFlow code\r\n",
      "- Linux Ubuntu 20.04\r\n",
      "- TensorFlow installed from binary\r\n",
      "\r\n",
      "```\r\n",
      "== env ==========================================================\r\n",
      "LD_LIBRARY_PATH is unset\r\n",
      "DYLD_LIBRARY_PATH is unset\r\n",
      "\r\n",
      "== nvidia-smi ===================================================\r\n",
      "Thu Jan 14 12:47:29 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 460.27.04    Driver Version: 460.27.04    CUDA Version: 11.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce GTX 166...  On   | 00000000:08:00.0  On |                  N/A |\r\n",
      "|  0%   49C    P8    12W / 120W |    535MiB /  5941MiB |      7%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      1217      G   /usr/lib/xorg/Xorg                346MiB |\r\n",
      "|    0   N/A  N/A      1943      G   /usr/bin/kwin_x11                  57MiB |\r\n",
      "|    0   N/A  N/A      1956      G   /usr/bin/plasmashell               72MiB |\r\n",
      "|    0   N/A  N/A      2367      G   ...gAAAAAAAAA --shared-files       10MiB |\r\n",
      "|    0   N/A  N/A      3078      G   ...AAAAAAAA== --shared-files       41MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "\r\n",
      "== cuda libs  ===================================================\r\n",
      "/usr/local/cuda-11.0/targets/x86_64-linux/lib/libcudart.so.11.0.221\r\n",
      "/usr/local/cuda-11.0/targets/x86_64-linux/lib/libcudart_static.a\r\n",
      "/usr/local/cuda-11.0/doc/man/man7/libcudart.7\r\n",
      "/usr/local/cuda-11.0/doc/man/man7/libcudart.so.7\r\n",
      "\r\n",
      "== tensorflow installed from info ==================\r\n",
      "Name: tensorflow\r\n",
      "Version: 2.4.0\r\n",
      "Summary: TensorFlow is an open source machine learning framework for everyone.\r\n",
      "Home-page: https://www.tensorflow.org/\r\n",
      "Author-email: packages@tensorflow.org\r\n",
      "License: Apache 2.0\r\n",
      "Location: /home/user/works/test-libs/venv/lib/python3.8/site-packages\r\n",
      "Required-by: \r\n",
      "\r\n",
      "== python version  ==============================================\r\n",
      "(major, minor, micro, releaselevel, serial)\r\n",
      "(3, 8, 5, 'final', 0)\r\n",
      "\r\n",
      "== bazel version  ===============================================\r\n",
      "```\r\n",
      "\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "I can't load my model when I use it. \r\n",
      "\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "I would like to be able to read a model with this layer. I know it's an experimental layer but it should work.  If you have no idea how to solve it, can you advise me of an alternative?\r\n",
      "\r\n",
      "Thank you :-)\r\n",
      "\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "```python\r\n",
      "import numpy as np\r\n",
      "import pandas as pd\r\n",
      "import tensorflow as tf\r\n",
      "from tensorflow import keras\r\n",
      "from tensorflow.keras import layers\r\n",
      "from tensorflow.keras.layers.experimental import preprocessing\r\n",
      "\r\n",
      "\r\n",
      "data = [{\"a\": 20.0, \"b\": 5.0, \"c\": 0.2972786923202068}, {\"a\": 20.0, \"b\": 10.0, \"c\": 0.10673704592967688}]\r\n",
      "train_dataset = pd.DataFrame.from_dict(data)\r\n",
      "\r\n",
      "train_features = train_dataset.copy()\r\n",
      "train_labels = train_features.pop('c')\r\n",
      "\r\n",
      "normalizer = preprocessing.Normalization()\r\n",
      "normalizer.adapt(np.array(train_features))\r\n",
      "\r\n",
      "dnn_model = keras.Sequential([\r\n",
      "    normalizer,\r\n",
      "    layers.Dense(64, activation='relu'),\r\n",
      "    layers.Dense(64, activation='relu'),\r\n",
      "    layers.Dense(1)\r\n",
      "])\r\n",
      "\r\n",
      "dnn_model.compile(loss='mean_absolute_error', optimizer=tf.keras.optimizers.Adam(0.001))\r\n",
      "dnn_model.save('file.h5')\r\n",
      "\r\n",
      "\r\n",
      "# ValueError: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.\r\n",
      "new_model = tf.keras.models.load_model('file.h5')\r\n",
      "```\r\n",
      "\r\n",
      "\r\n",
      "**logs**\r\n",
      "```\r\n",
      "2021-01-14 12:27:41.950145: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "2021-01-14 12:27:42.758932: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n",
      "2021-01-14 12:27:42.759555: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n",
      "2021-01-14 12:27:42.796836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n",
      "2021-01-14 12:27:42.797239: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\n",
      "pciBusID: 0000:08:00.0 name: GeForce GTX 1660 Ti computeCapability: 7.5\r\n",
      "coreClock: 1.8GHz coreCount: 24 deviceMemorySize: 5.80GiB deviceMemoryBandwidth: 268.26GiB/s\r\n",
      "2021-01-14 12:27:42.797264: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "2021-01-14 12:27:42.798974: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n",
      "2021-01-14 12:27:42.799021: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n",
      "2021-01-14 12:27:42.799743: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n",
      "2021-01-14 12:27:42.799901: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n",
      "2021-01-14 12:27:42.801635: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n",
      "2021-01-14 12:27:42.802036: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n",
      "2021-01-14 12:27:42.802131: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n",
      "2021-01-14 12:27:42.802239: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n",
      "2021-01-14 12:27:42.802663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n",
      "2021-01-14 12:27:42.802998: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n",
      "2021-01-14 12:27:42.803643: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n",
      "2021-01-14 12:27:42.803719: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n",
      "2021-01-14 12:27:42.804062: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\n",
      "pciBusID: 0000:08:00.0 name: GeForce GTX 1660 Ti computeCapability: 7.5\r\n",
      "coreClock: 1.8GHz coreCount: 24 deviceMemorySize: 5.80GiB deviceMemoryBandwidth: 268.26GiB/s\r\n",
      "2021-01-14 12:27:42.804079: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "2021-01-14 12:27:42.804092: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n",
      "2021-01-14 12:27:42.804101: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n",
      "2021-01-14 12:27:42.804110: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n",
      "2021-01-14 12:27:42.804119: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n",
      "2021-01-14 12:27:42.804128: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n",
      "2021-01-14 12:27:42.804136: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n",
      "2021-01-14 12:27:42.804145: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n",
      "2021-01-14 12:27:42.804191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n",
      "2021-01-14 12:27:42.804558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n",
      "2021-01-14 12:27:42.804884: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n",
      "2021-01-14 12:27:42.804907: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "2021-01-14 12:27:43.183218: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n",
      "2021-01-14 12:27:43.183266: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \r\n",
      "2021-01-14 12:27:43.183273: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \r\n",
      "2021-01-14 12:27:43.183488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n",
      "2021-01-14 12:27:43.183873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n",
      "2021-01-14 12:27:43.184214: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n",
      "2021-01-14 12:27:43.184529: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4868 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1660 Ti, pci bus id: 0000:08:00.0, compute capability: 7.5)\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py\", line 2873, in zeros\r\n",
      "    shape = constant_op._tensor_shape_tensor_conversion_function(\r\n",
      "  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\", line 355, in _tensor_shape_tensor_conversion_function\r\n",
      "    raise ValueError(\r\n",
      "ValueError: Cannot convert a partially known TensorShape to a Tensor: (None,)\r\n",
      "\r\n",
      "During handling of the above exception, another exception occurred:\r\n",
      "\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"save.py\", line 30, in <module>\r\n",
      "    new_model = tf.keras.models.load_model('file.h5')\r\n",
      "  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/keras/saving/save.py\", line 206, in load_model\r\n",
      "    return hdf5_format.load_model_from_hdf5(filepath, custom_objects,\r\n",
      "  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/keras/saving/hdf5_format.py\", line 183, in load_model_from_hdf5\r\n",
      "    model = model_config_lib.model_from_config(model_config,\r\n",
      "  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/keras/saving/model_config.py\", line 64, in model_from_config\r\n",
      "    return deserialize(config, custom_objects=custom_objects)\r\n",
      "  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/keras/layers/serialization.py\", line 173, in deserialize\r\n",
      "    return generic_utils.deserialize_keras_object(\r\n",
      "  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/keras/utils/generic_utils.py\", line 354, in deserialize_keras_object\r\n",
      "    return cls.from_config(\r\n",
      "  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py\", line 494, in from_config\r\n",
      "    model.add(layer)\r\n",
      "  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py\", line 517, in _method_wrapper\r\n",
      "    result = method(self, *args, **kwargs)\r\n",
      "  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py\", line 223, in add\r\n",
      "    output_tensor = layer(self.outputs[0])\r\n",
      "  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 951, in __call__\r\n",
      "    return self._functional_construction_call(inputs, args, kwargs,\r\n",
      "  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 1090, in _functional_construction_call\r\n",
      "    outputs = self._keras_tensor_symbolic_call(\r\n",
      "  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 822, in _keras_tensor_symbolic_call\r\n",
      "    return self._infer_output_signature(inputs, args, kwargs, input_masks)\r\n",
      "  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 862, in _infer_output_signature\r\n",
      "    self._maybe_build(inputs)\r\n",
      "  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 2710, in _maybe_build\r\n",
      "    self.build(input_shapes)  # pylint:disable=not-callable\r\n",
      "  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/keras/layers/preprocessing/normalization.py\", line 174, in build\r\n",
      "    self.mean = self._add_state_variable(\r\n",
      "  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_preprocessing_layer.py\", line 110, in _add_state_variable\r\n",
      "    weight = self.add_weight(\r\n",
      "  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 623, in add_weight\r\n",
      "    variable = self._add_variable_with_custom_getter(\r\n",
      "  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py\", line 805, in _add_variable_with_custom_getter\r\n",
      "    new_variable = getter(\r\n",
      "  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer_utils.py\", line 130, in make_variable\r\n",
      "    return tf_variables.VariableV1(\r\n",
      "  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/ops/variables.py\", line 260, in __call__\r\n",
      "    return cls._variable_v1_call(*args, **kwargs)\r\n",
      "  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/ops/variables.py\", line 206, in _variable_v1_call\r\n",
      "    return previous_getter(\r\n",
      "  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/ops/variables.py\", line 199, in <lambda>\r\n",
      "    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\r\n",
      "  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/ops/variable_scope.py\", line 2604, in default_variable_creator\r\n",
      "    return resource_variable_ops.ResourceVariable(\r\n",
      "  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/ops/variables.py\", line 264, in __call__\r\n",
      "    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n",
      "  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 1574, in __init__\r\n",
      "    self._init_from_args(\r\n",
      "  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 1712, in _init_from_args\r\n",
      "    initial_value = initial_value()\r\n",
      "  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/ops/init_ops.py\", line 114, in __call__\r\n",
      "    return array_ops.zeros(shape, dtype)\r\n",
      "  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\r\n",
      "    return target(*args, **kwargs)\r\n",
      "  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py\", line 2819, in wrapped\r\n",
      "    tensor = fun(*args, **kwargs)\r\n",
      "  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py\", line 2877, in zeros\r\n",
      "    shape = ops.convert_to_tensor(shape, dtype=dtypes.int32)\r\n",
      "  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/profiler/trace.py\", line 163, in wrapped\r\n",
      "    return func(*args, **kwargs)\r\n",
      "  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 1540, in convert_to_tensor\r\n",
      "    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n",
      "  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\", line 339, in _constant_tensor_conversion_function\r\n",
      "    return constant(v, dtype=dtype, name=name)\r\n",
      "  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\", line 264, in constant\r\n",
      "    return _constant_impl(value, dtype, shape, name, verify_shape=False,\r\n",
      "  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\", line 276, in _constant_impl\r\n",
      "    return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\r\n",
      "  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\", line 301, in _constant_eager_impl\r\n",
      "    t = convert_to_eager_tensor(value, ctx, dtype)\r\n",
      "  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\", line 98, in convert_to_eager_tensor\r\n",
      "    return ops.EagerTensor(value, ctx.device_name, dtype)\r\n",
      "ValueError: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.\r\n",
      "\r\n",
      "```\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:keras\n",
      "stat:awaiting tensorflower\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  [RNN] models cannot run in TFLite with delegates\n",
      "issue body -  **System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Qualcomm Snapdragon 865\r\n",
      "- TensorFlow installed from (source or binary): Source \r\n",
      "- TensorFlow version (or github SHA if from source): Nightly\r\n",
      "\r\n",
      "\r\n",
      "**Command used to run the converter or code if you’re using the Python API**\r\n",
      "If possible, please share a link to Colab/Jupyter/any notebook.\r\n",
      "\r\n",
      "```\r\n",
      "tf_model = tf.keras.models.load_model('./my_model.h5')\r\n",
      "\r\n",
      "for i in range(4):\r\n",
      "    tf_model.inputs[i].shape._dims[0] = tf_python.framework.tensor_shape.Dimension(1)\r\n",
      "\r\n",
      "model_func = tf.function(lambda a: tf_model(a))\r\n",
      "concrete_func = model_func.get_concrete_function([tf.TensorSpec(tf_model.inputs[0].shape, tf_model.inputs[0].dtype), tf.TensorSpec(tf_model.inputs[1].shape, tf_model.inputs[1].dtype), tf.TensorSpec(tf_model.inputs[2].shape, tf_model.inputs[2].dtype), tf.TensorSpec(tf_model.inputs[3].shape, tf_model.inputs[3].dtype)])\r\n",
      "\r\n",
      "tf_out = concrete_func([input_1, gru0_1_h_in, gru1_1_h_in, out_quats_gru1_h_in])\r\n",
      "converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\r\n",
      "tflite_model = converter.convert()\r\n",
      "```\r\n",
      "\r\n",
      "**The output from the converter invocation**\r\n",
      "\r\n",
      "```\r\n",
      "2021-01-14 11:16:28.325520: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:933] Optimization results for grappler item: graph_to_optimize\r\n",
      "  function_optimizer: Graph size after: 389 nodes (195), 590 edges (225), time = 6.75ms.\r\n",
      "  function_optimizer: Graph size after: 389 nodes (0), 590 edges (0), time = 3.64ms.\r\n",
      "Optimization results for grappler item: while_body_2339\r\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.004ms.\r\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n",
      "Optimization results for grappler item: while_cond_2338\r\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.004ms.\r\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n",
      "Optimization results for grappler item: while_cond_2708\r\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.003ms.\r\n",
      "  function_optimizer: function_optimizer did nothing. time = 0ms.\r\n",
      "Optimization results for grappler item: while_body_2709\r\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.003ms.\r\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n",
      "Optimization results for grappler item: while_body_1969\r\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.003ms.\r\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n",
      "Optimization results for grappler item: while_cond_1968\r\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.003ms.\r\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.002ms.\r\n",
      "\r\n",
      "2021-01-14 11:16:28.851727: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:332] Ignored output_format.\r\n",
      "2021-01-14 11:16:28.851756: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:335] Ignored drop_control_dependency.\r\n",
      "2021-01-14 11:16:28.895692: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:194] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\r\n",
      "\r\n",
      "```\r\n",
      "\r\n",
      "**Also, please include a link to the saved model or GraphDef**\r\n",
      "\r\n",
      "```\r\n",
      "My Keras model contains a GRU layer, here is what the Keras layer converts to in TFLite:\r\n",
      "\r\n",
      "![Screenshot from 2021-01-14 06-12-45](https://user-images.githubusercontent.com/67419721/104586228-348e6480-5633-11eb-9558-047cd559280b.png)\r\n",
      "\r\n",
      "\r\n",
      "```\r\n",
      "\r\n",
      "**Failure details**\r\n",
      "Model conversion was successful. and runs on CPU. However, when I try to initialize an OpenCL delegate, I get the following run time error:\r\n",
      "\r\n",
      "ERROR: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.\r\n",
      "interpreter->ModifyGraphWithDelegate failed\r\n",
      "\r\n",
      "I suspect the problem is with those `While` ops generated by the converter. My GRU layer is stateless, where I manually manage the layer state (tf.keras.layers.GRU is defined with arguments: return_sequences=True, stateful=False, unroll=False. It is called with one of the model's inputs passed to the initial_state argument), and with the sequence dimension of 1. Is it possible to tell the TFLite converter to generate a graph that doesn't contain a while loop? Since in my situation I don't actually need to have a loop in my GRU.\r\n",
      "\r\n",
      "**RNN conversion support**\r\n",
      "If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.\r\n",
      "\r\n",
      "**Any other info / logs**\r\n",
      "\r\n",
      "Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.5\n",
      "TFLiteGpuDelegate\n",
      "comp:lite\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  how to restore  part of the model,  then save the whole model after training?\n",
      "issue body -  I want to finetine a base model using a new dataset.\r\n",
      "The model have two part: part1 + part2\r\n",
      "I only want to restore the variables of part1.\r\n",
      "\r\n",
      "1.restore the model using:\r\n",
      "    saver = tf.train.Saver(variables_of_part1, max_to_keep=1000)\r\n",
      "2.Training\r\n",
      "3.Save the model\r\n",
      "But the new model saved lost the variables of part2.\r\n",
      "\r\n",
      "How can I realize \"restore part of a model, trainning, then save the whole model\"?\r\n",
      "\r\n",
      "My tensorflow version is 1.10.\n",
      "issue labels - \n",
      "comp:apis\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  writer_test of serialization failed for squeeznet\n",
      "issue body -  **System information**\r\n",
      "- OS Platform and Distribution  Linux Ubuntu 16.04\r\n",
      "- TensorFlow installed from source\r\n",
      "- TensorFlow version (use command below):\r\n",
      "- Python version: Python 3.5.2\r\n",
      "- Bazel version (if compiling from source): Build label: 3.7.2\r\n",
      "- GCC/Compiler version (if compiling from source) gcc (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "\r\n",
      "- download squeezenet (squeezenet.tflit)\r\n",
      "- build write test of serialization : bazel build -c opt //tensorflow/lite/tools/serialization:writer_test\r\n",
      "-  bazel-bin/tensorflow/lite/tools/serialization/writer_test [model folder]/squeezenet.tflite\r\n",
      "```\r\n",
      "ERROR: tensorflow/lite/kernels/reshape.cc:69 num_input_elements != num_output_elements (1001 != 1)\r\n",
      "ERROR: Node number 38 (RESHAPE) failed to prepare.\r\n",
      "\r\n",
      "AllocateTensors failed on the round-tripped model.\r\n",
      "```\r\n",
      "**Describe the expected behavior**\r\n",
      "pass write test\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "- download squeezenet (squeezenet.tflit)\r\n",
      "- build write test of serialization : bazel build -c opt //tensorflow/lite/tools/serialization:writer_test\r\n",
      "-  bazel-bin/tensorflow/lite/tools/serialization/writer_test [model folder]/squeezenet.tflite\r\n",
      "\r\n",
      "**Other info / logs** Include any logs or source code that would be helpful to\r\n",
      "```\r\n",
      "ERROR: tensorflow/lite/kernels/reshape.cc:69 num_input_elements != num_output_elements (1001 != 1)\r\n",
      "ERROR: Node number 38 (RESHAPE) failed to prepare.\r\n",
      "\r\n",
      "AllocateTensors failed on the round-tripped model.\r\n",
      "```\r\n",
      "A proposal of fix is at: https://github.com/tensorflow/tensorflow/pull/46422 \r\n",
      "\n",
      "issue labels - \n",
      "comp:lite\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Fix AllocateTensors fails for  tflite squeezenet\n",
      "issue body -  **[Purpose]**\r\n",
      "\r\n",
      "Fix AllocateTensors fails for  tflite squeezenet\r\n",
      "\r\n",
      "**[How to reproduce]**\r\n",
      "1. download squeezenet (squeezenet.tflit)\r\n",
      "2. build write test of serialization : bazel build -c opt //tensorflow/lite/tools/serialization:writer_test\r\n",
      "3. bazel-bin/tensorflow/lite/tools/serialization/writer_test [model folder]/squeezenet.tflite\r\n",
      "\r\n",
      "**[Result]**\r\n",
      "```\r\n",
      "ERROR: tensorflow/lite/kernels/reshape.cc:69 num_input_elements != num_output_elements (1001 != 1)\r\n",
      "ERROR: Node number 38 (RESHAPE) failed to prepare.\r\n",
      "\r\n",
      "AllocateTensors failed on the round-tripped model.\r\n",
      "```\r\n",
      "**[Root cause]**\r\n",
      "https://drive.google.com/file/d/1aJGy2lvEkfAMApQV4NrY67KKc4OYFpe1/view?usp=sharing\r\n",
      "\r\n",
      "```\r\n",
      "// Check if the shape tensor is valid. Shapes should be int32 vectors.\r\n",
      "inline bool ShapeIsVector(TfLiteContext* context, TfLiteNode* node) {\r\n",
      "  const TfLiteTensor* shape = GetInput(context, node, kShapeTensor);\r\n",
      "  return (shape != nullptr && shape->dims->size == 1 &&\r\n",
      "          shape->type == kTfLiteInt32);\r\n",
      "}\r\n",
      "```\r\n",
      "Notice that the reshape module has Shape Tensor's shape = 2 x 1.\r\n",
      "Compared to mobilenet, where the Shape Tensor's shape = 2.  \r\n",
      "The check within **ShapeIsVector** only considers the case where dim of shape needs to be 1.\r\n",
      "Obviously, squeezenet fails to satisfy the condition: \r\n",
      "```\r\n",
      "shape->dims->size == 1\r\n",
      "\r\n",
      "```\r\n",
      "Hence it uses GetOutputShapeFromParam to get output shape.\r\n",
      "```\r\n",
      "TfLiteIntArray* GetOutputShape(TfLiteContext* context, TfLiteNode* node) {\r\n",
      "  if (NumInputs(node) == 2 && ShapeIsVector(context, node)) {\r\n",
      "    return GetOutputShapeFromTensor(context, node);\r\n",
      "  } else {\r\n",
      "    return GetOutputShapeFromParam(context, node);\r\n",
      "  }\r\n",
      "``` \r\n",
      "However we don't have **TfLiteReshapeParams** so issue happens.\r\n",
      "\r\n",
      "**[Proposal of fix]**\r\n",
      "According to the semantic of function **ShapeIsVector**, allow additional ones(tailed).\r\n",
      "Only think of the Shape Tensor **is NOT a vector** when **there are more than one \"non-one\" dimensions**.\r\n",
      "\r\n",
      "**[Result]**\r\n",
      "squeezenet.tflite is O.K (bug fix)\r\n",
      "mobilenet.tflit is O.K (have no impact to the model which originally pass) \r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "cla: no\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  LSTM doesnot work after tf_upgrade_v2 with keras.models.load_model, but tf.keras.models.load_model works\n",
      "issue body -  **System information**\r\n",
      "- Linux Ubuntu 16.04\r\n",
      "- TensorFlow installed from pip\r\n",
      "- TensorFlow version: v2.1.0-rc2-17-ge5bf8de 2.1.0\r\n",
      "- Keras version: 2.3.1\r\n",
      "- Python version: 3.6.12\r\n",
      "- CUDA/cuDNN version: 11.1/7\r\n",
      "\r\n",
      "At first, I have codes written by tensorflow v1 that can't run. so I use \r\n",
      "\r\n",
      "```shell\r\n",
      "tf_upgrade_v2 --infile tensorflow_backend.py --outfile tensorflow_backend.py\r\n",
      "```\r\n",
      "to  upgrade the keras .py file in xx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/backend/\r\n",
      "\r\n",
      "it works after upgrade, however, later I found it goes wrong when load LSTM model(.h5)\r\n",
      "\r\n",
      "the LSTM model can be acessed [here](https://github.com/SingleBone/origin_models/blob/main/lstm0-sinewave_origin.h5)\r\n",
      "\r\n",
      "**the current behavior**\r\n",
      "when load a LSTM model with **keras.models.load_model**\r\n",
      "\r\n",
      "```python\r\n",
      "import keras\r\n",
      "model = keras.models.load_model('lstm0-sinewave_origin.h5')\r\n",
      "```\r\n",
      "\r\n",
      ", got RuntimeError\r\n",
      "```bash\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"<stdin>\", line 1, in <module>\r\n",
      "  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/saving.py\", line 492, in load_wrapper\r\n",
      "    return load_function(*args, **kwargs)\r\n",
      "  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/saving.py\", line 584, in load_model\r\n",
      "    model = _deserialize_model(h5dict, custom_objects, compile)\r\n",
      "  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/saving.py\", line 274, in _deserialize_model\r\n",
      "    model = model_from_config(model_config, custom_objects=custom_objects)\r\n",
      "  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/saving.py\", line 627, in model_from_config\r\n",
      "    return deserialize(config, custom_objects=custom_objects)\r\n",
      "  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/layers/__init__.py\", line 168, in deserialize\r\n",
      "    printable_module_name='layer')\r\n",
      "  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/utils/generic_utils.py\", line 147, in deserialize_keras_object\r\n",
      "    list(custom_objects.items())))\r\n",
      "  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/sequential.py\", line 302, in from_config\r\n",
      "    model.add(layer)\r\n",
      "  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/sequential.py\", line 166, in add\r\n",
      "    layer(x)\r\n",
      "  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/layers/recurrent.py\", line 541, in __call__\r\n",
      "    return super(RNN, self).__call__(inputs, **kwargs)\r\n",
      "  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 76, in symbolic_fn_wrapper\r\n",
      "    return func(*args, **kwargs)\r\n",
      "  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/base_layer.py\", line 463, in __call__\r\n",
      "    self.build(unpack_singleton(input_shapes))\r\n",
      "  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/layers/recurrent.py\", line 502, in build\r\n",
      "    self.cell.build(step_input_shape)\r\n",
      "  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/layers/recurrent.py\", line 1942, in build\r\n",
      "    constraint=self.bias_constraint)\r\n",
      "  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/base_layer.py\", line 282, in add_weight\r\n",
      "    constraint=constraint)\r\n",
      "  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 621, in variable\r\n",
      "    value, dtype=dtype, name=name, constraint=constraint)\r\n",
      "  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\", line 814, in variable\r\n",
      "    constraint=constraint)\r\n",
      "  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py\", line 260, in __call__\r\n",
      "    return cls._variable_v2_call(*args, **kwargs)\r\n",
      "  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py\", line 254, in _variable_v2_call\r\n",
      "    shape=shape)\r\n",
      "  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py\", line 235, in <lambda>\r\n",
      "    previous_getter = lambda **kws: default_variable_creator_v2(None, **kws)\r\n",
      "  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/variable_scope.py\", line 2645, in default_variable_creator_v2\r\n",
      "    shape=shape)\r\n",
      "  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py\", line 262, in __call__\r\n",
      "    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n",
      "  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py\", line 1411, in __init__\r\n",
      "    distribute_strategy=distribute_strategy)\r\n",
      "  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py\", line 1543, in _init_from_args\r\n",
      "    name=\"initial_value\", dtype=dtype)\r\n",
      "  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 1280, in convert_to_tensor\r\n",
      "    raise RuntimeError(\"Attempting to capture an EagerTensor without \"\r\n",
      "RuntimeError: Attempting to capture an EagerTensor without building a function.\r\n",
      "```\r\n",
      "\r\n",
      "**the expected behavior**\r\n",
      "when use **tensorflow.keras.models.load_model**\r\n",
      "\r\n",
      "```python\r\n",
      "import tensorflow\r\n",
      "model = tensorflow.keras.models.load_model('lstm0-sinewave_origin.h5')\r\n",
      "```\r\n",
      ", it works without error\r\n",
      "\r\n",
      "**the whole process to triger this issue**\r\n",
      "```bash\r\n",
      "(tensorflow) root@53125cc00c44:/data/origin_model# python\r\n",
      "Python 3.6.12 |Anaconda, Inc.| (default, Sep  8 2020, 23:10:56) \r\n",
      "[GCC 7.3.0] on linux\r\n",
      "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n",
      "# here is where it goes wrong\r\n",
      ">>> import keras\r\n",
      ">>> model = keras.models.load_model('lstm0-sinewave_origin.h5')\r\n",
      "WARNING:tensorflow:From /root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "If using Keras pass *_constraint arguments to layers.\r\n",
      "2021-01-14 07:36:41.711121: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n",
      "2021-01-14 07:36:41.728295: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\n",
      "pciBusID: 0000:02:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\n",
      "coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n",
      "2021-01-14 07:36:41.729095: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 1 with properties: \r\n",
      "pciBusID: 0000:84:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\n",
      "coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n",
      "2021-01-14 07:36:41.729486: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n",
      "2021-01-14 07:36:41.732935: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n",
      "2021-01-14 07:36:41.735810: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n",
      "2021-01-14 07:36:41.736327: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n",
      "2021-01-14 07:36:41.739862: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n",
      "2021-01-14 07:36:41.741934: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n",
      "2021-01-14 07:36:41.749515: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n",
      "2021-01-14 07:36:41.753192: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0, 1\r\n",
      "2021-01-14 07:36:41.753681: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n",
      "2021-01-14 07:36:41.772855: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2197465000 Hz\r\n",
      "2021-01-14 07:36:41.776310: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55b853a3c2f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n",
      "2021-01-14 07:36:41.776354: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n",
      "2021-01-14 07:36:42.013802: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55b853aa2730 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n",
      "2021-01-14 07:36:42.013851: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1\r\n",
      "2021-01-14 07:36:42.013862: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): GeForce GTX 1080 Ti, Compute Capability 6.1\r\n",
      "2021-01-14 07:36:42.015095: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\n",
      "pciBusID: 0000:02:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\n",
      "coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n",
      "2021-01-14 07:36:42.015643: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 1 with properties: \r\n",
      "pciBusID: 0000:84:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\n",
      "coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n",
      "2021-01-14 07:36:42.015709: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n",
      "2021-01-14 07:36:42.015745: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n",
      "2021-01-14 07:36:42.015784: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n",
      "2021-01-14 07:36:42.015809: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n",
      "2021-01-14 07:36:42.015829: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n",
      "2021-01-14 07:36:42.015849: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n",
      "2021-01-14 07:36:42.015883: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n",
      "2021-01-14 07:36:42.018395: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0, 1\r\n",
      "2021-01-14 07:36:42.018444: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n",
      "2021-01-14 07:36:42.020294: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n",
      "2021-01-14 07:36:42.020317: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 1 \r\n",
      "2021-01-14 07:36:42.020328: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N N \r\n",
      "2021-01-14 07:36:42.020337: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 1:   N N \r\n",
      "2021-01-14 07:36:42.022501: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10171 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1)\r\n",
      "2021-01-14 07:36:42.023440: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 2116 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:84:00.0, compute capability: 6.1)\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"<stdin>\", line 1, in <module>\r\n",
      "  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/saving.py\", line 492, in load_wrapper\r\n",
      "    return load_function(*args, **kwargs)\r\n",
      "  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/saving.py\", line 584, in load_model\r\n",
      "    model = _deserialize_model(h5dict, custom_objects, compile)\r\n",
      "  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/saving.py\", line 274, in _deserialize_model\r\n",
      "    model = model_from_config(model_config, custom_objects=custom_objects)\r\n",
      "  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/saving.py\", line 627, in model_from_config\r\n",
      "    return deserialize(config, custom_objects=custom_objects)\r\n",
      "  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/layers/__init__.py\", line 168, in deserialize\r\n",
      "    printable_module_name='layer')\r\n",
      "  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/utils/generic_utils.py\", line 147, in deserialize_keras_object\r\n",
      "    list(custom_objects.items())))\r\n",
      "  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/sequential.py\", line 302, in from_config\r\n",
      "    model.add(layer)\r\n",
      "  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/sequential.py\", line 166, in add\r\n",
      "    layer(x)\r\n",
      "  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/layers/recurrent.py\", line 541, in __call__\r\n",
      "    return super(RNN, self).__call__(inputs, **kwargs)\r\n",
      "  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 76, in symbolic_fn_wrapper\r\n",
      "    return func(*args, **kwargs)\r\n",
      "  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/base_layer.py\", line 463, in __call__\r\n",
      "    self.build(unpack_singleton(input_shapes))\r\n",
      "  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/layers/recurrent.py\", line 502, in build\r\n",
      "    self.cell.build(step_input_shape)\r\n",
      "  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/layers/recurrent.py\", line 1942, in build\r\n",
      "    constraint=self.bias_constraint)\r\n",
      "  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/base_layer.py\", line 282, in add_weight\r\n",
      "    constraint=constraint)\r\n",
      "  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 621, in variable\r\n",
      "    value, dtype=dtype, name=name, constraint=constraint)\r\n",
      "  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\", line 814, in variable\r\n",
      "    constraint=constraint)\r\n",
      "  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py\", line 260, in __call__\r\n",
      "    return cls._variable_v2_call(*args, **kwargs)\r\n",
      "  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py\", line 254, in _variable_v2_call\r\n",
      "    shape=shape)\r\n",
      "  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py\", line 235, in <lambda>\r\n",
      "    previous_getter = lambda **kws: default_variable_creator_v2(None, **kws)\r\n",
      "  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/variable_scope.py\", line 2645, in default_variable_creator_v2\r\n",
      "    shape=shape)\r\n",
      "  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py\", line 262, in __call__\r\n",
      "    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n",
      "  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py\", line 1411, in __init__\r\n",
      "    distribute_strategy=distribute_strategy)\r\n",
      "  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py\", line 1543, in _init_from_args\r\n",
      "    name=\"initial_value\", dtype=dtype)\r\n",
      "  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 1280, in convert_to_tensor\r\n",
      "    raise RuntimeError(\"Attempting to capture an EagerTensor without \"\r\n",
      "RuntimeError: Attempting to capture an EagerTensor without building a function.\r\n",
      "# tensorflow.keras.models.load_model \r\n",
      ">>> import tensorflow\r\n",
      ">>> model = tensorflow.keras.models.load_model('lstm0-sinewave_origin.h5')\r\n",
      "2021-01-14 07:39:55.578518: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\n",
      "pciBusID: 0000:02:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\n",
      "coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n",
      "2021-01-14 07:39:55.579080: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 1 with properties: \r\n",
      "pciBusID: 0000:84:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\n",
      "coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n",
      "2021-01-14 07:39:55.579141: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n",
      "2021-01-14 07:39:55.579165: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n",
      "2021-01-14 07:39:55.579186: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n",
      "2021-01-14 07:39:55.579207: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n",
      "2021-01-14 07:39:55.579224: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n",
      "2021-01-14 07:39:55.579241: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n",
      "2021-01-14 07:39:55.579259: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n",
      "2021-01-14 07:39:55.581588: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0, 1\r\n",
      "2021-01-14 07:39:55.581630: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n",
      "2021-01-14 07:39:55.581644: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 1 \r\n",
      "2021-01-14 07:39:55.581652: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N N \r\n",
      "2021-01-14 07:39:55.581659: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 1:   N N \r\n",
      "2021-01-14 07:39:55.583609: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10171 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1)\r\n",
      "2021-01-14 07:39:55.584122: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 2116 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:84:00.0, compute capability: 6.1)\r\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\r\n",
      ">>> import tensorflow\r\n",
      ">>> model = tensorflow.keras.models.load_model('lstm0-sinewave_origin.h5')\r\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\r\n",
      "```\n",
      "issue labels - \n",
      "TF 2.3\n",
      "comp:keras\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  PIP Tensorflow-gpu\n",
      "issue body -  I read in docs that for new version of tensorflow gpu support is integrated \r\n",
      "so if i run: pip install tensorflow, i have both cpu and gpu support.\r\n",
      "\r\n",
      "Now the question is: why is still present \r\n",
      "pip install tensorflow-gpu 2.4.0 ???\r\n",
      "https://pypi.org/project/tensorflow-gpu/\r\n",
      "\r\n",
      "anyway\r\n",
      "i installed  TS whit command \"pip install tensorflow\"\r\n",
      "\r\n",
      "if i run \r\n",
      "```\r\n",
      "C:\\Users\\Kit>nvcc -V \r\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\r\n",
      "Copyright (c) 2005-2020 NVIDIA Corporation\r\n",
      "Built on Wed_Jul_22_19:09:35_Pacific_Daylight_Time_2020\r\n",
      "Cuda compilation tools, release 11.0, V11.0.221\r\n",
      "Build cuda_11.0_bu.relgpu_drvr445TC445_37.28845127_0\r\n",
      "```\r\n",
      "so everithink seems fine\r\n",
      "also for \r\n",
      "C:\\Users\\Kit>nvidia-smi\r\n",
      "Thu Jan 14 08:02:30 2021\r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 460.89       Driver Version: 460.89       CUDA Version: 11.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce GTX 105... WDDM  | 00000000:01:00.0  On |                  N/A |\r\n",
      "| 35%   14C    P8    N/A /  75W |    758MiB /  4096MiB |      2%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "\r\n",
      "but when i run \r\n",
      "```\r\n",
      "from keras import backend as K\r\n",
      "print(tf.config.list_physical_devices('GPU'))\r\n",
      "or\r\n",
      "import tensorflow as tf\r\n",
      "print(tf.test.gpu_device_name())\r\n",
      "```\r\n",
      "i can see only cpu\r\n",
      "\r\n",
      "but if i install whit\r\n",
      "pip install tensorflow-gpu 2.4.0\r\n",
      "\r\n",
      "i can see cpu AND gpu in python\r\n",
      "\r\n",
      "I'M A BIT CONFUSED\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  TF2.4/XLA assertion in Electra model when deferring compilation on GPU\n",
      "issue body -  Attached are gzip versions of\r\n",
      "1) git.patch\r\n",
      "2) dummy_p1.tfrecord\r\n",
      "3) run_electra_bug.sh\r\n",
      "\r\n",
      "Environment:\r\n",
      "1) Google TF-2.4 container: tensorflow/tensorflow:2.4.0-gpu\r\n",
      "2) single GV100 32GB\r\n",
      "\r\n",
      "Reproduction steps:\r\n",
      "\r\n",
      "1) git clone git@github.com:NVIDIA/DeepLearningExamples.git\r\n",
      "2) cd DeepLearningExamples/TensorFlow2/LanguageModeling/ELECTRA/\r\n",
      "3) copy the attached git.patch file to ./\r\n",
      "4) cp the attached run_electra_bug file to ./scripts\r\n",
      "5) cp the attached dummy_p1.tfrecord file to ./data\r\n",
      "6) git apply git.patch\r\n",
      "7) bash scripts/docker/build.sh\r\n",
      "8) bash scripts/docker/launch.sh\r\n",
      "9) TF_XLA_FLAGS=--tf_xla_always_defer_compilation=true bash scripts/run_electra_bug.sh 1 6e-3 amp 1  (run this within the docker container started in 8).\r\n",
      "\r\n",
      "The resulting error:\r\n",
      "```\r\n",
      "0]<stderr>:Traceback (most recent call last):\r\n",
      "[0]<stderr>:  File \"/workspace/electra/run_pretraining.py\", line 493, in <module>\r\n",
      "[0]<stderr>:    args = main(start_time)\r\n",
      "[0]<stderr>:  File \"/workspace/electra/run_pretraining.py\", line 427, in main\r\n",
      "[0]<stderr>:    local_step==1, take_step=local_step % args.gradient_accumulation_steps == 0)\r\n",
      "[0]<stderr>:  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 828, in __call__\r\n",
      "[0]<stderr>:    result = self._call(*args, **kwds)\r\n",
      "[0]<stderr>:  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 888, in _call\r\n",
      "[0]<stderr>:    return self._stateless_fn(*args, **kwds)\r\n",
      "[0]<stderr>:  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 2943, in __call__\r\n",
      "[0]<stderr>:    filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\r\n",
      "[0]<stderr>:  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 1919, in _call_flat\r\n",
      "[0]<stderr>:    ctx, args, cancellation_manager=cancellation_manager))\r\n",
      "[0]<stderr>:  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 560, in call\r\n",
      "[0]<stderr>:    ctx=ctx)\r\n",
      "[0]<stderr>:  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\r\n",
      "[0]<stderr>:    inputs, attrs, num_outputs)\r\n",
      "[0]<stderr>:tensorflow.python.framework.errors_impl.CancelledError:  [_Derived_]RecvAsync is cancelled.\r\n",
      "[0]<stderr>:\t [[{{node cluster_11_1/merge_oidx_69/_148}}]] [Op:__inference_train_one_step_88467]\r\n",
      "[0]<stderr>:\r\n",
      "[0]<stderr>:Function call stack:\r\n",
      "[0]<stderr>:train_one_step\r\n",
      "[0]<stderr>:\r\n",
      "Process 0 exit with status code 1.\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/usr/local/bin/horovodrun\", line 8, in <module>\r\n",
      "    sys.exit(run_commandline())\r\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/horovod/runner/launch.py\", line 768, in run_commandline\r\n",
      "    _run(args)\r\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/horovod/runner/launch.py\", line 758, in _run\r\n",
      "    return _run_static(args)\r\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/horovod/runner/launch.py\", line 615, in _run_static\r\n",
      "    _launch_job(args, settings, nics, command)\r\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/horovod/runner/launch.py\", line 731, in _launch_job\r\n",
      "    args.verbose)\r\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/horovod/runner/launch.py\", line 704, in run_controller\r\n",
      "    gloo_run()\r\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/horovod/runner/launch.py\", line 720, in gloo_run_fn\r\n",
      "    gloo_run(settings, nics, env, driver_ip, command)\r\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/horovod/runner/gloo_run.py\", line 284, in gloo_run\r\n",
      "    launch_gloo(command, exec_command, settings, nics, env, server_ip)\r\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/horovod/runner/gloo_run.py\", line 271, in launch_gloo\r\n",
      "    .format(name=name, code=exit_code))\r\n",
      "```\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:gpu\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Update TFLM docker image to be based off Ubuntu 20.04\n",
      "issue body -  With this, we can easily install gcc-9, and Ubuntu is also part of the CI for the broader Tensorflow project.\n",
      "\n",
      "Fixes #46415\n",
      "\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  when i use custom loss and gpu, fit kernel died\n",
      "issue body -  <em>Please make sure that this is a bug. As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:bug_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): window 10\r\n",
      "- TensorFlow version (use command below): tf-nightly-gpu 2.5.0 210112\r\n",
      "- Python version:3.8\r\n",
      "- CUDA/cuDNN version: 11.1 / maybe 8(?)\r\n",
      "- GPU model and memory: RTX 3090 24GB MEM\r\n",
      "\r\n",
      "2. TF 2.0: v1.12.1-48890-g670cc3fa48f 2.5.0-dev20210113\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "my custom loss function looks like this,\r\n",
      "\r\n",
      "```\r\n",
      "import tensorflow as tf\r\n",
      "from keras import backend as K\r\n",
      "from tensorflow.keras.losses import Loss\r\n",
      "\r\n",
      "@tf.function\r\n",
      "def mase(y_true, y_pred, seasonality=1):\r\n",
      "    def _naive_forecasting(actual, seasonality: int = 1):\r\n",
      "        return actual[:-seasonality]\r\n",
      "    \r\n",
      "    def _error(actual, predicted):\r\n",
      "        return actual - predicted\r\n",
      "    \r\n",
      "    def _mae(actual, predicted):\r\n",
      "        return K.mean(K.abs(_error(actual, predicted)))\r\n",
      "    \r\n",
      "#     K.print_tensor(y_true,message='\\ny_true==')\r\n",
      "#     K.print_tensor(y_pred,message='\\ny_pred==')\r\n",
      "#     K.print_tensor(_mae(y_true, y_pred) / _mae(y_true[seasonality:], _naive_forecasting(y_true, seasonality)),message='\\nminus==')\r\n",
      "#     print(y_true, y_pred)\r\n",
      "    return _mae(y_true, y_pred) / _mae(y_true[seasonality:], _naive_forecasting(y_true, seasonality))\r\n",
      "\r\n",
      "```\r\n",
      "\r\n",
      "and use like this,\r\n",
      "```\r\n",
      "model.compile(loss=MASE(1), optimizer=Adam(lr=0.001))\r\n",
      "...\r\n",
      "model.fit(x_concat_data[train], y_concat_data[train], batch_size=batch_size, epochs=epoch, verbose=2, shuffle=True)\r\n",
      "...\r\n",
      "model.evaluate(x_concat_data[validation], y_concat_data[validation], batch_size=batch_size, callbacks=[early_stopping])\r\n",
      "```\r\n",
      "I use rtx 3090 and want to train with gpu\r\n",
      "\r\n",
      "cpu training is good!\r\n",
      "BUT! when i use gpu, python kernel is dead!\r\n",
      "kernel output like this\r\n",
      "\r\n",
      "2021-01-14 09:33:55.296929: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:127] None of the MLIR optimization passes are enabled (registered 2)\r\n",
      "Epoch 1/1000\r\n",
      "d:/>\r\n",
      "\r\n",
      "what is my problem?\n",
      "issue labels - \n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  TFLM CI is currently broken\n",
      "issue body -  @tensorflow/micro\r\n",
      "\r\n",
      "https://github.com/tensorflow/tensorflow/commit/02e9e26b27a59fef75f69575d54aa4af96605af5 added a compiler flag that [breaks the TFLM bazel build](https://source.cloud.google.com/results/invocations/b6a8674a-60c7-4426-8e9d-b5d96294f455/log) with the following error:\r\n",
      "```\r\n",
      "ERROR: /tmpfs/src/github/tensorflow/tensorflow/lite/kernels/internal/BUILD:685:11: C++ compilation of rule '//tensorflow/lite/kernels/internal:portable_tensor_utils' failed (Exit 1): gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 35 argument(s) skipped)\r\n",
      "gcc: error: unrecognized argument to -fsanitize= option: 'shift-base'\r\n",
      "```\r\n",
      "\r\n",
      "The underlying issue seems to be that the gcc version on the TFLM docker image (gcc (Debian 8.3.0-6) 8.3.0) does not support this argument.\r\n",
      "\r\n",
      "Depending on your local gcc version, this may or may not be reproducible locally.\r\n",
      "\r\n",
      "For example, for me: gcc (Debian 10.2.0-19) 10.2.0\r\n",
      "\r\n",
      "```\r\n",
      "bazel clean\r\n",
      "bazel build -s tensorflow/lite/c:common\r\n",
      "```\r\n",
      "\r\n",
      "has the shift-base argument:\r\n",
      "```\r\n",
      "/usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -MD -MF bazel-out/k8-opt/bin/tensorflow/lite/c/_objs/common/common.d '-frandom-seed=bazel-out/k8-opt/bin/tensorflow/lite/c/_objs/common/common.o' -iquote . -iquote bazel-out/k8-opt/bin -w -DAUTOLOAD_DYNAMIC_KERNELS -DFARMHASH_NO_CXX_STRING -Wno-sign-compare -O3 -fno-exceptions '-fno-sanitize=shift-base' -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c tensorflow/lite/c/common.c -o bazel-out/k8-opt/bin/tensorflow/lite/c/_objs/common/common.o)\r\n",
      "```\r\n",
      "\r\n",
      "but the build passes.\n",
      "issue labels - \n",
      "comp:micro\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  How the trim existed model for a given tflite model?\n",
      "issue body -  Given a tflite model, is there any way to trim the model?\r\n",
      "\r\n",
      "It is, for example, for squeezent, \r\n",
      "```\r\n",
      "1. Placeholder\r\n",
      "2. maxpool\r\n",
      "3. fire 1\r\n",
      "4. fire 2\r\n",
      "5. fire 3\r\n",
      ".....\r\n",
      "41. average pooling\r\n",
      "42. flattern\r\n",
      "43. softmax\r\n",
      "```\r\n",
      "I need a way to trim and keep \r\n",
      "```\r\n",
      "3. fire 1\r\n",
      "4. fire 2\r\n",
      "5. fire 3\r\n",
      "```\r\n",
      "for this case, the input is replaced to fire1 and output is as fire 3.\r\n",
      "\r\n",
      "I wonder if tensorflow provides such functionaliry?\r\n",
      "\r\n",
      "Thanks~\n",
      "issue labels - \n",
      "comp:lite\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:feature\n",
      "\n",
      "\n",
      "issue title -  Update xtensa fully_connected kernel to call into xa_nnlib.\n",
      "issue body -  This is the relevant code snippet from https://github.com/tensorflow/tensorflow/pull/44581 that is needed to use xa_nnlib with the fully_connected kernel for fusion_f1.\r\n",
      "\r\n",
      "Profiling the keyword benchmark shows a reduction of ~20,000 ticks for invoke (if we use the updated xa_nnlib from https://github.com/tensorflow/tensorflow/pull/44581)\r\n",
      "\r\n",
      "```\r\n",
      "make -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=fusion_f1 XTENSA_CORE=F1_190305_swupgrade test_keyword_benchmark -j8\r\n",
      "```\r\n",
      "\r\n",
      "gives:\r\n",
      "```\r\n",
      "InitializeKeywordRunner() took 279792 ticks (279 ms)\r\n",
      "KeywordRunNIerations(1) took 151304 ticks (151 ms)\r\n",
      "KeywordRunNIerations(10) took 1512547 ticks (1512 ms)\r\n",
      "```\r\n",
      "\r\n",
      "Also confirmed that the kernel_fully_connected_test passes:\r\n",
      "```\r\n",
      "make -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=fusion_f1 XTENSA_CORE=F1_190305_swupgrade test_kernel_fully_connected_test -j8\r\n",
      "```\r\n",
      "\r\n",
      "Progress towards http://b/177457688\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  Use CudnnRNNV3 for the GRU and LSTM\n",
      "issue body -  This PR replaces the use of CudnnRNN with CudnnRNNV3, which supports inputs with different layouts (time/batch major) and variable sequence lengths. With this PR, we can avoid unnecessary layout permutation and the code will be clearer.\r\n",
      "\r\n",
      "\r\n",
      "fyi. @nluehr  \n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:keras\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  TfLite: Internal error: Cannot create interpreter: Didn't find op for builtin opcode 'CONV_2D' version '5'\n",
      "issue body -  I was trying to create the digit classifier Android app from [codelabs](https://developer.android.com/codelabs/digit-classifier-tflite#0)\r\n",
      "\r\n",
      "At runtime, I get this error:\r\n",
      "\r\n",
      "E/AndroidRuntime: FATAL EXCEPTION: pool-2-thread-1\r\n",
      "    Process: org.tensorflow.lite.codelabs.digitclassifier, PID: 18122\r\n",
      "    java.lang.IllegalArgumentException: Internal error: Cannot create interpreter: Didn't find op for builtin opcode 'CONV_2D' version '5'\r\n",
      "    \r\n",
      "    Registration failed.\r\n",
      "    \r\n",
      "        at org.tensorflow.lite.NativeInterpreterWrapper.createInterpreter(Native Method)\r\n",
      "        at org.tensorflow.lite.NativeInterpreterWrapper.init(NativeInterpreterWrapper.java:72)\r\n",
      "        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:63)\r\n",
      "        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:237)\r\n",
      "        at org.tensorflow.lite.codelabs.digitclassifier.DigitClassifier.initializeInterpreter(DigitClassifier.kt:66)\r\n",
      "        at org.tensorflow.lite.codelabs.digitclassifier.DigitClassifier.access$initializeInterpreter(DigitClassifier.kt:31)\r\n",
      "        at org.tensorflow.lite.codelabs.digitclassifier.DigitClassifier$initialize$1.run(DigitClassifier.kt:48)\r\n",
      "        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1167)\r\n",
      "        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:641)\r\n",
      "        at java.lang.Thread.run(Thread.java:923)\r\n",
      "\r\n",
      "I have tried changing the tflite version on build.gradle. Any other ideas on how to resolve this issue?\n",
      "issue labels - \n",
      "comp:lite\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Support for directly importing Dense layer from keras API\n",
      "issue body -  **System information**\r\n",
      "- TensorFlow version (you are using): TF 4.0\r\n",
      "- Are you willing to contribute it (Yes/No): No\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**Describe the feature and the current behavior/state.**\r\n",
      "\r\n",
      "Why Dense?\r\n",
      "\r\n",
      "While i was writing code i noticed Dense layer is not supported in importing directly in keras but input layer is there. In that case anyhow we have to mention full path dependency because this layer is the most basic layer and frequently used while building the architecture even if not initially used then at least last few layers are dense layers only mostly. It can be used as feed forward neural network as well as in the output layer.so I think it will be good if this layer can be directly imported from keras without any path dependencies,\r\n",
      "\r\n",
      "[ `from tensorflow.python.keras.engine.input_layer import Input`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/__init__.py) \r\n",
      "\r\n",
      "Just adding Dense here like this will do\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**Will this change the current api? How?**\r\n",
      "\r\n",
      "Nothing is going to change in api. This will be just an additional functionality for the users to serve their purpose easily.\r\n",
      "\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "comp:keras\n",
      "type:feature\n",
      "\n",
      "\n",
      "issue title -  Cleanup `c/experimental/gradients/` part 3\n",
      "issue body -  @saxenasaurabh \r\n",
      "Could you take a look at this PR ? Thank you !\n",
      "issue labels - \n",
      "cla: yes\n",
      "ready to pull\n",
      "size:L\n",
      "\n",
      "\n",
      "issue title -  LSTM crashing during training - Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure\n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code: yes\r\n",
      "- OS Platform and Distribution: Win 10 Enterprice LTSC\r\n",
      "- TensorFlow installed from: binary\r\n",
      "- TensorFlow version: v2.4.0-rc4-71-g582c8d236cb 2.4.0\r\n",
      "- Python version: Python 3.8.5\r\n",
      "- Bazel version (if compiling from source): N/A\r\n",
      "- GCC/Compiler version (if compiling from source): N/A\r\n",
      "- CUDA/cuDNN version: Build cuda_11.0_bu.relgpu_drvr445TC445_37.28845127_0 / cuDNN v 8.0.5 for CUDA 11.0\r\n",
      "- GPU model and memory: GTX 960 (Asus Strix Direct DCU2 OC 4GB)\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "When training a network with LSTM layers it crashes randomly during epochs. \r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "https://github.com/Kalhama/tensorflow-lstm-crash\r\n",
      "\r\n",
      "**Things I've checked and are ok / have no effect**\r\n",
      "✅ Only occurs when training on GPU\r\n",
      "✅ Only occurs when using LSTM layers\r\n",
      "✅ Underclocking GPU, raising voltages etc.. (in case stock overclock would be unstable)\r\n",
      "✅ CPU and GPU temps (either never exceeds 60 deg C)\r\n",
      "✅ Different driver versions. I earlier had 460.89 but it's the same on 451.82\r\n",
      "✅ There is enough VRAM / RAM. (I used `tf.config.experimental.set_memory_growth()` and then Win task manager to check VRAM utilization. Only used max 1GB of VRAM on large networks)\r\n",
      "\r\n",
      "**Things I've checked and have some effect, but do not remove problem completely**\r\n",
      "✅ Different network sizes. All networks seem to have this issue but larger networks feel to be more prone to crash fast\r\n",
      "✅ Different batch sizes and different train data sizes. Overall larger batches and longer epochs are more stable. \r\n",
      "\r\n",
      "**Other info / logs**\r\n",
      "```2021-01-12 21:38:15.476512: E tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure\r\n",
      "2021-01-12 21:38:15.476581: E tensorflow/stream_executor/dnn.cc:616] CUDNN_STATUS_INTERNAL_ERROR\r\n",
      "in tensorflow/stream_executor/cuda/cuda_dnn.cc(1859): 'cudnnRNNForwardTraining( cudnn.handle(), rnn_desc.handle(), model_dims.max_seq_length, input_desc.handles(), input_data.opaque(), input_h_desc.handle(), input_h_data.opaque(), input_c_desc.handle(), input_c_data.opaque(), rnn_desc.params_handle(), params.opaque(), output_desc.handles(), output_data->opaque(), output_h_desc.handle(), output_h_data->opaque(), output_c_desc.handle(), output_c_data->opaque(), workspace.opaque(), workspace.size(), reserve_space.opaque(), reserve_space.size())'\r\n",
      "2021-01-12 21:38:15.483573: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:220] Unexpected Event status: 1\r\n",
      "\r\n",
      "2\r\n",
      "```\r\n",
      "\r\n",
      "Sometimes the error looks like this:\r\n",
      "```\r\n",
      "2021-01-12 21:41:41.096689: E tensorflow/stream_executor/dnn.cc:616] CUDNN_STATUS_INTERNAL_ERROR\r\n",
      "in tensorflow/stream_executor/cuda/cuda_dnn.cc(1972): 'cudnnRNNBackwardData( cudnn.handle(), rnn_desc.handle(), model_dims.max_seq_length, output_desc.handles(), output_data.opaque(), output_desc.handles(), output_backprop_data.opaque(), output_h_desc.handle(), output_h_backprop_data.opaque(), output_c_desc.handle(), output_c_backprop_data.opaque(), rnn_desc.params_handle(), params.opaque(), input_h_desc.handle(), input_h_data.opaque(), input_c_desc.handle(), input_c_data.opaque(), input_desc.handles(), input_backprop_data->opaque(), input_h_desc.handle(), input_h_backprop_data->opaque(), input_c_desc.handle(), input_c_backprop_data->opaque(), workspace.opaque(), workspace.size(), reserve_space_data->opaque(), reserve_space_data->size())'\r\n",
      "2021-01-12 21:41:41.127515: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at cudnn_rnn_ops.cc:1926 : Internal: Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 3, 200, 1, 200, 32, 200]\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"c:/Users/User/Code/tensorflow/main.py\", line 61, in <module>\r\n",
      "    main()\r\n",
      "  File \"c:/Users/User/Code/tensorflow/main.py\", line 40, in main\r\n",
      "    model.train(\r\n",
      "  File \"c:\\Users\\User\\Code\\tensorflow\\core\\model.py\", line 47, in train\r\n",
      "    self.model.fit(\r\n",
      "  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1100, in fit\r\n",
      "    tmp_logs = self.train_function(iterator)\r\n",
      "  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 828, in __call__\r\n",
      "    result = self._call(*args, **kwds)\r\n",
      "  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 855, in _call\r\n",
      "    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\r\n",
      "  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2942, in __call__\r\n",
      "    return graph_function._call_flat(\r\n",
      "  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1918, in _call_flat\r\n",
      "    return self._build_call_outputs(self._inference_function.call(\r\n",
      "  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\", line 555, in call\r\n",
      "    outputs = execute.execute(\r\n",
      "  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 59, in quick_execute\r\n",
      "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n",
      "tensorflow.python.framework.errors_impl.InternalError:    Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 3, 200, 1, 200, 32, 200]\r\n",
      "         [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]\r\n",
      "         [[Adam/gradients/PartitionedCall_2]] [Op:__inference_train_function_7900]\r\n",
      "\r\n",
      "Function call stack:\r\n",
      "train_function -> train_function -> train_function\r\n",
      "```\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:gpu\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Add support of tf.qint16 and tf.quint16 for tf.stack\n",
      "issue body -  This PR is part of the effort for #26069 where `tf.stack` support most of the qtypes (`tf.qint8/tf.quint8/tf.qint32`)\r\n",
      "but not `tf.qint16` and `tf.quint16`. The reason was that `TF_CALL_QUANTIZED_TYPES` does not include `qint16` and `quint16`.\r\n",
      "\r\n",
      "This PR also update to add `qint32` for tf.equal and tf.not_equal to be consistent with other ops.\r\n",
      "\r\n",
      "Signed-off-by: Yong Tang <yong.tang.github@outlook.com>\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:core\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  IndexError webcam object detection\n",
      "issue body -  Hi,\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.10\r\n",
      "- TensorFlow version (use command below): v2.4.0-rc4-71-g582c8d236cb 2.4.0\r\n",
      "- Python version: 3.8.6\r\n",
      "- GPU model and memory: None\r\n",
      "\r\n",
      "I followed the tutorial [here](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/auto_examples/object_detection_camera.html#sphx-glr-download-auto-examples-object-detection-camera-py) to make a real time object detection. I followed all the step to install tensorflow and tensorflow object detection API and everything was working on their test files. I came back to the tutorial and launch the code everything was working good (model downloaded, ...) but I came with an IndexError issue.\r\n",
      "\r\n",
      "Error :\r\n",
      "```\r\n",
      "object_detection_camera.py:167 detect_fn  *\r\n",
      "        image, shapes = detection_model.preprocess(image)\r\n",
      "    /home/name/.local/lib/python3.8/site-packages/object_detection/meta_architectures/ssd_meta_arch.py:482 preprocess  *\r\n",
      "        normalized_inputs = self._feature_extractor.preprocess(inputs)\r\n",
      "    /home/name/.local/lib/python3.8/site-packages/object_detection/models/ssd_resnet_v1_fpn_keras_feature_extractor.py:204 preprocess  *\r\n",
      "        if resized_inputs.shape.as_list()[3] == 3:\r\n",
      "\r\n",
      "    IndexError: list index out of range\r\n",
      "```\r\n",
      "\r\n",
      "Here the code :\r\n",
      "```\r\n",
      "#!/usr/bin/env python\r\n",
      "# coding: utf-8\r\n",
      "\"\"\"\r\n",
      "Detect Objects Using Your Webcam\r\n",
      "================================\r\n",
      "\"\"\"\r\n",
      "\r\n",
      "# %%\r\n",
      "# This demo will take you through the steps of running an \"out-of-the-box\" detection model to\r\n",
      "# detect objects in the video stream extracted from your camera.\r\n",
      "\r\n",
      "# %%\r\n",
      "# Create the data directory\r\n",
      "# ~~~~~~~~~~~~~~~~~~~~~~~~~\r\n",
      "# The snippet shown below will create the ``data`` directory where all our data will be stored. The\r\n",
      "# code will create a directory structure as shown bellow:\r\n",
      "#\r\n",
      "# .. code-block:: bash\r\n",
      "#\r\n",
      "#     data\r\n",
      "#     └── models\r\n",
      "#\r\n",
      "# where the ``models`` folder will will contain the downloaded models.\r\n",
      "import os\r\n",
      "\r\n",
      "DATA_DIR = os.path.join(os.getcwd(), 'data')\r\n",
      "MODELS_DIR = os.path.join(DATA_DIR, 'models')\r\n",
      "for dir in [DATA_DIR, MODELS_DIR]:\r\n",
      "    if not os.path.exists(dir):\r\n",
      "        os.mkdir(dir)\r\n",
      "\r\n",
      "# %%\r\n",
      "# Download the model\r\n",
      "# ~~~~~~~~~~~~~~~~~~\r\n",
      "# The code snippet shown below is used to download the object detection model checkpoint file,\r\n",
      "# as well as the labels file (.pbtxt) which contains a list of strings used to add the correct\r\n",
      "# label to each detection (e.g. person).\r\n",
      "#\r\n",
      "# The particular detection algorithm we will use is the `SSD ResNet101 V1 FPN 640x640`. More\r\n",
      "# models can be found in the `TensorFlow 2 Detection Model Zoo <https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md>`_.\r\n",
      "# To use a different model you will need the URL name of the specific model. This can be done as\r\n",
      "# follows:\r\n",
      "#\r\n",
      "# 1. Right click on the `Model name` of the model you would like to use;\r\n",
      "# 2. Click on `Copy link address` to copy the download link of the model;\r\n",
      "# 3. Paste the link in a text editor of your choice. You should observe a link similar to ``download.tensorflow.org/models/object_detection/tf2/YYYYYYYY/XXXXXXXXX.tar.gz``;\r\n",
      "# 4. Copy the ``XXXXXXXXX`` part of the link and use it to replace the value of the ``MODEL_NAME`` variable in the code shown below;\r\n",
      "# 5. Copy the ``YYYYYYYY`` part of the link and use it to replace the value of the ``MODEL_DATE`` variable in the code shown below.\r\n",
      "#\r\n",
      "# For example, the download link for the model used below is: ``download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet101_v1_fpn_640x640_coco17_tpu-8.tar.gz``\r\n",
      "import tarfile\r\n",
      "import urllib.request\r\n",
      "\r\n",
      "# Download and extract model\r\n",
      "MODEL_DATE = '20200711'\r\n",
      "MODEL_NAME = 'ssd_resnet101_v1_fpn_640x640_coco17_tpu-8'\r\n",
      "MODEL_TAR_FILENAME = MODEL_NAME + '.tar.gz'\r\n",
      "MODELS_DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/tf2/'\r\n",
      "MODEL_DOWNLOAD_LINK = MODELS_DOWNLOAD_BASE + MODEL_DATE + '/' + MODEL_TAR_FILENAME\r\n",
      "PATH_TO_MODEL_TAR = os.path.join(MODELS_DIR, MODEL_TAR_FILENAME)\r\n",
      "PATH_TO_CKPT = os.path.join(MODELS_DIR, os.path.join(MODEL_NAME, 'checkpoint/'))\r\n",
      "PATH_TO_CFG = os.path.join(MODELS_DIR, os.path.join(MODEL_NAME, 'pipeline.config'))\r\n",
      "if not os.path.exists(PATH_TO_CKPT):\r\n",
      "    print('Downloading model. This may take a while... ', end='')\r\n",
      "    urllib.request.urlretrieve(MODEL_DOWNLOAD_LINK, PATH_TO_MODEL_TAR)\r\n",
      "    tar_file = tarfile.open(PATH_TO_MODEL_TAR)\r\n",
      "    tar_file.extractall(MODELS_DIR)\r\n",
      "    tar_file.close()\r\n",
      "    os.remove(PATH_TO_MODEL_TAR)\r\n",
      "    print('Done')\r\n",
      "\r\n",
      "# Download labels file\r\n",
      "LABEL_FILENAME = 'mscoco_label_map.pbtxt'\r\n",
      "LABELS_DOWNLOAD_BASE = \\\r\n",
      "    'https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/data/'\r\n",
      "PATH_TO_LABELS = os.path.join(MODELS_DIR, os.path.join(MODEL_NAME, LABEL_FILENAME))\r\n",
      "if not os.path.exists(PATH_TO_LABELS):\r\n",
      "    print('Downloading label file... ', end='')\r\n",
      "    urllib.request.urlretrieve(LABELS_DOWNLOAD_BASE + LABEL_FILENAME, PATH_TO_LABELS)\r\n",
      "    print('Done')\r\n",
      "\r\n",
      "# %%\r\n",
      "# Load the model\r\n",
      "# ~~~~~~~~~~~~~~\r\n",
      "# Next we load the downloaded model\r\n",
      "\r\n",
      "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'    # Suppress TensorFlow logging\r\n",
      "import tensorflow as tf\r\n",
      "from object_detection.utils import label_map_util\r\n",
      "from object_detection.utils import config_util\r\n",
      "from object_detection.utils import visualization_utils as viz_utils\r\n",
      "from object_detection.builders import model_builder\r\n",
      "\r\n",
      "tf.get_logger().setLevel('ERROR')           # Suppress TensorFlow logging (2)\r\n",
      "\r\n",
      "# Enable GPU dynamic memory allocation\r\n",
      "gpus = tf.config.experimental.list_physical_devices('GPU')\r\n",
      "for gpu in gpus:\r\n",
      "    tf.config.experimental.set_memory_growth(gpu, True)\r\n",
      "\r\n",
      "# Load pipeline config and build a detection model\r\n",
      "configs = config_util.get_configs_from_pipeline_file(PATH_TO_CFG)\r\n",
      "model_config = configs['model']\r\n",
      "detection_model = model_builder.build(model_config=model_config, is_training=False)\r\n",
      "\r\n",
      "# Restore checkpoint\r\n",
      "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\r\n",
      "ckpt.restore(os.path.join(PATH_TO_CKPT, 'ckpt-0')).expect_partial()\r\n",
      "\r\n",
      "@tf.function\r\n",
      "def detect_fn(image):\r\n",
      "    \"\"\"Detect objects in image.\"\"\"\r\n",
      "\r\n",
      "    image, shapes = detection_model.preprocess(image)\r\n",
      "    prediction_dict = detection_model.predict(image, shapes)\r\n",
      "    detections = detection_model.postprocess(prediction_dict, shapes)\r\n",
      "\r\n",
      "    return detections, prediction_dict, tf.reshape(shapes, [-1])\r\n",
      "\r\n",
      "\r\n",
      "# %%\r\n",
      "# Load label map data (for plotting)\r\n",
      "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n",
      "# Label maps correspond index numbers to category names, so that when our convolution network\r\n",
      "# predicts `5`, we know that this corresponds to `airplane`.  Here we use internal utility\r\n",
      "# functions, but anything that returns a dictionary mapping integers to appropriate string labels\r\n",
      "# would be fine.\r\n",
      "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS,\r\n",
      "                                                                    use_display_name=True)\r\n",
      "\r\n",
      "# %%\r\n",
      "# Define the video stream\r\n",
      "# ~~~~~~~~~~~~~~~~~~~~~~~\r\n",
      "# We will use `OpenCV <https://pypi.org/project/opencv-python/>`_ to capture the video stream\r\n",
      "# generated by our webcam. For more information you can refer to the `OpenCV-Python Tutorials <https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_gui/py_video_display/py_video_display.html#capture-video-from-camera>`_\r\n",
      "import cv2\r\n",
      "\r\n",
      "cap = cv2.VideoCapture(0)\r\n",
      "\r\n",
      "# %%\r\n",
      "# Putting everything together\r\n",
      "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n",
      "# The code shown below loads an image, runs it through the detection model and visualizes the\r\n",
      "# detection results, including the keypoints.\r\n",
      "#\r\n",
      "# Note that this will take a long time (several minutes) the first time you run this code due to\r\n",
      "# tf.function's trace-compilation --- on subsequent runs (e.g. on new images), things will be\r\n",
      "# faster.\r\n",
      "#\r\n",
      "# Here are some simple things to try out if you are curious:\r\n",
      "#\r\n",
      "# * Modify some of the input images and see if detection still works. Some simple things to try out here (just uncomment the relevant portions of code) include flipping the image horizontally, or converting to grayscale (note that we still expect the input image to have 3 channels).\r\n",
      "# * Print out `detections['detection_boxes']` and try to match the box locations to the boxes in the image.  Notice that coordinates are given in normalized form (i.e., in the interval [0, 1]).\r\n",
      "# * Set ``min_score_thresh`` to other values (between 0 and 1) to allow more detections in or to filter out more detections.\r\n",
      "import numpy as np\r\n",
      "\r\n",
      "while True:\r\n",
      "    # Read frame from camera\r\n",
      "    ret, image_np = cap.read()\r\n",
      "\r\n",
      "    # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\r\n",
      "    image_np_expanded = np.expand_dims(image_np, axis=0)\r\n",
      "\r\n",
      "    # Things to try:\r\n",
      "    # Flip horizontally\r\n",
      "    # image_np = np.fliplr(image_np).copy()\r\n",
      "\r\n",
      "    # Convert image to grayscale\r\n",
      "    # image_np = np.tile(\r\n",
      "    #     np.mean(image_np, 2, keepdims=True), (1, 1, 3)).astype(np.uint8)\r\n",
      "\r\n",
      "    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\r\n",
      "    detections, predictions_dict, shapes = detect_fn(input_tensor)\r\n",
      "\r\n",
      "    label_id_offset = 1\r\n",
      "    image_np_with_detections = image_np.copy()\r\n",
      "\r\n",
      "    viz_utils.visualize_boxes_and_labels_on_image_array(\r\n",
      "          image_np_with_detections,\r\n",
      "          detections['detection_boxes'][0].numpy(),\r\n",
      "          (detections['detection_classes'][0].numpy() + label_id_offset).astype(int),\r\n",
      "          detections['detection_scores'][0].numpy(),\r\n",
      "          category_index,\r\n",
      "          use_normalized_coordinates=True,\r\n",
      "          max_boxes_to_draw=200,\r\n",
      "          min_score_thresh=.30,\r\n",
      "          agnostic_mode=False)\r\n",
      "\r\n",
      "    # Display output\r\n",
      "    cv2.imshow('object detection', cv2.resize(image_np_with_detections, (800, 600)))\r\n",
      "\r\n",
      "    if cv2.waitKey(25) & 0xFF == ord('q'):\r\n",
      "        break\r\n",
      "\r\n",
      "cap.release()\r\n",
      "cv2.destroyAllWindows()\r\n",
      "```\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Installation of OpenCL backend Tensorflow Lite runtime package\n",
      "issue body -  **System information**\r\n",
      "- OS Platform and Distribution: Ubuntu 18.04 aarch64\r\n",
      "- Mobile device:  RK3399 with Mali T860 GPU\r\n",
      "- TensorFlow installed from (source or binary): source\r\n",
      "- TensorFlow version: 2.4.0\r\n",
      "- Python version: 3.7\r\n",
      "- Installed using virtualenv? pip? conda?:  virtualenv\r\n",
      "- GCC/Compiler version (if compiling from source): gcc 7.5\r\n",
      "- CUDA/cuDNN version: None\r\n",
      "- GPU model and memory: Mali T860\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "\r\n",
      "I am going to install OpenCL backend TensorFlow Lite runtime for Python on RK3399 Ubuntu 18.04 aarch64 platform.\r\n",
      "I looked at the document and blogs but could NOT find any method to install OpenCL backend TensorFlow Lite runtime for Python.\r\n",
      "I think that OpenCL backend TensorflowLite for Python should be installed from the source.\r\n",
      "If so, how should it be built and installed?\r\n",
      "Could u provide a proper method asap?\r\n",
      "Thanks\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:lite\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  Add RFC for pre-allocated tensors.\n",
      "issue body -  Add a RFC-document for the pre-allocated tensors feature.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  Switching to Cadence HiFi 4 NN Library v2.3.0\n",
      "issue body -  Using the latest version of HiFi 4 NN Library.\r\n",
      "\r\n",
      "This version has optimized implementation of FC and softmax for int8 datatype.\r\n",
      "\r\n",
      "Tested the change using following commands:\r\n",
      "\r\n",
      "```\r\n",
      "make -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=hifi4 XTENSA_TOOLS_VERSION=RI-2020.5-linux XTENSA_CORE=AE_HiFi4_LE5_FP_XC clean_downloads\r\n",
      "\r\n",
      "make -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=hifi4 XTENSA_TOOLS_VERSION=RI-2020.5-linux XTENSA_CORE=AE_HiFi4_LE5_FP_XC test_kernel_softmax_test\r\n",
      "```\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Dynamic shape for block3 to support input_shape (None, None, 3)\n",
      "issue body -  This PR is a part of #37146, the error I have addressed at [this comment](https://github.com/tensorflow/tensorflow/pull/37146#issuecomment-759351768)\r\n",
      "\r\n",
      "For ResNext models which use block3 function to build up. When the input shape is `(None, None, 3)`, the error will occur.\r\n",
      "I changed the function to get dynamic shape and reshape with None dimension.\r\n",
      "\r\n",
      "```python\r\n",
      "ResNeXt50(include_top=False, input_shape=(224, 224,3)).summary()\r\n",
      "```\r\n",
      "![image](https://user-images.githubusercontent.com/36766404/104455554-0af51f00-55da-11eb-8ed1-8c705fd236f9.png)\r\n",
      "\r\n",
      "```python\r\n",
      "ResNeXt50(include_top=False, input_shape=(None, None,3)).summary()\r\n",
      "```\r\n",
      "![image](https://user-images.githubusercontent.com/36766404/104455487-ebf68d00-55d9-11eb-83b2-74c05d54a48b.png)\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:keras\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Converting Tensorflow to TFLite error\n",
      "issue body -  **System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n",
      "- TensorFlow installed from (source or binary):\r\n",
      "- TensorFlow version (or github SHA if from source):\r\n",
      "\r\n",
      "\r\n",
      "**Provide the text output from tflite_convert**\r\n",
      "\r\n",
      "```\r\n",
      "Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, CONV_2D, EXP, LEAKY_RELU, LOGISTIC, MAX_POOL_2D, MUL, PAD, RESHAPE, RESIZE_BILINEAR, SPLIT_V, STRIDED_SLICE, TANH. Here is a list of operators for which you will need custom implementations: Softplus.\r\n",
      "```\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue** \r\n",
      "import tensorflow as tf\r\n",
      "from absl import app, flags, logging\r\n",
      "from absl.flags import FLAGS\r\n",
      "import numpy as np\r\n",
      "import cv2\r\n",
      "from core.yolov4 import YOLOv4, YOLOv3, YOLOv3_tiny, decode\r\n",
      "import core.utils as utils\r\n",
      "import os\r\n",
      "from core.config import cfg\r\n",
      "\r\n",
      "flags.DEFINE_string('weights', './checkpoints/yolov4-416', 'path to weights file')\r\n",
      "flags.DEFINE_string('output', './checkpoints/yolov4-416-fp32.tflite', 'path to output')\r\n",
      "flags.DEFINE_integer('input_size', 416, 'path to output')\r\n",
      "flags.DEFINE_string('quantize_mode', 'float32', 'quantize mode (int8, float16, float32)')\r\n",
      "flags.DEFINE_string('dataset', \"/Volumes/Elements/data/coco_dataset/coco/5k.txt\", 'path to dataset')\r\n",
      "\r\n",
      "def representative_data_gen():\r\n",
      "  fimage = open(FLAGS.dataset).read().split()\r\n",
      "  for input_value in range(10):\r\n",
      "    if os.path.exists(fimage[input_value]):\r\n",
      "      original_image=cv2.imread(fimage[input_value])\r\n",
      "      original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\r\n",
      "      image_data = utils.image_preprocess(np.copy(original_image), [FLAGS.input_size, FLAGS.input_size])\r\n",
      "      img_in = image_data[np.newaxis, ...].astype(np.float32)\r\n",
      "      print(\"calibration image {}\".format(fimage[input_value]))\r\n",
      "      yield [img_in]\r\n",
      "    else:\r\n",
      "      continue\r\n",
      "\r\n",
      "def save_tflite():\r\n",
      "  converter = tf.lite.TFLiteConverter.from_saved_model(FLAGS.weights)\r\n",
      "\r\n",
      "  if FLAGS.quantize_mode == 'float16':\r\n",
      "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n",
      "    converter.target_spec.supported_types = [tf.compat.v1.lite.constants.FLOAT16]\r\n",
      "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\n",
      "    converter.allow_custom_ops = True\r\n",
      "  elif FLAGS.quantize_mode == 'int8':\r\n",
      "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n",
      "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n",
      "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\n",
      "    converter.allow_custom_ops = True\r\n",
      "    converter.representative_dataset = representative_data_gen\r\n",
      "\r\n",
      "  tflite_model = converter.convert()\r\n",
      "  open(FLAGS.output, 'wb').write(tflite_model)\r\n",
      "\r\n",
      "  logging.info(\"model saved to: {}\".format(FLAGS.output))\r\n",
      "\r\n",
      "def demo():\r\n",
      "  interpreter = tf.lite.Interpreter(model_path=FLAGS.output)\r\n",
      "  interpreter.allocate_tensors()\r\n",
      "  logging.info('tflite model loaded')\r\n",
      "\r\n",
      "  input_details = interpreter.get_input_details()\r\n",
      "  print(input_details)\r\n",
      "  output_details = interpreter.get_output_details()\r\n",
      "  print(output_details)\r\n",
      "\r\n",
      "  input_shape = input_details[0]['shape']\r\n",
      "\r\n",
      "  input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\r\n",
      "\r\n",
      "  interpreter.set_tensor(input_details[0]['index'], input_data)\r\n",
      "  interpreter.invoke()\r\n",
      "  output_data = [interpreter.get_tensor(output_details[i]['index']) for i in range(len(output_details))]\r\n",
      "\r\n",
      "  print(output_data)\r\n",
      "\r\n",
      "def main(_argv):\r\n",
      "  save_tflite()\r\n",
      "  demo()\r\n",
      "\r\n",
      "if __name__ == '__main__':\r\n",
      "    try:\r\n",
      "        app.run(main)\r\n",
      "    except SystemExit:\r\n",
      "        pass\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**Any other info / logs**\r\n",
      "\r\n",
      "Include any logs or source code that would be helpful to diagnose the problem.\r\n",
      "If including tracebacks, please include the full traceback. Large logs and files\r\n",
      "should be attached.\r\n",
      "\n",
      "issue labels - \n",
      "TFLiteConverter\n",
      "comp:lite\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  YOLOv4 not completely quantized -> Edge TPU compilation fails\n",
      "issue body -  **System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n",
      "- TensorFlow installed from (source or binary): Binary\r\n",
      "- TensorFlow version (or github SHA if from source): tf-nightly\r\n",
      "\r\n",
      "\r\n",
      "**Command used to run the converter or code if you’re using the Python API**\r\n",
      "If possible, please share a link to Colab/Jupyter/any notebook.\r\n",
      "\r\n",
      "```\r\n",
      "import tensorflow as tf\r\n",
      "import numpy as np\r\n",
      "\r\n",
      "def representative_dataset_gen():\r\n",
      "\ti = 1\r\n",
      "\tfor image in range(100):\r\n",
      "\t\timage = np.random.random((1, 416, 416, 3)).astype('float32')\r\n",
      "\t\tprint(i, image.shape)\r\n",
      "\t\ti += 1\r\n",
      "\t\tyield [image]\r\n",
      "\r\n",
      "converter = tf.lite.TFLiteConverter.from_saved_model('/content/drive/MyDrive/YOLO/saved_model')\r\n",
      "converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n",
      "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n",
      "converter.inference_input_type = tf.uint8\r\n",
      "converter.inference_output_type = tf.uint8\r\n",
      "converter._experimental_new_quantizer = True\r\n",
      "converter.allow_custom_ops = True\r\n",
      "converter.representative_dataset = representative_dataset_gen\r\n",
      "tflite_quant_model = converter.convert()\r\n",
      "with open('/content/drive/MyDrive/YOLO/saved_model/yolov4_full_integer_quant.tflite', 'wb') as w:\r\n",
      "\tw.write(tflite_quant_model)\r\n",
      "print(\"Full Integer Quantization complete! - yolov4_full_integer_quant.tflite\")\r\n",
      "```\r\n",
      "\r\n",
      "**The output from the converter invocation**\r\n",
      "\r\n",
      "```\r\n",
      "There is no issue here\r\n",
      "```\r\n",
      "\r\n",
      "**Also, please include a link to the saved model or GraphDef**\r\n",
      "\r\n",
      "The link contains the Saved_Model and the quantized TFLite model.\r\n",
      "https://drive.google.com/drive/folders/1m4KOjdWHmtBpHPau8Oxu7bC5efkdCRy_?usp=sharing\r\n",
      "\r\n",
      "**Failure details**\r\n",
      "If the conversion is successful, but the generated model is wrong,\r\n",
      "state what is wrong:\r\n",
      "- Despite using the target spec BUILTINS_INT8, the model is not quantized completely (exp and log operations)\r\n",
      "- This then causes an issue when using the EdgeTPU compiler, with an error\r\n",
      "```\r\n",
      "Edge TPU Compiler version 15.0.340273435\r\n",
      "Invalid model: /content/drive/MyDrive/Clutterbot/YOLO/saved_model/yolov4_full_integer_quant.tflite\r\n",
      "Model not quantized\r\n",
      "```\r\n",
      "\r\n",
      "@abattery @jingpu Can you please help me out here? I thought Exp and Log operations were common enough to have been quantized, or am I missing something here?\r\n",
      "\n",
      "issue labels - \n",
      "ModelOptimizationToolkit\n",
      "TF 2.5\n",
      "TFLiteConverter\n",
      "comp:lite\n",
      "stat:awaiting tensorflower\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Checksum error with GCC download\n",
      "issue body -  @tensorflow/micro\r\n",
      "\r\n",
      "**Short description**\r\n",
      "\r\n",
      "```\r\n",
      "tensorflow/lite/micro/tools/make/targets/apollo3evb_makefile.inc:14: *** Something went wrong with the GCC download: Bad checksum. Expected: bc8ae26d7c429f30d583a605a4bcf9bc, Got: e588d21be5a0cc9caa60938d2422b058.  Stop.\r\n",
      "```\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Catalina\r\n",
      "- TensorFlow installed from (source or binary): n/a\r\n",
      "- Tensorflow version (commit SHA if source): latest from git clone b860885da901ed641015bc75b9ac06fc4a1c5a57\r\n",
      "- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): SparkFun_edge\r\n",
      "\r\n",
      "**Reproduction instructions**\r\n",
      "\r\n",
      "```\r\n",
      "make -f tensorflow/lite/micro/tools/make/Makefile TARGET=sparkfun_edge hello_world_bin\r\n",
      "```\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "\r\n",
      "**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "comp:micro\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  merge\n",
      "issue body -  **System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n",
      "- TensorFlow installed from (source or binary):\r\n",
      "- TensorFlow version (or github SHA if from source):\r\n",
      "\r\n",
      "\r\n",
      "**Provide the text output from tflite_convert**\r\n",
      "\r\n",
      "```\r\n",
      "# Copy and paste here\r\n",
      "```\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue** \r\n",
      "Provide a reproducible test case that is the bare minimum necessary to generate\r\n",
      "the problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n",
      "\r\n",
      "Also, please include a link to a GraphDef or the model if possible.\r\n",
      "\r\n",
      "**Any other info / logs**\r\n",
      "\r\n",
      "Include any logs or source code that would be helpful to diagnose the problem.\r\n",
      "If including tracebacks, please include the full traceback. Large logs and files\r\n",
      "should be attached.\r\n",
      "\n",
      "issue labels - \n",
      "invalid\n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  Support for LSTM and GRU\n",
      "issue body -  Hello,\r\n",
      "\r\n",
      "I wonder if there is support for LSTM and GRU within TensorFlow Lite for Microcontrollers (audio NN models) or any plan to have these operations? \r\n",
      "\r\n",
      "Thank you.\r\n",
      "\r\n",
      "Best regards,\r\n",
      "Peter\n",
      "issue labels - \n",
      "comp:micro\n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  TF2.4 ParameterServerStrategy tf.range in step_fn is slower than np.arange\n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):**TF on yarn**(running machine system is CentOS 7)\r\n",
      "- TensorFlow installed from (source or binary):binary\r\n",
      "- TensorFlow version (use command below):2.4.0\r\n",
      "- Python version:3.6\r\n",
      "- CUDA/cuDNN version: no gpu\r\n",
      "- GPU model and memory: no gpu\r\n",
      "- CPU info: please see it below\r\n",
      "- Training config: chief:  8cpus,25G memory; worker: 30 instances, 8 cpus per instance, 25G memory per instance;ps: 8 instances, 12 cpus per instance, 25G memory per instance\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "\r\n",
      "I built a model using ParameterServerStrategy, and I used tf.range(steps_per_invocation) in step_fn to execute more steps. But I found there was a about 0.4 second between one loop end and next loop start, but when I changed to np.arnage(steps_per_invocation), the gap can be ignored.This is very similar to https://github.com/tensorflow/tensorflow/issues/40708.\r\n",
      "\r\n",
      "```python\r\n",
      "@tf.function\r\n",
      "def train_fn(iterator):\r\n",
      "    print(\"Flag_A_retrace!!!\", time.time())\r\n",
      "    tf.print(\"Flag_A\", tf.timestamp())\r\n",
      "    losses = 0.0\r\n",
      "    for inner_index in tf.range(FLAGS.steps_per_invocation):\r\n",
      "        tf.print(\"Flag_G\", tf.timestamp(), inner_index)\r\n",
      "        parsed_feature_dict, label_dict, weight_dict = next(iterator)\r\n",
      "        tf.print(\"Flag_H\", tf.timestamp(), inner_index)\r\n",
      "        replica_fn = train_step(model, metrics, optimizer)\r\n",
      "        tf.print(\"Flag_I\", tf.timestamp(), inner_index)\r\n",
      "        losses = strategy.run(replica_fn, args=(parsed_feature_dict, label_dict, weight_dict)) # we just need last loss\r\n",
      "        tf.print(\"Flag_J\", tf.timestamp(), inner_index)\r\n",
      "    tf.print(\"Flag_B\", tf.timestamp())\r\n",
      "    return strategy.reduce(tf.distribute.ReduceOp.SUM, losses, axis=None)\r\n",
      "```\r\n",
      "\r\n",
      "The time between Flag_J and next Flag_G is 0.4 second\r\n",
      "\r\n",
      "```python\r\n",
      "@tf.function\r\n",
      "def train_fn(iterator):\r\n",
      "    print(\"Flag_A_retrace!!!\", time.time())\r\n",
      "    tf.print(\"Flag_A\", tf.timestamp())\r\n",
      "    losses = 0.0\r\n",
      "    for inner_index in np.arange(FLAGS.steps_per_invocation):\r\n",
      "        tf.print(\"Flag_G\", tf.timestamp(), inner_index)\r\n",
      "        parsed_feature_dict, label_dict, weight_dict = next(iterator)\r\n",
      "        tf.print(\"Flag_H\", tf.timestamp(), inner_index)\r\n",
      "        replica_fn = train_step(model, metrics, optimizer)\r\n",
      "        tf.print(\"Flag_I\", tf.timestamp(), inner_index)\r\n",
      "        losses = strategy.run(replica_fn, args=(parsed_feature_dict, label_dict, weight_dict)) # we just need last loss\r\n",
      "        tf.print(\"Flag_J\", tf.timestamp(), inner_index)\r\n",
      "    tf.print(\"Flag_B\", tf.timestamp())\r\n",
      "    return strategy.reduce(tf.distribute.ReduceOp.SUM, losses, axis=None)\r\n",
      "```\r\n",
      "The time between Flag_J and next Flag_G can be ignored.\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "1.  tf.range performance should be the same as np.arange\r\n",
      "2. 0.4 second in just one step is very slow because it accouts for abount 94% of one step execute time.\r\n",
      "\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:dist-strat\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:performance\n",
      "\n",
      "\n",
      "issue title -  Tensorflow Lite Build issue on RK3399 Ubuntu 18.04 aarch64\n",
      "issue body -  @tensorflow/micro\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 18.04 aarch64\r\n",
      "- TensorFlow installed from (source or binary):  source\r\n",
      "- Tensorflow version (commit SHA if source):  098231f\r\n",
      "- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):  RK3399 Ubuntu 18.04 aarch64\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "I was going to build the tensorflow lite with CMake on RK3399 Ubuntu 18.04 aarch64 device.\r\n",
      "\r\n",
      "OS: Ubuntu 18.04 aarch64\r\n",
      "CMake: 3.18.0\r\n",
      "gcc(g++): 10.1.0\r\n",
      "GPU: Mali T860\r\n",
      "OpenCL: 1.2\r\n",
      "\r\n",
      "I cloned the TensorFlow and was going to build tflite supporting ARM GPU (OpenCL) with CMake.\r\n",
      "\r\n",
      "_git clone https://github.com/tensorflow/tensorflow.git\r\n",
      "cd tensorflow\r\n",
      "mkdir tflite_build\r\n",
      "cd tflite_build\r\n",
      "cmake ../tensorflow/lite -DTFLITE_ENABLE_GPU=ON_\r\n",
      "\r\n",
      "The configuration was successful.\r\n",
      "I started to build the command \"cmake --build . -j\" but I met the following error.\r\n",
      "\r\n",
      "![image](https://user-images.githubusercontent.com/47862419/104396628-00109f00-5586-11eb-9fcb-0f57eee17347.png)\r\n",
      "\r\n",
      "\r\n",
      "Could u please let me know how to fix it?\r\n",
      "Thanks\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "comp:lite\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  tensorflow 2.4.0  the predict function outputs an error, when the same code works on another TF versions. \n",
      "issue body -  <em>Please make sure that this is a bug. As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:bug_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n",
      "- OS Platform : Windows 10\r\n",
      "- TensorFlow installed from ): 2.4.0, using instal_tensorflow(version = \"2.4.0\" )\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version: 11.0 / 8.0.2\r\n",
      "- GPU model and memory: RTX 3070\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "I can run this code https://blogs.rstudio.com/ai/posts/2019-08-23-unet/ on the kaggles notebook (which runs TF 2.3.0),\r\n",
      "but on my PC, intel + nvidia 3070, TF 2.4, the code can run and train.  BUT the PREDICT function doesnt work. \r\n",
      "The exact same code the predict function works as expected on Kaggles\r\n",
      "\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "The predicitions functions results in predictions, not errors\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "https://www.kaggle.com/rpsantosakaggle/unet-carvana\r\n",
      "\r\n",
      "**Other info / logs** \r\n",
      " predictions <- predict(model, batch)\r\n",
      "Error in py_call_impl(callable, dots$args, dots$keywords) : \r\n",
      "  ValueError: in user code:\r\n",
      "\r\n",
      "    C:\\Users\\rpsan\\AppData\\Local\\R-MINI~1\\envs\\R-RETI~1\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1478 predict_function  *\r\n",
      "        return step_function(self, iterator)\r\n",
      "    C:\\Users\\rpsan\\AppData\\Local\\R-MINI~1\\envs\\R-RETI~1\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1468 step_function  **\r\n",
      "        outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n",
      "    C:\\Users\\rpsan\\AppData\\Local\\R-MINI~1\\envs\\R-RETI~1\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1259 run\r\n",
      "        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n",
      "    C:\\Users\\rpsan\\AppData\\Local\\R-MINI~1\\envs\\R-RETI~1\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2730 call_for_each_replica\r\n",
      "        return self._call_for_each_replica(fn, args, kwargs)\r\n",
      "    C:\\Users\\rpsan\\AppData\\Local\\R-MINI~1\\envs\\R-RETI~1\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3417 _call_for_each_re\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "*have run many other codes that worked seamlesly, but with this one, it bugs. \r\n",
      " tf_config()\r\n",
      "TensorFlow v2.4.0 ()\r\n",
      "Python v3.6 (C:/Users/rpsan/AppData/Local/r-miniconda/envs/r-reticulate/python.exe)\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:keras\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Port micro operator EXP and its test code\n",
      "issue body -  Combined PR4 and PR5 for issue #45415. This PR, if merged, should finish the issue.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "prtype:bugfix\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  TF-TRT Test ConvertConcat in dynamic shape mode\n",
      "issue body -  This PR adds explicit batch and dynamic shape mode tests to ConvertConcat.\r\n",
      "\r\n",
      "Tagging @bixia1 for review and @DEKHTIARJonathan for visibility.\r\n",
      "\r\n",
      "Tracker: #45481\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:gpu:tensorrt\n",
      "ready to pull\n",
      "size:L\n",
      "\n",
      "\n",
      "issue title -  TF-TRT improve documentation of execution context handling\n",
      "issue body -  This PR improves the description of the execution context handling by TF-TRT.\r\n",
      "\r\n",
      "The synchronization related to execution context management was discussed in issue #36959. Relevant parts from that discussion are added as comments to the EngineContext class.\r\n",
      "\r\n",
      "Tagging @bixia1 for review.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:gpu:tensorrt\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Model Subclassing 'NoneType' object has no attribute 'shape'\n",
      "issue body -  <em>Please make sure that this is a bug. As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:bug_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "- TensorFlow installed from (source or binary):\r\n",
      "- TensorFlow version (use command below):\r\n",
      "- Python version:\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version:\r\n",
      "- GPU model and memory:\r\n",
      "\r\n",
      "You can collect some of this information using our environment capture\r\n",
      "[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n",
      "You can also obtain the TensorFlow version with:\r\n",
      "1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n",
      "2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n",
      "\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "class MyVGG(tf.keras.Model):\r\n",
      "   I don't understand why is the problem please help!\r\n",
      " \r\n",
      "    def __init__(self, num_classes):\r\n",
      "        super(MyVGG, self).__init__()\r\n",
      "        \r\n",
      "        #self.input_l = tf.keras.layers.Conv2D(filters=64,kernel_size=3, activation='relu', input_shape=(150,150,3))\r\n",
      "        #create blocks of Vgg \r\n",
      "        self.block_a = Block(filters=64 ,  kernel_size=3, repetitions= 2) \r\n",
      "        self.block_b = Block(filters=128 , kernel_size=3 , repetitions= 2) \r\n",
      "        self.block_c = Block(filters=256 , kernel_size=3, repetitions= 3) \r\n",
      "        self.block_d = Block(filters=512 , kernel_size=3 , repetitions= 3) \r\n",
      "        self.block_e = Block(filters=512 , kernel_size=3 , repetitions= 3) \r\n",
      "        \r\n",
      "        # Define a Flatten layer\r\n",
      "        self.flatten = tf.keras.layers.Flatten()\r\n",
      "        # Define a Dense Layer \r\n",
      "        self.dense = tf.keras.layers.Dense(256, activation='relu')\r\n",
      "        # Add  the softmax classifier using a Dense layer\r\n",
      "        self.classifier = tf.keras.layers.Dense(num_classes, activation='softmax')\r\n",
      "    \r\n",
      "    def call(self, inputs):\r\n",
      "    \r\n",
      "        #x = self.input_l(inputs)\r\n",
      "        #Chain all the layers \r\n",
      "        x = self.block_a(inputs) \r\n",
      "        x = self.block_b(x) \r\n",
      "        x = self.block_c(x) \r\n",
      "        x = self.block_d(x) \r\n",
      "        x = self.block_e(x) \r\n",
      "        x = self.flatten(x) \r\n",
      "        x = self.dense(x) \r\n",
      "        x = self.classifier(x) \r\n",
      "        return x\r\n",
      "model_vgg = MyVGG(num_classes=2)\r\n",
      "\r\n",
      "model_vgg.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\r\n",
      "\r\n",
      "history = model_vgg.fit_generator(train_generator, \r\n",
      "                                  epochs=10, \r\n",
      "                                  verbose=1, \r\n",
      "                                  validation_data = validation_generator)\r\n",
      "\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py in _maybe_build(self, inputs)\r\n",
      "   2616     if not self.built:\r\n",
      "   2617       input_spec.assert_input_compatibility(\r\n",
      "-> 2618           self.input_spec, inputs, self.name)\r\n",
      "   2619       input_list = nest.flatten(inputs)\r\n",
      "   2620       if input_list and self._dtype_policy.compute_dtype is None:\r\n",
      "\r\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/input_spec.py in assert_input_compatibility(input_spec, inputs, layer_name)\r\n",
      "    164         spec.min_ndim is not None or\r\n",
      "    165         spec.max_ndim is not None):\r\n",
      "--> 166       if x.shape.ndims is None:\r\n",
      "    167         raise ValueError('Input ' + str(input_index) + ' of layer ' +\r\n",
      "    168                          layer_name + ' is incompatible with the layer: '\r\n",
      "\r\n",
      "AttributeError: 'NoneType' object has no attribute 'shape'\r\n",
      "\r\n",
      "**Other info / logs** Include any logs or source code that would be helpful to\r\n",
      "diagnose the problem. If including tracebacks, please include the full\r\n",
      "traceback. Large logs and files should be attached.\r\n",
      "\n",
      "issue labels - \n",
      "comp:keras\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Problem with pypi package installation in linux debian(parrot os)\n",
      "issue body -  <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Parrot Os 4.10 64bit\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "- TensorFlow installed from (source or binary):pypi\r\n",
      "- TensorFlow version:\r\n",
      "- Python version:3.9.1\r\n",
      "- Installed using virtualenv? pip? conda?:\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version:\r\n",
      "- GPU model and memory:\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "When i run the following commands either with pip3 or just pip basically, i run into an issue like :\r\n",
      "`ERROR: Could not find a version that satisfies the requirement tensorflow\r\n",
      "ERROR: No matching distribution found for tensorflow\r\n",
      "`\r\n",
      "I even read the article from this website\r\n",
      "[https://tutorialforlinux.com/2020/10/02/step-by-step-tensorflow-parrot-linux-installation-guide/](tutorialforlinux)\r\n",
      "I have run the various commands yet to no avail\r\n",
      "**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n",
      "pip install tensorflow\r\n",
      "pip install tensorflow-cpu\r\n",
      "pip install tf-nightly\r\n",
      "pip install tf-nightly-gpu\r\n",
      "\r\n",
      "**Any other info / logs**\r\n",
      "Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n",
      "\n",
      "issue labels - \n",
      "stat:awaiting response\n",
      "subtype: ubuntu/linux\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  [TFTRT - Dynamic Shape Phase 3] Add Dynamic Shape Testing for ConvertResize & Various TF2TRT Node Convert Unittest Improvements\n",
      "issue body -  @bixia1 @tfeher for review\r\n",
      "<!--\r\n",
      "- add Dynamic Shape Testing for ConvertResize.\r\n",
      "- add `DebugString(const TrtPrecisionMode)` in file `tensorflow/compiler/tf2tensorrt/convert/utils.[h/cc]`\r\n",
      "- add `DebugString(const DataType)` in file `tensorflow/compiler/tf2tensorrt/convert/utils.[h/cc]`\r\n",
      "- add `DebugString(const TrtTestMode)` in file `tensorflow/compiler/tf2tensorrt/convert/convert_nodes_test.cc`\r\n",
      "- add LOG(INFO) data for each GTest executed from the base class `ParameterizedOpConverterTestBase`\r\n",
      "  - `tf_type_`\r\n",
      "  - `trt_mode_`\r\n",
      "  - `converter_precision_`\r\n",
      "- add C++ public getters in class `ParameterizedOpConverterTestBase` for the attributes:\r\n",
      "  - `tf_type_`\r\n",
      "  - `trt_mode_`\r\n",
      "  - `converter_precision_`\r\n",
      "  -->\r\n",
      "ConvertResize now requires a static shape for the tensor representing the resized size to workaround the fact that we can't support shape tensors as inputs yet.\r\n",
      "Add dynamic shape test cases for ConvertResize.\r\n",
      "\r\n",
      "\r\n",
      "Feature Tracker: #45481\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:gpu:tensorrt\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  [Cherrypick:2.4] Fix an exception when profiler is stopped twice\n",
      "issue body -  None\n",
      "issue labels - \n",
      "cla: yes\n",
      "waiting for patch release\n",
      "\n",
      "\n",
      "issue title -  Fix\n",
      "issue body -  None\n",
      "issue labels - \n",
      "cla: yes\n",
      "\n",
      "\n",
      "issue title -  Change from model.predict_proba to  using model.predict\n",
      "issue body -  model.predict_proba is deprecated in the Keras code base, and should be replaced by model.predict\r\n",
      "This replaces the usage of the deprecated call in the scikit_learn wrapper\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:keras\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Exporting a TF Estimator with a `hub.text_embedding_column_v2` does not include the pretrained model\n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): AI Platform runtime 2.3 or Google Colab\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n",
      "- TensorFlow installed from (source or binary): pip\r\n",
      "- TensorFlow version (use command below): 2.4\r\n",
      "- Python version: 3.6.9\r\n",
      "- Bazel version (if compiling from source): N/A\r\n",
      "- GCC/Compiler version (if compiling from source): N/A\r\n",
      "- CUDA/cuDNN version: N/A\r\n",
      "- GPU model and memory: N/A\r\n",
      "\r\n",
      "This issue has been observed with the estimator API only, not Keras. I'm not entirely sure if it's a problem with estimator, export, or a tf hub issue.\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "\r\n",
      "When exporting a TF Estimator which uses a `hub.text_embedding_column_v2` (the new version of tf-hub's `text_embedding_column_v2` that supports pretrained models in TF 2 SavedModel format), the export directory does not include the pretrained model asset files. Instead, the model seems to reference the local path the model was trained with.\r\n",
      "When `text_embedding_column_v2` is fed with a url (i.e. `tfhub.dev/../my-model`), the model references the cached local path.\r\n",
      "\r\n",
      "When the local path doesn't exist anymore, or when using the exported model in a different machine altogether, calling `tf.saved_model.load` fails with a `FileNotFound` exception:\r\n",
      "\r\n",
      "When defining the embedding column with a local path to the pretrained model:\r\n",
      "```\r\n",
      " ...\r\n",
      " hub.text_embedding_column_v2('/home/.../my-local-model')\r\n",
      " ...\r\n",
      " estimator = ...\r\n",
      " ...\r\n",
      " tf.estimator.export_saved_model(...)\r\n",
      "\r\n",
      "...\r\n",
      " # on another machine or when removing /home/.../my-local-model:\r\n",
      "...\r\n",
      "tf.saved_model.load(...)\r\n",
      "\r\n",
      "NotFoundError:  /content/local_nnlm50_2/assets/tokens.txt; No such file or directory\r\n",
      "\t [[{{node dnn/input_from_feature_columns/input_layer/text_feature0_hub_module_embedding/StatefulPartitionedCall_1/StatefulPartitionedCall/text_file_init/InitializeTableFromTextFileV2}}]] [Op:__inference_pruned_10883]\r\n",
      "```\r\n",
      "\r\n",
      "When defining the embedding column with a cached pretrained model from a `tfhub.dev` url (not reproducible in Colab since tf hub cache doesn't kick in there):\r\n",
      "```\r\n",
      " ...\r\n",
      " hub.text_embedding_column_v2('https://tfhub.dev/google/nnlm-en-dim50/2')\r\n",
      " ...\r\n",
      " estimator = ...\r\n",
      " ...\r\n",
      " tf.estimator.export_saved_model(...)\r\n",
      "\r\n",
      "...\r\n",
      " # on another machine or when removing cache dir /tmp/tfhub_modules:\r\n",
      "...\r\n",
      "tf.saved_model.load(...)\r\n",
      "\r\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\r\n",
      "    inputs, attrs, num_outputs)\r\n",
      "tensorflow.python.framework.errors_impl.NotFoundError:  /tmp/tfhub_modules/74a841d6eb84e8d93d913d716fb5440d020cc291/assets/tokens.txt; No such file or directory\r\n",
      "         [[{{node dnn/input_from_feature_columns/input_layer/topClickQuery_1st_hub_module_embedding/StatefulPartitionedCall_1/StatefulPartitionedCall/text_file_init/InitializeTableFromTextFileV2}}]] [Op:__inference_pruned_1658]\r\n",
      "```\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "\r\n",
      "The expected behavior is for the export directory to include the pretrained model's assets, like it does when using `hub.text_embedding_column` (v1) with TF 1 format modules. And more importantly: loading the exported estimators should not fail even when loaded in a different machine, as well as not when the original local path to the pretrained model doesn't exist anymore.\r\n",
      "The entire exported estimator should be contained within the export directory. \r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "Here's a Colab replicating the issue:\r\n",
      "https://colab.research.google.com/drive/1Tr5M0m_EVRd5sFdgDX-EbVsk47UokPxD?usp=sharing\r\n",
      "\r\n",
      "Overview of Colab:\r\n",
      "- Download both versions of nnlm50 to local (TF 1 module, TF 2 SavedModel)\r\n",
      "- Define two estimators: one with `text_embedding_column` (v1) and the local TF 1 format pretrained model, and the other with `text_embedding_column_v2` and the local TF 2 format pretrained model\r\n",
      "- Train and export the estimators to SavedModels\r\n",
      "- Print contents of export dir: **TF 1 format has the nnlm50 assets, TF 2 format does not**\r\n",
      "- Try to load both estimators from disk: **success**\r\n",
      "- Delete both local nnlm50 directories used for training the estimators\r\n",
      "- Try to load the v1 estimator from disk again: **success**\r\n",
      "- Try to load the v2 estimator from disk again: **fails**\r\n",
      "\r\n",
      "**Other info / logs** Include any logs or source code that would be helpful to\r\n",
      "diagnose the problem. If including tracebacks, please include the full\r\n",
      "traceback. Large logs and files should be attached.\r\n",
      "\n",
      "issue labels - \n",
      "Fixed in Nightly\n",
      "TF 2.4\n",
      "comp:apis\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  [INTEL MKL] DNN 0.x final code cleanup\n",
      "issue body -  This final DNN 0.x code cleanup removes the header file which defines MACRO's that are not usable any more\r\n",
      "   1. delete util/mkl_types.h\r\n",
      "   2. update util/mkl_util.h, to un-include mkl_types.h\r\n",
      "   3. update util/BUILD\r\n",
      "\r\n",
      "It also addresses previous merge problems (code cleanup was accidently overwritten, or somehow missed).  with the following source files:\r\n",
      "\r\n",
      "   1. kernels/mkl/mkl_matmul_ops_common.h \r\n",
      "   2. kernels/mkl/mkl_dequantize_op_test.cc\r\n",
      "   3. util/mkl_util_test.cc\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:mkl\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  Add missing space in error message\n",
      "issue body -  \n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:keras\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  [ROCm] Fix for breakage in ROCm support - 210112\n",
      "issue body -  The following commit introduces a new unit-test `//tensorflow/compiler/xla/service/gpu/tests:mlir_sorting_test`, which fails on the ROCm platform.\r\n",
      "\r\n",
      "https://github.com/tensorflow/tensorflow/commit/70502be4a5941b6ffa2b8c33bb549657b33976da\r\n",
      "\r\n",
      "The test fails on the ROCm platform because the underlying code for it is CUDA-centric. This PR/commit updates the test to make it work on the ROCm platform as well.\r\n",
      "\r\n",
      "----------------------------------------------------\r\n",
      "\r\n",
      "/cc @cheshire @chsigg @nvining-work \n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:gpu\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  MultiWorkerMirroredStrategy documents old communication=tf.distribute.experimental.CollectiveCommunication.NCCL\n",
      "issue body -  ## URL(s) with the issue:\r\n",
      "\r\n",
      "https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras\r\n",
      "\r\n",
      "## Description of issue (what needs changing):\r\n",
      "\r\n",
      "The page says\r\n",
      "> To override the automatic choice, specify a valid value to the communication parameter of MultiWorkerMirroredStrategy's constructor, e.g. communication=tf.distribute.experimental.CollectiveCommunication.NCCL.\r\n",
      "\r\n",
      "However `MultiWorkerMirroredStrategy`s ctor has changed to expect a `communication_options` parameter instead.\r\n",
      "\r\n",
      "It should be described on this page how to actually change the distribution implementation with current TF 2.4\r\n",
      "\r\n",
      "Note that the deprecated `tf.distribute.experimental.MultiWorkerMirroredStrategy()` still has that parameter but TF warns about usage of this class and the tutorial page doesn't mention that (old) class.\n",
      "issue labels - \n",
      "comp:dist-strat\n",
      "stat:awaiting response\n",
      "type:docs-bug\n",
      "\n",
      "\n",
      "issue title -  add support for concatenated gzip in ZlibInputStream\n",
      "issue body -  This PR will fix #45137.\r\n",
      "\r\n",
      "To support concatenated gzip, we need to call `inflateReset()` when `inflate()` returns `Z_STREAM_END`, as the zlib's doc explained:\r\n",
      "> Unlike the gunzip utility and gzread() (see below), inflate() will not automatically decode concatenated gzip streams. inflate() will return Z_STREAM_END at the end of the gzip stream. The state would need to be reset to continue decoding a subsequent gzip stream.\r\n",
      "\r\n",
      "from https://github.com/madler/zlib/blob/cacf7f1d4e3d44d871b605da3b647f07d718623f/zlib.h#L868\r\n",
      "\r\n",
      "Thank you for your time on reviewing this pr.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:core\n",
      "prtype:bugfix\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Kernel Silently dies while Generating an Image\n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n",
      "- TensorFlow installed from (source or binary): binary\r\n",
      "- TensorFlow version (use command below):2.5.0-dev20210111\r\n",
      "- Python version: 3.8.6\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version: 11.1 ,8.0.5\r\n",
      "- GPU model and memory: RTX 3070 8GB\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "The Kernel silently dies when it tried to generate the image made via the generator.\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "Should generate the image and continue the code\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "[Using the example here on the Tensorflow site for GANs](https://www.tensorflow.org/tutorials/generative/dcgan)\r\n",
      "While executing it till we generate the image via the untrained generator. The notebook just stops, looking at the logs shows that the kernel then gets restarted.\r\n",
      "\r\n",
      "**Other info / logs**\r\n",
      "```\r\n",
      "[I 2021-01-12 16:02:30.618 ServerApp] Kernel started: 0227f971-d1fe-44a3-8db7-a948217de0bb\r\n",
      "2021-01-12 16:02:51.057848: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n",
      "2021-01-12 16:02:54.205477: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n",
      "2021-01-12 16:02:54.226746: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1760] Found device 0 with properties:\r\n",
      "pciBusID: 0000:0a:00.0 name: GeForce RTX 3070 computeCapability: 8.6\r\n",
      "coreClock: 1.77GHz coreCount: 46 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n",
      "2021-01-12 16:02:54.226918: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n",
      "2021-01-12 16:02:54.258395: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n",
      "2021-01-12 16:02:54.258550: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n",
      "2021-01-12 16:02:54.279408: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n",
      "2021-01-12 16:02:54.284138: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n",
      "2021-01-12 16:02:54.345037: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n",
      "2021-01-12 16:02:54.348822: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n",
      "2021-01-12 16:02:54.350356: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n",
      "2021-01-12 16:02:54.350512: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1902] Adding visible gpu devices: 0\r\n",
      "2021-01-12 16:02:57.436724: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\r\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n",
      "2021-01-12 16:02:57.437783: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1760] Found device 0 with properties:\r\n",
      "pciBusID: 0000:0a:00.0 name: GeForce RTX 3070 computeCapability: 8.6\r\n",
      "coreClock: 1.77GHz coreCount: 46 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n",
      "2021-01-12 16:02:57.438036: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1902] Adding visible gpu devices: 0\r\n",
      "2021-01-12 16:02:57.821516: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n",
      "2021-01-12 16:02:57.821707: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1306]      0\r\n",
      "2021-01-12 16:02:57.821818: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1319] 0:   N\r\n",
      "2021-01-12 16:02:57.822077: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1446] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5474 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3070, pci bus id: 0000:0a:00.0, compute capability: 8.6)\r\n",
      "2021-01-12 16:03:00.249533: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n",
      "2021-01-12 16:03:00.804621: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n",
      "2021-01-12 16:03:00.805722: I tensorflow/stream_executor/cuda/cuda_blas.cc:1838] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\r\n",
      "2021-01-12 16:03:00.810208: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n",
      "2021-01-12 16:03:01.400017: I tensorflow/stream_executor/cuda/cuda_dnn.cc:334] Loaded cuDNN version 8005\r\n",
      "```\r\n",
      "\r\n",
      "The kernel then crashes after this and jupyter restarts it\r\n",
      "\r\n",
      "```\r\n",
      "[I 2021-01-12 16:03:39.603 ServerApp] KernelRestarter: restarting kernel (1/5), keep random ports\r\n",
      "kernel 0227f971-d1fe-44a3-8db7-a948217de0bb restarted\r\n",
      "kernel 0227f971-d1fe-44a3-8db7-a948217de0bb restarted\r\n",
      "[I 2021-01-12 16:03:39.626 ServerApp] Starting buffering for 0227f971-d1fe-44a3-8db7-a948217de0bb:c751d222-0788-4d6b-8fb6-b564f4dd14f5\r\n",
      "```\n",
      "issue labels - \n",
      "TF 2.5\n",
      "comp:gpu\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  ModuleNotFoundError: No module named 'tensorflow.core'\n",
      "issue body -  <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Windows 10):\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "- Installed from whl:  tensorflow_gpu-2.4.0-cp36-cp36m-win_amd64.whl\r\n",
      "- TensorFlow version: 2.4.0\r\n",
      "- Python version: 3.6\r\n",
      "- Installed using virtualenv? pip? conda?: pip within conda\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version: 10.1\r\n",
      "- GPU model and memory: NVIDIA GeForce RTX 2080 Ti\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "\r\n",
      "I have installed the following packages in a conda environment\r\n",
      "absl-py                   0.4.1                     <pip>\r\n",
      "absl-py                   0.7.1                    py36_0    <unknown>\r\n",
      "absl-py                   0.10.0                    <pip>\r\n",
      "astunparse                1.6.3                     <pip>\r\n",
      "beautifulsoup4            4.9.3                     <pip>\r\n",
      "cachetools                4.1.0                     <pip>\r\n",
      "certifi                   2018.4.16                py36_0\r\n",
      "chardet                   4.0.0                     <pip>\r\n",
      "Cython                    0.29.14                   <pip>\r\n",
      "flatbuffers               1.12                      <pip>\r\n",
      "gast                      0.3.3                     <pip>\r\n",
      "google                    3.0.0                     <pip>\r\n",
      "google-auth               1.14.1                    <pip>\r\n",
      "google-auth-oauthlib      0.4.2                     <pip>\r\n",
      "google-pasta              0.2.0                     <pip>\r\n",
      "grpcio                    1.32.0                    <pip>\r\n",
      "h5py                      2.10.0                    <pip>\r\n",
      "idna                      2.10                      <pip>\r\n",
      "Keras                     2.4.3                     <pip>\r\n",
      "Keras-Preprocessing       1.1.2                     <pip>\r\n",
      "Markdown                  3.2.1                     <pip>\r\n",
      "numpy                     1.19.2                    <pip>\r\n",
      "oauthlib                  3.1.0                     <pip>\r\n",
      "opt-einsum                3.3.0                     <pip>\r\n",
      "packaging                 20.8                      <pip>\r\n",
      "pip                       10.0.1                   py36_0\r\n",
      "pkgconfig                 1.5.1                     <pip>\r\n",
      "protobuf                  3.14.0                    <pip>\r\n",
      "pyasn1                    0.4.5                      py_0    <unknown>\r\n",
      "pyasn1                    0.4.8                     <pip>\r\n",
      "pyasn1-modules            0.2.8                     <pip>\r\n",
      "pyparsing                 2.4.7                     <pip>\r\n",
      "python                    3.6.5                h0c2934d_0\r\n",
      "PyYAML                    5.3                       <pip>\r\n",
      "requests                  2.25.1                    <pip>\r\n",
      "requests-oauthlib         1.3.0                     <pip>\r\n",
      "rsa                       4.0                       <pip>\r\n",
      "scipy                     1.5.2                     <pip>\r\n",
      "setuptools                39.1.0                   py36_0\r\n",
      "setuptools                46.2.0                    <pip>\r\n",
      "six                       1.15.0                    <pip>\r\n",
      "soupsieve                 2.1                       <pip>\r\n",
      "tensorboard               2.4.0                     <pip>\r\n",
      "tensorboard-plugin-wit    1.7.0                     <pip>\r\n",
      "tensorflow-estimator      2.4.0                     <pip>\r\n",
      "tensorflow-gpu            2.4.0                     <pip>\r\n",
      "termcolor                 1.1.0                     <pip>\r\n",
      "typing-extensions         3.7.4.1                   <pip>\r\n",
      "urllib3                   1.26.2                    <pip>\r\n",
      "vc                        14                   h0510ff6_3\r\n",
      "vs2015_runtime            14.0.25123                    3\r\n",
      "Werkzeug                  1.0.1                     <pip>\r\n",
      "wheel                     0.35.0                    <pip>\r\n",
      "wheel                     0.31.1                   py36_0\r\n",
      "wincertstore              0.2              py36h7fe50ca_0\r\n",
      "wrapt                     1.12.1                    <pip>\r\n",
      "\r\n",
      "I am working within a firewall so packages are installed painstakingly one by one. It takes days to get it right\r\n",
      "\r\n",
      "Running python code\r\n",
      "import numpy as np\r\n",
      "import tensorflow as tf\r\n",
      "import google\r\n",
      "import datetime\r\n",
      "from tensorflow import keras\r\n",
      "import os\r\n",
      "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\r\n",
      "\r\n",
      "(Running from command line)\r\n",
      "\r\n",
      "I get:\r\n",
      "\r\n",
      " c:\\Users\\mosheho\\NLP>python Keras_embeddings.py\r\n",
      "2021-01-12 14:24:20.125275: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"Keras_embeddings.py\", line 51, in <module>\r\n",
      "    from tensorflow import keras\r\n",
      "  File \"C:\\Users\\mosheho\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow2_test\\lib\\site-packages\\tensorflow\\keras\\__init__.py\", line 14, in <module>\r\n",
      "    from . import activations\r\n",
      "  File \"C:\\Users\\mosheho\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow2_test\\lib\\site-packages\\tensorflow\\keras\\activations\\__init__.py\", line 10, in <module>\r\n",
      "    from tensorflow.python.keras.activations import deserialize\r\n",
      "  File \"C:\\Users\\mosheho\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow2_test\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 41, in <module>\r\n",
      "    from tensorflow.python.eager import context\r\n",
      "  File \"C:\\Users\\mosheho\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow2_test\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\", line 32, in <module>\r\n",
      "    from tensorflow.core.framework import function_pb2\r\n",
      "ModuleNotFoundError: No module named 'tensorflow.core'\r\n",
      "\r\n",
      "I do not know where to start. This is in TF 2.4 where most issues have been in TF 1 versions\r\n",
      "Thanks\r\n",
      "\r\n",
      "**Any other info / logs**\r\n",
      "Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "stalled\n",
      "stat:awaiting response\n",
      "subtype:windows\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  added double braces to initialization\n",
      "issue body -  Not raised an issue, however this fixes and error with the `expected_output_data.h`.\r\n",
      "\r\n",
      "Issue came about when running:\r\n",
      "\r\n",
      "```\r\n",
      "make -f tensorflow/lite/micro/tools/make/Makefile test\r\n",
      "```\r\n",
      "\r\n",
      "Error message was:\r\n",
      "\r\n",
      "```\r\n",
      "warning: suggest braces around initialization of subobject [-Wmissing-braces]\r\n",
      "```\r\n",
      "\r\n",
      "This failed on:\r\n",
      "\r\n",
      "```\r\n",
      "Apple clang version 12.0.0 (clang-1200.0.32.27)\r\n",
      "Target: x86_64-apple-darwin19.6.0\r\n",
      "Thread model: posix\r\n",
      "InstalledDir: /Library/Developer/CommandLineTools/usr/bin\r\n",
      "```\r\n",
      "\r\n",
      "Fix:\r\n",
      "\r\n",
      "added set of braces around the initialisation.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  [tf.data] Implement SkipInternal for TFRecordDataset\n",
      "issue body -  This is a PR from JIZHI Team & TaiJi AI platform in Tencent.\r\n",
      "\r\n",
      "This PR is a follow-up of #40963 and #41222. In those 2 PRs, I added the `Skip` method to `IteratorBase` and `RecordReader` so that we can potentially reduce the IO when user is using `tf.data.Dataset.shard()` method on samples instead of filenames.\r\n",
      "\r\n",
      "This PR adds the `SkipInternal` for `TFRecordDatasetOp`. The implementation is very similar to `GetNextInternal`. And to test the new method, I added the helper functions and macros for `Skip`, which are also very similar to that of `GetNext`.\r\n",
      "\r\n",
      "I'm not sure the proper way to deal with `metrics::GetTFDataBytesReadCounter` in `SkipInternal`, so it is omitted for now. It would be great if you could give me some hints on it.\r\n",
      "\r\n",
      "Sorry for this late follow-up, I would try to add the meaningful `SkipInternal`s in the coming months.\r\n",
      "\r\n",
      "cc @aaudiber @jsimsa \n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:data\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  how to use doc\n",
      "issue body -  Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\n",
      "policy, we only address code/doc bugs, performance issues, feature requests, and\r\n",
      "build/installation issues on GitHub.\r\n",
      "\r\n",
      "The TensorFlow docs are open source! To get involved, read the documentation\r\n",
      "contributor guide: https://www.tensorflow.org/community/contribute/docs\r\n",
      "\r\n",
      "## URL(s) with the issue:\r\n",
      "\r\n",
      "Please provide a link to the documentation entry, for example:\r\n",
      "https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod\r\n",
      "\r\n",
      "## Description of issue (what needs changing):\r\n",
      "\r\n",
      "### Clear description\r\n",
      "\r\n",
      "For example, why should someone use this method? How is it useful?\r\n",
      "\r\n",
      "### Correct links\r\n",
      "\r\n",
      "Is the link to the source code correct?\r\n",
      "\r\n",
      "### Parameters defined\r\n",
      "\r\n",
      "Are all parameters defined and formatted correctly?\r\n",
      "\r\n",
      "### Returns defined\r\n",
      "\r\n",
      "Are return values defined?\r\n",
      "\r\n",
      "### Raises listed and defined\r\n",
      "\r\n",
      "Are the errors defined? For example,\r\n",
      "https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n",
      "\r\n",
      "### Usage example\r\n",
      "\r\n",
      "Is there a usage example?\r\n",
      "\r\n",
      "See the API guide: https://www.tensorflow.org/community/contribute/docs_ref\r\n",
      "on how to write testable usage examples.\r\n",
      "\r\n",
      "### Request visuals, if applicable\r\n",
      "\r\n",
      "Are there currently visuals? If not, will it clarify the content?\r\n",
      "\r\n",
      "### Submit a pull request?\r\n",
      "\r\n",
      "Are you planning to also submit a pull request to fix the issue? See the docs\r\n",
      "contributor guide: https://www.tensorflow.org/community/contribute/docs,\r\n",
      "docs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\n",
      "docs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n",
      "\n",
      "issue labels - \n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  '<=' not supported between instances of 'list' and 'int' when using StringLookup\n",
      "issue body -  tf version: 2.4.0\r\n",
      "\r\n",
      "```\r\n",
      "vocab = [\"a\", \"b\", \"c\", \"d\"]\r\n",
      "table = tf.keras.layers.experimental.preprocessing.StringLookup(vocab)\r\n",
      "```\r\n",
      "\r\n",
      "> ---------------------------------------------------------------------------\r\n",
      "> TypeError                                 Traceback (most recent call last)\r\n",
      "> <ipython-input-98-cc6cfb4d8df0> in <module>\r\n",
      "> ----> 1 table = tf.keras.layers.experimental.preprocessing.StringLookup(vocab)\r\n",
      "> \r\n",
      "> /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/preprocessing/string_lookup.py in __init__(self, max_tokens, num_oov_indices, mask_token, oov_token, vocabulary, encoding, invert, **kwargs)\r\n",
      ">     197         vocabulary=vocabulary,\r\n",
      ">     198         invert=invert,\r\n",
      "> --> 199         **kwargs)\r\n",
      ">     200     base_preprocessing_layer._kpl_gauge.get_cell(\"V2\").set(\"StringLookup\")\r\n",
      ">     201\r\n",
      "> \r\n",
      "> /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/preprocessing/index_lookup.py in __init__(self, max_tokens, num_oov_indices, mask_token, oov_token, vocabulary, invert, **kwargs)\r\n",
      ">      92     # If max_tokens is set, the value must be greater than 1 - otherwise we\r\n",
      ">      93     # are creating a 0-element vocab, which doesn't make sense.\r\n",
      "> ---> 94     if max_tokens is not None and max_tokens <= 1:\r\n",
      ">      95       raise ValueError(\"If set, `max_tokens` must be greater than 1.\")\r\n",
      ">      96\r\n",
      "> \r\n",
      "> TypeError: '<=' not supported between instances of 'list' and 'int'\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:keras\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Docker+tf-gpu2.1.0: No such file or directory; /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "issue body -  <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- OS Platform and Distribution : linux ubuntu 18.04\r\n",
      "- TensorFlow installed from (source or binary): binary\r\n",
      "- TensorFlow version: 2.1.0\r\n",
      "- Python version: 3.6.3\r\n",
      "- Installed using virtualenv? pip? conda?:  container\r\n",
      "- Bazel version (if compiling from source): \r\n",
      "- GCC/Compiler version (if compiling from source): 7.5\r\n",
      "- CUDA/cuDNN version: 10.1\r\n",
      "- GPU model and memory: rtx2080ti  12GB\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "\r\n",
      "I try to install through the mirror of NVIDIA, \r\n",
      "image info:\r\n",
      "nvidia/cuda: 10.1-cudnn7-devel-ubuntu18.04\r\n",
      "server diver info:\r\n",
      " NVIDIA-SMI 418.56       Driver Version: 418.56       CUDA Version: 10.1\r\n",
      "`nvdia-smi` can work.\r\n",
      "After finishing installing tf,\r\n",
      "\r\n",
      "\r\n",
      "**Any other info / logs**\r\n",
      " W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\n",
      "\r\n",
      "W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\n",
      "\r\n",
      " W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.1\n",
      "comp:gpu\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Merge pull request #1 from tensorflow/master\n",
      "issue body -  Bring up to date with upstream\n",
      "issue labels - \n",
      "cla: no\n",
      "invalid\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Rename directories to have underscores instead of dashes (cmsis-nn -> cmsis_nn and ethos-u -> ethos_u)\n",
      "issue body -  Fixes http://b/168824958\r\n",
      "- Rename ethos-u to ethos_u.\r\n",
      "- cmsis-nn -> cmsis_nn\r\n",
      "\r\n",
      "@mansnils @freddan80 : There is one more cleanup that might be worth addressing as part of this cleanup, and that is the use of OPTIMIZED_KERNEL_DIR instead of TAGS:\r\n",
      "\r\n",
      "The issue at hand is that we are moving towards explicitly including the specific target and optimized_kernel dirs that are needed (as opposed to including all the *.inc and then filtering based on tags):\r\n",
      "https://github.com/tensorflow/tensorflow/blob/161fcca9a7767a63a304a875ff26d22026d5e1b5/tensorflow/lite/micro/tools/make/Makefile#L560-L583\r\n",
      "\r\n",
      "\r\n",
      "What this means is that we would like to change the make command to be:\r\n",
      "```\r\n",
      "make TARGET=cortex_m_generic OPTIMIZED_KERNEL_DIR=cmsis_nn\r\n",
      "```\r\n",
      "\r\n",
      "The catch is what to do about ethos_u since with this new approach we won't be including ext_libs/ethos_u.inc or specializing kernels/ethos_u/ethosu.cc unless we did something special.\r\n",
      "\r\n",
      "An option would be to have cortex_m_generic_makefile.inc include ext_libs/ethos_u.inc (depending on a command line option). And have ethos_u.inc explicitly specialize kernels/ethos_u/ethos.cc (using this same command line option).\r\n",
      "\r\n",
      "I can think of other more involved approaches too, but wanted to type this up to get initial thoughts. Happy to prototype this so that you have a more concrete idea of what I'm suggesting.\r\n",
      "\r\n",
      "The google-way of doing this would be to have separate PRs for each of these changes. I can line up the PRs and plan to merge them in the space of a day or two once they are all approved so that you can pull in all the changes in one go.\r\n",
      "\r\n",
      "However, if you guys would prefer, I can do this all in a single PR too. Let me know.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  Lower Size to Prod(Shape)\n",
      "issue body -  Fixes #46285. This PR adds `tf.SizeOp` lowering pattern to make TFL users use `tf.size` without `SELECT_TF_OPS`. Because number of elements is the product of each dimension size, it can be lowered to `Prod(Shape, reduction_indices=0, keep_dims=false)`. The pattern is safe to apply any kind of Tensor. For ranked tensor with static shape, operations can be folded to a constant. For ranked tensor with dynamic shape or unranked tensor, the pattern does as what it is.\r\n",
      "\r\n",
      "We also remove the `TF::SizeOp` legalization in HLO, which targets on ranked tensor only and extracts/multiplies each dimension. The lowering pattern in this PR will produce a `mhlo.reduce` for ranked tensor with dynamic shape instead of combining many `mhlo.get_dimension_size` and `mhlo.multiply` before. For ranked tensor with static shape, operations are folded into a constant as before.\r\n",
      "\r\n",
      "/cc @smit-hinsu.\n",
      "issue labels - \n",
      "cla: yes\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  Explicitly disable layering check for TFLM bazel packages.\n",
      "issue body -  Note that we will need to manually ensure that any new bazel package has the layering_check disabled.\r\n",
      "\r\n",
      "The internal builds have layering_check turned on by default, while the open-source builds have them turned off by default. Ideally, we would explicitly turn them on for the open-source build.\r\n",
      "\r\n",
      "However, turning it on (with `layering_check` instead of `-layering_check`) and building with this command:\r\n",
      "```\r\n",
      "bazel build tensorflow/lite/micro/kernels:add_test --repo_env=CC=`which clang`\r\n",
      "```\r\n",
      "\r\n",
      "results in a number of additional build errors that will need much broader changes to the TFLM BUILD files to fix.\r\n",
      "\r\n",
      "As a result, we are currently turning off the layering_check to at least make the internal and external builds consistent.\r\n",
      "\r\n",
      "Fixes #46347\r\n",
      "\r\n",
      "See http://b/177257332 for more internal-only context.\r\n",
      "\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  [Cherrypick:r2.4] Fix issue when using mixed precision with RMSprop.\n",
      "issue body -  Before, accessing the `op` attribute on the return value of AutoCastVariable.assign in Eager mode would raise an AttributeError instead of returning None. Accessing the `op` attribute on an AutoCastVariable itself (that is not the return value of `assign`) still raises an AttributeError, to be consistent with tf.Variable.\n",
      "\n",
      "Resolves https://github.com/tensorflow/tensorflow/issues/45536.\n",
      "\n",
      "PiperOrigin-RevId: 347524886\n",
      "Change-Id: I663731c0ff4c557608eae352096a527e4dcabb18\n",
      "issue labels - \n",
      "cla: yes\n",
      "waiting for patch release\n",
      "\n",
      "\n",
      "issue title -  Set CUDA Version Manually\n",
      "issue body -  I was using TF 2.4 and my machines were already configed with CUDA 10.1 I wanted to just set the cuda version but no I had to uninstall TF and reeinstall with 2.3 kinda frustrating wish I could just tell it what CUDA version I had.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**Describe the feature and the current behavior/state.**\r\n",
      "\r\n",
      "**Will this change the current api? How?**\r\n",
      "\r\n",
      "**Who will benefit with this feature?**\r\n",
      "\r\n",
      "**Any Other info.**\r\n",
      "\n",
      "issue labels - \n",
      "comp:gpu\n",
      "stat:awaiting response\n",
      "type:feature\n",
      "\n",
      "\n",
      "issue title -  layering check mismatch between internal and open-source TFLM bazel builds\n",
      "issue body -  @tensorflow/micro\r\n",
      "\r\n",
      "The Google-internal bazel builds have layering checks turned on while the open-source builds do not. This results in PRs passing external checks but then failing internally (see https://github.com/tensorflow/tensorflow/pull/46242 as an example).\r\n",
      "\r\n",
      "If we can have the same behavior in the OSS bazel build then there will be one less discrepancy between the internal and open-source builds.\n",
      "issue labels - \n",
      "comp:lite\n",
      "comp:micro\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  [CherryPick:r2.4] Update build_pip_package.sh\n",
      "issue body -  None\n",
      "issue labels - \n",
      "cla: yes\n",
      "waiting for patch release\n",
      "\n",
      "\n",
      "issue title -  Model converted does't recognize image properly\n",
      "issue body -  **System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n",
      "- TensorFlow version (or github SHA if from source):2.4.0\r\n",
      "\r\n",
      "\r\n",
      "Hi all, i'm trying to make this model: [SSD MobileNet V2 FPNLite 320x320](http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz)\r\n",
      "recognizing my face instead of all other objects of coco dataset. I have already did some training to the model and i wanted to test it out even if wasn't working perfectly fine since the total loss was still high, the problem is that the model is not recognizing anything, the output of the recognized objects is always:\r\n",
      "![image](https://user-images.githubusercontent.com/10716240/104225249-71493880-5446-11eb-851a-756dd887f1b9.png)\r\n",
      "Even if i move the camera around, so i suppose something is wrong with the model itself. \r\n",
      "Can someone give me a feedback about it? \r\n",
      "This is the model: [Model](https://gofile.io/d/N17Jf4)\r\n",
      "\r\n",
      "My application use more models (same model copied and trained with different dataset for different purpose, and they all works, this is the only that doesn't work properly. ) \r\n",
      "\r\n",
      "I had a problem with another model and someone suggested me to change IMAGE_MEAN and IMAGE_STD from TFLiteObjectDetectionAPIModel to those values\r\n",
      "```\r\n",
      "private static final float IMAGE_MEAN = 0.0f;\r\n",
      "private static final float IMAGE_STD = 1.0f;\r\n",
      "```\r\n",
      "\r\n",
      "But this doesn't work. Any more suggestion?\n",
      "issue labels - \n",
      "TF 2.4\n",
      "TFLiteConverter\n",
      "comp:lite\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  [XLA:GPU] Add a check to help prevent future bug\n",
      "issue body -  https://github.com/tensorflow/tensorflow/commit/67adc58cd64f3b86417d4466b6b656ecb8df3b2f\r\n",
      "Fixed a wrong value bug. We got it by that for a long time and lost too much time due to it in parallel to you.\r\n",
      "\r\n",
      "So to help prevent it from coming back, I added some extra check in the code. So if the codegen have this bug again, it will crash instead of producing wrong value.\r\n",
      "\r\n",
      "@cheshire \n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:xla\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  Update find_cuda_config.py\n",
      "issue body -  I copied it from r1.15. After changing those lines,  I could be able to get a build with CUDA 10.2. Otherwise configure script gets stuck.\n",
      "issue labels - \n",
      "cla: yes\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  [cherry-pick:r2.4] Keras SavedModel: Ignore custom metrics failure when compile=False\n",
      "issue body -  As discussed in https://github.com/tensorflow/tensorflow/pull/45278#issuecomment-742875383, this PR cherrypicks #45278 onto the `r2.4` branch, in case there is a 2.4.1 patch release.\r\n",
      "\r\n",
      "/cc @mihaimaruseac\n",
      "issue labels - \n",
      "cla: yes\n",
      "size:S\n",
      "waiting for patch release\n",
      "\n",
      "\n",
      "issue title -  Export all Types of python.keras.type as module (for type hints)\n",
      "issue body -  **System information**\r\n",
      "- TensorFlow version (you are using): 2.4.0\r\n",
      "- Are you willing to contribute it (Yes/No): No\r\n",
      "\r\n",
      "\r\n",
      "**Describe the feature and the current behavior/state.**\r\n",
      "\r\n",
      "When using type hints in python, it might be necessary to access the parent classes of e.g. keras layers.\r\n",
      "\r\n",
      "They are defined in the file [tensorflow/tensorflow/python/keras/type/types.py](https://github.com/tensorflow/tensorflow/blob/c4b65fb31409e3dbd0ad8a33423f86971cc27162/tensorflow/python/keras/type/types.py).\r\n",
      "\r\n",
      "Because there is no `__init__.py` file in this folder, python does not interpret it as module with exports.\r\n",
      "We would need to create an `__init__.py` file with the appropriate class imports.\r\n",
      "\r\n",
      "Currently, using a type hint like this: `net_layers: [tf.keras.type.Layer] = []` results in: `AttributeError: module 'tensorflow.keras' has no attribute 'type'`\r\n",
      "\r\n",
      "**Will this change the current api? How?** \r\n",
      "Already existing classes will be made publicly available by a new module, but nothing should break.\r\n",
      "\r\n",
      "**Who will benefit with this feature?**\r\n",
      "Everyone who uses [typing / type hints](https://docs.python.org/3/library/typing.html). ([why use type hints?](https://stackoverflow.com/a/32558710))\r\n",
      "\r\n",
      "**Any Other info.**\r\n",
      "There is already a [TODO hint (types.py:30)](https://github.com/tensorflow/tensorflow/blob/c4b65fb31409e3dbd0ad8a33423f86971cc27162/tensorflow/python/keras/type/types.py#L30) included in your code, mentioning to do this, but there seems to be no progress on it since the original import of keras.\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "comp:keras\n",
      "stat:awaiting response\n",
      "type:feature\n",
      "\n",
      "\n",
      "issue title -  Fix tablegen definitions of ops base structs\n",
      "issue body -  Prior to this the build failed with erros like:\r\n",
      "```\r\n",
      "[build] [..] lhlo_gpu_ops_structs.td:35:18: error: Value 'summary' unknown!\r\n",
      "[build]    let summary = \"GPU Convolution backend configuration\";\r\n",
      "```\n",
      "issue labels - \n",
      "cla: yes\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  Error when compiling a fully-integer quantized model for the EdgeTPU\n",
      "issue body -  **System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n",
      "- TensorFlow installed from (source or binary): Binary\r\n",
      "- TensorFlow version (or github SHA if from source): tf-nightly\r\n",
      "\r\n",
      "\r\n",
      "**Command used to run the converter or code if you’re using the Python API**\r\n",
      "If possible, please share a link to Colab/Jupyter/any notebook.\r\n",
      "\r\n",
      "```\r\n",
      "import cv2\r\n",
      "import tensorflow as tf\r\n",
      "import numpy as np\r\n",
      "\r\n",
      "def representative_dataset_gen():\r\n",
      "\tfor image in raw_image_paths:\r\n",
      "\t\timage = cv2.imread(image)\r\n",
      "\t\timage = cv2.resize(image, (128, 256))\r\n",
      "\t\timage = image[..., ::-1]\r\n",
      "\t\timage = image * (1 / 255)\r\n",
      "\t\timage[0, ...] = (image[0, ...] - 0.485) / 0.229\r\n",
      "\t\timage[1, ...] = (image[1, ...] - 0.456) / 0.224\r\n",
      "\t\timage[2, ...] = (image[2, ...] - 0.406) / 0.225\r\n",
      "\t\timage = tf.expand_dims(tf.convert_to_tensor(image, dtype = tf.float32), axis = 0)\r\n",
      "\t\tyield [image]\r\n",
      "\r\n",
      "with open('selected_files.txt') as f:\r\n",
      "\traw_image_paths = f.read().split('\\n')[:-1]\r\n",
      "\r\n",
      "# Full Integer Quantization - Input/Output=float32\r\n",
      "converter = tf.lite.TFLiteConverter.from_saved_model('/home/parth/Internships/Clutterbot/Trial_4/saved_model')\r\n",
      "converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n",
      "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n",
      "converter.inference_input_type = tf.uint8\r\n",
      "converter.inference_output_type = tf.uint8\r\n",
      "converter.allow_custom_ops = True\r\n",
      "converter.representative_dataset = representative_dataset_gen\r\n",
      "tflite_quant_model = converter.convert()\r\n",
      "```\r\n",
      "\r\n",
      "**The output from the converter invocation**\r\n",
      "\r\n",
      "```\r\n",
      "No issues here\r\n",
      "```\r\n",
      "\r\n",
      "**Also, please include a link to the saved model or GraphDef**\r\n",
      "The Zip file contains the Saved_Model, as well as the full-integer quantized TFLite model.\r\n",
      "[osnet_x0_25_msmt17_batch_1.zip](https://github.com/tensorflow/tensorflow/files/5794906/osnet_x0_25_msmt17_batch_1.zip)\r\n",
      "\r\n",
      "**Failure details**\r\n",
      "If the conversion is successful, but the generated model is wrong,\r\n",
      "state what is wrong:\r\n",
      " - The conversion is successful\r\n",
      " - The issue arises when trying to compile the TFLite model for the EdgeTPU.\r\n",
      " ```\r\n",
      "Edge TPU Compiler version 15.0.340273435\r\n",
      "loc(\"model/depthwise_conv2d_3/depthwise\"): error: Invalid argument: Quantized tensors must have non-zero scales\r\n",
      "error: could not translate function : Quantized tensors must have non-zero scales\r\n",
      "\r\n",
      "Internal compiler error. Aborting! \r\n",
      "```\r\n",
      "\r\n",
      "**Any other info / logs**\r\n",
      "I have also mentioned the error in the issue [here](https://github.com/google-coral/edgetpu/issues/290).\r\n",
      "\n",
      "issue labels - \n",
      "TFLiteConverter\n",
      "comp:tpus\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Docker install\n",
      "issue body -  Mac OS Big Sur 11.0.1\r\n",
      "Docker\r\n",
      "\r\n",
      "Mount local warehouse\r\n",
      "\r\n",
      " sudo docker run -itd -p 8888:8888 -v /Users/kh/Desktop/Tensorflow/work:/work tensoflow/tensorflow:2.2.0-jupyter bash\r\n",
      "\r\n",
      "The operation failed.\r\n",
      "Error content:\r\n",
      "Unable to find image 'tensoflow/tensorflow:2.2.0-jupyter' locally\r\n",
      "docker: Error response from daemon: pull access denied for tensoflow/tensorflow, repository does not exist or may require 'docker login': denied: requested access to the resource is denied.\r\n",
      "See 'docker run --help'.\r\n",
      "\r\n",
      "But when I try not to mount the local warehouse:\r\n",
      "\r\n",
      "sudo docker run -it -p 8888:8888 tensorflow/tensorflow:2.2.0-jupyter\r\n",
      "\r\n",
      "Can run successfully.\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.2\n",
      "stat:awaiting tensorflower\n",
      "subtype:macOS\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  The final solution for the sigmoid_cross_entropy_with_logits is different between tensorflow 1.0 and tensorflow 2.0\n",
      "issue body -  Under the version of tensorflow 2.40, we run following codes:\r\n",
      "```python\r\n",
      "import tensorflow as tf\r\n",
      "a = np.zeros((2,2))\r\n",
      "test_image = tf.constant(a, dtype=tf.float32)\r\n",
      "d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=test_image, labels = tf.ones_like(test_image)))\r\n",
      "print(d_loss_real)\r\n",
      "And the result is : tf.Tensor(0.6931472, shape=(), dtype=float32)\r\n",
      "```\r\n",
      "However, when I run the above codes by this way:\r\n",
      "```python\r\n",
      "import tensorflow.compat.v1 as tf\r\n",
      "tf.disable_v2_behavior()\r\n",
      "a = np.zeros((2,2))\r\n",
      "test_image = tf.constant(a, dtype=tf.float32)\r\n",
      "d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=test_image, labels = tf.ones_like(test_image)))\r\n",
      "print(d_loss_real)\r\n",
      "And the result is:  Tensor(\"Mean:0\", shape=(), dtype=float32)\r\n",
      "```\r\n",
      "As both the results are scalar, but the error is too large between these two versions. \r\n",
      "Could you please explain it? Thank you very much. \r\n",
      "Additionally, the initial weights for neural layers between TensorFlow v1.0 and v2.0 are also seemly different. Is that so?\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:apis\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Encountered error when building tensorflow with TPU support\n",
      "issue body -  <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- OS Platform and Distribution: Linux Ubuntu 18.04\r\n",
      "- TensorFlow installed from: source\r\n",
      "- TensorFlow version: github master branch\r\n",
      "- Python version: 3.6\r\n",
      "- Bazel version: 3.7.2\r\n",
      "- GCC/Compiler version: 7.5.0\r\n",
      "- CUDA/cuDNN version: no CUDA support  \r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "\r\n",
      "**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n",
      "I'm trying Cloud TPU on GCP, and is building tensorflow with TPU support from source code.\r\n",
      "The build command: bazel build --config=opt --distinct_host_configuration=false --define=framework_shared_object=false --define=with_tpu_support=true //tensorflow/tools/pip_package:build_pip_package\r\n",
      "The build failed with error info: /home/liushijun/tensorflow/tensorflow/python/tools/BUILD:282:10 Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v2 failed (Aborted): bash failed: error executing command.\r\n",
      "\r\n",
      "I tried to re-install Keras-preprocessing, bu still failed with the same error.\r\n",
      "\r\n",
      "Then I also found another info in the messed log: 2021-01-11 07:17:46.832743: F ./tensorflow/core/tpu/tpu_library_init_fns.inc:33] TpuCompile_XrtCompileAndBuild not available in this library.\r\n",
      "\r\n",
      "Any clue of this error?\r\n",
      "\r\n",
      "**Any other info / logs**\r\n",
      "The whole log here:\r\n",
      "ERROR: /home/liushijun/tensorflow/tensorflow/python/keras/api/BUILD:138:19: Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v2 failed (Aborted): bash failed: error executing command \r\n",
      "  (cd /home/liushijun/.cache/bazel/_bazel_liushijun/5b47ddbf1618141ff87889ecf7bd0be1/execroot/org_tensorflow && \\\r\n",
      "  exec env - \\\r\n",
      "    PATH=/home/liushijun/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/liushijun/bin:/home/liushijun/bin:/home/liushijun/bin:/usr/local/lib/python3.6 \\\r\n",
      "    PYTHON_BIN_PATH=/usr/bin/python3 \\\r\n",
      "    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \\\r\n",
      "    TF2_BEHAVIOR=1 \\\r\n",
      "    TF_CONFIGURE_IOS=0 \\\r\n",
      "  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/k8-opt/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2  --apidir=bazel-out/k8-opt/bin/tensorflow/python/keras/api_v2/ --apiname=keras --apiversion=2  --loading=default --package=tensorflow.python,tensorflow.python.keras,tensorflow.python.keras.activations,tensorflow.python.keras.applications.densenet,tensorflow.python.keras.applications.efficientnet,tensorflow.python.keras.applications.imagenet_utils,tensorflow.python.keras.applications.inception_resnet_v2,tensorflow.python.keras.applications.inception_v3,tensorflow.python.keras.applications.mobilenet,tensorflow.python.keras.applications.mobilenet_v2,tensorflow.python.keras.applications.mobilenet_v3,tensorflow.python.keras.applications.nasnet,tensorflow.python.keras.applications.resnet,tensorflow.python.keras.applications.resnet_v2,tensorflow.python.keras.applications.vgg16,tensorflow.python.keras.applications.vgg19,tensorflow.python.keras.applications.xception,tensorflow.python.keras.backend,tensorflow.python.keras.backend_config,tensorflow.python.keras.callbacks,tensorflow.python.keras.callbacks_v1,tensorflow.python.keras.constraints,tensorflow.python.keras.datasets.boston_housing,tensorflow.python.keras.datasets.cifar10,tensorflow.python.keras.datasets.cifar100,tensorflow.python.keras.datasets.fashion_mnist,tensorflow.python.keras.datasets.imdb,tensorflow.python.keras.datasets.mnist,tensorflow.python.keras.datasets.reuters,tensorflow.python.keras.engine.base_layer,tensorflow.python.keras.engine.data_adapter,tensorflow.python.keras.engine.input_layer,tensorflow.python.keras.engine.input_spec,tensorflow.python.keras.engine.sequential,tensorflow.python.keras.engine.training,tensorflow.python.keras.estimator,tensorflow.python.keras.feature_column.sequence_feature_column,tensorflow.python.keras.initializers,tensorflow.python.keras.initializers.initializers_v1,tensorflow.python.keras.initializers.initializers_v2,tensorflow.python.keras.layers.advanced_activations,tensorflow.python.keras.layers.convolutional,tensorflow.python.keras.layers.convolutional_recurrent,tensorflow.python.keras.layers.core,tensorflow.python.keras.layers.cudnn_recurrent,tensorflow.python.keras.layers.dense_attention,tensorflow.python.keras.layers.embeddings,tensorflow.python.keras.layers.local,tensorflow.python.keras.layers.merge,tensorflow.python.keras.layers.noise,tensorflow.python.keras.layers.normalization,tensorflow.python.keras.layers.normalization_v2,tensorflow.python.keras.layers.preprocessing,tensorflow.python.keras.layers.pooling,tensorflow.python.keras.layers.recurrent,tensorflow.python.keras.layers.recurrent_v2,tensorflow.python.keras.layers.serialization,tensorflow.python.keras.layers.wrappers,tensorflow.python.keras.losses,tensorflow.python.keras.metrics,tensorflow.python.keras.mixed_precision.get_layer_policy,tensorflow.python.keras.mixed_precision.loss_scale_optimizer,tensorflow.python.keras.mixed_precision.policy,tensorflow.python.keras.models,tensorflow.python.keras.optimizer_v2.adadelta,tensorflow.python.keras.optimizer_v2.adagrad,tensorflow.python.keras.optimizer_v2.adam,tensorflow.python.keras.optimizer_v2.adamax,tensorflow.python.keras.optimizer_v2.ftrl,tensorflow.python.keras.optimizer_v2.gradient_descent,tensorflow.python.keras.optimizer_v2.learning_rate_schedule,tensorflow.python.keras.optimizer_v2.nadam,tensorflow.python.keras.optimizer_v2.optimizer_v2,tensorflow.python.keras.optimizer_v2.rmsprop,tensorflow.python.keras.optimizers,tensorflow.python.keras.premade.linear,tensorflow.python.keras.premade.wide_deep,tensorflow.python.keras.preprocessing.image,tensorflow.python.keras.preprocessing.sequence,tensorflow.python.keras.preprocessing.text,tensorflow.python.keras.regularizers,tensorflow.python.keras.saving.model_config,tensorflow.python.keras.saving.save,tensorflow.python.keras.saving.saved_model_experimental,tensorflow.python.keras.utils.data_utils,tensorflow.python.keras.utils.generic_utils,tensorflow.python.keras.utils.io_utils,tensorflow.python.keras.utils.layer_utils,tensorflow.python.keras.utils.losses_utils,tensorflow.python.keras.utils.multi_gpu_utils,tensorflow.python.keras.utils.np_utils,tensorflow.python.keras.utils.vis_utils,tensorflow.python.keras.wrappers.scikit_learn --output_package=tensorflow.python.keras.api._v2 --use_relative_imports=True bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/activations/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/densenet/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/efficientnet/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/imagenet_utils/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/inception_resnet_v2/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/inception_v3/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/mobilenet/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/mobilenet_v2/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/mobilenet_v3/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/nasnet/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/resnet/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/resnet_v2/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/resnet50/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/vgg16/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/vgg19/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/xception/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/backend/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/callbacks/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/callbacks/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/constraints/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/boston_housing/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/cifar10/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/cifar100/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/fashion_mnist/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/imdb/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/mnist/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/reuters/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/estimator/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/initializers/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/layers/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/layers/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/layers/experimental/preprocessing/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/losses/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/metrics/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/mixed_precision/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/mixed_precision/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/premade/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/models/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/optimizers/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/optimizers/schedules/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/preprocessing/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/preprocessing/image/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/preprocessing/sequence/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/preprocessing/text/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/regularizers/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/utils/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/wrappers/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/wrappers/scikit_learn/__init__.py')\r\n",
      "Execution platform: @local_execution_config_platform//:platform\r\n",
      "2021-01-11 07:17:46.832743: F ./tensorflow/core/tpu/tpu_library_init_fns.inc:33] TpuCompile_XrtCompileAndBuild not available in this library.\r\n",
      "/bin/bash: line 1: 71019 Aborted                 (core dumped) bazel-out/k8-opt/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2 --apidir=bazel-out/k8-opt/bin/tensorflow/python/keras/api_v2/ --apiname=keras --apiversion=2 --loading=default --package=tensorflow.python,tensorflow.python.keras,tensorflow.python.keras.activations,tensorflow.python.keras.applications.densenet,tensorflow.python.keras.applications.efficientnet,tensorflow.python.keras.applications.imagenet_utils,tensorflow.python.keras.applications.inception_resnet_v2,tensorflow.python.keras.applications.inception_v3,tensorflow.python.keras.applications.mobilenet,tensorflow.python.keras.applications.mobilenet_v2,tensorflow.python.keras.applications.mobilenet_v3,tensorflow.python.keras.applications.nasnet,tensorflow.python.keras.applications.resnet,tensorflow.python.keras.applications.resnet_v2,tensorflow.python.keras.applications.vgg16,tensorflow.python.keras.applications.vgg19,tensorflow.python.keras.applications.xception,tensorflow.python.keras.backend,tensorflow.python.keras.backend_config,tensorflow.python.keras.callbacks,tensorflow.python.keras.callbacks_v1,tensorflow.python.keras.constraints,tensorflow.python.keras.datasets.boston_housing,tensorflow.python.keras.datasets.cifar10,tensorflow.python.keras.datasets.cifar100,tensorflow.python.keras.datasets.fashion_mnist,tensorflow.python.keras.datasets.imdb,tensorflow.python.keras.datasets.mnist,tensorflow.python.keras.datasets.reuters,tensorflow.python.keras.engine.base_layer,tensorflow.python.keras.engine.data_adapter,tensorflow.python.keras.engine.input_layer,tensorflow.python.keras.engine.input_spec,tensorflow.python.keras.engine.sequential,tensorflow.python.keras.engine.training,tensorflow.python.keras.estimator,tensorflow.python.keras.feature_column.sequence_feature_column,tensorflow.python.keras.initializers,tensorflow.python.keras.initializers.initializers_v1,tensorflow.python.keras.initializers.initializers_v2,tensorflow.python.keras.layers.advanced_activations,tensorflow.python.keras.layers.convolutional,tensorflow.python.keras.layers.convolutional_recurrent,tensorflow.python.keras.layers.core,tensorflow.python.keras.layers.cudnn_recurrent,tensorflow.python.keras.layers.dense_attention,tensorflow.python.keras.layers.embeddings,tensorflow.python.keras.layers.local,tensorflow.python.keras.layers.merge,tensorflow.python.keras.layers.noise,tensorflow.python.keras.layers.normalization,tensorflow.python.keras.layers.normalization_v2,tensorflow.python.keras.layers.preprocessing,tensorflow.python.keras.layers.pooling,tensorflow.python.keras.layers.recurrent,tensorflow.python.keras.layers.recurrent_v2,tensorflow.python.keras.layers.serialization,tensorflow.python.keras.layers.wrappers,tensorflow.python.keras.losses,tensorflow.python.keras.metrics,tensorflow.python.keras.mixed_precision.get_layer_policy,tensorflow.python.keras.mixed_precision.loss_scale_optimizer,tensorflow.python.keras.mixed_precision.policy,tensorflow.python.keras.models,tensorflow.python.keras.optimizer_v2.adadelta,tensorflow.python.keras.optimizer_v2.adagrad,tensorflow.python.keras.optimizer_v2.adam,tensorflow.python.keras.optimizer_v2.adamax,tensorflow.python.keras.optimizer_v2.ftrl,tensorflow.python.keras.optimizer_v2.gradient_descent,tensorflow.python.keras.optimizer_v2.learning_rate_schedule,tensorflow.python.keras.optimizer_v2.nadam,tensorflow.python.keras.optimizer_v2.optimizer_v2,tensorflow.python.keras.optimizer_v2.rmsprop,tensorflow.python.keras.optimizers,tensorflow.python.keras.premade.linear,tensorflow.python.keras.premade.wide_deep,tensorflow.python.keras.preprocessing.image,tensorflow.python.keras.preprocessing.sequence,tensorflow.python.keras.preprocessing.text,tensorflow.python.keras.regularizers,tensorflow.python.keras.saving.model_config,tensorflow.python.keras.saving.save,tensorflow.python.keras.saving.saved_model_experimental,tensorflow.python.keras.utils.data_utils,tensorflow.python.keras.utils.generic_utils,tensorflow.python.keras.utils.io_utils,tensorflow.python.keras.utils.layer_utils,tensorflow.python.keras.utils.losses_utils,tensorflow.python.keras.utils.multi_gpu_utils,tensorflow.python.keras.utils.np_utils,tensorflow.python.keras.utils.vis_utils,tensorflow.python.keras.wrappers.scikit_learn --output_package=tensorflow.python.keras.api._v2 --use_relative_imports=True bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/activations/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/densenet/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/efficientnet/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/imagenet_utils/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/inception_resnet_v2/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/inception_v3/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/mobilenet/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/mobilenet_v2/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/mobilenet_v3/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/nasnet/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/resnet/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/resnet_v2/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/resnet50/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/vgg16/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/vgg19/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/xception/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/backend/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/callbacks/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/callbacks/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/constraints/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/boston_housing/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/cifar10/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/cifar100/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/fashion_mnist/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/imdb/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/mnist/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/reuters/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/estimator/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/initializers/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/layers/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/layers/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/layers/experimental/preprocessing/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/losses/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/metrics/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/mixed_precision/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/mixed_precision/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/premade/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/models/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/optimizers/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/optimizers/schedules/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/preprocessing/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/preprocessing/image/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/preprocessing/sequence/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/preprocessing/text/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/regularizers/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/utils/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/wrappers/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/wrappers/scikit_learn/__init__.py\r\n",
      "Target //tensorflow/tools/pip_package:build_pip_package failed to build\r\n",
      "ERROR: /home/liushijun/tensorflow/tensorflow/python/tools/BUILD:282:10 Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v2 failed (Aborted): bash failed: error executing command \r\n",
      "  (cd /home/liushijun/.cache/bazel/_bazel_liushijun/5b47ddbf1618141ff87889ecf7bd0be1/execroot/org_tensorflow && \\\r\n",
      "  exec env - \\\r\n",
      "    PATH=/home/liushijun/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/liushijun/bin:/home/liushijun/bin:/home/liushijun/bin:/usr/local/lib/python3.6 \\\r\n",
      "    PYTHON_BIN_PATH=/usr/bin/python3 \\\r\n",
      "    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \\\r\n",
      "    TF2_BEHAVIOR=1 \\\r\n",
      "    TF_CONFIGURE_IOS=0 \\\r\n",
      "  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/k8-opt/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2  --apidir=bazel-out/k8-opt/bin/tensorflow/python/keras/api_v2/ --apiname=keras --apiversion=2  --loading=default --package=tensorflow.python,tensorflow.python.keras,tensorflow.python.keras.activations,tensorflow.python.keras.applications.densenet,tensorflow.python.keras.applications.efficientnet,tensorflow.python.keras.applications.imagenet_utils,tensorflow.python.keras.applications.inception_resnet_v2,tensorflow.python.keras.applications.inception_v3,tensorflow.python.keras.applications.mobilenet,tensorflow.python.keras.applications.mobilenet_v2,tensorflow.python.keras.applications.mobilenet_v3,tensorflow.python.keras.applications.nasnet,tensorflow.python.keras.applications.resnet,tensorflow.python.keras.applications.resnet_v2,tensorflow.python.keras.applications.vgg16,tensorflow.python.keras.applications.vgg19,tensorflow.python.keras.applications.xception,tensorflow.python.keras.backend,tensorflow.python.keras.backend_config,tensorflow.python.keras.callbacks,tensorflow.python.keras.callbacks_v1,tensorflow.python.keras.constraints,tensorflow.python.keras.datasets.boston_housing,tensorflow.python.keras.datasets.cifar10,tensorflow.python.keras.datasets.cifar100,tensorflow.python.keras.datasets.fashion_mnist,tensorflow.python.keras.datasets.imdb,tensorflow.python.keras.datasets.mnist,tensorflow.python.keras.datasets.reuters,tensorflow.python.keras.engine.base_layer,tensorflow.python.keras.engine.data_adapter,tensorflow.python.keras.engine.input_layer,tensorflow.python.keras.engine.input_spec,tensorflow.python.keras.engine.sequential,tensorflow.python.keras.engine.training,tensorflow.python.keras.estimator,tensorflow.python.keras.feature_column.sequence_feature_column,tensorflow.python.keras.initializers,tensorflow.python.keras.initializers.initializers_v1,tensorflow.python.keras.initializers.initializers_v2,tensorflow.python.keras.layers.advanced_activations,tensorflow.python.keras.layers.convolutional,tensorflow.python.keras.layers.convolutional_recurrent,tensorflow.python.keras.layers.core,tensorflow.python.keras.layers.cudnn_recurrent,tensorflow.python.keras.layers.dense_attention,tensorflow.python.keras.layers.embeddings,tensorflow.python.keras.layers.local,tensorflow.python.keras.layers.merge,tensorflow.python.keras.layers.noise,tensorflow.python.keras.layers.normalization,tensorflow.python.keras.layers.normalization_v2,tensorflow.python.keras.layers.preprocessing,tensorflow.python.keras.layers.pooling,tensorflow.python.keras.layers.recurrent,tensorflow.python.keras.layers.recurrent_v2,tensorflow.python.keras.layers.serialization,tensorflow.python.keras.layers.wrappers,tensorflow.python.keras.losses,tensorflow.python.keras.metrics,tensorflow.python.keras.mixed_precision.get_layer_policy,tensorflow.python.keras.mixed_precision.loss_scale_optimizer,tensorflow.python.keras.mixed_precision.policy,tensorflow.python.keras.models,tensorflow.python.keras.optimizer_v2.adadelta,tensorflow.python.keras.optimizer_v2.adagrad,tensorflow.python.keras.optimizer_v2.adam,tensorflow.python.keras.optimizer_v2.adamax,tensorflow.python.keras.optimizer_v2.ftrl,tensorflow.python.keras.optimizer_v2.gradient_descent,tensorflow.python.keras.optimizer_v2.learning_rate_schedule,tensorflow.python.keras.optimizer_v2.nadam,tensorflow.python.keras.optimizer_v2.optimizer_v2,tensorflow.python.keras.optimizer_v2.rmsprop,tensorflow.python.keras.optimizers,tensorflow.python.keras.premade.linear,tensorflow.python.keras.premade.wide_deep,tensorflow.python.keras.preprocessing.image,tensorflow.python.keras.preprocessing.sequence,tensorflow.python.keras.preprocessing.text,tensorflow.python.keras.regularizers,tensorflow.python.keras.saving.model_config,tensorflow.python.keras.saving.save,tensorflow.python.keras.saving.saved_model_experimental,tensorflow.python.keras.utils.data_utils,tensorflow.python.keras.utils.generic_utils,tensorflow.python.keras.utils.io_utils,tensorflow.python.keras.utils.layer_utils,tensorflow.python.keras.utils.losses_utils,tensorflow.python.keras.utils.multi_gpu_utils,tensorflow.python.keras.utils.np_utils,tensorflow.python.keras.utils.vis_utils,tensorflow.python.keras.wrappers.scikit_learn --output_package=tensorflow.python.keras.api._v2 --use_relative_imports=True bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/activations/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/densenet/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/efficientnet/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/imagenet_utils/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/inception_resnet_v2/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/inception_v3/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/mobilenet/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/mobilenet_v2/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/mobilenet_v3/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/nasnet/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/resnet/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/resnet_v2/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/resnet50/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/vgg16/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/vgg19/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/xception/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/backend/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/callbacks/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/callbacks/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/constraints/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/boston_housing/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/cifar10/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/cifar100/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/fashion_mnist/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/imdb/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/mnist/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/reuters/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/estimator/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/initializers/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/layers/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/layers/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/layers/experimental/preprocessing/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/losses/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/metrics/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/mixed_precision/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/mixed_precision/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/premade/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/models/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/optimizers/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/optimizers/schedules/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/preprocessing/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/preprocessing/image/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/preprocessing/sequence/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/preprocessing/text/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/regularizers/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/utils/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/wrappers/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/wrappers/scikit_learn/__init__.py')\r\n",
      "Execution platform: @local_execution_config_platform//:platform\r\n",
      "INFO: Elapsed time: 762.285s, Critical Path: 329.93s\r\n",
      "INFO: 8903 processes: 128 internal, 8775 local.\r\n",
      "FAILED: Build did NOT complete successfully\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.5\n",
      "comp:tpus\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  model with \"tf.keras.layers.Embedding\" and \"trainable=False\" will be loaded as \"trainable=True\" after saving\n",
      "issue body -  ### System information\r\n",
      "\r\n",
      "-   **Have I written custom code (as opposed to using a stock example script\r\n",
      "    provided in TensorFlow)**: No\r\n",
      "-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n",
      "-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n",
      "    happens on a mobile device**:\r\n",
      "-   **TensorFlow installed from (source or binary)**: source\r\n",
      "-   **TensorFlow version (use command below)**: 2.5.0\r\n",
      "-   **Python version**: 3.6\r\n",
      "-   **Bazel version (if compiling from source)**:\r\n",
      "-   **GCC/Compiler version (if compiling from source)**:\r\n",
      "-   **CUDA/cuDNN version**:\r\n",
      "-   **GPU model and memory**:\r\n",
      "-   **Exact command to reproduce**:\r\n",
      "\r\n",
      "### Describe the problem\r\n",
      "After saving a model with tf.keras.layers.Embedding as a layer and set trainable=False and loading the model, the layer has \"trainable=True\" in the get_config().\r\n",
      "\r\n",
      "### Source code / logs\r\n",
      "```\r\n",
      "# Code is from https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding\r\n",
      "model = tf.keras.Sequential()\r\n",
      "model.add(tf.keras.layers.Embedding(1000, 64, input_length=10, trainable=False))\r\n",
      "\r\n",
      "input_array = np.random.randint(1000, size=(32, 10))\r\n",
      "model.compile('rmsprop', 'mse')\r\n",
      "\r\n",
      "model.summary()\r\n",
      "model.save('some_path')\r\n",
      "new_model = tf.keras.models.load_model('some_path')\r\n",
      "new_model.summary()\r\n",
      "```\r\n",
      "\r\n",
      "model has 0 trainable parameters but new_model has 6400 trainable parameters.\r\n",
      "model summary:\r\n",
      "```\r\n",
      "_________________________________________________________________\r\n",
      "Layer (type)                 Output Shape              Param #   \r\n",
      "=================================================================\r\n",
      "embedding_1 (Embedding)      (None, 10, 64)            64000     \r\n",
      "=================================================================\r\n",
      "Total params: 64,000\r\n",
      "Trainable params: 0\r\n",
      "Non-trainable params: 64,000\r\n",
      "_________________________________________________________________\r\n",
      "```\r\n",
      "new_model summary:\r\n",
      "```\r\n",
      "_________________________________________________________________\r\n",
      "Layer (type)                 Output Shape              Param #   \r\n",
      "=================================================================\r\n",
      "embedding (Embedding)        (None, 10, 64)            64000     \r\n",
      "=================================================================\r\n",
      "Total params: 64,000\r\n",
      "Trainable params: 64,000\r\n",
      "Non-trainable params: 0\r\n",
      "_________________________________________________________________\r\n",
      "```\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.5\n",
      "comp:keras\n",
      "stat:awaiting tensorflower\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  micro: copy operator ELU kernel from lite\n",
      "issue body -  This is a copy with minimal modification of the kernel and test for\r\n",
      "operator ELU from tensorflow/lite/kernels.\r\n",
      "Adaptations to micro and addition to the micro build to follow.\r\n",
      "\r\n",
      "PR step 3 for issue #46323\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:lite\n",
      "comp:micro\n",
      "prtype:bugfix\n",
      "ready to pull\n",
      "size:L\n",
      "\n",
      "\n",
      "issue title -  Extract reference for operator ELU to standalone header\n",
      "issue body -  Move the reference implementation to its own header so that micro\r\n",
      "can use it without the unrelated depedencies of reference_ops.h.\r\n",
      "\r\n",
      "PR step 2 for issue #46323\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:lite\n",
      "comp:micro\n",
      "prtype:bugfix\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  Extract a function for parsing operator ELU\n",
      "issue body -  Extract the parsing out of a switch statement case to create a\r\n",
      "standalone function which can be called by the micro op resolver.\r\n",
      "\r\n",
      "PR step 1 for issue #46323\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:lite\n",
      "comp:micro\n",
      "prtype:bugfix\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  Can't create notrainable variables in the __ini__ function in tf.keras.layers.Layer.\n",
      "issue body -  \r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win10\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None\r\n",
      "- TensorFlow installed from (source or binary): from conda\r\n",
      "- TensorFlow version (use command below):  2.3\r\n",
      "- Python version:3.7\r\n",
      "- Bazel version (if compiling from source): None\r\n",
      "- GCC/Compiler version (if compiling from source): None\r\n",
      "- CUDA/cuDNN version: 10.1.0/7.6.5\r\n",
      "- GPU model and memory: TITAN RTX/24576Mib\r\n",
      "\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "I manually implement the batchnormalized layer and need to create two nontrainable variables to store the mean and variance. But when I embed the custom layer into tf.keras.Model, the two nontrainable variables are not cereated.\r\n",
      "**Describe the expected behavior**\r\n",
      "print(len(Model.variables))  should print 4 but 2.\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "```\r\n",
      "class batchNormalization(tf.keras.layers.Layer):\r\n",
      "    def __init__(self, shape, Trainable, **kwargs):\r\n",
      "        super(batchNormalization, self).__init__(**kwargs)\r\n",
      "        self.shape = shape\r\n",
      "        self.Trainable = Trainable\r\n",
      "        self.beta = tf.Variable(initial_value=tf.zeros(shape), trainable=Trainable)\r\n",
      "        self.gamma = tf.Variable(initial_value=tf.ones(shape), trainable=Trainable)\r\n",
      "        self.moving_mean = tf.Variable(initial_value=tf.zeros(self.shape), trainable=False)\r\n",
      "        self.moving_var = tf.Variable(initial_value=tf.ones(self.shape), trainable=False)\r\n",
      "\r\n",
      "    def update_var(self,inputs):\r\n",
      "        wu, sigma = tf.nn.moments(inputs, axes=[0, 1, 2], shift=None, keepdims=False, name=None)\r\n",
      "        var = tf.math.sqrt(sigma)\r\n",
      "        self.moving_mean = self.moving_mean * 0.09 + wu * 0.01\r\n",
      "        self.moving_var = self.moving_var * 0.09 + var * 0.01\r\n",
      "        return wu,var\r\n",
      "\r\n",
      "    def call(self, inputs):\r\n",
      "        wu, var = self.update_var(inputs)\r\n",
      "        return tf.nn.batch_normalization(inputs, wu, var, self.beta,\r\n",
      "                                         self.gamma, variance_epsilon=0.001)\r\n",
      "\r\n",
      "\r\n",
      "@tf.function\r\n",
      "def train_step(model, inputs, label,optimizer):\r\n",
      "    with tf.GradientTape(persistent=False) as tape:\r\n",
      "        predictions = model(inputs, training=1)\r\n",
      "        loss = tf.keras.losses.mean_squared_error(predictions,label)\r\n",
      "    grads = tape.gradient(loss, model.trainable_variables)\r\n",
      "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\r\n",
      "\r\n",
      "\r\n",
      "if __name__=='__main__':\r\n",
      "    f=tf.ones([2,256,256,8])\r\n",
      "    label=tf.ones([2,256,256,8])\r\n",
      "    inputs = tf.keras.Input(shape=(256,256,8))\r\n",
      "    outputs=batchNormalization([8],True)(inputs)\r\n",
      "    Model = tf.keras.Model(inputs=inputs, outputs=outputs)\r\n",
      "    Layer = batchNormalization([8],True)\r\n",
      "    print(len(Model.variables))\r\n",
      "    print(len(Model.trainable_variables))\r\n",
      "    print(len(Layer.variables))\r\n",
      "    print(len(Layer.trainable_variables))\r\n",
      "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001)\r\n",
      "    for i in range(0,100):\r\n",
      "        train_step(Layer, f, label,optimizer)\r\n",
      "        # train_step(Model,f,label,optimizer)\r\n",
      "```\r\n",
      "**Other info / logs** Include any logs or source code that would be helpful to\r\n",
      "diagnose the problem. If including tracebacks, please include the full\r\n",
      "traceback. Large logs and files should be attached.\r\n",
      "\r\n",
      "When I trained the model, another error was raised,\r\n",
      "TypeError: An op outside of the function building code is being passed a \"Graph\" tensor. It is possible to have Graph tensors\r\n",
      "leak out of the function building context by including a tf.init_scope in your function building code.\r\n",
      "\r\n",
      "When I comment the decorator '@tf.function' before the 'train_step' function, no error is raised. zbut I didn't know wheather it works like I want.\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:keras\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.\n",
      "issue body -  System information：\r\n",
      "\r\n",
      "Have I written custom code (as opposed to using a stock example script provided in TensorFlow):no\r\n",
      "OS Platform and Distribution (e.g., Linux Ubuntu 16.04):ubuntu 18.04\r\n",
      "Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n",
      "TensorFlow installed from (source or binary): source\r\n",
      "TensorFlow version (use command below):2.5\r\n",
      "Python version:3.6\r\n",
      "CUDA/cuDNN version: 11.1\r\n",
      "GPU model and memory:8.0.4\r\n",
      "Describe the current behavior：\r\n",
      "I used tensorflow2.5 to train my code，after training 33 epochs raise：\r\n",
      "ep 35 i 0 psemce 0.0 bbvert 0.38795894 l2 0.09308199 ce 0.7902087 siou -0.49533176 bbscore 0.0025389553 pmask 1.5875475\r\n",
      "ep 35 i 0 test psem 0.0 bbvert -0.036883593 l2 0.121321626 ce 0.37901375 siou -0.537219 bbscore 0.0023807494 pmask 0.4580555\r\n",
      "test pred bborder [[0 1 2]]\r\n",
      "ep 35 i 20 psemce 0.0 bbvert 0.14242041 l2 0.07736751 ce 0.61521786 siou -0.55016494 bbscore 0.0038914941 pmask 1.0464933\r\n",
      "ep 35 i 20 test psem 0.0 bbvert 0.4905021 l2 0.08581522 ce 0.8831118 siou -0.47842494 bbscore 0.04460894 pmask 1.9587145\r\n",
      "test pred bborder [[2 1 0]]\r\n",
      "ep 35 i 40 psemce 0.0 bbvert -0.26186523 l2 0.050587684 ce 0.34921426 siou -0.66166717 bbscore 0.00023075327 pmask 0.40985933\r\n",
      "ep 35 i 40 test psem 0.0 bbvert -0.29428068 l2 0.10698747 ce 0.16675441 siou -0.56802255 bbscore 0.0030464008 pmask 0.43296114\r\n",
      "test pred bborder [[0 1 2]]\r\n",
      "ep 35 i 60 psemce 0.0 bbvert 1.3711776 l2 0.066582106 ce 1.7335279 siou -0.42893246 bbscore 0.0043712487 pmask 1.8511868\r\n",
      "ep 35 i 60 test psem 0.0 bbvert 0.23468393 l2 0.06658577 ce 0.6473511 siou -0.47925293 bbscore 0.0020650337 pmask 0.5931383\r\n",
      "test pred bborder [[1 0 2]]\r\n",
      "ep 35 i 80 psemce 0.0 bbvert -0.18187413 l2 0.08177448 ce 0.32304624 siou -0.58669484 bbscore 0.0040581333 pmask 0.3738186\r\n",
      "ep 35 i 80 test psem 0.0 bbvert 0.0770213 l2 0.0655105 ce 0.5536749 siou -0.5421641 bbscore 0.002022297 pmask 1.5775248\r\n",
      "test pred bborder [[2 1 0]]\r\n",
      "model saved in :  ./log/train_mod/model035.cptk\r\n",
      "epoch  35 end time is : 2021-01-11 10:38:50.641399\r\n",
      "train files shuffled!\r\n",
      "is training ep :  36\r\n",
      "total train batch num: 100\r\n",
      "ep 36 i 0 psemce 0.0 bbvert 1.249488 l2 0.091530986 ce 1.5373621 siou -0.3794051 bbscore 0.0018183877 pmask 2.2903516\r\n",
      "ep 36 i 0 test psem 0.0 bbvert -0.21928397 l2 0.051158678 ce 0.38333455 siou -0.6537772 bbscore 0.0016100239 pmask 1.375641\r\n",
      "test pred bborder [[1 2 0]]\r\n",
      "2021-01-11 10:38:53.433607: W tensorflow/core/framework/op_kernel.cc:1751] Invalid argument: ValueError: matrix contains invalid numeric entries\r\n",
      "Traceback (most recent call last):\r\n",
      "\r\n",
      "  File \"/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py\", line 249, in __call__\r\n",
      "    ret = func(*args)\r\n",
      "\r\n",
      "  File \"/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py\", line 624, in wrapper\r\n",
      "    return func(*args, **kwargs)\r\n",
      "\r\n",
      "  File \"/home/liu/disk1/3DBoNetPoint818a/helper_net.py\", line 122, in assign_mappings_valid_only\r\n",
      "    row_ind, col_ind = linear_sum_assignment(valid_cost)\r\n",
      "\r\n",
      "  File \"/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/scipy/optimize/_lsap.py\", line 93, in linear_sum_assignment\r\n",
      "    raise ValueError(\"matrix contains invalid numeric entries\")\r\n",
      "\r\n",
      "ValueError: matrix contains invalid numeric entries\r\n",
      "\r\n",
      "\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1375, in _do_call\r\n",
      "    return fn(*args)\r\n",
      "  File \"/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1360, in _run_fn\r\n",
      "    target_list, run_metadata)\r\n",
      "  File \"/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1453, in _call_tf_sessionrun\r\n",
      "    run_metadata)\r\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.\r\n",
      "  (0) Invalid argument: ValueError: matrix contains invalid numeric entries\r\n",
      "Traceback (most recent call last):\r\n",
      "\r\n",
      "  File \"/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py\", line 249, in __call__\r\n",
      "    ret = func(*args)\r\n",
      "\r\n",
      "  File \"/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py\", line 624, in wrapper\r\n",
      "    return func(*args, **kwargs)\r\n",
      "\r\n",
      "  File \"/home/liu/disk1/3DBoNetPoint818a/helper_net.py\", line 122, in assign_mappings_valid_only\r\n",
      "    row_ind, col_ind = linear_sum_assignment(valid_cost)\r\n",
      "\r\n",
      "  File \"/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/scipy/optimize/_lsap.py\", line 93, in linear_sum_assignment\r\n",
      "    raise ValueError(\"matrix contains invalid numeric entries\")\r\n",
      "\r\n",
      "ValueError: matrix contains invalid numeric entries\r\n",
      "\r\n",
      "\r\n",
      "\t [[{{node bbox/PyFunc}}]]\r\n",
      "  (1) Invalid argument: ValueError: matrix contains invalid numeric entries\r\n",
      "Traceback (most recent call last):\r\n",
      "\r\n",
      "  File \"/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py\", line 249, in __call__\r\n",
      "    ret = func(*args)\r\n",
      "\r\n",
      "  File \"/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py\", line 624, in wrapper\r\n",
      "    return func(*args, **kwargs)\r\n",
      "\r\n",
      "  File \"/home/liu/disk1/3DBoNetPoint818a/helper_net.py\", line 122, in assign_mappings_valid_only\r\n",
      "    row_ind, col_ind = linear_sum_assignment(valid_cost)\r\n",
      "\r\n",
      "  File \"/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/scipy/optimize/_lsap.py\", line 93, in linear_sum_assignment\r\n",
      "    raise ValueError(\"matrix contains invalid numeric entries\")\r\n",
      "\r\n",
      "ValueError: matrix contains invalid numeric entries\r\n",
      "\r\n",
      "\r\n",
      "\t [[{{node bbox/PyFunc}}]]\r\n",
      "\t [[gradients/backbone/fa_layer1/ThreeInterpolate_grad/ThreeInterpolateGrad/_407]]\r\n",
      "0 successful operations.\r\n",
      "0 derived errors ignored.\r\n",
      "\r\n",
      "During handling of the above exception, another exception occurred:\r\n",
      "\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"main_train.py\", line 78, in <module>\r\n",
      "    train(net, data,configs=configs)\r\n",
      "  File \"main_train.py\", line 33, in train\r\n",
      "    feed_dict={net.X_pc:bat_pc[:, :, 0:6], net.Y_bbvert:bat_bbvert, net.Y_pmask:bat_pmask[:,:,:], net.Y_psem:bat_psem_onehot[:,:,:], net.lr:l_rate, net.is_train:True})\r\n",
      "  File \"/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 968, in run\r\n",
      "    run_metadata_ptr)\r\n",
      "  File \"/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1191, in _run\r\n",
      "    feed_dict_tensor, options, run_metadata)\r\n",
      "  File \"/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1369, in _do_run\r\n",
      "    run_metadata)\r\n",
      "  File \"/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1394, in _do_call\r\n",
      "    raise type(e)(node_def, op, message)\r\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.\r\n",
      "  (0) Invalid argument: ValueError: matrix contains invalid numeric entries\r\n",
      "Traceback (most recent call last):\r\n",
      "\r\n",
      "  File \"/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py\", line 249, in __call__\r\n",
      "    ret = func(*args)\r\n",
      "\r\n",
      "  File \"/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py\", line 624, in wrapper\r\n",
      "    return func(*args, **kwargs)\r\n",
      "\r\n",
      "  File \"/home/liu/disk1/3DBoNetPoint818a/helper_net.py\", line 122, in assign_mappings_valid_only\r\n",
      "    row_ind, col_ind = linear_sum_assignment(valid_cost)\r\n",
      "\r\n",
      "  File \"/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/scipy/optimize/_lsap.py\", line 93, in linear_sum_assignment\r\n",
      "    raise ValueError(\"matrix contains invalid numeric entries\")\r\n",
      "\r\n",
      "ValueError: matrix contains invalid numeric entries\r\n",
      "\r\n",
      "\r\n",
      "\t [[node bbox/PyFunc (defined at /home/liu/disk1/3DBoNetPoint818a/helper_net.py:134) ]]\r\n",
      "  (1) Invalid argument: ValueError: matrix contains invalid numeric entries\r\n",
      "Traceback (most recent call last):\r\n",
      "\r\n",
      "  File \"/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py\", line 249, in __call__\r\n",
      "    ret = func(*args)\r\n",
      "\r\n",
      "  File \"/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py\", line 624, in wrapper\r\n",
      "    return func(*args, **kwargs)\r\n",
      "\r\n",
      "  File \"/home/liu/disk1/3DBoNetPoint818a/helper_net.py\", line 122, in assign_mappings_valid_only\r\n",
      "    row_ind, col_ind = linear_sum_assignment(valid_cost)\r\n",
      "\r\n",
      "  File \"/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/scipy/optimize/_lsap.py\", line 93, in linear_sum_assignment\r\n",
      "    raise ValueError(\"matrix contains invalid numeric entries\")\r\n",
      "\r\n",
      "ValueError: matrix contains invalid numeric entries\r\n",
      "\r\n",
      "\r\n",
      "\t [[node bbox/PyFunc (defined at /home/liu/disk1/3DBoNetPoint818a/helper_net.py:134) ]]\r\n",
      "\t [[gradients/backbone/fa_layer1/ThreeInterpolate_grad/ThreeInterpolateGrad/_407]]\r\n",
      "0 successful operations.\r\n",
      "0 derived errors ignored.\r\n",
      "\r\n",
      "Errors may have originated from an input operation.\r\n",
      "Input Source operations connected to node bbox/PyFunc:\r\n",
      " Y_bbvert (defined at /home/liu/disk1/3DBoNetPoint818a/main_3D_BoNet.py:226)\t\r\n",
      " bbox/add_11 (defined at /home/liu/disk1/3DBoNetPoint818a/helper_net.py:190)\r\n",
      "\r\n",
      "Input Source operations connected to node bbox/PyFunc:\r\n",
      " Y_bbvert (defined at /home/liu/disk1/3DBoNetPoint818a/main_3D_BoNet.py:226)\t\r\n",
      " bbox/add_11 (defined at /home/liu/disk1/3DBoNetPoint818a/helper_net.py:190)\r\n",
      "\r\n",
      "Original stack trace for 'bbox/PyFunc':\r\n",
      "  File \"main_train.py\", line 72, in <module>\r\n",
      "    net.build_graph()\r\n",
      "  File \"/home/liu/disk1/3DBoNetPoint818a/main_3D_BoNet.py\", line 246, in build_graph\r\n",
      "    self.y_bbvert_pred, self.pred_bborder = Ops.bbvert_association(self.X_pc,  self.y_bbvert_pred_raw, self.Y_bbvert, label=bbox_criteria)\r\n",
      "  File \"/home/liu/disk1/3DBoNetPoint818a/helper_net.py\", line 208, in bbvert_association\r\n",
      "    pred_bborder, association_score_min = Ops.hungarian(associate_maxtrix, bb_gt=Y_bbvert)\r\n",
      "  File \"/home/liu/disk1/3DBoNetPoint818a/helper_net.py\", line 134, in hungarian\r\n",
      "    ordering, loss_total = tf.compat.v1.py_func(assign_mappings_valid_only, [loss_matrix, bb_gt], [tf.int32, tf.float32])\r\n",
      "  File \"/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 337, in new_func\r\n",
      "    return func(*args, **kwargs)\r\n",
      "  File \"/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py\", line 206, in wrapper\r\n",
      "    return target(*args, **kwargs)\r\n",
      "  File \"/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py\", line 636, in py_func\r\n",
      "    return py_func_common(func, inp, Tout, stateful, name=name)\r\n",
      "  File \"/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py\", line 617, in py_func_common\r\n",
      "    func=func, inp=inp, Tout=Tout, stateful=stateful, eager=False, name=name)\r\n",
      "  File \"/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py\", line 352, in _internal_py_func\r\n",
      "    input=inp, token=token, Tout=Tout, name=name)\r\n",
      "  File \"/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/ops/gen_script_ops.py\", line 150, in py_func\r\n",
      "    \"PyFunc\", input=input, token=token, Tout=Tout, name=name)\r\n",
      "  File \"/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 750, in _apply_op_helper\r\n",
      "    attrs=attr_protos, op_def=op_def)\r\n",
      "  File \"/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3543, in _create_op_internal\r\n",
      "    op_def=op_def)\r\n",
      "  File \"/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2023, in __init__\r\n",
      "    self._traceback = tf_stack.extract_stack_for_node(self._c_op)\r\n",
      "\r\n",
      "main_train.py is as below：\r\n",
      "# _*_ coding:utf-8 _*_\r\n",
      "import os\r\n",
      "# from helper_data_plot import Plot as Plot\r\n",
      "\r\n",
      "import glob\r\n",
      "import datetime\r\n",
      "from tensorflow.compat.v1 import ConfigProto\r\n",
      "from tensorflow.compat.v1 import InteractiveSession\r\n",
      "config = ConfigProto()\r\n",
      "config.gpu_options.allow_growth = True\r\n",
      "session = InteractiveSession(config=config)\r\n",
      "def train(net, data,configs):\r\n",
      "\tprint(\"train start time is : \",datetime.datetime.now())\r\n",
      "\tfor ep in range(configs.epoch):\r\n",
      "\t\tl_rate = max(0.0005/(2**(ep//20)), 0.00001) #学习率\r\n",
      "\r\n",
      "\t\tdata.shuffle_train_files(ep) #打乱顺序\r\n",
      "\r\n",
      "\t\ttotal_train_batch_num = data.total_train_batch_num \r\n",
      "\t\tprint(\"is training ep : \",ep)\r\n",
      "\t\tprint('total train batch num:', total_train_batch_num)\r\n",
      "\t\t\r\n",
      "\t\tfor i in range(total_train_batch_num):\r\n",
      "\t\t\t###### training\r\n",
      "\t\t\tbat_pc,bat_psem_onehot, bat_bbvert, bat_pmask = data.load_train_one_batch(i)\r\n",
      "\t\t\t# print(\"point num is : \",bat_pc.shape[1])\r\n",
      "\t\t\t# print(\"bat_pc : \",bat_pc.shape)#(4,4096,12) (1,11322,9)\r\n",
      "\t\t\t# print(\"bat_psem_onehot : \",bat_psem_onehot.shape)#(4,4096,13) (1,11322,2)\r\n",
      "\t\t\t# print(\"bat_bbvert : \",bat_bbvert.shape)#(4,24,2,3) (1,24,2,3)\r\n",
      "\t\t\t# print(\"bat_pmask : \",bat_pmask.shape)#(4,24,4096) (1,24,11322)\r\n",
      "\r\n",
      "\t\t\t_, ls_psemce, ls_bbvert_all, ls_bbvert_l2, ls_bbvert_ce, ls_bbvert_iou, ls_bbscore, ls_pmask = net.sess.run([\r\n",
      "\t\t\tnet.optim, net.psemce_loss, net.bbvert_loss, net.bbvert_loss_l2, net.bbvert_loss_ce, net.bbvert_loss_iou,net.bbscore_loss, net.pmask_loss],\r\n",
      "\t\t\tfeed_dict={net.X_pc:bat_pc[:, :, 0:6], net.Y_bbvert:bat_bbvert, net.Y_pmask:bat_pmask[:,:,:], net.Y_psem:bat_psem_onehot[:,:,:], net.lr:l_rate, net.is_train:True})\r\n",
      "\r\n",
      "\t\t\tif i%20==0:#测试val数据\r\n",
      "\t\t\t\tsum_train = net.sess.run(net.sum_merged,\r\n",
      "\t\t\t\tfeed_dict={net.X_pc: bat_pc[:, :, 0:6], net.Y_bbvert: bat_bbvert, net.Y_pmask: bat_pmask, net.Y_psem: bat_psem_onehot, net.lr: l_rate, net.is_train: False})\r\n",
      "\t\t\t\r\n",
      "\t\t\t\tprint ('ep', ep, 'i', i, 'psemce', ls_psemce, 'bbvert', ls_bbvert_all, 'l2', ls_bbvert_l2, 'ce', ls_bbvert_ce, 'siou', ls_bbvert_iou, 'bbscore', ls_bbscore, 'pmask', ls_pmask)\r\n",
      "\r\n",
      "\t\t\t###### random testing\r\n",
      "\t\t\tif i%20==0:\r\n",
      "\r\n",
      "\t\t\t\t#相比 训练的时候 少了 net.optim 多了 net.sum_merged, net.pred_bborder\r\n",
      "\t\t\t\tbat_pc, bat_psem_onehot, bat_bbvert, bat_pmask = data.load_test_one_batch()\r\n",
      "\r\n",
      "\t\t\t\tls_psemce, ls_bbvert_all, ls_bbvert_l2, ls_bbvert_ce, ls_bbvert_iou, ls_bbscore, ls_pmask, sum_test, pred_bborder = net.sess.run([\r\n",
      "\t\t\t\tnet.psemce_loss, net.bbvert_loss, net.bbvert_loss_l2, net.bbvert_loss_ce, net.bbvert_loss_iou, net.bbscore_loss, net.pmask_loss, net.sum_merged, net.pred_bborder],\r\n",
      "\t\t\t\tfeed_dict={net.X_pc:bat_pc[:, :, 0:6], net.Y_bbvert:bat_bbvert, net.Y_pmask:bat_pmask, net.Y_psem:bat_psem_onehot, net.is_train:False})\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\tprint('ep',ep,'i',i,'test psem', ls_psemce, 'bbvert', ls_bbvert_all, 'l2', ls_bbvert_l2, 'ce', ls_bbvert_ce, 'siou', ls_bbvert_iou, 'bbscore', ls_bbscore, 'pmask', ls_pmask)\r\n",
      "\t\t\t\tprint('test pred bborder', pred_bborder)\r\n",
      "\r\n",
      "\t\t\t###### saving model\r\n",
      "\t\t\tif ep % 1 == 0 and i == total_train_batch_num - 1:\r\n",
      "\t\t\t\tsave_path=net.train_mod_dir + 'model' + str(ep).zfill(3) + '.cptk'\r\n",
      "\t\t\t\tnet.saver.save(net.sess, save_path=save_path)\r\n",
      "\t\t\t\tprint(\"model saved in : \",save_path)\r\n",
      "\t\t\t\tprint(\"epoch \",ep,\"end time is :\",datetime.datetime.now())\r\n",
      "\r\n",
      "############\r\n",
      "if __name__=='__main__':\r\n",
      "\tos.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\r\n",
      "\tos.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'  ## specify the GPU to use\r\n",
      "\r\n",
      "\tfrom main_3D_BoNet import BoNet\r\n",
      "\tfrom helper_data_s3dis import Data_Configs as Data_Configs\r\n",
      "\r\n",
      "\tconfigs = Data_Configs()\r\n",
      "\tnet = BoNet(configs = configs)\r\n",
      "\tnet.creat_folders(name='log', re_train=True)\r\n",
      "\tnet.build_graph()\r\n",
      "\r\n",
      "\t####\r\n",
      "\tfrom helper_data_s3dis import Dataset_PointCloud as Data\r\n",
      "\tdata=Data(configs = configs)\r\n",
      "\tdata.check_mat_file_exists()# check the data file for  input \r\n",
      "\ttrain(net, data,configs=configs)\r\n",
      "main_3D_Bonet.py is as below：\r\n",
      "# _*_ coding:utf-8 _*_\r\n",
      "import tensorflow.compat.v1 as tf\r\n",
      "tf.disable_v2_behavior()\r\n",
      "import os\r\n",
      "import shutil\r\n",
      "from helper_net import Ops as Ops\r\n",
      "\r\n",
      "class BoNet:\r\n",
      "\tdef __init__(self, configs):\r\n",
      "\t\tself.points_cc = configs.points_cc#6\r\n",
      "\t\tself.sem_num = configs.sem_num#2\r\n",
      "\t\tself.bb_num = configs.ins_max_num#24\r\n",
      "\r\n",
      "\tdef creat_folders(self, name='log', re_train=False):\r\n",
      "\t\tself.train_mod_dir = './'+name+'/train_mod/'\r\n",
      "\t\tself.train_sum_dir = './'+name+'/train_sum/'\r\n",
      "\t\tself.test_sum_dir = './'+name+'/test_sum/'\r\n",
      "\t\tprint (\"re_train:\", re_train)\r\n",
      "\t\tdef tp(path):\r\n",
      "\t\t\tif os.path.exists(path):\r\n",
      "\t\t\t\tif re_train:\r\n",
      "\t\t\t\t\tprint (path, \": files kept!\")\r\n",
      "\t\t\t\telse:\r\n",
      "\t\t\t\t\tshutil.rmtree(path)\r\n",
      "\t\t\t\t\tos.makedirs(path)\r\n",
      "\t\t\t\t\tprint (path, ': deleted and then created!')\r\n",
      "\t\t\telse:\r\n",
      "\t\t\t\tos.makedirs(path)\r\n",
      "\t\t\t\tprint (path, ': created!')\r\n",
      "\t\ttp(self.test_sum_dir)\r\n",
      "\t\ttp(self.train_sum_dir)\r\n",
      "\t\ttp(self.train_mod_dir)\r\n",
      "\r\n",
      "\t######  1. backbone + sem\r\n",
      "\tdef backbone_pointnet(self, X_pc, is_train):\r\n",
      "\t\t[_, _, points_cc] = X_pc.get_shape()\r\n",
      "\t\tpoints_num = tf.shape(X_pc)[1]\r\n",
      "\t\tX_pc = tf.reshape(X_pc, [-1, points_num, int(points_cc), 1])\r\n",
      "\r\n",
      "\t\tl1 = Ops.xxlu(Ops.conv2d(X_pc, k=(1, points_cc), out_c=64, str=1, pad='VALID', name='l1'), label='lrelu')\r\n",
      "\t\tl2 = Ops.xxlu(Ops.conv2d(l1, k=(1, 1), out_c=64, str=1, pad='VALID', name='l2'), label='lrelu')\r\n",
      "\t\tl3 = Ops.xxlu(Ops.conv2d(l2, k=(1, 1), out_c=64, str=1, pad='VALID', name='l3'), label='lrelu')\r\n",
      "\t\tl4 = Ops.xxlu(Ops.conv2d(l3, k=(1, 1), out_c=128, str=1, pad='VALID', name='l4'), label='lrelu')\r\n",
      "\t\tl5 = Ops.xxlu(Ops.conv2d(l4, k=(1, 1), out_c=1024, str=1, pad='VALID', name='l5'), label='lrelu')\r\n",
      "\t\tglobal_features = tf.reduce_max(l5, axis=1, name='maxpool')\r\n",
      "\t\tglobal_features = tf.reshape(global_features, [-1, int(l5.shape[-1])])\r\n",
      "\t\tpoint_features = tf.reshape(l5, [-1, points_num, int(l5.shape[-1])])\r\n",
      "\r\n",
      "\t\t####  sem\r\n",
      "\t\tg1 = Ops.xxlu(Ops.fc(global_features, out_d=256, name='semg1'), label='lrelu')\r\n",
      "\t\tg2 = Ops.xxlu(Ops.fc(g1, out_d=128, name='semg2'), label='lrelu')\r\n",
      "\t\tsem1 = tf.tile(g2[:,None,None,:], [1, points_num, 1, 1])\r\n",
      "\t\tsem1 = tf.concat([l5, sem1], axis=-1)\r\n",
      "\t\tsem1 = Ops.xxlu(Ops.conv2d(sem1, k=(1,1), out_c=512, str=1, pad='VALID', name='sem1'), label='lrelu')\r\n",
      "\t\tsem2 = Ops.xxlu(Ops.conv2d(sem1, k=(1, 1), out_c=256, str=1, pad='VALID', name='sem2'), label='lrelu')\r\n",
      "\t\tsem3 = Ops.xxlu(Ops.conv2d(sem2, k=(1, 1), out_c=128, str=1, pad='VALID', name='sem3'), label='lrelu')\r\n",
      "\t\tsem3 = Ops.dropout(sem3, keep_prob=0.5, is_train=is_train, name='sem3_dropout')\r\n",
      "\t\tsem4 = Ops.conv2d(sem3, k=(1, 1), out_c=self.sem_num, str=1, pad='VALID', name='sem4')\r\n",
      "\t\tsem4 = tf.reshape(sem4, [-1, points_num, self.sem_num])\r\n",
      "\t\tself.y_psem_logits = sem4\r\n",
      "\t\ty_sem_pred = tf.nn.softmax(self.y_psem_logits, name='y_sem_pred')\r\n",
      "\r\n",
      "\t\treturn point_features, global_features, y_sem_pred\r\n",
      "\r\n",
      "\t# def backbone_pointnet2(self, X_pc, is_train=None):\r\n",
      "\t# \timport helper_pointnet2 as pnet2\r\n",
      "\t# \tpoints_num = tf.shape(X_pc)[1] #每个batch的点的数量 不定\r\n",
      "\t# \tl0_xyz = X_pc[:,:,0:3] # xyz\r\n",
      "\t# \t#可调参数：\r\n",
      "\t# \tl1_xyz, l1_points, l1_indices = pnet2.pointnet_sa_module(l0_xyz, None, npoint=16384, radius=0.025, nsample=8,\r\n",
      "\t# \t\tmlp=[8, 8, 16], mlp2=None, group_all=False, is_training=None, bn_decay=None, scope='layer1')\r\n",
      "\t# \tl2_xyz, l2_points, l2_indices = pnet2.pointnet_sa_module(l1_xyz, l1_points, npoint=4096, radius=0.05, nsample=16,\r\n",
      "\t# \t\tmlp=[16, 16,32], mlp2=None, group_all=False, is_training=None, bn_decay=None, scope='layer2')\r\n",
      "\t# \tl3_xyz, l3_points, l3_indices = pnet2.pointnet_sa_module(l2_xyz, l2_points, npoint=1024, radius=0.1, nsample=32,\r\n",
      "\t# \t    mlp=[32, 32, 64], mlp2=None, group_all=False, is_training=None, bn_decay=None, scope='layer3')\r\n",
      "\t# \tl4_xyz, l4_points, l4_indices = pnet2.pointnet_sa_module(l3_xyz, l3_points, npoint=256, radius=0.2, nsample=64,\r\n",
      "\t# \t    mlp=[64, 64, 128], mlp2=None, group_all=False, is_training=None, bn_decay=None, scope='layer4')\r\n",
      "\t# \tl5_xyz, l5_points, l5_indices = pnet2.pointnet_sa_module(l4_xyz, l4_points, npoint=64, radius=0.4, nsample=128,\r\n",
      "\t# \t    mlp=[128, 128, 256], mlp2=None, group_all=False, is_training=None, bn_decay=None, scope='layer5') \r\n",
      "\r\n",
      "\t# \tl6_xyz, l6_points, l6_indices = pnet2.pointnet_sa_module(l5_xyz, l5_points, npoint=None, radius=None, nsample=None,\r\n",
      "\t# \t\tmlp=[256, 256, 512], mlp2=None, group_all=True, is_training=None, bn_decay=None, scope='layer6')\r\n",
      "\r\n",
      "\t# \t# Feature Propagation layers   \r\n",
      "\t# \tl5_points = pnet2.pointnet_fp_module(l5_xyz, l6_xyz, l5_points, l6_points, [256, 256], is_training=None, bn_decay=None, scope='fa_layer1')\r\n",
      "\t# \tl4_points = pnet2.pointnet_fp_module(l4_xyz, l5_xyz, l4_points, l5_points, [256, 256], is_training=None, bn_decay=None,scope='fa_layer2')\r\n",
      "\t# \tl3_points = pnet2.pointnet_fp_module(l3_xyz, l4_xyz, l3_points, l4_points, [256, 256], is_training=None, bn_decay=None,scope='fa_layer3')\r\n",
      "\t# \tl2_points = pnet2.pointnet_fp_module(l2_xyz, l3_xyz, l2_points, l3_points, [256, 256], is_training=None, bn_decay=None,scope='fa_layer4')\r\n",
      "\t# \tl1_points = pnet2.pointnet_fp_module(l1_xyz, l2_xyz, l1_points, l2_points, [256, 128], is_training=None, bn_decay=None,scope='fa_layer5')\r\n",
      "\t# \tl0_points = pnet2.pointnet_fp_module(l0_xyz, l1_xyz, l0_xyz, l1_points,[128, 128, 128, 128], is_training=None, bn_decay=None, scope='fa_layer6')\r\n",
      "\t# \tglobal_features = tf.reshape(l6_points, [-1, 512])\r\n",
      "\t# \tpoint_features = l0_points\r\n",
      "\r\n",
      "\t# \t# sem\r\n",
      "\t# \tl0_points = l0_points[:,:,None,:]\r\n",
      "\t# \tsem1 = Ops.xxlu(Ops.conv2d(l0_points, k=(1, 1), out_c=128, str=1, pad='VALID', name='sem1'), label='lrelu')\r\n",
      "\t# \tsem2 = Ops.xxlu(Ops.conv2d(sem1, k=(1, 1), out_c=64, str=1, pad='VALID', name='sem2'), label='lrelu')\r\n",
      "\t# \tsem2 = Ops.dropout(sem2, keep_prob=0.5, is_train=is_train, name='sem2_dropout')\r\n",
      "\t# \tsem3 = Ops.conv2d(sem2, k=(1, 1), out_c=self.sem_num, str=1, pad='VALID', name='sem3')\r\n",
      "\t# \tsem3 = tf.reshape(sem3, [-1, points_num, self.sem_num])\r\n",
      "\t# \tself.y_psem_logits = sem3\r\n",
      "\t# \ty_sem_pred = tf.nn.softmax(self.y_psem_logits, name='y_sem_pred')\r\n",
      "\r\n",
      "\t# \treturn point_features, global_features, y_sem_pred\r\n",
      "\r\n",
      "\tdef backbone_pointnet2(self, X_pc, is_train=None):\r\n",
      "\t\timport helper_pointnet2 as pnet2\r\n",
      "\t\tpoints_num = tf.shape(X_pc)[1]\r\n",
      "\t\tl0_xyz = X_pc[:,:,0:3]\r\n",
      "\r\n",
      "\t\tl1_xyz, l1_points, l1_indices = pnet2.pointnet_sa_module(l0_xyz, None, npoint=1024, radius=0.1, nsample=32,\r\n",
      "\t\t\tmlp=[32, 32, 64], mlp2=None, group_all=False, is_training=None, bn_decay=None, scope='layer1')\r\n",
      "\t\tl2_xyz, l2_points, l2_indices = pnet2.pointnet_sa_module(l1_xyz, l1_points, npoint=256, radius=0.2, nsample=64,\r\n",
      "\t\t\tmlp=[64, 64, 128], mlp2=None, group_all=False, is_training=None, bn_decay=None, scope='layer2')\r\n",
      "\t\tl3_xyz, l3_points, l3_indices = pnet2.pointnet_sa_module(l2_xyz, l2_points, npoint=64, radius=0.4, nsample=128,\r\n",
      "\t\t    mlp=[128, 128, 256], mlp2=None, group_all=False, is_training=None, bn_decay=None, scope='layer3')\r\n",
      "\t\tl4_xyz, l4_points, l4_indices = pnet2.pointnet_sa_module(l3_xyz, l3_points, npoint=None, radius=None, nsample=None,\r\n",
      "\t\t\tmlp=[256, 256, 512], mlp2=None, group_all=True, is_training=None, bn_decay=None, scope='layer4')\r\n",
      "\r\n",
      "\t\t# Feature Propagation layers\r\n",
      "\t\tl3_points = pnet2.pointnet_fp_module(l3_xyz, l4_xyz, l3_points, l4_points, [256, 256], is_training=None, bn_decay=None, scope='fa_layer1')\r\n",
      "\t\tl2_points = pnet2.pointnet_fp_module(l2_xyz, l3_xyz, l2_points, l3_points, [256, 256], is_training=None, bn_decay=None,scope='fa_layer2')\r\n",
      "\t\tl1_points = pnet2.pointnet_fp_module(l1_xyz, l2_xyz, l1_points, l2_points, [256, 128], is_training=None, bn_decay=None,scope='fa_layer3')\r\n",
      "\t\tl0_points = pnet2.pointnet_fp_module(l0_xyz, l1_xyz, l0_xyz, l1_points,[128, 128, 128, 128], is_training=None, bn_decay=None, scope='fa_layer6')\r\n",
      "\t\tglobal_features = tf.reshape(l4_points, [-1, 512])\r\n",
      "\t\tpoint_features = l0_points\r\n",
      "\r\n",
      "\t\t# sem\r\n",
      "\t\tl0_points = l0_points[:,:,None,:]\r\n",
      "\t\tsem1 = Ops.xxlu(Ops.conv2d(l0_points, k=(1, 1), out_c=128, str=1, pad='VALID', name='sem1'), label='lrelu')\r\n",
      "\t\tsem2 = Ops.xxlu(Ops.conv2d(sem1, k=(1, 1), out_c=64, str=1, pad='VALID', name='sem2'), label='lrelu')\r\n",
      "\t\tsem2 = Ops.dropout(sem2, keep_prob=0.5, is_train=is_train, name='sem2_dropout')\r\n",
      "\t\tsem3 = Ops.conv2d(sem2, k=(1, 1), out_c=self.sem_num, str=1, pad='VALID', name='sem3')\r\n",
      "\t\tsem3 = tf.reshape(sem3, [-1, points_num, self.sem_num])\r\n",
      "\t\tself.y_psem_logits = sem3\r\n",
      "\t\ty_sem_pred = tf.nn.softmax(self.y_psem_logits, name='y_sem_pred')\r\n",
      "\r\n",
      "\t\treturn point_features, global_features, y_sem_pred\r\n",
      "\r\n",
      "\tdef backbone_pointconv(self, X_pc, is_training=None,sigma=0.05,bn_decay=None,weight_decay=None):\r\n",
      "\t\timport helper_pointnet2 as pnet2\r\n",
      "\t\tpoints_num = tf.shape(X_pc)[1] #每个batch的点的数量 不定\r\n",
      "\t\tl0_xyz = X_pc[:,:,0:3] # xyz\r\n",
      "\t\tl0_points=l0_xyz\r\n",
      "\t\t# k=16\r\n",
      "\t\t#可调参数：\r\n",
      "\t\tl1_xyz, l1_points = pnet2.feature_encoding_layer(l0_xyz, l0_points, npoint=1024, radius = 0.1, sigma = sigma, K=32, mlp=[32,32,64], is_training=is_training, bn_decay=bn_decay, weight_decay = weight_decay, scope='layer1')\r\n",
      "\t\tl2_xyz, l2_points = pnet2.feature_encoding_layer(l1_xyz, l1_points, npoint=256, radius = 0.2, sigma = 2 * sigma, K=32, mlp=[64,64,128], is_training=is_training, bn_decay=bn_decay, weight_decay = weight_decay, scope='layer2')\r\n",
      "\t\tl3_xyz, l3_points = pnet2.feature_encoding_layer(l2_xyz, l2_points, npoint=64, radius = 0.4, sigma = 4 * sigma, K=32, mlp=[128,128,256], is_training=is_training, bn_decay=bn_decay, weight_decay = weight_decay, scope='layer3')\r\n",
      "\t\tl4_xyz, l4_points = pnet2.feature_encoding_layer(l3_xyz, l3_points, npoint=36, radius = 0.8, sigma = 8 * sigma, K=32, mlp=[256,256,512], is_training=is_training, bn_decay=bn_decay, weight_decay = weight_decay, scope='layer4')\r\n",
      "\t\tl5_xyz, l5_points, l5_indices = pnet2.pointnet_sa_module(l4_xyz, l4_points, npoint=None, radius=None, nsample=None,mlp=[256,  512], mlp2=None, group_all=True, is_training=None, bn_decay=None, scope='layer5')\r\n",
      "\t\t# Feature decoding layers\r\n",
      "\t\tl3_points = pnet2.feature_decoding_layer(l3_xyz, l4_xyz, l3_points, l4_points, 0.8, 8 * sigma, 32, [256,256], is_training, bn_decay, weight_decay, scope='fa_layer1')\r\n",
      "\t\tl2_points = pnet2.feature_decoding_layer(l2_xyz, l3_xyz, l2_points, l3_points, 0.4, 4 * sigma, 32, [256,256], is_training, bn_decay, weight_decay, scope='fa_layer2')\r\n",
      "\t\tl1_points = pnet2.feature_decoding_layer(l1_xyz, l2_xyz, l1_points, l2_points, 0.2, 2 * sigma, 32, [256,128], is_training, bn_decay, weight_decay, scope='fa_layer3')\r\n",
      "\t\tl0_points = pnet2.feature_decoding_layer(l0_xyz, l1_xyz, l0_points, l1_points, 0.1, sigma, 32, [128,128,128], is_training, bn_decay, weight_decay, scope='fa_layer4')\r\n",
      "\r\n",
      "\t\tglobal_features = tf.reshape((l5_points), [-1, 512])\r\n",
      "\t\tpoint_features = l0_points\r\n",
      "\t\t# sem\r\n",
      "\t\tl0_points = l0_points[:,:,None,:]\r\n",
      "\t\tsem1 = Ops.xxlu(Ops.conv2d(l0_points, k=(1, 1), out_c=128, str=1, pad='VALID', name='sem1'), label='lrelu')\r\n",
      "\t\tsem2 = Ops.xxlu(Ops.conv2d(sem1, k=(1, 1), out_c=64, str=1, pad='VALID', name='sem2'), label='lrelu')\r\n",
      "\t\tsem2 = Ops.dropout(sem2, keep_prob=0.5, is_train=is_training, name='sem2_dropout')\r\n",
      "\t\tsem3 = Ops.conv2d(sem2, k=(1, 1), out_c=self.sem_num, str=1, pad='VALID', name='sem3')\r\n",
      "\t\tsem3 = tf.reshape(sem3, [-1, points_num, self.sem_num])\r\n",
      "\t\tself.y_psem_logits = sem3\r\n",
      "\t\ty_sem_pred = tf.nn.softmax(self.y_psem_logits, name='y_sem_pred')\r\n",
      "\r\n",
      "\t\treturn point_features, global_features, y_sem_pred\r\n",
      "\t######  2. bbox\r\n",
      "\tdef bbox_net(self, global_features):\r\n",
      "\t\t'''由全局特征直接输出固定数量的bb 和相应的分数 \r\n",
      "\t\t'''\r\n",
      "\t\tb1 = Ops.xxlu(Ops.fc(global_features, out_d= 512, name='b1'), label='lrelu')\r\n",
      "\t\tb2 = Ops.xxlu(Ops.fc(b1, out_d= 256, name='b2'), label='lrelu')\r\n",
      "\r\n",
      "\t\t#### sub branch 1\r\n",
      "\t\tb3 = Ops.xxlu(Ops.fc(b2, out_d=256, name='b3'), label='lrelu')\r\n",
      "\t\tbbvert = Ops.fc(b3, out_d=self.bb_num * 2 * 3, name='bbvert')\r\n",
      "\t\tbbvert = tf.reshape(bbvert, [-1, self.bb_num, 2, 3])\r\n",
      "\t\tpoints_min = tf.reduce_min(bbvert, axis=-2)[:, :, None, :]\r\n",
      "\t\tpoints_max = tf.reduce_max(bbvert, axis=-2)[:, :, None, :]\r\n",
      "\t\ty_bbvert_pred = tf.concat([points_min, points_max], axis=-2, name='y_bbvert_pred')\r\n",
      "\r\n",
      "\t\t#### sub branch 2\r\n",
      "\t\tb4 = Ops.xxlu(Ops.fc(b2, out_d=256, name='b4'), label='lrelu')\r\n",
      "\t\ty_bbscore_pred = tf.sigmoid(Ops.fc(b4, out_d=self.bb_num * 1, name='y_bbscore_pred'))\r\n",
      "\r\n",
      "\t\treturn y_bbvert_pred, y_bbscore_pred\r\n",
      "\r\n",
      "\t######  3. pmask\r\n",
      "\tdef pmask_net(self, point_features, global_features, bbox, bboxscore):\r\n",
      "\t\tp_f_num = int(point_features.shape[-1])\r\n",
      "\t\tp_num = tf.shape(point_features)[1]\r\n",
      "\t\tbb_num = int(bbox.shape[1])\r\n",
      "\r\n",
      "\t\tglobal_features = tf.tile(Ops.xxlu(Ops.fc(global_features, out_d=256, name='down_g1'), label='lrelu')[:,None,None,:], [1, p_num, 1, 1])\r\n",
      "\t\tpoint_features = Ops.xxlu(Ops.conv2d(point_features[:,:,:,None],k=(1, p_f_num), out_c=256, str=1,name='down_p1',pad='VALID'), label='lrelu')\r\n",
      "\t\tpoint_features = tf.concat([point_features, global_features], axis=-1)\r\n",
      "\t\tpoint_features = Ops.xxlu(Ops.conv2d(point_features, k=(1,int(point_features.shape[-2])), out_c=128, str=1, pad='VALID', name='down_p2'), label='lrelu')\r\n",
      "\t\tpoint_features = Ops.xxlu(Ops.conv2d(point_features, k=(1, int(point_features.shape[-2])), out_c=128, str=1, pad='VALID',name='down_p3'), label='lrelu')\r\n",
      "\t\tpoint_features = tf.squeeze(point_features, axis=-2)\r\n",
      "\r\n",
      "\t\tbbox_info = tf.tile(tf.concat([tf.reshape(bbox, [-1, bb_num, 6]), bboxscore[:,:,None]],axis=-1)[:,:,None,:], [1,1,p_num,1])\r\n",
      "\t\tpmask0 = tf.tile(point_features[:,None,:,:], [1, bb_num, 1, 1])\r\n",
      "\t\tpmask0 = tf.concat([pmask0, bbox_info], axis=-1)\r\n",
      "\t\tpmask0 = tf.reshape(pmask0, [-1, p_num, int(pmask0.shape[-1]), 1])\r\n",
      "\r\n",
      "\t\tpmask1 = Ops.xxlu(Ops.conv2d(pmask0, k=(1,int(pmask0.shape[-2])), out_c=64, str=1, pad='VALID', name='pmask1'), label='lrelu')\r\n",
      "\t\tpmask2 = Ops.xxlu(Ops.conv2d(pmask1, k=(1, 1), out_c=32, str=1, pad='VALID', name='pmask2'),label='lrelu')\r\n",
      "\t\tpmask3 = Ops.conv2d(pmask2, k=(1,1), out_c=1, str=1, pad='VALID', name='pmask3')\r\n",
      "\t\tpmask3 = tf.reshape(pmask3, [-1, bb_num, p_num])\r\n",
      "\r\n",
      "\t\ty_pmask_logits = pmask3\r\n",
      "\t\ty_pmask_pred = tf.nn.sigmoid(y_pmask_logits, name='y_pmask_pred')\r\n",
      "\r\n",
      "\t\treturn y_pmask_pred\r\n",
      "\r\n",
      "\tdef build_graph(self, GPU='0'):\r\n",
      "\t\t'''\r\n",
      "\t\t\t整个的网络结构都在这里\r\n",
      "\t\t'''\r\n",
      "\t\t#######   1. define inputs\r\n",
      "\t\tself.X_pc = tf.placeholder(shape=[None, None, self.points_cc], dtype=tf.float32, name='X_pc')\r\n",
      "\t\tself.Y_bbvert = tf.placeholder(shape=[None, self.bb_num, 2, 3], dtype=tf.float32, name='Y_bbvert')\r\n",
      "\t\tself.Y_pmask = tf.placeholder(shape=[None, self.bb_num, None], dtype=tf.float32, name='Y_pmask')\r\n",
      "\t\tself.Y_psem = tf.placeholder(shape=[None, None, self.sem_num], dtype=tf.float32, name='Y_psem')\r\n",
      "\t\tself.is_train = tf.placeholder(dtype=tf.bool, name='is_train')\r\n",
      "\t\tself.lr = tf.placeholder(dtype=tf.float32, name='lr')\r\n",
      "\r\n",
      "\t\t#######  2. define networks, losses\r\n",
      "\t\twith tf.variable_scope('backbone'):\r\n",
      "\t\t\t#self.point_features, self.global_features, self.y_psem_pred = self.backbone_pointnet(self.X_pc, self.is_train)\r\n",
      "\t\t\tself.point_features, self.global_features, self.y_psem_pred = self.backbone_pointnet2(self.X_pc, self.is_train)\r\n",
      "\t\t\t# self.point_features, self.global_features, self.y_psem_pred =self.backbone_pointconv(self.X_pc, is_training=self.is_train)\r\n",
      "\r\n",
      "\t\t\t### loss\r\n",
      "\t\t\tself.psemce_loss = Ops.get_loss_psem_ce(self.y_psem_logits, self.Y_psem)\r\n",
      "\t\t\tself.sum_psemce_loss = tf.summary.scalar('psemce_loss', self.psemce_loss)\r\n",
      "\r\n",
      "\t\twith tf.variable_scope('bbox'):\r\n",
      "\t\t\tself.y_bbvert_pred_raw, self.y_bbscore_pred_raw = self.bbox_net(self.global_features)\r\n",
      "\t\t\t#### association, only used for training\r\n",
      "\t\t\tbbox_criteria = 'use_all_ce_l2_iou'\r\n",
      "\t\t\tself.y_bbvert_pred, self.pred_bborder = Ops.bbvert_association(self.X_pc,  self.y_bbvert_pred_raw, self.Y_bbvert, label=bbox_criteria)\r\n",
      "\t\t\tself.y_bbscore_pred = Ops.bbscore_association(self.y_bbscore_pred_raw, self.pred_bborder)\r\n",
      "\r\n",
      "\t\t\t### loss\r\n",
      "\t\t\tself.bbvert_loss, self.bbvert_loss_l2, self.bbvert_loss_ce, self.bbvert_loss_iou = \\\r\n",
      "\t\t\t\tOps.get_loss_bbvert(self.X_pc, self.y_bbvert_pred, self.Y_bbvert, label=bbox_criteria)\r\n",
      "\t\t\tself.bbscore_loss = Ops.get_loss_bbscore(self.y_bbscore_pred, self.Y_bbvert)\r\n",
      "\t\t\t\r\n",
      "\t\t\tself.sum_bbox_vert_loss = tf.summary.scalar('bbvert_loss', self.bbvert_loss)\r\n",
      "\t\t\tself.sum_bbox_vert_loss_l2 = tf.summary.scalar('bbvert_loss_l2', self.bbvert_loss_l2)\r\n",
      "\t\t\tself.sum_bbox_vert_loss_ce = tf.summary.scalar('bbvert_loss_ce', self.bbvert_loss_ce)\r\n",
      "\t\t\tself.sum_bbox_vert_loss_iou = tf.summary.scalar('bbvert_loss_iou', self.bbvert_loss_iou)\r\n",
      "\t\t\tself.sum_bbox_score_loss = tf.summary.scalar('bbscore_loss', self.bbscore_loss)\r\n",
      "\r\n",
      "\t\twith tf.variable_scope('pmask'):\r\n",
      "\t\t\tself.y_pmask_pred = self.pmask_net(self.point_features, self.global_features, self.y_bbvert_pred, self.y_bbscore_pred)\r\n",
      "\r\n",
      "\t\t\t### loss\r\n",
      "\t\t\tself.pmask_loss = Ops.get_loss_pmask(self.X_pc, self.y_pmask_pred, self.Y_pmask)\r\n",
      "\t\t\tself.sum_pmask_loss = tf.summary.scalar('pmask_loss', self.pmask_loss)\r\n",
      "\r\n",
      "\t\twith tf.variable_scope('pmask', reuse=True):\r\n",
      "\t\t\t#### during testing, no need to associate, use unordered predictions\r\n",
      "\t\t\tself.y_pmask_pred_raw = self.pmask_net(self.point_features, self.global_features, self.y_bbvert_pred_raw, self.y_bbscore_pred_raw)\r\n",
      "\r\n",
      "\t\t######   3. define optimizers\r\n",
      "\t\tvar_backbone = [var for var in tf.trainable_variables() if var.name.startswith('backbone') and not var.name.startswith('backbone/sem')]\r\n",
      "\t\tvar_sem = [var for var in tf.trainable_variables() if var.name.startswith('backbone/sem')]\r\n",
      "\t\tvar_bbox = [var for var in tf.trainable_variables() if var.name.startswith('bbox')]\r\n",
      "\t\tvar_pmask = [var for var in tf.trainable_variables() if var.name.startswith('pmask')]\r\n",
      "\r\n",
      "\t\tend_2_end_loss = self.bbvert_loss + self.bbscore_loss  + self.pmask_loss + self.psemce_loss\r\n",
      "\t\tself.optim = tf.train.AdamOptimizer(learning_rate=self.lr).minimize(end_2_end_loss, var_list = var_bbox+var_pmask +var_backbone+ var_sem)\r\n",
      "\r\n",
      "\t\t######   4. others\r\n",
      "\t\tprint(Ops.variable_count())\r\n",
      "\t\tself.saver = tf.train.Saver(max_to_keep=0)\r\n",
      "\t\tconfig = tf.ConfigProto(allow_soft_placement=True)\r\n",
      "\t\tconfig.gpu_options.visible_device_list = GPU\r\n",
      "\t\tself.sess = tf.Session(config=config)\r\n",
      "\t\tself.sum_writer_train = tf.summary.FileWriter(self.train_sum_dir, self.sess.graph)\r\n",
      "\t\tself.sum_write_test = tf.summary.FileWriter(self.test_sum_dir)\r\n",
      "\t\tself.sum_merged = tf.summary.merge_all()\r\n",
      "\r\n",
      "\t\tpath = self.train_mod_dir\r\n",
      "\t\tmodelsaved='model025.cptk.data-00000-of-00001'\r\n",
      "\t\tif os.path.isfile(path + modelsaved):\r\n",
      "\t\t\tprint (\"restoring saved model\"+modelsaved)\r\n",
      "\t\t\tself.saver.restore(self.sess, path + 'model025.cptk')\r\n",
      "\t\telse:\r\n",
      "\t\t\tprint (\"model not found, all weights are initilized\")\r\n",
      "\t\t\tself.sess.run(tf.global_variables_initializer())\r\n",
      "\r\n",
      "\t\treturn 0\r\n",
      "helper_net is as below：\r\n",
      "# _*_ coding:utf-8 _*_\r\n",
      "import numpy as np\r\n",
      "import tensorflow as tf\r\n",
      "from scipy.optimize import linear_sum_assignment\r\n",
      "import tensorflow as tf\r\n",
      "#tf.compat.v1.disable_v2_behavior()\r\n",
      "#import tensorflow.compat.v1 as tf\r\n",
      "#tf.disable_v2_behavior()\r\n",
      "class Ops:\r\n",
      "\r\n",
      "    @staticmethod\r\n",
      "    def lrelu(x, leak=0.2):\r\n",
      "        f1 = 0.5 * (1 + leak)\r\n",
      "        f2 = 0.5 * (1 - leak)\r\n",
      "        return f1 * x + f2 * abs(x)\r\n",
      "\r\n",
      "    @staticmethod\r\n",
      "    def relu(x):\r\n",
      "        return tf.nn.relu(x)\r\n",
      "\r\n",
      "    @staticmethod\r\n",
      "    def xxlu(x,label,name=None):\r\n",
      "        if label =='relu':\r\n",
      "            return  Ops.relu(x)\r\n",
      "        if label =='lrelu':\r\n",
      "            return  Ops.lrelu(x,leak=0.2)\r\n",
      "\r\n",
      "    @staticmethod\r\n",
      "    def variable_sum(var, name):\r\n",
      "        with tf.compat.v1.name_scope(name):\r\n",
      "            mean = tf.reduce_mean(input_tensor=var)\r\n",
      "            tf.compat.v1.summary.scalar('mean', mean)\r\n",
      "            stddev = tf.sqrt(tf.reduce_mean(input_tensor=tf.square(var - mean)))\r\n",
      "            tf.compat.v1.summary.scalar('stddev', stddev)\r\n",
      "            tf.compat.v1.summary.scalar('max', tf.reduce_max(input_tensor=var))\r\n",
      "            tf.compat.v1.summary.scalar('min', tf.reduce_min(input_tensor=var))\r\n",
      "            tf.compat.v1.summary.histogram('histogram', var)\r\n",
      "\r\n",
      "    @staticmethod\r\n",
      "    def variable_count():\r\n",
      "        total_para = 0\r\n",
      "        for variable in tf.compat.v1.trainable_variables():\r\n",
      "            shape = variable.get_shape()\r\n",
      "            variable_para = 1\r\n",
      "            for dim in shape:\r\n",
      "                variable_para *= dim.value\r\n",
      "            total_para += variable_para\r\n",
      "        return total_para\r\n",
      "\r\n",
      "    @staticmethod\r\n",
      "    def fc(x, out_d, name):\r\n",
      "        #xavier_init = tf.contrib.layers.xavier_initializer()\r\n",
      "        #xavier_init = tf.truncated_normal_initializer()\r\n",
      "        xavier_init = tf.initializers.GlorotUniform()\r\n",
      "        zero_init = tf.compat.v1.zeros_initializer()\r\n",
      "        in_d = x.get_shape()[1]\r\n",
      "        with tf.device('/cpu:0'):  # to create Variables stored on CPU memory\r\n",
      "            w = tf.compat.v1.get_variable(name + '_w', [in_d, out_d], initializer=xavier_init)\r\n",
      "            b = tf.compat.v1.get_variable(name + '_b', [out_d], initializer=zero_init)\r\n",
      "        y = tf.nn.bias_add(tf.matmul(x, w), b)\r\n",
      "        Ops.variable_sum(w, name)\r\n",
      "        return y\r\n",
      "\t\r\n",
      "    @staticmethod\r\n",
      "    def conv2d(x, k=(1,1), out_c=1, str=1, name='',pad='SAME'):\r\n",
      "        #xavier_init = tf.contrib.layers.xavier_initializer()\r\n",
      "        #xavier_init = tf.truncated_normal_initializer()\r\n",
      "        xavier_init = tf.initializers.GlorotUniform()\r\n",
      "        zero_init = tf.compat.v1.zeros_initializer()\r\n",
      "        in_c = x.get_shape()[3]\r\n",
      "        with tf.device('/cpu:0'):  # to create Variables stored on CPU memory\r\n",
      "            w = tf.compat.v1.get_variable(name + '_w', [k[0], k[1], in_c, out_c], initializer=xavier_init)\r\n",
      "            b = tf.compat.v1.get_variable(name + '_b', [out_c], initializer=zero_init)\r\n",
      "\r\n",
      "        stride = [1, str, str, 1]\r\n",
      "        y = tf.nn.bias_add(tf.nn.conv2d(input=x, filters=w, strides=stride, padding=pad), b)\r\n",
      "        Ops.variable_sum(w, name)\r\n",
      "        return y\r\n",
      "\r\n",
      "    @staticmethod\r\n",
      "    def dropout(x, is_train, keep_prob, name):\r\n",
      "        y = tf.cond(pred=is_train, true_fn=lambda: tf.nn.dropout(x, rate=1 - (keep_prob), name=name), false_fn=lambda: x)\r\n",
      "        return y\r\n",
      "\r\n",
      "    ####################################\r\n",
      "    @staticmethod\r\n",
      "    def gather_tensor_along_2nd_axis(bat_bb_pred, bat_bb_indices):\r\n",
      "        bat_size = tf.shape(input=bat_bb_pred)[0]\r\n",
      "        [_, ins_max_num, d1, d2] = bat_bb_pred.get_shape()\r\n",
      "        bat_size_range = tf.range(bat_size)\r\n",
      "        bat_size_range_flat = tf.reshape(bat_size_range, [-1,1])\r\n",
      "        bat_size_range_flat_repeat = tf.tile(bat_size_range_flat, [1, int(ins_max_num)])\r\n",
      "        bat_size_range_flat_repeat = tf.reshape(bat_size_range_flat_repeat, [-1])\r\n",
      "        \r\n",
      "        indices_2d_flat = tf.reshape(bat_bb_indices, [-1])\r\n",
      "        indices_2d_flat_repeat = bat_size_range_flat_repeat*int(ins_max_num) + indices_2d_flat\r\n",
      "\r\n",
      "        bat_bb_pred = tf.reshape(bat_bb_pred, [-1, int(d1), int(d2)])\r\n",
      "        bat_bb_pred_new = tf.gather(bat_bb_pred, indices_2d_flat_repeat)\r\n",
      "        bat_bb_pred_new = tf.reshape(bat_bb_pred_new, [bat_size, int(ins_max_num), int(d1), int(d2)])\r\n",
      "   \r\n",
      "        return bat_bb_pred_new\r\n",
      "\r\n",
      "    @staticmethod\r\n",
      "    def hungarian(loss_matrix, bb_gt):\r\n",
      "        box_mask = np.array([[0, 0, 0], [0, 0, 0]])\r\n",
      "\r\n",
      "        def assign_mappings_valid_only(cost, gt_boxes):\r\n",
      "            # return ordering : batch_size x num_instances\r\n",
      "            loss_total = 0.\r\n",
      "            batch_size, num_instances = cost.shape[:2]\r\n",
      "            ordering = np.zeros(shape=[batch_size, num_instances]).astype(np.int32)\r\n",
      "            for idx in range(batch_size):\r\n",
      "                ins_gt_boxes = gt_boxes[idx]\r\n",
      "                ins_count = 0\r\n",
      "                for box in ins_gt_boxes:\r\n",
      "                    if np.array_equal(box, box_mask):\r\n",
      "                        break\r\n",
      "                    else:\r\n",
      "                        ins_count += 1\r\n",
      "                valid_cost = cost[idx][:ins_count]\r\n",
      "                row_ind, col_ind = linear_sum_assignment(valid_cost)\r\n",
      "                unmapped = num_instances - ins_count\r\n",
      "                if unmapped > 0:\r\n",
      "                    rest = np.array(range(ins_count, num_instances))\r\n",
      "                    row_ind = np.concatenate([row_ind, rest])\r\n",
      "                    unmapped_ind = np.array(list(set(range(num_instances)) - set(col_ind)))\r\n",
      "                    col_ind = np.concatenate([col_ind, unmapped_ind])\r\n",
      "\r\n",
      "                loss_total += cost[idx][row_ind, col_ind].sum()\r\n",
      "                ordering[idx] = np.reshape(col_ind, [1, -1])\r\n",
      "            return ordering, (loss_total / float(batch_size * num_instances)).astype(np.float32)\r\n",
      "        ######\r\n",
      "        ordering, loss_total = tf.compat.v1.py_func(assign_mappings_valid_only, [loss_matrix, bb_gt], [tf.int32, tf.float32])\r\n",
      "\r\n",
      "        return ordering, loss_total\r\n",
      "\r\n",
      "    @staticmethod\r\n",
      "    def bbvert_association(X_pc, y_bbvert_pred, Y_bbvert, label=''):\r\n",
      "        points_num = tf.shape(input=X_pc)[1]\r\n",
      "        bbnum = int(y_bbvert_pred.shape[1])\r\n",
      "        points_xyz = X_pc[:, :, 0:3]\r\n",
      "        points_xyz = tf.tile(points_xyz[:, None, :, :], [1, bbnum, 1, 1])\r\n",
      "\r\n",
      "        ##### get points hard mask in each gt bbox\r\n",
      "        gt_bbox_min_xyz = Y_bbvert[:, :, 0, :]\r\n",
      "        gt_bbox_max_xyz = Y_bbvert[:, :, 1, :]\r\n",
      "        gt_bbox_min_xyz = tf.tile(gt_bbox_min_xyz[:, :, None, :], [1, 1, points_num, 1])\r\n",
      "        gt_bbox_max_xyz = tf.tile(gt_bbox_max_xyz[:, :, None, :], [1, 1, points_num, 1])\r\n",
      "        tp1_gt = gt_bbox_min_xyz - points_xyz\r\n",
      "        tp2_gt = points_xyz - gt_bbox_max_xyz\r\n",
      "        tp_gt = tp1_gt * tp2_gt\r\n",
      "        points_in_gt_bbox_prob = tf.cast(tf.equal(tf.reduce_mean(input_tensor=tf.cast(tf.greater_equal(tp_gt, 0.), tf.float32), axis=-1), 1.0), tf.float32)\r\n",
      "\r\n",
      "        ##### get points soft mask in each pred bbox ---> Algorithm 1\r\n",
      "        pred_bbox_min_xyz = y_bbvert_pred[:, :, 0, :]\r\n",
      "        pred_bbox_max_xyz = y_bbvert_pred[:, :, 1, :]\r\n",
      "        pred_bbox_min_xyz = tf.tile(pred_bbox_min_xyz[:, :, None, :], [1, 1, points_num, 1])\r\n",
      "        pred_bbox_max_xyz = tf.tile(pred_bbox_max_xyz[:, :, None, :], [1, 1, points_num, 1])\r\n",
      "        tp1_pred = pred_bbox_min_xyz - points_xyz\r\n",
      "        tp2_pred = points_xyz - pred_bbox_max_xyz\r\n",
      "        tp_pred = 100 * tp1_pred * tp2_pred\r\n",
      "        tp_pred = tf.maximum(tf.minimum(tp_pred, 20.0), -20.0)\r\n",
      "        points_in_pred_bbox_prob = 1.0/(1.0 + tf.exp(-1.0 * tp_pred))\r\n",
      "        points_in_pred_bbox_prob = tf.reduce_min(input_tensor=points_in_pred_bbox_prob, axis=-1)\r\n",
      "\r\n",
      "        ##### get bbox cross entropy scores\r\n",
      "        prob_gt = tf.tile(points_in_gt_bbox_prob[:, :, None, :], [1, 1, bbnum, 1])\r\n",
      "        prob_pred = tf.tile(points_in_pred_bbox_prob[:, None, :, :], [1, bbnum, 1, 1])\r\n",
      "        ce_scores_matrix = - prob_gt * tf.math.log(prob_pred + 1e-8) - (1 - prob_gt) * tf.math.log(1 - prob_pred + 1e-8)\r\n",
      "        ce_scores_matrix = tf.reduce_mean(input_tensor=ce_scores_matrix, axis=-1)\r\n",
      "\r\n",
      "        ##### get bbox soft IOU\r\n",
      "        TP = tf.reduce_sum(input_tensor=prob_gt * prob_pred, axis=-1)\r\n",
      "        FP = tf.reduce_sum(input_tensor=prob_pred, axis=-1) - TP\r\n",
      "        FN = tf.reduce_sum(input_tensor=prob_gt, axis=-1) - TP\r\n",
      "        iou_scores_matrix = TP/ (TP + FP + FN + 1e-6)\r\n",
      "        # iou_scores_matrix = 1.0/iou_scores_matrix  # bad, don't use\r\n",
      "        iou_scores_matrix = -1.0 * iou_scores_matrix  # to minimize\r\n",
      "\r\n",
      "        ##### get bbox l2 scores\r\n",
      "        l2_gt = tf.tile(Y_bbvert[:, :, None, :, :], [1, 1, bbnum, 1, 1])\r\n",
      "        l2_pred = tf.tile(y_bbvert_pred[:, None, :, :, :], [1, bbnum, 1, 1, 1])\r\n",
      "        l2_gt = tf.reshape(l2_gt, [-1, bbnum, bbnum, 2 * 3])\r\n",
      "        l2_pred = tf.reshape(l2_pred, [-1, bbnum, bbnum, 2 * 3])\r\n",
      "        l2_scores_matrix = tf.reduce_mean(input_tensor=(l2_gt - l2_pred) ** 2, axis=[-1])\r\n",
      "\r\n",
      "        ##### bbox association\r\n",
      "        if label == 'use_all_ce_l2_iou':\r\n",
      "            associate_maxtrix = ce_scores_matrix + l2_scores_matrix + iou_scores_matrix\r\n",
      "        elif label == 'use_both_ce_l2':\r\n",
      "            associate_maxtrix = ce_scores_matrix + l2_scores_matrix\r\n",
      "        elif label == 'use_both_ce_iou':\r\n",
      "            associate_maxtrix = ce_scores_matrix + iou_scores_matrix\r\n",
      "        elif label == 'use_both_l2_iou':\r\n",
      "            associate_maxtrix = l2_scores_matrix + iou_scores_matrix\r\n",
      "        elif label == 'use_only_ce':\r\n",
      "            associate_maxtrix = ce_scores_matrix\r\n",
      "        elif label == 'use_only_l2':\r\n",
      "            associate_maxtrix = l2_scores_matrix\r\n",
      "        elif label == 'use_only_iou':\r\n",
      "            associate_maxtrix = iou_scores_matrix\r\n",
      "        else:\r\n",
      "            associate_maxtrix=None\r\n",
      "            print('association label error!'); exit()\r\n",
      "\r\n",
      "        ######\r\n",
      "        pred_bborder, association_score_min = Ops.hungarian(associate_maxtrix, bb_gt=Y_bbvert)\r\n",
      "        pred_bborder = tf.cast(pred_bborder, dtype=tf.int32)\r\n",
      "        y_bbvert_pred_new = Ops.gather_tensor_along_2nd_axis(y_bbvert_pred, pred_bborder)\r\n",
      "\r\n",
      "        return y_bbvert_pred_new, pred_bborder\r\n",
      "\r\n",
      "    @staticmethod\r\n",
      "    def bbscore_association(y_bbscore_pred_raw, pred_bborder):\r\n",
      "        y_bbscore_pred_raw = y_bbscore_pred_raw[:,:,None,None]\r\n",
      "        y_bbscore_pred_new = Ops.gather_tensor_along_2nd_axis(y_bbscore_pred_raw, pred_bborder)\r\n",
      "\r\n",
      "        y_bbscore_pred_new = tf.reshape(y_bbscore_pred_new, [-1, int(y_bbscore_pred_new.shape[1])])\r\n",
      "        return y_bbscore_pred_new\r\n",
      "\r\n",
      "    ####################################  sem loss\r\n",
      "    @staticmethod\r\n",
      "    def get_loss_psem_ce(y_psem_logits, Y_psem):\r\n",
      "        psemce_loss = tf.nn.softmax_cross_entropy_with_logits(logits=y_psem_logits, labels=tf.stop_gradient(Y_psem))\r\n",
      "        psemce_loss = tf.reduce_mean(input_tensor=psemce_loss)\r\n",
      "        return psemce_loss\r\n",
      "\r\n",
      "    ####################################  bbox loss\r\n",
      "    @staticmethod\r\n",
      "    def get_loss_bbvert(X_pc, y_bbvert_pred, Y_bbvert, label=''):\r\n",
      "        points_num = tf.shape(input=X_pc)[1]\r\n",
      "        bb_num = int(Y_bbvert.shape[1])\r\n",
      "        points_xyz = X_pc[:, :, 0:3]\r\n",
      "        points_xyz = tf.tile(points_xyz[:, None, :, :], [1, bb_num, 1, 1])\r\n",
      "\r\n",
      "        ##### get points hard mask in each gt bbox\r\n",
      "        gt_bbox_min_xyz = Y_bbvert[:, :, 0, :]\r\n",
      "        gt_bbox_max_xyz = Y_bbvert[:, :, 1, :]\r\n",
      "        gt_bbox_min_xyz = tf.tile(gt_bbox_min_xyz[:, :, None, :], [1, 1, points_num, 1])\r\n",
      "        gt_bbox_max_xyz = tf.tile(gt_bbox_max_xyz[:, :, None, :], [1, 1, points_num, 1])\r\n",
      "        tp1_gt = gt_bbox_min_xyz - points_xyz\r\n",
      "        tp2_gt = points_xyz - gt_bbox_max_xyz\r\n",
      "        tp_gt = tp1_gt * tp2_gt\r\n",
      "        points_in_gt_bbox_prob = tf.cast(tf.equal(tf.reduce_mean(input_tensor=tf.cast(tf.greater_equal(tp_gt, 0.), tf.float32), axis=-1), 1.0), tf.float32)\r\n",
      "\r\n",
      "        ##### get points soft mask in each pred bbox\r\n",
      "        pred_bbox_min_xyz = y_bbvert_pred[:, :, 0, :]\r\n",
      "        pred_bbox_max_xyz = y_bbvert_pred[:, :, 1, :]\r\n",
      "        pred_bbox_min_xyz = tf.tile(pred_bbox_min_xyz[:, :, None, :], [1, 1, points_num, 1])\r\n",
      "        pred_bbox_max_xyz = tf.tile(pred_bbox_max_xyz[:, :, None, :], [1, 1, points_num, 1])\r\n",
      "        tp1_pred = pred_bbox_min_xyz - points_xyz\r\n",
      "        tp2_pred = points_xyz - pred_bbox_max_xyz\r\n",
      "        tp_pred = 100*tp1_pred*tp2_pred\r\n",
      "        tp_pred = tf.maximum(tf.minimum(tp_pred, 20.0), -20.0)\r\n",
      "        points_in_pred_bbox_prob = 1.0/(1.0 + tf.exp(-1.0 * tp_pred))\r\n",
      "        points_in_pred_bbox_prob = tf.reduce_min(input_tensor=points_in_pred_bbox_prob, axis=-1)\r\n",
      "\r\n",
      "        ##### helper -> the valid bbox (the gt boxes are zero-padded during data processing, pickup valid ones here)\r\n",
      "        Y_bbox_helper = tf.reduce_sum(input_tensor=tf.reshape(Y_bbvert, [-1, bb_num, 6]), axis=-1)\r\n",
      "        Y_bbox_helper = tf.cast(tf.greater(Y_bbox_helper, 0.), tf.float32)\r\n",
      "\r\n",
      "        ##### 1. get ce loss of valid/positive bboxes, don't count the ce_loss of invalid/negative bboxes\r\n",
      "        Y_bbox_helper_tp1 = tf.tile(Y_bbox_helper[:, :, None], [1, 1, points_num])\r\n",
      "        bbox_loss_ce_all = -points_in_gt_bbox_prob * tf.math.log(points_in_pred_bbox_prob + 1e-8) \\\r\n",
      "                       -(1.-points_in_gt_bbox_prob)*tf.math.log(1.-points_in_pred_bbox_prob + 1e-8)\r\n",
      "        bbox_loss_ce_pos = tf.reduce_sum(input_tensor=bbox_loss_ce_all*Y_bbox_helper_tp1)/tf.reduce_sum(input_tensor=Y_bbox_helper_tp1)\r\n",
      "        bbox_loss_ce = bbox_loss_ce_pos\r\n",
      "\r\n",
      "        ##### 2. get iou loss of valid/positive bboxes\r\n",
      "        TP = tf.reduce_sum(input_tensor=points_in_pred_bbox_prob * points_in_gt_bbox_prob, axis=-1)\r\n",
      "        FP = tf.reduce_sum(input_tensor=points_in_pred_bbox_prob, axis=-1) - TP\r\n",
      "        FN = tf.reduce_sum(input_tensor=points_in_gt_bbox_prob, axis=-1) - TP\r\n",
      "        bbox_loss_iou_all = TP/(TP + FP + FN + 1e-6)\r\n",
      "        bbox_loss_iou_all = -1.0*bbox_loss_iou_all\r\n",
      "        bbox_loss_iou_pos = tf.reduce_sum(input_tensor=bbox_loss_iou_all*Y_bbox_helper)/tf.reduce_sum(input_tensor=Y_bbox_helper)\r\n",
      "        bbox_loss_iou = bbox_loss_iou_pos\r\n",
      "\r\n",
      "        ##### 3. get l2 loss of both valid/positive bboxes\r\n",
      "        bbox_loss_l2_all = (Y_bbvert - y_bbvert_pred)**2\r\n",
      "        bbox_loss_l2_all = tf.reduce_mean(input_tensor=tf.reshape(bbox_loss_l2_all, [-1, bb_num, 6]), axis=-1)\r\n",
      "        bbox_loss_l2_pos = tf.reduce_sum(input_tensor=bbox_loss_l2_all*Y_bbox_helper)/tf.reduce_sum(input_tensor=Y_bbox_helper)\r\n",
      "\r\n",
      "        ## to minimize the 3D volumn of invalid/negative bboxes, it serves as a regularizer to penalize false pred bboxes\r\n",
      "        ## it turns out to be quite helpful, but not discussed in the paper\r\n",
      "        bbox_pred_neg = tf.tile((1.- Y_bbox_helper)[:,:,None,None], [1,1,2,3])*y_bbvert_pred\r\n",
      "        bbox_loss_l2_neg = (bbox_pred_neg[:,:,0,:]-bbox_pred_neg[:,:,1,:])**2\r\n",
      "        bbox_loss_l2_neg = tf.reduce_sum(input_tensor=bbox_loss_l2_neg)/(tf.reduce_sum(input_tensor=1.-Y_bbox_helper)+1e-8)\r\n",
      "\r\n",
      "        bbox_loss_l2 = bbox_loss_l2_pos + bbox_loss_l2_neg\r\n",
      "\r\n",
      "        #####\r\n",
      "        if label == 'use_all_ce_l2_iou':\r\n",
      "            bbox_loss = bbox_loss_ce + bbox_loss_l2 + bbox_loss_iou\r\n",
      "        elif label == 'use_both_ce_l2':\r\n",
      "            bbox_loss = bbox_loss_ce + bbox_loss_l2\r\n",
      "        elif label == 'use_both_ce_iou':\r\n",
      "            bbox_loss = bbox_loss_ce + bbox_loss_iou\r\n",
      "        elif label == 'use_both_l2_iou':\r\n",
      "            bbox_loss = bbox_loss_l2 + bbox_loss_iou\r\n",
      "        elif label == 'use_only_ce':\r\n",
      "            bbox_loss = bbox_loss_ce\r\n",
      "        elif label == 'use_only_l2':\r\n",
      "            bbox_loss = bbox_loss_l2\r\n",
      "        elif label == 'use_only_iou':\r\n",
      "            bbox_loss = bbox_loss_iou\r\n",
      "        else:\r\n",
      "            bbox_loss = None\r\n",
      "            print('bbox loss label error!'); exit()\r\n",
      "\r\n",
      "        return bbox_loss, bbox_loss_l2, bbox_loss_ce, bbox_loss_iou\r\n",
      "\r\n",
      "    @staticmethod\r\n",
      "    def get_loss_bbscore(y_bbscore_pred, Y_bbvert):\r\n",
      "        bb_num = int(Y_bbvert.shape[1])\r\n",
      "\r\n",
      "        ##### helper -> the valid bbox\r\n",
      "        Y_bbox_helper = tf.reduce_sum(input_tensor=tf.reshape(Y_bbvert, [-1, bb_num, 6]), axis=-1)\r\n",
      "        Y_bbox_helper = tf.cast(tf.greater(Y_bbox_helper, 0.), tf.float32)\r\n",
      "\r\n",
      "        ##### bbox score loss\r\n",
      "        bbox_loss_score = tf.reduce_mean(input_tensor=-Y_bbox_helper * tf.math.log(y_bbscore_pred + 1e-8)\r\n",
      "                                         -(1. - Y_bbox_helper) * tf.math.log(1. - y_bbscore_pred + 1e-8))\r\n",
      "        return bbox_loss_score\r\n",
      "\r\n",
      "    ####################################  pmask loss\r\n",
      "    @staticmethod\r\n",
      "    def get_loss_pmask(X_pc, y_pmask_pred, Y_pmask):\r\n",
      "        points_num = tf.shape(input=X_pc)[1]\r\n",
      "        ##### valid ins\r\n",
      "        Y_pmask_helper = tf.reduce_sum(input_tensor=Y_pmask, axis=-1)\r\n",
      "        Y_pmask_helper = tf.cast(tf.greater(Y_pmask_helper, 0.), tf.float32)\r\n",
      "        Y_pmask_helper = tf.tile(Y_pmask_helper[:, :, None], [1, 1, points_num])\r\n",
      "\r\n",
      "        Y_pmask = Y_pmask * Y_pmask_helper\r\n",
      "        y_pmask_pred = y_pmask_pred * Y_pmask_helper\r\n",
      "\r\n",
      "        ##### focal loss\r\n",
      "        alpha = 0.75\r\n",
      "        gamma = 2\r\n",
      "        pmask_loss_focal_all = -Y_pmask*alpha*((1.-y_pmask_pred)**gamma)*tf.math.log(y_pmask_pred+1e-8)\\\r\n",
      "                               -(1.-Y_pmask)*(1.-alpha)*(y_pmask_pred**gamma)*tf.math.log(1.-y_pmask_pred+1e-8)\r\n",
      "        pmask_loss_focal = tf.reduce_sum(input_tensor=pmask_loss_focal_all*Y_pmask_helper)/tf.reduce_sum(input_tensor=Y_pmask_helper)\r\n",
      "\r\n",
      "        ## the above \"alpha\" makes the loss to be small\r\n",
      "        ## then use a constant, so it's numerically comparable with other losses (e.g., semantic loss, bbox loss)\r\n",
      "        pmask_loss = 30*pmask_loss_focal\r\n",
      "\r\n",
      "        return pmask_loss\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.5\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  micro: port op ELU from lite\n",
      "issue body -  @tensorflow/micro\r\n",
      "\r\n",
      "This issue tracks my work porting operator ELU from lite to micro.\r\n",
      "\r\n",
      "The port will be submitted in a number of PRs. Here's a rough flight plan per @advaitjain and @petewarden:\r\n",
      "\r\n",
      "PR 1: Extract the code for parsing the op from a flatbuffer out of ParseOpDataTfLite in tensorflow/lite/core/api/flatbuffer_conversions.cc into a standalone function that can be called from micro's op resolver\r\n",
      "PR 2: Extract the reference implementation out of tensorflow/lite/kernels/internal/reference/reference_ops.h into its own header which can be included without dragging in reference_ops.h's dependences\r\n",
      "PR 3: Copy operator from lite to micro making minimal changes and not including in the build\r\n",
      "PR 4: Delete extra code from the micro copy of the operator\r\n",
      "PR 5: Port micro copy of operator as necessary and add a corresponding test\r\n",
      "PR 6: Extract common activation code into activations.cc and activation_utils.h files.  Extract common test code into activation_test_utils.h file.\r\n",
      "\n",
      "issue labels - \n",
      "comp:lite\n",
      "comp:micro\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  [TFL] Convert Size to Prod(Shape)\n",
      "issue body -  This PR converts `TF_SizeOp` to `TF_ProdOp(TF_ShapeOp)`. It makes TFL users can run models with `tf.size` without `SELECT_TF_OPS`. Fixes #46285.\n",
      "issue labels - \n",
      "cla: no\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  Fix InvalidArgumentError error when mixed precision policy is used in Attention/AdditiveAttention layer\n",
      "issue body -  This PR tries to address the issue raised in #46064 where\r\n",
      "InvalidArgumentError error is thrown when mixed precision policy is used\r\n",
      "in keras Attention/AdditiveAttention layer.\r\n",
      "\r\n",
      "This PR fixes #46064.\r\n",
      "\r\n",
      "This PR also fixes  #43261.\r\n",
      "\r\n",
      "Signed-off-by: Yong Tang <yong.tang.github@outlook.com>\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:keras\n",
      "prtype:bugfix\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  [tf.data] Support eager mode benchmarking\n",
      "issue body -  This PR adds support for eager execution based benchmarks based on the benchmarking context. This is achieved by checking for the `eager_execution()` value within the benchmarking context and iterating over the dataset eagerly in the `DatasetBenchmarkBase` class.\r\n",
      "\r\n",
      "Additionally, the `ListFilesBenchmark` in `python/data/benchmarks/list_files_benchmark.py` has been refactored to use the `DatasetBenchmarkBase` class.\r\n",
      "\r\n",
      "A sample comparison w.r.t `RangeBenchmark` is as follows:\r\n",
      "\r\n",
      "GRAPH MODE:\r\n",
      "```python\r\n",
      "entry {\r\n",
      "  name: \"RangeBenchmark.modeling_on.graph\"\r\n",
      "  iters: 5\r\n",
      "  wall_time: 1.7887322902679443e-07\r\n",
      "  extras {\r\n",
      "    key: \"num_elements\"\r\n",
      "    value {\r\n",
      "      double_value: 10000000.0\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "\r\n",
      "entry {\r\n",
      "  name: \"RangeBenchmark.modeling_off.graph\"\r\n",
      "  iters: 5\r\n",
      "  wall_time: 1.541191530227661e-07\r\n",
      "  extras {\r\n",
      "    key: \"num_elements\"\r\n",
      "    value {\r\n",
      "      double_value: 50000000.0\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "```\r\n",
      "\r\n",
      "EAGER MODE:\r\n",
      "```python\r\n",
      "entry {\r\n",
      "  name: \"RangeBenchmark.modeling_on.eager\"\r\n",
      "  iters: 5\r\n",
      "  wall_time: 2.0390429496765137e-07\r\n",
      "  extras {\r\n",
      "    key: \"num_elements\"\r\n",
      "    value {\r\n",
      "      double_value: 10000000.0\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "\r\n",
      "entry {\r\n",
      "  name: \"RangeBenchmark.modeling_off.eager\"\r\n",
      "  iters: 5\r\n",
      "  wall_time: 1.8014323711395263e-07\r\n",
      "  extras {\r\n",
      "    key: \"num_elements\"\r\n",
      "    value {\r\n",
      "      double_value: 50000000.0\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "```\r\n",
      "\r\n",
      "w.r.t  `ListFilesBenchmark`\r\n",
      "```python\r\n",
      "\r\n",
      "GRAPH MODE\r\n",
      "entry {\r\n",
      "  name: \"ListFilesBenchmark.nested_directory(1024*16).graph\"\r\n",
      "  iters: 3\r\n",
      "  wall_time: 8.092029020190239e-07\r\n",
      "  extras {\r\n",
      "    key: \"num_elements\"\r\n",
      "    value {\r\n",
      "      double_value: 2048.0\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "\r\n",
      "EAGER MODE\r\n",
      "entry {\r\n",
      "  name: \"ListFilesBenchmark.nested_directory(1024*16).eager\"\r\n",
      "  iters: 3\r\n",
      "  wall_time: 7.745111361145973e-07\r\n",
      "  extras {\r\n",
      "    key: \"num_elements\"\r\n",
      "    value {\r\n",
      "      double_value: 2048.0\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "```\r\n",
      "\r\n",
      "\r\n",
      "NOTE: Description has been updated as per review comments.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:data\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  Optimization using Tensorflow issue\n",
      "issue body -  We had given below test case, we written all steps as per instructions. nit sure when we went wrong. Please guide us -\r\n",
      "\r\n",
      "We will look at three scenarios here -\r\n",
      "\r\n",
      "- Tuning different optimisation algorithms, \r\n",
      "- Tuning learning rate and momentum of SGD optimizer\r\n",
      "- Tuning beta values of Adam optimizer\r\n",
      "\r\n",
      "```\r\n",
      "import numpy as np\r\n",
      "from sklearn.model_selection import GridSearchCV\r\n",
      "from keras.models import Sequential\r\n",
      "from keras.layers import Dense\r\n",
      "from keras.wrappers.scikit_learn import KerasClassifier\r\n",
      "from sklearn.datasets import load_iris\r\n",
      "from sklearn.model_selection import train_test_split\r\n",
      "from sklearn.preprocessing import LabelEncoder,StandardScaler\r\n",
      "from sklearn.utils import shuffle\r\n",
      "from keras.utils.np_utils import to_categorical\r\n",
      "from keras.optimizers import SGD,Adam\r\n",
      "from matplotlib import pyplot\r\n",
      "import seaborn as sns\r\n",
      "import pandas as pd\r\n",
      "from keras.models import model_from_json\r\n",
      "\r\n",
      "```\r\n",
      "\r\n",
      "\r\n",
      "- Load the Dataset\r\n",
      "- Load the iris dataset using load_iris() function.\r\n",
      "- Store the data of the iris dataset in the variable X.\r\n",
      "- Store the target of the iris dataset in the variable y.\r\n",
      "- Convert the variable y into categorial variable using function to_categorial and save it in variable y.\r\n",
      "- Set seed value as 7 in the variable seed and use random.seed function in numpy to set seed value.\r\n",
      "- Now shuffle the data X and y using shuffle function and save it in variables X ,Y.\r\n",
      "\r\n",
      "```\r\n",
      "iris = load_iris()\r\n",
      "X=  iris.data\r\n",
      "y = iris.target\r\n",
      "y = to_categorical(iris.target,3)\r\n",
      "seed = 7\r\n",
      "np.random.seed(seed)\r\n",
      "X, Y =  shuffle(X, y)\r\n",
      "```\r\n",
      "\r\n",
      "In variable optimizer pass the following optimizers as a list -\r\n",
      "SGD,RMSprop,Adam,Nadam\r\n",
      "In param_grid pass parameter optimizer as optimizer using dict\r\n",
      "\r\n",
      "```\r\n",
      "optimizer = ['SGD', 'RMSprop',  'Adam', 'Nadam']\r\n",
      "param_grid = dict(optimizer=optimizer)\r\n",
      "```\r\n",
      "\r\n",
      "Create a sequential model\r\n",
      "The model expects rows of data with 4 variables (the input_dim=4 argument)\r\n",
      "The first hidden layer has 64 nodes and uses the relu activation function.\r\n",
      "The second hidden layer has 32 nodes and uses the relu activation function.\r\n",
      "The third hidden layer has 16 nodes and uses the relu activation function.\r\n",
      "The output layer has 3 nodes and uses the softmax activation function.\r\n",
      "While comipling the model pass the following parameters -\r\n",
      "     -optimizer as optimizer\r\n",
      "     -loss as categorical cross entropy \r\n",
      "     -metrics as accuracy.\r\n",
      "Return the compiled model\r\n",
      "\r\n",
      "```\r\n",
      "def create_model(optimizer='adam'):\r\n",
      "    model = Sequential()\r\n",
      "    model.add(Dense(4, input_dim=4))\r\n",
      "    model.add(Dense(64, activation='relu'))\r\n",
      "    model.add(Dense(32, activation='relu'))\r\n",
      "    model.add(Dense(16, activation='relu'))\r\n",
      "    model.add(Dense(3, activation='softmax'))\r\n",
      "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy']) \r\n",
      "    return model\r\n",
      "```\r\n",
      "\r\n",
      "Use the KerasClassifier function to call the model function with following parameters -\r\n",
      "build_fn as create_model\r\n",
      "batch_size as 10\r\n",
      "verbose as 0\r\n",
      "epochs as 10\r\n",
      "Save the above in the variable model\r\n",
      "\r\n",
      "`model = KerasClassifier(build_fn=create_model, batch_size=10, verbose=0, epochs=10)`\r\n",
      "\r\n",
      "In grid use the GridSearchCV function and pass the following parameters -\r\n",
      "estimator as model\r\n",
      "param_grid as param_grid\r\n",
      "n_jobs as 1\r\n",
      "Now fit the model with X and Y using grid and save it in grid_result\r\n",
      "\r\n",
      "```\r\n",
      "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\r\n",
      "grid_result = grid.fit(X, Y)\r\n",
      "```\r\n",
      "\r\n",
      "\r\n",
      "```\r\n",
      "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\r\n",
      "means = grid_result.cv_results_['mean_test_score']\r\n",
      "stds = grid_result.cv_results_['std_test_score']\r\n",
      "params = grid_result.cv_results_['params']\r\n",
      "for mean, stdev, param in zip(means, stds, params):\r\n",
      "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\r\n",
      "```\r\n",
      "\r\n",
      "Best: 0.946667 using {'optimizer': 'RMSprop'}\r\n",
      "0.753333 (0.082192) with: {'optimizer': 'SGD'}\r\n",
      "0.946667 (0.047140) with: {'optimizer': 'RMSprop'}\r\n",
      "0.913333 (0.083799) with: {'optimizer': 'Adam'}\r\n",
      "0.866667 (0.106249) with: {'optimizer': 'Nadam'}\r\n",
      "\r\n",
      "\r\n",
      "```\r\n",
      "pyplot.figure(figsize=(10,8))\r\n",
      "#pyplot.xticks(grid_result1.cv_results_['mean_test_score'])\r\n",
      "pyplot.title(\"Performance metrics of each optimiser\")\r\n",
      "plot=sns.barplot(grid_result.cv_results_['mean_test_score'],optimizer)\r\n",
      "pyplot.show() \r\n",
      "```\r\n",
      "\r\n",
      "In variable learn_rate pass the following learn rates as a list -\r\n",
      "0.001, 0.01,0.3\r\n",
      "In variable momentum pass the following momentums as a list -\r\n",
      "0.0, 0.4, 0.9\r\n",
      "\r\n",
      "```\r\n",
      "learn_rate = [0.001, 0.01, 0.3]\r\n",
      "momentum = [0.0, 0.4, 0.9]\r\n",
      "```\r\n",
      "\r\n",
      "Use the same model parameters as above to construct the model in the function create_model1\r\n",
      "In the variable optimizer pass the following parameters using the optimizer SGD\r\n",
      " -lr as learn_rate\r\n",
      " -momentum as momentum\r\n",
      "While comipling the model pass the following parameters -\r\n",
      " -optimizer as optimizer\r\n",
      " -loss as categorical cross entropy \r\n",
      " -metrics as accuracy.\r\n",
      "Return the compiled model\r\n",
      "\r\n",
      "```\r\n",
      "def create_model1(learn_rate=0.01, momentum=0):\r\n",
      "    model1 = Sequential()\r\n",
      "    model1.add(Dense(4, input_dim=4))\r\n",
      "    model1.add(Dense(64, activation='relu'))\r\n",
      "    model1.add(Dense(32, activation='relu'))\r\n",
      "    model1.add(Dense(16, activation='relu'))\r\n",
      "    model1.add(Dense(3, activation='softmax'))\r\n",
      "    optimizer = SGD(lr=learn_rate, momentum=momentum)\r\n",
      "    model1.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])    \r\n",
      "    return model1\r\n",
      "```\r\n",
      "\r\n",
      "\r\n",
      "`model1 = KerasClassifier(build_fn=create_model1, batch_size=10, verbose=0, epochs=10)`\r\n",
      "\r\n",
      "```\r\n",
      "param_grid1 = dict(learn_rate=learn_rate,momentum=momentum )\r\n",
      "grid1 = GridSearchCV(estimator=model1, param_grid=param_grid1, n_jobs=1)\r\n",
      "```\r\n",
      "\r\n",
      "Use random.seed function in numpy to set seed value.\r\n",
      "Now fit the model with X and Y using grid1 and save it in grid_result1\r\n",
      "\r\n",
      "```\r\n",
      "np.random.seed(seed)\r\n",
      "grid_result1 = grid1.fit(X, Y)\r\n",
      "```\r\n",
      "\r\n",
      "```\r\n",
      "print(\"Best: %f using %s\" % (grid_result1.best_score_, grid_result1.best_params_))\r\n",
      "means1 = grid_result1.cv_results_['mean_test_score']\r\n",
      "stds1 = grid_result1.cv_results_['std_test_score']\r\n",
      "params1 = grid_result1.cv_results_['params']\r\n",
      "for mean, stdev, param in zip(means1, stds1, params1):\r\n",
      "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\r\n",
      "```\r\n",
      "\r\n",
      "Best: 0.893333 using {'learn_rate': 0.01, 'momentum': 0.9}\r\n",
      "0.360000 (0.032660) with: {'learn_rate': 0.001, 'momentum': 0.0}\r\n",
      "0.666667 (0.037712) with: {'learn_rate': 0.001, 'momentum': 0.4}\r\n",
      "0.780000 (0.081650) with: {'learn_rate': 0.001, 'momentum': 0.9}\r\n",
      "0.760000 (0.016330) with: {'learn_rate': 0.01, 'momentum': 0.0}\r\n",
      "0.866667 (0.160278) with: {'learn_rate': 0.01, 'momentum': 0.4}\r\n",
      "0.893333 (0.067987) with: {'learn_rate': 0.01, 'momentum': 0.9}\r\n",
      "0.413333 (0.188562) with: {'learn_rate': 0.3, 'momentum': 0.0}\r\n",
      "0.333333 (0.018856) with: {'learn_rate': 0.3, 'momentum': 0.4}\r\n",
      "0.333333 (0.018856) with: {'learn_rate': 0.3, 'momentum': 0.9}\r\n",
      "\r\n",
      "```\r\n",
      "params1=pd.DataFrame(params1)\r\n",
      "pyplot.figure(figsize=(10,8))\r\n",
      "#pyplot.xticks(grid_result1.cv_results_['mean_test_score'])\r\n",
      "pyplot.title(\"Performance metrics of SGD optimiser with different learning rates and momentum\")\r\n",
      "plot1=sns.barplot(params1[\"learn_rate\"],grid_result1.cv_results_['mean_test_score'],hue=params1[\"momentum\"])\r\n",
      "plot1.set(ylabel='Score')\r\n",
      "pyplot.show() \r\n",
      "```\r\n",
      "\r\n",
      "```\r\n",
      "beta_1 = [0.001, 0.01, 0.3]\r\n",
      "beta_2 = [0.0, 0.4, 0.9] \r\n",
      "```\r\n",
      "\r\n",
      "```\r\n",
      "def create_model2(beta_1=0.01, beta_2=0):\r\n",
      "    model2 = Sequential()\r\n",
      "    model2.add(Dense(4, input_dim=4))\r\n",
      "    model2.add(Dense(64, activation='relu'))\r\n",
      "    model2.add(Dense(32, activation='relu'))\r\n",
      "    model2.add(Dense(16, activation='relu'))\r\n",
      "    model2.add(Dense(3, activation='softmax'))\r\n",
      "    optimizer = Adam(beta_1=beta_1,beta_2=beta_2)\r\n",
      "    model2.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\r\n",
      "    return model2\r\n",
      "```\r\n",
      "\r\n",
      "`model2 =  KerasClassifier(build_fn=create_model2,batch_size=10, verbose=0,  epochs=10)`\r\n",
      "\r\n",
      "```\r\n",
      "param_grid2 = dict(beta_1=beta_1, beta_2=beta_2)\r\n",
      "grid2 = GridSearchCV(estimator=model2, param_grid=param_grid2, n_jobs=1)\r\n",
      "```\r\n",
      "\r\n",
      "Use random.seed function in numpy to set seed value.\r\n",
      "Now fit the model with X and Y using grid2 and save it in grid_result2\r\n",
      "\r\n",
      "```\r\n",
      "np.random.seed(seed)\r\n",
      "grid_result2 = grid2.fit(X, Y)\r\n",
      "```\r\n",
      "\r\n",
      "```\r\n",
      "print(\"Best: %f using %s\" % (grid_result2.best_score_, grid_result2.best_params_))\r\n",
      "means2 = grid_result2.cv_results_['mean_test_score']\r\n",
      "stds2 = grid_result2.cv_results_['std_test_score']\r\n",
      "params2 = grid_result2.cv_results_['params']\r\n",
      "for mean, stdev, param in zip(means2, stds2, params2):\r\n",
      "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\r\n",
      "```\r\n",
      "\r\n",
      "Best: 0.966667 using {'beta_2': 0.4, 'beta_1': 0.001}\r\n",
      "0.526667 (0.264491) with: {'beta_2': 0.0, 'beta_1': 0.001}\r\n",
      "0.966667 (0.009428) with: {'beta_2': 0.4, 'beta_1': 0.001}\r\n",
      "0.840000 (0.133666) with: {'beta_2': 0.9, 'beta_1': 0.001}\r\n",
      "0.320000 (0.032660) with: {'beta_2': 0.0, 'beta_1': 0.01}\r\n",
      "0.813333 (0.111156) with: {'beta_2': 0.4, 'beta_1': 0.01}\r\n",
      "0.766667 (0.151731) with: {'beta_2': 0.9, 'beta_1': 0.01}\r\n",
      "0.360000 (0.032660) with: {'beta_2': 0.0, 'beta_1': 0.3}\r\n",
      "0.926667 (0.041096) with: {'beta_2': 0.4, 'beta_1': 0.3}\r\n",
      "0.766667 (0.151731) with: {'beta_2': 0.9, 'beta_1': 0.3}\r\n",
      "\r\n",
      "\r\n",
      "```\r\n",
      "params2=pd.DataFrame(params2)\r\n",
      "pyplot.figure(figsize=(10,8))\r\n",
      "#pyplot.xticks(grid_result1.cv_results_['mean_test_score'])\r\n",
      "pyplot.title(\"Performance metrics of SGD optimiser with different beta values\")\r\n",
      "plot2=sns.barplot(params2[\"beta_1\"],grid_result2.cv_results_['mean_test_score'],hue=params2[\"beta_2\"])\r\n",
      "plot2.set(ylabel='Score')\r\n",
      "pyplot.show() \r\n",
      "```\r\n",
      "\r\n",
      "```\r\n",
      "with open(\"score.txt\",\"w\") as f:\r\n",
      "    f.write(str(round(grid_result.best_score_,2)))\r\n",
      "with open(\"params.txt\",\"w\") as f:\r\n",
      "    f.write(str(grid_result.best_params_))\r\n",
      "\r\n",
      "with open(\"score1.txt\",\"w\") as f:\r\n",
      "    f.write(str(round(grid_result1.best_score_,2)))\r\n",
      "with open(\"params1.txt\",\"w\") as f:\r\n",
      "    f.write(str(grid_result1.best_params_))\r\n",
      "    \r\n",
      "with open(\"score2.txt\",\"w\") as f:\r\n",
      "    f.write(str(round(grid_result2.best_score_,2)))\r\n",
      "with open(\"params2.txt\",\"w\") as f:\r\n",
      "    f.write(str(grid_result2.best_params_))\r\n",
      "\r\n",
      "```        \r\n",
      "\r\n",
      "```\r\n",
      "def save_model(model):\r\n",
      "    # saving model\r\n",
      "    json_model = model.to_json()\r\n",
      "    open('model.json', 'w').write(json_model)\r\n",
      "    # saving weights\r\n",
      "    model.save_weights('model.h5', overwrite=True)\r\n",
      "classifier=create_model()\r\n",
      "save_model(classifier)\r\n",
      "```\r\n",
      "\r\n",
      " \n",
      "issue labels - \n",
      "comp:keras\n",
      "stat:awaiting response\n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  code's\n",
      "issue body -  <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "- TensorFlow installed from (source or binary):\r\n",
      "- TensorFlow version:\r\n",
      "- Python version:\r\n",
      "- Installed using virtualenv? pip? conda?:\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version:\r\n",
      "- GPU model and memory:\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "\r\n",
      "**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n",
      "\r\n",
      "\r\n",
      "**Any other info / logs**\r\n",
      "Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n",
      "\n",
      "issue labels - \n",
      "stat:awaiting response\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  In different tf2 versions, the weight naming rules of creating keras model are different\n",
      "issue body -  **System information**\r\n",
      "- OS Platform and Distribution : Linux Ubuntu 18.04\r\n",
      "- TensorFlow installed from (source or binary): pip install tensorflow-cpu\r\n",
      "- TensorFlow version (use command below): 2.1, 2.2, 2.3, 2.4\r\n",
      "- Python version: 3.6 / 3.8\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "I create keras model using same code, but got different results(different weight names) in different tf version.\r\n",
      "This will prevent me from loading network weights (based on variable name) in different tf version.\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "I can get same results in different tf version.\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "\r\n",
      "I create kears model:\r\n",
      "```python\r\n",
      "from tensorflow.keras import layers, Model, Sequential\r\n",
      "\r\n",
      "\r\n",
      "class ConvBNReLU(layers.Layer):\r\n",
      "    def __init__(self, out_channel, kernel_size=3, stride=1, **kwargs):\r\n",
      "        super(ConvBNReLU, self).__init__(**kwargs)\r\n",
      "        layers_list = [layers.Conv2D(filters=out_channel, kernel_size=kernel_size,\r\n",
      "                                     strides=stride, padding='SAME', use_bias=False, name='Conv2d'),\r\n",
      "                       layers.BatchNormalization(momentum=0.9, epsilon=1e-5, name='BatchNorm'),\r\n",
      "                       layers.ReLU(max_value=6.0)]\r\n",
      "\r\n",
      "        self.combine_layer = Sequential(layers_list, name=\"combine\")\r\n",
      "\r\n",
      "    def call(self, inputs, training=False, **kwargs):\r\n",
      "        x = self.combine_layer(inputs, training=training)\r\n",
      "        return x\r\n",
      "\r\n",
      "\r\n",
      "def main():\r\n",
      "    input_image = layers.Input(shape=(224, 224, 3), dtype='float32')\r\n",
      "    # conv1\r\n",
      "    x = ConvBNReLU(32, stride=2)(input_image)\r\n",
      "    output = ConvBNReLU(64, stride=2)(x)\r\n",
      "    model = Model(inputs=input_image, outputs=output)\r\n",
      "\r\n",
      "    for i in model.weights:\r\n",
      "        print(i.name)\r\n",
      "\r\n",
      "\r\n",
      "if __name__ == '__main__':\r\n",
      "    main()\r\n",
      "\r\n",
      "```\r\n",
      "\r\n",
      "In tf2.0, 2.1 and 2.2, the printed weight name information is as follows:\r\n",
      "```\r\n",
      "conv_bn_re_lu/combine/Conv2d/kernel:0\r\n",
      "conv_bn_re_lu/combine/BatchNorm/gamma:0\r\n",
      "conv_bn_re_lu/combine/BatchNorm/beta:0\r\n",
      "conv_bn_re_lu/combine/BatchNorm/moving_mean:0\r\n",
      "conv_bn_re_lu/combine/BatchNorm/moving_variance:0\r\n",
      "conv_bn_re_lu_1/combine/Conv2d/kernel:0\r\n",
      "conv_bn_re_lu_1/combine/BatchNorm/gamma:0\r\n",
      "conv_bn_re_lu_1/combine/BatchNorm/beta:0\r\n",
      "conv_bn_re_lu_1/combine/BatchNorm/moving_mean:0\r\n",
      "conv_bn_re_lu_1/combine/BatchNorm/moving_variance:0\r\n",
      "```\r\n",
      "\r\n",
      "But in tf2.3 and 2.4, I got different results:\r\n",
      "```\r\n",
      "Conv2d/kernel:0\r\n",
      "BatchNorm/gamma:0\r\n",
      "BatchNorm/beta:0\r\n",
      "BatchNorm/moving_mean:0\r\n",
      "BatchNorm/moving_variance:0\r\n",
      "Conv2d/kernel:0\r\n",
      "BatchNorm/gamma:0\r\n",
      "BatchNorm/beta:0\r\n",
      "BatchNorm/moving_mean:0\r\n",
      "BatchNorm/moving_variance:0\r\n",
      "```\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:keras\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Keras model conflicting with multiprocessing\n",
      "issue body -  This is not necessarily a bug with tensorflow / keras but it may be classified as incompatibility issue. I'm getting a weird error when I'm trying to run 2 class methods concurrently in a third method. After eliminating large chunks of code, one at a time, I was surprised to find out that having keras model as a class attribute in the example, leads to the error.\r\n",
      "\r\n",
      "**Things to note:** \r\n",
      "\r\n",
      " - I must have a model as a class attribute, I cannot change that.\r\n",
      " - I need both tasks to run concurrently and I cannot get these 2 tasks out of the class because they interact with other class members\r\n",
      " - I get the same error using `multiprocessing.Process()`, so that also will not fix the problem.\r\n",
      "___\r\n",
      "\r\n",
      "    from concurrent.futures import ProcessPoolExecutor, as_completed\r\n",
      "    \r\n",
      "    from tensorflow.keras.models import Model\r\n",
      "    \r\n",
      "    \r\n",
      "    class Example:\r\n",
      "        def __init__(self):\r\n",
      "            self.model = Model()\r\n",
      "            # comment out the line above and uncomment the line below, the error is gone\r\n",
      "            # self.model = None\r\n",
      "    \r\n",
      "        def task1(self):\r\n",
      "            pass\r\n",
      "    \r\n",
      "        def task2(self):\r\n",
      "            pass\r\n",
      "    \r\n",
      "        def process(\r\n",
      "            self,\r\n",
      "        ):\r\n",
      "            with ProcessPoolExecutor(2) as executor:\r\n",
      "                future_items = [\r\n",
      "                    executor.submit(self.task1),\r\n",
      "                    executor.submit(self.task2),\r\n",
      "                ]\r\n",
      "                results = [\r\n",
      "                    future_item.result() for future_item in as_completed(future_items)\r\n",
      "                ]\r\n",
      "                print(results)\r\n",
      "    \r\n",
      "    \r\n",
      "    if __name__ == '__main__':\r\n",
      "        ex = Example()\r\n",
      "        ex.process()\r\n",
      "\r\n",
      "**Results in:**\r\n",
      "\r\n",
      "    2021-01-10 08:10:04.315386: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n",
      "    2021-01-10 08:10:04.315897: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\n",
      "    To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n",
      "    concurrent.futures.process._RemoteTraceback: \r\n",
      "    \"\"\"\r\n",
      "    Traceback (most recent call last):\r\n",
      "      File \"/usr/local/Cellar/python@3.8/3.8.7/Frameworks/Python.framework/Versions/3.8/lib/python3.8/multiprocessing/queues.py\", line 239, in _feed\r\n",
      "        obj = _ForkingPickler.dumps(obj)\r\n",
      "      File \"/usr/local/Cellar/python@3.8/3.8.7/Frameworks/Python.framework/Versions/3.8/lib/python3.8/multiprocessing/reduction.py\", line 51, in dumps\r\n",
      "        cls(buf, protocol).dump(obj)\r\n",
      "    TypeError: cannot pickle 'weakref' object\r\n",
      "    \"\"\"\r\n",
      "    \r\n",
      "    The above exception was the direct cause of the following exception:\r\n",
      "    \r\n",
      "    Traceback (most recent call last):\r\n",
      "      File \"/Users/emadboctor/Desktop/code/drl-algos/scratch.py\", line 34, in <module>\r\n",
      "        ex.process()\r\n",
      "      File \"/Users/emadboctor/Desktop/code/drl-algos/scratch.py\", line 26, in process\r\n",
      "        results = [\r\n",
      "      File \"/Users/emadboctor/Desktop/code/drl-algos/scratch.py\", line 27, in <listcomp>\r\n",
      "        future_item.result() for future_item in as_completed(future_items)\r\n",
      "      File \"/usr/local/Cellar/python@3.8/3.8.7/Frameworks/Python.framework/Versions/3.8/lib/python3.8/concurrent/futures/_base.py\", line 432, in result\r\n",
      "        return self.__get_result()\r\n",
      "      File \"/usr/local/Cellar/python@3.8/3.8.7/Frameworks/Python.framework/Versions/3.8/lib/python3.8/concurrent/futures/_base.py\", line 388, in __get_result\r\n",
      "        raise self._exception\r\n",
      "      File \"/usr/local/Cellar/python@3.8/3.8.7/Frameworks/Python.framework/Versions/3.8/lib/python3.8/multiprocessing/queues.py\", line 239, in _feed\r\n",
      "        obj = _ForkingPickler.dumps(obj)\r\n",
      "      File \"/usr/local/Cellar/python@3.8/3.8.7/Frameworks/Python.framework/Versions/3.8/lib/python3.8/multiprocessing/reduction.py\", line 51, in dumps\r\n",
      "        cls(buf, protocol).dump(obj)\r\n",
      "    TypeError: cannot pickle 'weakref' object\r\n",
      "\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:keras\n",
      "stat:awaiting tensorflower\n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  Add tf.qint32 support for tf.zeros\n",
      "issue body -  This PR is part of #26069 where `tf.zeros` does not support\r\n",
      "basic type of `tf.qint32` while all other qtypes have been supported\r\n",
      "(tf.{qint8|qint16|quint8|quint16} supported).\r\n",
      "\r\n",
      "Signed-off-by: Yong Tang <yong.tang.github@outlook.com>\n",
      "issue labels - \n",
      "cla: yes\n",
      "prtype:bugfix\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  micro: prepare to port operator FLOOR_MOD kernel from lite with test\n",
      "issue body -  Implement skeleton (non-working) code for operator and test.\r\n",
      "Header files changed.\r\n",
      "Namespaces changed.\r\n",
      "Some original code deleted.\r\n",
      "Some original code modified.\r\n",
      "\r\n",
      "This represents PR step 4 of the work to port operator FLOOR_MOD as tracked in Issue #45749\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:lite\n",
      "comp:micro\n",
      "prtype:bugfix\n",
      "ready to pull\n",
      "size:L\n",
      "\n",
      "\n",
      "issue title -  micro: copy operator FLOOR_MOD kernel from lite\n",
      "issue body -  This is a copy with minimal modification of the kernel and test for\r\n",
      "operator FLOOR_MOD from tensorflow/lite/kernels.\r\n",
      "Adaptations to micro and addition to the micro build to follow.\r\n",
      "\r\n",
      "PR step 3 for issue #45749\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:lite\n",
      "comp:micro\n",
      "prtype:bugfix\n",
      "ready to pull\n",
      "size:L\n",
      "\n",
      "\n",
      "issue title -  Dynamic library  libcudnn.so.8 and libcusolver.so.10 dont load\n",
      "issue body -  <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 20.04):\r\n",
      "- TensorFlow installed from: pip in anaconda env\r\n",
      "- TensorFlow version: 2.4.0\r\n",
      "- Python version: 3.8.5\r\n",
      "- CUDA/cuDNN version:  CUDA:11.0 and cuDNN 8.05 \r\n",
      "- GPU model and memory: Dual 1080 ti 12 gb\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "Fresh build with newly installed Ubuntu. Had to downgrade the existing nvidia driver  and cuda   from 4.60 > 4.50 and cuda from 11.2 to 11.1. since cuDNN didnt list CUDA 11.2  as compatible and i ran into more issues. Running a simple model (make blobs classification) returns these errors when compiling the model :+1: \r\n",
      "\r\n",
      "libcudnn.so.8 and libcusolver.so.10 dont load (code below run in jupyter)\r\n",
      "\r\n",
      "```\r\n",
      "Kernel started: 074c56de-e52a-48bd-b7bb-20ab27829401, name: python3\r\n",
      "2021-01-09 12:25:03.574107: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "2021-01-09 12:25:08.195326: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n",
      "2021-01-09 12:25:08.195728: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n",
      "2021-01-09 12:25:08.220794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n",
      "2021-01-09 12:25:08.221463: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\n",
      "pciBusID: 0000:23:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\n",
      "coreClock: 1.6325GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n",
      "2021-01-09 12:25:08.221502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n",
      "2021-01-09 12:25:08.222043: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: \r\n",
      "pciBusID: 0000:2d:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\n",
      "coreClock: 1.6325GHz coreCount: 28 deviceMemorySize: 10.91GiB deviceMemoryBandwidth: 451.17GiB/s\r\n",
      "2021-01-09 12:25:08.222053: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "2021-01-09 12:25:08.222983: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n",
      "2021-01-09 12:25:08.223005: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n",
      "2021-01-09 12:25:08.223355: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n",
      "2021-01-09 12:25:08.223461: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n",
      "2021-01-09 12:25:08.223514: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] _Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory_\r\n",
      "2021-01-09 12:25:08.223746: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n",
      "2021-01-09 12:25:08.223775: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] _Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory_\r\n",
      "2021-01-09 12:25:08.223780: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\n",
      "Skipping registering GPU devices...\r\n",
      "2021-01-09 12:25:08.224169: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n",
      "2021-01-09 12:25:08.224183: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n",
      "2021-01-09 12:25:08.224188: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]    \r\n",
      "```\r\n",
      "\r\n",
      "**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n",
      "I changed the existing drier from 4.60 to 4.50 by the software page. prior to this  cuda 11.2 was installed but was not in /usr/local/\r\n",
      "I installed cuda 11.0 from .deb after purging and autoremoving cuda. \r\n",
      "\r\n",
      "I've tried adding the the path following the nvidia documenation replacing the 11.2 with 11.1\r\n",
      "```export PATH=/usr/local/cuda-11.2/bin${PATH:+:${PATH}}```\r\n",
      "but it hasnt helped\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:gpu\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\n",
      "issue body -  \r\n",
      "**System information**\r\n",
      "- Windows 10 Pro 2004\r\n",
      "- TensorFlow  installed from (pip):\r\n",
      "- TensorFlow version (2.4.0):\r\n",
      "- Python version 3.8.1:\r\n",
      "- Installed in anaconda venv\r\n",
      "- CUDA version 11.0, cuDNN version 8.0.4\r\n",
      "- GPU model gtx 1660ti, 6Gb vram:\r\n",
      "\r\n",
      "\r\n",
      "**The code**\r\n",
      "`import os\r\n",
      "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\r\n",
      "import tensorflow as tf\r\n",
      "import tensorflow_datasets as tfds\r\n",
      "\r\n",
      "import tensorflow_addons as tfa\r\n",
      "import math\r\n",
      "from tensorflow.keras import layers\r\n",
      "from tensorflow import keras\r\n",
      "\r\n",
      "print(tf.__version__)\r\n",
      "if tf.test.gpu_device_name():\r\n",
      "    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))\r\n",
      "else:\r\n",
      "    print(\"Please install GPU version of TF\")\r\n",
      "physical_devices = tf.config.list_physical_devices('GPU')\r\n",
      "print(physical_devices)\r\n",
      "tf.config.experimental.set_memory_growth(physical_devices[0], True)\r\n",
      "\r\n",
      "(ds_train, ds_test), ds_info = tfds.load('mnist',\r\n",
      "                                         split=['train', 'test'],\r\n",
      "                                         shuffle_files=False,\r\n",
      "                                         as_supervised=True,\r\n",
      "                                         with_info=True,)\r\n",
      "\r\n",
      "@tf.function\r\n",
      "def normalize_img(image, label):\r\n",
      "    return tf.cast(image, tf.float32) / 255.0, label\r\n",
      "\r\n",
      "@tf.function\r\n",
      "def rotate(img, max_degrees=25):\r\n",
      "    degrees = tf.random.uniform([], -max_degrees, max_degrees, dtype=tf.float32)\r\n",
      "    img = tfa.image.rotate(img, degrees*math.pi / 180, interpolation='BILINEAR')\r\n",
      "    return img\r\n",
      "\r\n",
      "#\r\n",
      "@tf.function\r\n",
      "def augment(image, label):\r\n",
      "    image = tf.image.resize(image, size=[28, 28])\r\n",
      "    image = rotate(image)\r\n",
      "\r\n",
      "    # coloring of image\r\n",
      "    image = tf.image.random_brightness(image, max_delta=0.2)\r\n",
      "    image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\r\n",
      "    return image, label\r\n",
      "\r\n",
      "AUTOTUNE = tf.data.experimental.AUTOTUNE\r\n",
      "BATCH_SIZE = 32\r\n",
      "\r\n",
      "ds_train = ds_train.cache()\r\n",
      "ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\r\n",
      "ds_train = ds_train.map(normalize_img, num_parallel_calls=AUTOTUNE)\r\n",
      "ds_train = ds_train.map(augment, num_parallel_calls=AUTOTUNE)\r\n",
      "ds_train = ds_train.batch(BATCH_SIZE)\r\n",
      "ds_train = ds_train.prefetch(AUTOTUNE)\r\n",
      "\r\n",
      "ds_test = ds_test.map(normalize_img, num_parallel_calls=AUTOTUNE)\r\n",
      "ds_test = ds_test.batch(BATCH_SIZE)\r\n",
      "ds_test = ds_test.prefetch(AUTOTUNE)\r\n",
      "\r\n",
      "def my_model():\r\n",
      "    inputs = keras.Input(shape=(28, 28, 1))\r\n",
      "    x = layers.Conv2D(32, 3)(inputs)\r\n",
      "    x = layers.BatchNormalization()(x)\r\n",
      "    x = keras.activations.relu(x)\r\n",
      "    x = layers.MaxPooling2D()(x)\r\n",
      "    x = layers.Conv2D(64, 3)(x)\r\n",
      "    x = layers.BatchNormalization()(x)\r\n",
      "    x = keras.activations.relu(x)\r\n",
      "    x = layers.MaxPooling2D()(x)\r\n",
      "    x = layers.Conv2D(128, 3)(x)\r\n",
      "    x = layers.BatchNormalization()(x)\r\n",
      "    x = keras.activations.relu(x)\r\n",
      "    x = layers.Flatten()(x)\r\n",
      "    x = layers.Dense(64, activation='relu')(x)\r\n",
      "    outputs = layers.Dense(10, activation='softmax')(x)\r\n",
      "    return keras.Model(inputs=inputs, outputs=outputs)\r\n",
      "\r\n",
      "model = my_model()\r\n",
      "# compile model\r\n",
      "model.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\r\n",
      "              optimizer=keras.optimizers.Adam(lr=1e-4),\r\n",
      "              metrics=['accuracy'])\r\n",
      "# model.fit\r\n",
      "model.fit(ds_train, epochs=30, verbose=2)\r\n",
      "# model.evaluate\r\n",
      "model.evaluate(ds_test)\r\n",
      "model.save('model')\r\n",
      "\r\n",
      "`\r\n",
      "**The error it produced**\r\n",
      "> 2.4.0\r\n",
      "Default GPU Device:/device:GPU:0\r\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\r\n",
      "Epoch 1/30\r\n",
      "2021-01-09 11:59:09.630855: E tensorflow/stream_executor/cuda/cuda_dnn.cc:336] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\r\n",
      "2021-01-09 11:59:09.631034: E tensorflow/stream_executor/cuda/cuda_dnn.cc:340] Error retrieving driver version: Unimplemented: kernel reported driver version not implemented on Windows\r\n",
      "2021-01-09 11:59:09.632741: E tensorflow/stream_executor/cuda/cuda_dnn.cc:336] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\r\n",
      "2021-01-09 11:59:09.632910: E tensorflow/stream_executor/cuda/cuda_dnn.cc:340] Error retrieving driver version: Unimplemented: kernel reported driver version not implemented on Windows\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"D:/Projects/pythonProject/main.py\", line 85, in <module>\r\n",
      "    model.fit(ds_train, epochs=30, verbose=2)\r\n",
      "  File \"C:\\Users\\.\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1100, in fit\r\n",
      "    tmp_logs = self.train_function(iterator)\r\n",
      "  File \"C:\\Users\\.\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 828, in __call__\r\n",
      "    result = self._call(*args, **kwds)\r\n",
      "  File \"C:\\Users\\.\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 888, in _call\r\n",
      "    return self._stateless_fn(*args, **kwds)\r\n",
      "  File \"C:\\Users\\.\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2942, in __call__\r\n",
      "    return graph_function._call_flat(\r\n",
      "  File \"C:\\Users\\.\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1918, in _call_flat\r\n",
      "    return self._build_call_outputs(self._inference_function.call(\r\n",
      "  File \"C:\\Users\\.\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\", line 555, in call\r\n",
      "    outputs = execute.execute(\r\n",
      "  File \"C:\\Users\\.\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 59, in quick_execute\r\n",
      "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n",
      "tensorflow.python.framework.errors_impl.UnknownError:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n",
      "\t [[node model/conv2d/Conv2D (defined at D:/Projects/pythonProject/main.py:85) ]] [Op:__inference_train_function_1489]\r\n",
      "Function call stack:\r\n",
      "train_function\r\n",
      "\r\n",
      "\r\n",
      "Verified versions of drivers, cuda toolkit and cudnn several times, and reinstalled them also several times.\r\n",
      "Any advice or suggestion will be appriciated.\r\n",
      "Thx\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:gpu\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Error at importing! \n",
      "issue body -  ### My 'tensorflow-GPU' was working fine yesterday but when I import it now it says-\r\n",
      " `Traceback (most recent call last):\r\n",
      "  File \"<stdin>\", line 1, in <module>\r\n",
      "  File \"C:\\Users\\Sahil Singh\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n",
      "    from tensorflow.python.tools import module_util as _module_util\r\n",
      "  File \"C:\\Users\\Sahil Singh\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 40, in <module>\r\n",
      "    from tensorflow.python.eager import context\r\n",
      "  File \"C:\\Users\\Sahil Singh\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\", line 32, in <module>\r\n",
      "    from tensorflow.core.framework import function_pb2\r\n",
      "  File \"C:\\Users\\Sahil Singh\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\core\\framework\\function_pb2.py\", line 7, in <module>\r\n",
      "    from google.protobuf import descriptor as _descriptor\r\n",
      "  File \"C:\\Users\\Sahil Singh\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\google\\protobuf\\__init__.py\", line 37, in <module>\r\n",
      "    __import__('pkg_resources').declare_namespace(__name__)\r\n",
      "  File \"C:\\Users\\Sahil Singh\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pkg_resources.py\", line 1479, in <module>\r\n",
      "    register_loader_type(importlib_bootstrap.SourceFileLoader, DefaultProvider)'`\r\n",
      "`AttributeError: module 'importlib._bootstrap' has no attribute 'SourceFileLoader`\n",
      "issue labels - \n",
      "comp:gpu\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  I can't find the tensorflow-lite model at the specified location(new->other->tensorflow-lite model) What should i do?\n",
      "issue body -  Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\n",
      "policy, we only address code/doc bugs, performance issues, feature requests, and\r\n",
      "build/installation issues on GitHub.\r\n",
      "\r\n",
      "The TensorFlow docs are open source! To get involved, read the documentation\r\n",
      "contributor guide: https://www.tensorflow.org/community/contribute/docs\r\n",
      "\r\n",
      "## URL(s) with the issue:\r\n",
      "\r\n",
      "Please provide a link to the documentation entry, for example:\r\n",
      "https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod\r\n",
      "\r\n",
      "## Description of issue (what needs changing):\r\n",
      "\r\n",
      "### Clear description\r\n",
      "\r\n",
      "For example, why should someone use this method? How is it useful?\r\n",
      "\r\n",
      "### Correct links\r\n",
      "\r\n",
      "Is the link to the source code correct?\r\n",
      "\r\n",
      "### Parameters defined\r\n",
      "\r\n",
      "Are all parameters defined and formatted correctly?\r\n",
      "\r\n",
      "### Returns defined\r\n",
      "\r\n",
      "Are return values defined?\r\n",
      "\r\n",
      "### Raises listed and defined\r\n",
      "\r\n",
      "Are the errors defined? For example,\r\n",
      "https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n",
      "\r\n",
      "### Usage example\r\n",
      "\r\n",
      "Is there a usage example?\r\n",
      "\r\n",
      "See the API guide: https://www.tensorflow.org/community/contribute/docs_ref\r\n",
      "on how to write testable usage examples.\r\n",
      "\r\n",
      "### Request visuals, if applicable\r\n",
      "\r\n",
      "Are there currently visuals? If not, will it clarify the content?\r\n",
      "\r\n",
      "### Submit a pull request?\r\n",
      "\r\n",
      "Are you planning to also submit a pull request to fix the issue? See the docs\r\n",
      "contributor guide: https://www.tensorflow.org/community/contribute/docs,\r\n",
      "docs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\n",
      "docs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n",
      "\n",
      "issue labels - \n",
      "comp:lite\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  ModuleNotFoundError: No module named 'tensorflow.models'\n",
      "issue body -  hi,dear all friends\r\n",
      "when I run the [cifar rp](https://github.com/tensorflow/tensorflow/blob/r0.7/tensorflow/models/image/cifar10/cifar10_train.py),\r\n",
      "I find the problem down\r\n",
      "could you please help me ?\r\n",
      "\n",
      "issue labels - \n",
      "stat:awaiting response\n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  Merge pull request #1 from tensorflow/master\n",
      "issue body -  Merge from master\n",
      "issue labels - \n",
      "cla: no\n",
      "invalid\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Include C env API into part of the libtensorflow_framework.so\n",
      "issue body -  `env` only uses some transitive dependencies of `c_api` ( It doesn't even call any function in `c_api.h` ) and `//tensorflow/core:lib`. So I replace these `deps` by its actual `deps`.\r\n",
      "\r\n",
      "It is required by `gcs_filesystem` plugin.\r\n",
      "\r\n",
      "Part of https://github.com/tensorflow/io/issues/1183\r\n",
      "\r\n",
      "/cc: @yongtang, @mihaimaruseac \n",
      "issue labels - \n",
      "cla: yes\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  Tf32 fixes\n",
      "issue body -  Currently, enable_tensor_float_32_execution(False) does not fully disable TensorFloat32 evaluation in RNNs. This PR fixes this and also disabled TF32 execution for a few additional tests. \r\n",
      "\r\n",
      "Attn: @reedwm \n",
      "issue labels - \n",
      "awaiting review\n",
      "cla: yes\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  TF-TRT Test ConvertTopK in dynamic shape mode\n",
      "issue body -  This PR adds explicit batch and dynamic shape unit tests for TopK op converter.\r\n",
      "\r\n",
      "Additionally the `RunValidationAndConversion` test routine is extended to handle multiple output tensors. \r\n",
      "\r\n",
      "Tagging @bixia1 for review and @DEKHTIARJonathan for visibility.\r\n",
      "\r\n",
      "Tracker: #45481\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:gpu:tensorrt\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  Allow more of the internal checks to have open-source counterparts.\n",
      "issue body -   * gtest includes will be flagged as errors\r\n",
      " * use of the error reporter without the wrapper macros will be an error (https://github.com/tensorflow/tensorflow/pull/45457#discussion_r547434839)\r\n",
      " * assert can not be used (static_assert is ok).\r\n",
      "\r\n",
      "Manually tested by adding the disallowed strings to the code and confirmed that an error is raised.\r\n",
      "\r\n",
      "Fixes #46297\r\n",
      "Fixes http://b/175657165\r\n",
      "\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  gtest headers result in conflicting clang-format requirements\n",
      "issue body -  @tensorflow/micro\r\n",
      "\r\n",
      "While porting OPs from lite to micro, one of the intermediate steps results in test code copied over from lite that has gtest headers. While this code is not (and can not) be compiled for TFLM it still trips up the formatting checks as described in https://github.com/tensorflow/tensorflow/pull/46159#discussion_r553568396.\r\n",
      "\r\n",
      "Deleting these includes (the workaround in https://github.com/tensorflow/tensorflow/pull/46159#discussion_r553568396) works just fine and it would be nice to give pull request authors this feedback directly via the TF Micro CI instead of waiting for the change to be imported internally before the error is detected.\r\n",
      "\r\n",
      "The overarching goal is to get to a place where if a pull request passes the external CI, it also passes the internal CI (unless the code that a PR is breaking is internal-only).\n",
      "issue labels - \n",
      "comp:micro\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Linear aglebra C++ API\n",
      "issue body -  <em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n",
      "\r\n",
      "\r\n",
      "**System information**\r\n",
      "- TensorFlow version (you are using): r2.1 c++ API\r\n",
      "- Are you willing to contribute it (Yes/No): Yes\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**Describe the feature and the current behavior/state.**\r\n",
      "\r\n",
      "Is there any plan to extend the current c++ API to include ops such as those from tf.linalg in the python API? Atm I only see a Inv op but computes the inverse element-wise. My solution at the moment would be to write a new op that uses eigen to perform the different operations but it seems that eigen is able to run in a CUDA kernel yet. \r\n",
      "\r\n",
      "**Will this change the current api? How?**\r\n",
      "\r\n",
      "Adding a new set of OPs.\r\n",
      "\r\n",
      "**Who will benefit with this feature?**\r\n",
      "\r\n",
      "It would be very handy for graphs that do not only use neural networks. Use case: I'm currently implementing a sampled based controller for a robot that will run on a GPU. Hence I don't want to run the python interpreter as we will collect a lot of data from sensors and need to process everything on the fly. The controller performs a bunch of linear algebra calculation and run a NN in the middle of it, all of the operations being performed once for every sample. The high number of samples requires a GPU to be a real time controller. \n",
      "issue labels - \n",
      "comp:apis\n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  Input y of 'Greater' op has type float32 that does not match type int64 of x\n",
      "issue body -  I have this code below:\r\n",
      "```\r\n",
      "tf.contrib.metrics.precision_at_recall(tf.cast(tf.argmax(tf.nn.softmax(query_preds[i], dim=1), axis=1), tf.float32), tf.argmax(query_y, axis=1), target_recall=0.3)\r\n",
      "```\r\n",
      "\r\n",
      "It keeps giving me this error. I tried casting as you can see but still not working.\n",
      "issue labels - \n",
      "contrib\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Add int8 and int16x8 support for GATHER_ND operator\n",
      "issue body -  \n",
      "issue labels - \n",
      "cla: yes\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  How to convert TF1.12 checkpoints to saved/keras model format.\n",
      "issue body -  I have a TTS model checkpoints trained on TFv1.12. For inferencing, I have used \r\n",
      "`self.session = tf.Session(config=config)\r\n",
      "self.session.run(tf.global_variables_initializer())\r\n",
      "saver = tf.train.Saver()\r\n",
      "saver.restore(self.session, checkpoint_path)\r\n",
      "`\r\n",
      "For its TFlite conversion, saved/keras model is required. How do I convert this model into TFLite.\n",
      "issue labels - \n",
      "TF 1.12\n",
      "comp:lite\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  TF2.1 .h5 file to .pb\n",
      "issue body -  ```\r\n",
      "from tensorflow.python.keras.models import load_model\r\n",
      "import tensorflow as tf\r\n",
      "\r\n",
      "\r\n",
      "H5Path = \"xxx.h5\"\r\n",
      "YoloV5s_model = load_model(H5Path, compile=False)\r\n",
      "YoloV5s_model.summary()\r\n",
      "YoloV5s_model.save(\"a.pb\")\r\n",
      "```\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "2021-01-08 20:36:37.248006: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"TFLoadH5AndInfer.py\", line 18, in <module>\r\n",
      "    YoloV5s_model.save(\"Yolov5s_1.pb\")\r\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\network.py\", line 1008, in save\r\n",
      "    signatures, options)\r\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\save.py\", line 115, in save_model\r\n",
      "    signatures, options)\r\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\saved_model\\save.py\", line 78, in save\r\n",
      "    save_lib.save(model, filepath, signatures, options)\r\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\saved_model\\save.py\", line 899, in save\r\n",
      "    _ = _SaveableView(checkpoint_graph_view)\r\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\saved_model\\save.py\", line 165, in __init__\r\n",
      "    self.checkpoint_view.objects_ids_and_slot_variables())\r\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\training\\tracking\\graph_view.py\", line 415, in objects_ids_and_slot_variables\r\n",
      "    trackable_objects, path_to_root = self._breadth_first_traversal()\r\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\training\\tracking\\graph_view.py\", line 199, in _breadth_first_traversal\r\n",
      "    for name, dependency in self.list_dependencies(current_trackable):\r\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\saved_model\\save.py\", line 113, in list_dependencies\r\n",
      "    for name, dep in super(_AugmentedGraphView, self).list_dependencies(obj):\r\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\training\\tracking\\graph_view.py\", line 159, in list_dependencies\r\n",
      "    return obj._checkpoint_dependencies\r\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\training\\tracking\\data_structures.py\", line 744, in __getattribute__\r\n",
      "    return object.__getattribute__(self, name)\r\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\training\\tracking\\data_structures.py\", line 786, in _checkpoint_dependencies\r\n",
      "    \"ignored.\" % (self,))\r\n",
      "ValueError: Unable to save the object {1: ListWrapper([0, 0, 0, 0]), 2: ListWrapper([0, 0, 0, 0]), 3: ListWrapper([1, 2, 2, 1])} (a dictionary wrapper constructed automatically on attribute assignment). The wrapped dictionary contains a non-string key which maps to a trackable object or mutable data structure.\r\n",
      "\r\n",
      "If you don't need this dictionary checkpointed, wrap it in a tf.contrib.checkpoint.NoDependency object; it will be automatically un-wrapped and subsequently ignored.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "comp:keras\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  TFLu: Update downloaded GCC version to 10\n",
      "issue body -  * Add new GCC download script.\r\n",
      "* Enable GCC 10 for all targets.\r\n",
      "* Update GCC flags for cortex_m_generic target.\r\n",
      "\r\n",
      "This intends to fix: https://github.com/tensorflow/tensorflow/issues/43725 so that M55 can also be compiled with GCC.\r\n",
      "Only targets, cortex_m_generic and stm32f4 has been tested.\r\n",
      "It has only been tested on Linux.\r\n",
      "\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  Cannot create tf.constant inside tf.function with integer tensor.\n",
      "issue body -  <em>Please make sure that this is a bug. As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:bug_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **yes**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 18.04**\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **N/A**\r\n",
      "- TensorFlow installed from (source or binary): **binary (Google Colab)**\r\n",
      "- TensorFlow version (use command below): **v2.4.0-0-g582c8d236cb 2.4.0**\r\n",
      "- Python version: **Python 3.6.9**\r\n",
      "- Bazel version (if compiling from source): **N/A**\r\n",
      "- GCC/Compiler version (if compiling from source): **N/A**\r\n",
      "- CUDA/cuDNN version: **N/A**\r\n",
      "- GPU model and memory: **N/A**\r\n",
      "\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "It is impossible to create a `tf.constant` inside a function wrapped by `tf.function` if the argument to `tf.constant` is an integer Tensor.\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "It is expected that such operations do not raise an error. For example in case of slightly more advanced postprocessing.\r\n",
      "Unless this behaviour is desired, this issue can be closed. I would however, greatly appreciate an explanation.\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "The following snippet will work with eager execution:\r\n",
      "```python\r\n",
      "def function():\r\n",
      "    a = int(tf.random.normal(shape=()))\r\n",
      "    tf.print(a)\r\n",
      "\r\n",
      "    constant = tf.constant([a])\r\n",
      "    tf.print(constant)\r\n",
      "```\r\n",
      "Will output:\r\n",
      "```\r\n",
      "-1\r\n",
      "[-1]\r\n",
      "```\r\n",
      "\r\n",
      "However, after wrapping in `tf.function` an error is raised:\r\n",
      "```python\r\n",
      "wrapped = tf.function(function)\r\n",
      "wrapped()\r\n",
      "```\r\n",
      "\r\n",
      "Raises:\r\n",
      "```\r\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)\r\n",
      "    975           except Exception as e:  # pylint:disable=broad-except\r\n",
      "    976             if hasattr(e, \"ag_error_metadata\"):\r\n",
      "--> 977               raise e.ag_error_metadata.to_exception(e)\r\n",
      "    978             else:\r\n",
      "    979               raise\r\n",
      "\r\n",
      "TypeError: in user code:\r\n",
      "\r\n",
      "    <ipython-input-97-0acfdad4a1be>:5 function  *\r\n",
      "        constant = tf.constant([a])\r\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py:265 constant  **\r\n",
      "        allow_broadcast=True)\r\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py:283 _constant_impl\r\n",
      "        allow_broadcast=allow_broadcast))\r\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:457 make_tensor_proto\r\n",
      "        _AssertCompatible(values, dtype)\r\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:334 _AssertCompatible\r\n",
      "        raise TypeError(\"Expected any non-tensor type, got a tensor instead.\")\r\n",
      "\r\n",
      "    TypeError: Expected any non-tensor type, got a tensor instead.\r\n",
      "```\r\n",
      "\r\n",
      "**Other info / logs**: **N/A**\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:autograph\n",
      "comp:ops\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  'tf.Size' op is neither a custom op nor a flex op\n",
      "issue body -  ### System information\r\n",
      "\r\n",
      "-   **Have I written custom code (as opposed to using a stock example script\r\n",
      "    provided in TensorFlow)**: No\r\n",
      "-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10.0.18363\r\n",
      "-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n",
      "    happens on a mobile device**: n/a\r\n",
      "-   **TensorFlow installed from (source or binary)**: binary\r\n",
      "-   **TensorFlow version (use command below)**: tensorflow-2.4.0\r\n",
      "-   **Python version**: 3.8.5\r\n",
      "-   **Bazel version (if compiling from source)**: n/a\r\n",
      "-   **GCC/Compiler version (if compiling from source)**: n/a\r\n",
      "-   **CUDA/cuDNN version**: CUDA 11.0 / CuDNN 8.0.5\r\n",
      "-   **GPU model and memory**: Quadro P2000 4GB\r\n",
      "-   **Exact command to reproduce**: Run \"Convert a SavedModel\" from https://www.tensorflow.org/lite/convert\r\n",
      "\r\n",
      "### Describe the problem\r\n",
      "For some unknown reason (to me) I am no longer able to convert a model to TFLite for usage on a Raspberry Pi. When I run the script mentioned in the link above, the following error appears:\r\n",
      "\r\n",
      "```\r\n",
      "loc(callsite(callsite(\"Postprocessor/BatchMultiClassNonMaxSuppression/MultiClassNonMaxSuppression/SortByField_1/Size@__inference___call___23519\" at \"StatefulPartitionedCall@__inference_signature_wrapper_25508\") at \"StatefulPartitionedCall\")): error: 'tf.Size' op is neither a custom op nor a flex op\r\n",
      "error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):\r\n",
      "        tf.Size {device = \"\"}\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"C:\\Users\\Reno\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py\", line 210, in toco_convert_protos\r\n",
      "    model_str = wrap_toco.wrapped_toco_convert(model_flags_str,\r\n",
      "  File \"C:\\Users\\Reno\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\lite\\python\\wrap_toco.py\", line 32, in wrapped_toco_convert\r\n",
      "    return _pywrap_toco_api.TocoConvert(\r\n",
      "Exception: <unknown>:0: error: loc(callsite(callsite(\"Postprocessor/BatchMultiClassNonMaxSuppression/MultiClassNonMaxSuppression/SortByField_1/Size@__inference___call___23519\" at \"StatefulPartitionedCall@__inference_signature_wrapper_25508\") at \"StatefulPartitionedCall\")): 'tf.Size' op is neither a custom op nor a flex op\r\n",
      "<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n",
      "<unknown>:0: error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):\r\n",
      "        tf.Size {device = \"\"}\r\n",
      " \r\n",
      " \r\n",
      "During handling of the above exception, another exception occurred:\r\n",
      " \r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"convert_to_tflite_v2.py\", line 7, in <module>\r\n",
      "    tflite_model = converter.convert()\r\n",
      "  File \"C:\\Users\\Reno\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\", line 739, in convert\r\n",
      "    result = _convert_saved_model(**converter_kwargs)\r\n",
      "  File \"C:\\Users\\Reno\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py\", line 632, in convert_saved_model\r\n",
      "    data = toco_convert_protos(\r\n",
      "  File \"C:\\Users\\Reno\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py\", line 216, in toco_convert_protos\r\n",
      "    raise ConverterError(str(e))\r\n",
      "tensorflow.lite.python.convert.ConverterError: <unknown>:0: error: loc(callsite(callsite(\"Postprocessor/BatchMultiClassNonMaxSuppression/MultiClassNonMaxSuppression/SortByField_1/Size@__inference___call___23519\" at \"StatefulPartitionedCall@__inference_signature_wrapper_25508\") at \"StatefulPartitionedCall\")): 'tf.Size' op is neither a custom op nor a flex op\r\n",
      "<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n",
      "<unknown>:0: error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):\r\n",
      "        tf.Size {device = \"\"}\r\n",
      "```\r\n",
      "\r\n",
      "I have read more issues regarding Flex Ops, unfortunately I haven't found a suitable solution as of now. I tried adding the following line:\r\n",
      "\r\n",
      "```\r\n",
      "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\n",
      "```\r\n",
      "\r\n",
      "When this line is present, the model converts very smoothly to a TFlite model, but then the TFLite model doesn't work anymore on a Raspberry Pi, with the following error: \r\n",
      "\r\n",
      "```\r\n",
      "Unexpected failure when preparing tensor allocations: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.\r\n",
      "```\r\n",
      "\r\n",
      "I never had this before for any other model. The model works perfectly fine with regular TensorFlow. It's just the TFlite part which causes issues for me.\r\n",
      "\r\n",
      "I am using a pre-trained model (SSD MobileNet V2 FPNLite 320x320) from the Detection Model Zoo at https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md\r\n",
      "\r\n",
      "### Source code / logs\r\n",
      "I started off by installing a fresh install of Tensorflow. See:\r\n",
      "\r\n",
      "```\r\n",
      "(base) C:\\Users\\Reno>conda activate tensorflow\r\n",
      " \r\n",
      "(tensorflow) C:\\Users\\Reno>pip cache purge\r\n",
      "ERROR: No matching packages\r\n",
      " \r\n",
      "(tensorflow) C:\\Users\\Reno>pip list\r\n",
      "Package      Version\r\n",
      "------------ -------------------\r\n",
      "certifi      2020.12.5\r\n",
      "pip          20.3.3\r\n",
      "setuptools   51.0.0.post20201207\r\n",
      "wheel        0.36.2\r\n",
      "wincertstore 0.2\r\n",
      " \r\n",
      "(tensorflow) C:\\Users\\Reno>pip install --upgrade tensorflow\r\n",
      "Collecting tensorflow\r\n",
      "  Downloading tensorflow-2.4.0-cp38-cp38-win_amd64.whl (370.7 MB)\r\n",
      "     |████████████████████████████████| 370.7 MB 21 kB/s\r\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\users\\reno\\.conda\\envs\\tensorflow\\lib\\site-packages (from tensorflow) (0.36.2)\r\n",
      "Collecting gast==0.3.3\r\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\r\n",
      "Collecting absl-py~=0.10\r\n",
      "  Downloading absl_py-0.11.0-py3-none-any.whl (127 kB)\r\n",
      "     |████████████████████████████████| 127 kB 2.2 MB/s\r\n",
      "Collecting astunparse~=1.6.3\r\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\r\n",
      "Collecting flatbuffers~=1.12.0\r\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\r\n",
      "Collecting google-pasta~=0.2\r\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\r\n",
      "     |████████████████████████████████| 57 kB 1.5 MB/s\r\n",
      "Collecting grpcio~=1.32.0\r\n",
      "  Downloading grpcio-1.32.0-cp38-cp38-win_amd64.whl (2.6 MB)\r\n",
      "     |████████████████████████████████| 2.6 MB 2.2 MB/s\r\n",
      "Collecting h5py~=2.10.0\r\n",
      "  Downloading h5py-2.10.0-cp38-cp38-win_amd64.whl (2.5 MB)\r\n",
      "     |████████████████████████████████| 2.5 MB 803 kB/s\r\n",
      "Collecting keras-preprocessing~=1.1.2\r\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\r\n",
      "     |████████████████████████████████| 42 kB 3.2 MB/s\r\n",
      "Collecting numpy~=1.19.2\r\n",
      "  Downloading numpy-1.19.5-cp38-cp38-win_amd64.whl (13.3 MB)\r\n",
      "     |████████████████████████████████| 13.3 MB 3.3 MB/s\r\n",
      "Collecting opt-einsum~=3.3.0\r\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\r\n",
      "     |████████████████████████████████| 65 kB 2.2 MB/s\r\n",
      "Collecting protobuf>=3.9.2\r\n",
      "  Downloading protobuf-3.14.0-py2.py3-none-any.whl (173 kB)\r\n",
      "     |████████████████████████████████| 173 kB 2.2 MB/s\r\n",
      "Collecting six~=1.15.0\r\n",
      "  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\r\n",
      "Collecting tensorboard~=2.4\r\n",
      "  Downloading tensorboard-2.4.0-py3-none-any.whl (10.6 MB)\r\n",
      "     |████████████████████████████████| 10.6 MB 2.2 MB/s\r\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\reno\\.conda\\envs\\tensorflow\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (51.0.0.post20201207)\r\n",
      "Collecting google-auth<2,>=1.6.3\r\n",
      "  Downloading google_auth-1.24.0-py2.py3-none-any.whl (114 kB)\r\n",
      "     |████████████████████████████████| 114 kB 2.2 MB/s\r\n",
      "Collecting cachetools<5.0,>=2.0.0\r\n",
      "  Downloading cachetools-4.2.0-py3-none-any.whl (12 kB)\r\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\r\n",
      "  Downloading google_auth_oauthlib-0.4.2-py2.py3-none-any.whl (18 kB)\r\n",
      "Collecting markdown>=2.6.8\r\n",
      "  Downloading Markdown-3.3.3-py3-none-any.whl (96 kB)\r\n",
      "     |████████████████████████████████| 96 kB 3.2 MB/s\r\n",
      "Collecting pyasn1-modules>=0.2.1\r\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\r\n",
      "     |████████████████████████████████| 155 kB 2.2 MB/s\r\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\r\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\r\n",
      "     |████████████████████████████████| 77 kB 2.6 MB/s\r\n",
      "Collecting requests<3,>=2.21.0\r\n",
      "  Downloading requests-2.25.1-py2.py3-none-any.whl (61 kB)\r\n",
      "     |████████████████████████████████| 61 kB 2.0 MB/s\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\reno\\.conda\\envs\\tensorflow\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2020.12.5)\r\n",
      "Collecting chardet<5,>=3.0.2\r\n",
      "  Downloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\r\n",
      "     |████████████████████████████████| 178 kB 2.2 MB/s\r\n",
      "Collecting idna<3,>=2.5\r\n",
      "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\r\n",
      "     |████████████████████████████████| 58 kB 2.0 MB/s\r\n",
      "Collecting requests-oauthlib>=0.7.0\r\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\r\n",
      "Collecting oauthlib>=3.0.0\r\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\r\n",
      "     |████████████████████████████████| 147 kB 2.2 MB/s\r\n",
      "Collecting rsa<5,>=3.1.4\r\n",
      "  Downloading rsa-4.6-py3-none-any.whl (47 kB)\r\n",
      "     |████████████████████████████████| 47 kB 3.2 MB/s\r\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\r\n",
      "  Downloading tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779 kB)\r\n",
      "     |████████████████████████████████| 779 kB 2.2 MB/s\r\n",
      "Collecting tensorflow-estimator<2.5.0,>=2.4.0rc0\r\n",
      "  Downloading tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462 kB)\r\n",
      "     |████████████████████████████████| 462 kB 2.2 MB/s\r\n",
      "Collecting termcolor~=1.1.0\r\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\r\n",
      "Collecting typing-extensions~=3.7.4\r\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\r\n",
      "Collecting urllib3<1.27,>=1.21.1\r\n",
      "  Downloading urllib3-1.26.2-py2.py3-none-any.whl (136 kB)\r\n",
      "     |████████████████████████████████| 136 kB 2.2 MB/s\r\n",
      "Collecting werkzeug>=0.11.15\r\n",
      "  Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\r\n",
      "     |████████████████████████████████| 298 kB 2.2 MB/s\r\n",
      "Collecting wrapt~=1.12.1\r\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\r\n",
      "Building wheels for collected packages: termcolor, wrapt\r\n",
      "  Building wheel for termcolor (setup.py) ... done\r\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=841391132c29825394e6b0ab2a1e79ffb1f6a1867cd2a214dc65cc3c4aa5b688\r\n",
      "  Stored in directory: c:\\users\\reno\\appdata\\local\\pip\\cache\\wheels\\a0\\16\\9c\\5473df82468f958445479c59e784896fa24f4a5fc024b0f501\r\n",
      "  Building wheel for wrapt (setup.py) ... done\r\n",
      "  Created wheel for wrapt: filename=wrapt-1.12.1-cp38-cp38-win_amd64.whl size=33672 sha256=7efbaa96078fc7ecd811c170d66cc642ab8fd8f88a41af40ba118c5b7c16765f\r\n",
      "  Stored in directory: c:\\users\\reno\\appdata\\local\\pip\\cache\\wheels\\5f\\fd\\9e\\b6cf5890494cb8ef0b5eaff72e5d55a70fb56316007d6dfe73\r\n",
      "Successfully built termcolor wrapt\r\n",
      "Installing collected packages: urllib3, pyasn1, idna, chardet, six, rsa, requests, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, google-auth, werkzeug, tensorboard-plugin-wit, protobuf, numpy, markdown, grpcio, google-auth-oauthlib, absl-py, wrapt, typing-extensions, termcolor, tensorflow-estimator, tensorboard, opt-einsum, keras-preprocessing, h5py, google-pasta, gast, flatbuffers, astunparse, tensorflow\r\n",
      "Successfully installed absl-py-0.11.0 astunparse-1.6.3 cachetools-4.2.0 chardet-4.0.0 flatbuffers-1.12 gast-0.3.3 google-auth-1.24.0 google-auth-oauthlib-0.4.2 google-pasta-0.2.0 grpcio-1.32.0 h5py-2.10.0 idna-2.10 keras-preprocessing-1.1.2 markdown-3.3.3 numpy-1.19.5 oauthlib-3.1.0 opt-einsum-3.3.0 protobuf-3.14.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.25.1 requests-oauthlib-1.3.0 rsa-4.6 six-1.15.0 tensorboard-2.4.0 tensorboard-plugin-wit-1.7.0 tensorflow-2.4.0 tensorflow-estimator-2.4.0 termcolor-1.1.0 typing-extensions-3.7.4.3 urllib3-1.26.2 werkzeug-1.0.1 wrapt-1.12.1\r\n",
      "\r\n",
      "(tensorflow) C:\\Users\\Reno>cd C:\\Users\\Reno\\Documents\\TensorFlow\\workspace\\training\r\n",
      "\r\n",
      "(tensorflow) C:\\Users\\Reno\\Documents\\TensorFlow\\workspace\\training>python convert_to_tflite_v2.py\r\n",
      "\r\n",
      "2021-01-08 11:30:11.595435: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n",
      "2021-01-08 11:30:19.711062: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n",
      "2021-01-08 11:30:19.715933: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n",
      "2021-01-08 11:30:20.065510: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:\r\n",
      "pciBusID: 0000:01:00.0 name: Quadro P2000 computeCapability: 6.1\r\n",
      "coreClock: 1.607GHz coreCount: 6 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 89.53GiB/s\r\n",
      "2021-01-08 11:30:20.072148: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n",
      "2021-01-08 11:30:20.083133: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n",
      "2021-01-08 11:30:20.088475: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n",
      "2021-01-08 11:30:20.099554: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n",
      "2021-01-08 11:30:20.105734: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n",
      "2021-01-08 11:30:20.118062: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n",
      "2021-01-08 11:30:20.124535: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n",
      "2021-01-08 11:30:20.130482: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n",
      "2021-01-08 11:30:20.133532: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n",
      "2021-01-08 11:30:20.136101: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\r\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n",
      "2021-01-08 11:30:20.144289: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:\r\n",
      "pciBusID: 0000:01:00.0 name: Quadro P2000 computeCapability: 6.1\r\n",
      "coreClock: 1.607GHz coreCount: 6 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 89.53GiB/s\r\n",
      "2021-01-08 11:30:20.152136: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n",
      "2021-01-08 11:30:20.155789: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n",
      "2021-01-08 11:30:20.159025: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n",
      "2021-01-08 11:30:20.162087: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n",
      "2021-01-08 11:30:20.165806: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n",
      "2021-01-08 11:30:20.171067: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n",
      "2021-01-08 11:30:20.174766: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n",
      "2021-01-08 11:30:20.180898: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n",
      "2021-01-08 11:30:20.185452: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n",
      "2021-01-08 11:30:20.767230: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n",
      "2021-01-08 11:30:20.771172: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0\r\n",
      "2021-01-08 11:30:20.773087: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N\r\n",
      "2021-01-08 11:30:20.775197: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2982 MB memory) -> physical GPU (device: 0, name: Quadro P2000, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n",
      "2021-01-08 11:30:20.783136: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n",
      "2021-01-08 11:31:15.040028: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:316] Ignored output_format.\r\n",
      "2021-01-08 11:31:15.046343: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:319] Ignored drop_control_dependency.\r\n",
      "2021-01-08 11:31:15.052724: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:325] Ignored change_concat_input_ranges.\r\n",
      "2021-01-08 11:31:15.059159: I tensorflow/cc/saved_model/reader.cc:32] Reading SavedModel from: C:/Users/Reno/Documents/TensorFlow/workspace/training_walnoot/exported-models/walnoot_model_lite/saved_model/\r\n",
      "2021-01-08 11:31:15.152959: I tensorflow/cc/saved_model/reader.cc:55] Reading meta graph with tags { serve }\r\n",
      "2021-01-08 11:31:15.155845: I tensorflow/cc/saved_model/reader.cc:93] Reading SavedModel debug info (if present) from: C:/Users/Reno/Documents/TensorFlow/workspace/training_walnoot/exported-models/walnoot_model_lite/saved_model/\r\n",
      "2021-01-08 11:31:15.163162: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n",
      "2021-01-08 11:31:15.167694: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]\r\n",
      "2021-01-08 11:31:15.170225: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n",
      "2021-01-08 11:31:15.517776: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:196] None of the MLIR optimization passes are enabled (registered 0 passes)\r\n",
      "2021-01-08 11:31:15.565923: I tensorflow/cc/saved_model/loader.cc:206] Restoring SavedModel bundle.\r\n",
      "2021-01-08 11:31:16.326616: I tensorflow/cc/saved_model/loader.cc:190] Running initialization op on SavedModel bundle at path: C:/Users/Reno/Documents/TensorFlow/workspace/training_walnoot/exported-models/walnoot_model_lite/saved_model/\r\n",
      "2021-01-08 11:31:16.703255: I tensorflow/cc/saved_model/loader.cc:277] SavedModel load for tags { serve }; Status: success: OK. Took 1644097 microseconds.\r\n",
      "2021-01-08 11:31:20.471768: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:194] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\r\n",
      "2021-01-08 11:31:21.438534: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:\r\n",
      "pciBusID: 0000:01:00.0 name: Quadro P2000 computeCapability: 6.1\r\n",
      "coreClock: 1.607GHz coreCount: 6 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 89.53GiB/s\r\n",
      "2021-01-08 11:31:21.445277: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n",
      "2021-01-08 11:31:21.452203: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n",
      "2021-01-08 11:31:21.456431: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n",
      "2021-01-08 11:31:21.461100: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n",
      "2021-01-08 11:31:21.467734: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n",
      "2021-01-08 11:31:21.473807: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n",
      "2021-01-08 11:31:21.479188: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n",
      "2021-01-08 11:31:21.485787: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n",
      "2021-01-08 11:31:21.491625: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n",
      "2021-01-08 11:31:21.495347: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n",
      "2021-01-08 11:31:21.499727: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0\r\n",
      "2021-01-08 11:31:21.502749: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N\r\n",
      "2021-01-08 11:31:21.505539: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2982 MB memory) -> physical GPU (device: 0, name: Quadro P2000, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n",
      "2021-01-08 11:31:21.513866: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n",
      "loc(callsite(callsite(\"Postprocessor/BatchMultiClassNonMaxSuppression/MultiClassNonMaxSuppression/SortByField_1/Size@__inference___call___23519\" at \"StatefulPartitionedCall@__inference_signature_wrapper_25508\") at \"StatefulPartitionedCall\")): error: 'tf.Size' op is neither a custom op nor a flex op\r\n",
      "error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):\r\n",
      "        tf.Size {device = \"\"}\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"C:\\Users\\Reno\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py\", line 210, in toco_convert_protos\r\n",
      "    model_str = wrap_toco.wrapped_toco_convert(model_flags_str,\r\n",
      "  File \"C:\\Users\\Reno\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\lite\\python\\wrap_toco.py\", line 32, in wrapped_toco_convert\r\n",
      "    return _pywrap_toco_api.TocoConvert(\r\n",
      "Exception: <unknown>:0: error: loc(callsite(callsite(\"Postprocessor/BatchMultiClassNonMaxSuppression/MultiClassNonMaxSuppression/SortByField_1/Size@__inference___call___23519\" at \"StatefulPartitionedCall@__inference_signature_wrapper_25508\") at \"StatefulPartitionedCall\")): 'tf.Size' op is neither a custom op nor a flex op\r\n",
      "<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n",
      "<unknown>:0: error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):\r\n",
      "        tf.Size {device = \"\"}\r\n",
      " \r\n",
      " \r\n",
      "During handling of the above exception, another exception occurred:\r\n",
      " \r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"convert_to_tflite_v2.py\", line 7, in <module>\r\n",
      "    tflite_model = converter.convert()\r\n",
      "  File \"C:\\Users\\Reno\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\", line 739, in convert\r\n",
      "    result = _convert_saved_model(**converter_kwargs)\r\n",
      "  File \"C:\\Users\\Reno\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py\", line 632, in convert_saved_model\r\n",
      "    data = toco_convert_protos(\r\n",
      "  File \"C:\\Users\\Reno\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py\", line 216, in toco_convert_protos\r\n",
      "    raise ConverterError(str(e))\r\n",
      "tensorflow.lite.python.convert.ConverterError: <unknown>:0: error: loc(callsite(callsite(\"Postprocessor/BatchMultiClassNonMaxSuppression/MultiClassNonMaxSuppression/SortByField_1/Size@__inference___call___23519\" at \"StatefulPartitionedCall@__inference_signature_wrapper_25508\") at \"StatefulPartitionedCall\")): 'tf.Size' op is neither a custom op nor a flex op\r\n",
      "<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n",
      "<unknown>:0: error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):\r\n",
      "        tf.Size {device = \"\"}\r\n",
      " \r\n",
      " \r\n",
      "(tensorflow) C:\\Users\\Reno\\Documents\\TensorFlow\\workspace\\training>\r\n",
      "```\r\n",
      "\r\n",
      "Why is this error appearing just now, what does it mean, and how can I convert a model to TFlite without any of these nasty errors and subsequently run it on a Raspberry Pi without any hassle.\r\n",
      "\r\n",
      "Thanks in advance.\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:lite\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  TFLite: GPU delegate (OpenGL) outputs errors when different context is active.\n",
      "issue body -  <em>Please make sure that this is a bug. As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:bug_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung Galaxy\r\n",
      "- TensorFlow installed from (source or binary): source\r\n",
      "- TensorFlow version (use command below): 2.3\r\n",
      "- Python version: -\r\n",
      "- Bazel version (if compiling from source): 2.0.0\r\n",
      "- GCC/Compiler version (if compiling from source): \r\n",
      "- CUDA/cuDNN version: -\r\n",
      "- GPU model and memory: -\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "I have a problem running TFLite's GPU delegate on a mobile device without OpenCL support (which means the delegate falls back to the OpenGL implementation obviously).\r\n",
      "```\r\n",
      "I/tflite: Initialized TensorFlow Lite runtime.\r\n",
      "I/tflite: Created TensorFlow Lite delegate for GPU.\r\n",
      "E/tflite: OpenCL library not loaded - dlopen failed: library \"libOpenCL-pixel.so\" not found\r\n",
      "E/tflite: Falling back to OpenGL\r\n",
      "E/libEGL: call to OpenGL ES API with no current context (logged once per thread)\r\n",
      "```\r\n",
      "Got error:\r\n",
      "```\r\n",
      "E/tflite: TfLiteGpuDelegate Invoke: ToTensorConverter: input data size does not match expected size.\r\n",
      "E/tflite: Node number 130 (TfLiteGpuDelegateV2) failed to invoke.\r\n",
      "E/tflite: TfLiteGpuDelegate Invoke: Write to buffer failed. Source data is larger than buffer.\r\n",
      "E/tflite: Node number 87 (TfLiteGpuDelegateV2) failed to invoke.\r\n",
      "```\r\n",
      "Problem is that my app (which is using Unity3D engine) is creating its own OpenGL context (I don't have access to rendering context, so I can not change it back to TFLite one after rendering) which is active and TFLite can not copy data to GPU.\r\n",
      "I think TFLite should activate its own context before making any operations on GPU when running OpenGL. \r\n",
      "**Describe the expected behavior**\r\n",
      "TFLite works when app changes current OpenGL context to different than the one used by TFLite.\n",
      "issue labels - \n",
      "TF 2.3\n",
      "comp:lite\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Implement support for ragged tensors in mean_squared_error loss function.\n",
      "issue body -  First cut at addressing Issue #45403.\r\n",
      "\r\n",
      "This PR adds support for RaggedTensors to \"mse\" loss. Tensors with shapes of (batch, ragged_dimension) or (batch, ragged_dimension, feature dims) are handled on a per batch basis.\r\n",
      "\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:keras\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found\n",
      "issue body -  System information：\r\n",
      "\r\n",
      "Have I written custom code (as opposed to using a stock example script provided in TensorFlow):no\r\n",
      "OS Platform and Distribution (e.g., Linux Ubuntu 16.04):ubuntu 18.04\r\n",
      "Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n",
      "TensorFlow installed from (source or binary): source\r\n",
      "TensorFlow version (use command below):1.15.0\r\n",
      "Python version:3.6\r\n",
      "CUDA/cuDNN version: 10.0\r\n",
      "GPU model and memory:7.4\r\n",
      "\r\n",
      "Describe the current behavior：\r\n",
      "I used tensorflow1.15 to train my code，after training 33 epochs raise \"tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found\"\r\n",
      "\r\n",
      "epoch  32 end time is : 2021-01-08 14:21:29.075227\r\n",
      "train files shuffled!\r\n",
      "is training ep :  33\r\n",
      "total train batch num: 100\r\n",
      "ep 33 i 0 psemce 0.0 bbvert -0.23279694 l2 0.060497742 ce 0.26028115 siou -0.5535758 bbscore 0.0038582273 pmask 0.6467816\r\n",
      "ep 33 i 0 test psem 0.0 bbvert 1.9851223 l2 0.059121773 ce 2.2534707 siou -0.3274702 bbscore 0.0048341155 pmask 3.4519947\r\n",
      "test pred bborder [[2 1 0]]\r\n",
      "ep 33 i 20 psemce 0.0 bbvert -0.44303733 l2 0.030050844 ce 0.16678412 siou -0.6398723 bbscore 0.0026201883 pmask 0.63400114\r\n",
      "ep 33 i 20 test psem 0.0 bbvert -0.4450612 l2 0.04898257 ce 0.23810822 siou -0.732152 bbscore 0.0016184862 pmask 0.47476012\r\n",
      "test pred bborder [[2 0 1]]\r\n",
      "ep 33 i 40 psemce 0.0 bbvert 0.43996847 l2 0.08725857 ce 0.8109299 siou -0.45822 bbscore 0.0034747643 pmask 0.9270937\r\n",
      "ep 33 i 40 test psem 0.0 bbvert -0.07955924 l2 0.040802542 ce 0.4581066 siou -0.5784684 bbscore 0.0087565 pmask 1.0110209\r\n",
      "test pred bborder [[2 0 1]]\r\n",
      "ep 33 i 60 psemce 0.0 bbvert 0.057684183 l2 0.071036406 ce 0.5709717 siou -0.58432394 bbscore 0.00046247765 pmask 0.48829234\r\n",
      "ep 33 i 60 test psem 0.0 bbvert -0.3817188 l2 0.03684793 ce 0.2770622 siou -0.69562894 bbscore 0.0019779946 pmask 0.54431504\r\n",
      "test pred bborder [[2 0 1]]\r\n",
      "ep 33 i 80 psemce 0.0 bbvert 0.038050413 l2 0.034902867 ce 0.6071266 siou -0.60397905 bbscore 0.015431552 pmask 0.8978894\r\n",
      "ep 33 i 80 test psem 0.0 bbvert 1.1076844 l2 0.07785928 ce 1.3761435 siou -0.34631833 bbscore 0.033992507 pmask 1.7343999\r\n",
      "test pred bborder [[2 0 1]]\r\n",
      "model saved in :  ./log/train_mod/model033.cptk\r\n",
      "epoch  33 end time is : 2021-01-08 14:21:44.245053\r\n",
      "train files shuffled!\r\n",
      "is training ep :  34\r\n",
      "total train batch num: 100\r\n",
      "ep 34 i 0 psemce 0.0 bbvert -0.41581324 l2 0.057975773 ce 0.28829214 siou -0.76208115 bbscore 0.0003172583 pmask 0.34254307\r\n",
      "ep 34 i 0 test psem 0.0 bbvert 1.7912706 l2 0.08668331 ce 2.017744 siou -0.31315675 bbscore 0.00576146 pmask 2.1253805\r\n",
      "test pred bborder [[0 2 1]]\r\n",
      "ep 34 i 20 psemce 0.0 bbvert -0.14073128 l2 0.034625944 ce 0.4937689 siou -0.6691261 bbscore 0.0047056335 pmask 0.7088615\r\n",
      "ep 34 i 20 test psem 0.0 bbvert 1.9534252 l2 0.0907397 ce 2.1705353 siou -0.3078499 bbscore 0.0019757028 pmask 2.682012\r\n",
      "test pred bborder [[0 2 1]]\r\n",
      "ep 34 i 40 psemce 0.0 bbvert 0.27091432 l2 0.053299602 ce 0.7194822 siou -0.5018675 bbscore 0.001409175 pmask 0.60204554\r\n",
      "ep 34 i 40 test psem 0.0 bbvert -0.28416353 l2 0.09286666 ce 0.19349718 siou -0.5705274 bbscore 0.003192804 pmask 0.21589296\r\n",
      "test pred bborder [[1 0 2]]\r\n",
      "2021-01-08 14:21:52.432564: W tensorflow/core/framework/op_kernel.cc:1639] Invalid argument: ValueError: matrix contains invalid numeric entries\r\n",
      "Traceback (most recent call last):\r\n",
      "\r\n",
      "  File \"/home/liu/anaconda3/envs/tf1.15/lib/python3.6/site-packages/tensorflow_core/python/ops/script_ops.py\", line 235, in __call__\r\n",
      "    ret = func(*args)\r\n",
      "\r\n",
      "  File \"/home/liu/disk1/Life/3DBoNetPoint818a(linux)/helper_net.py\", line 115, in assign_mappings_valid_only\r\n",
      "    row_ind, col_ind = linear_sum_assignment(valid_cost)\r\n",
      "\r\n",
      "  File \"/home/liu/anaconda3/envs/tf1.15/lib/python3.6/site-packages/scipy/optimize/_hungarian.py\", line 93, in linear_sum_assignment\r\n",
      "    raise ValueError(\"matrix contains invalid numeric entries\")\r\n",
      "\r\n",
      "ValueError: matrix contains invalid numeric entries\r\n",
      "\r\n",
      "\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/liu/anaconda3/envs/tf1.15/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\", line 1365, in _do_call\r\n",
      "    return fn(*args)\r\n",
      "  File \"/home/liu/anaconda3/envs/tf1.15/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\", line 1350, in _run_fn\r\n",
      "    target_list, run_metadata)\r\n",
      "  File \"/home/liu/anaconda3/envs/tf1.15/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\", line 1443, in _call_tf_sessionrun\r\n",
      "    run_metadata)\r\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.\r\n",
      "  (0) Invalid argument: ValueError: matrix contains invalid numeric entries\r\n",
      "Traceback (most recent call last):\r\n",
      "\r\n",
      "  File \"/home/liu/anaconda3/envs/tf1.15/lib/python3.6/site-packages/tensorflow_core/python/ops/script_ops.py\", line 235, in __call__\r\n",
      "    ret = func(*args)\r\n",
      "\r\n",
      "  File \"/home/liu/disk1/Life/3DBoNetPoint818a(linux)/helper_net.py\", line 115, in assign_mappings_valid_only\r\n",
      "    row_ind, col_ind = linear_sum_assignment(valid_cost)\r\n",
      "\r\n",
      "  File \"/home/liu/anaconda3/envs/tf1.15/lib/python3.6/site-packages/scipy/optimize/_hungarian.py\", line 93, in linear_sum_assignment\r\n",
      "    raise ValueError(\"matrix contains invalid numeric entries\")\r\n",
      "\r\n",
      "ValueError: matrix contains invalid numeric entries\r\n",
      "\r\n",
      "\r\n",
      "     [[{{node bbox/PyFunc}}]]\r\n",
      "     [[gradients/backbone/fa_layer1/ThreeInterpolate_grad/ThreeInterpolateGrad/_425]]\r\n",
      "  (1) Invalid argument: ValueError: matrix contains invalid numeric entries\r\n",
      "Traceback (most recent call last):\r\n",
      "\r\n",
      "  File \"/home/liu/anaconda3/envs/tf1.15/lib/python3.6/site-packages/tensorflow_core/python/ops/script_ops.py\", line 235, in __call__\r\n",
      "    ret = func(*args)\r\n",
      "\r\n",
      "  File \"/home/liu/disk1/Life/3DBoNetPoint818a(linux)/helper_net.py\", line 115, in assign_mappings_valid_only\r\n",
      "    row_ind, col_ind = linear_sum_assignment(valid_cost)\r\n",
      "\r\n",
      "  File \"/home/liu/anaconda3/envs/tf1.15/lib/python3.6/site-packages/scipy/optimize/_hungarian.py\", line 93, in linear_sum_assignment\r\n",
      "    raise ValueError(\"matrix contains invalid numeric entries\")\r\n",
      "\r\n",
      "ValueError: matrix contains invalid numeric entries\n",
      "issue labels - \n",
      "TF 1.15\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  delete pycharm project issue(Pycharm project deletion problem due to cache deletion)\n",
      "issue body -  The project was deleted while using pycharm for tensorflow developer certification. It seems that the pycharm project was deleted while deleting the Mac OS cache and junk. Developers preparing for certification should be aware of this issue.\n",
      "issue labels - \n",
      "stat:awaiting response\n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  TF2.4 MultiWorkerMirroredStrategy and ParameterServerStrategy training very slow for large mode and large data compared with TF1.14\n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):**TF on yarn**(running machine system is CentOS 7)\r\n",
      "- TensorFlow installed from (source or binary):binary\r\n",
      "- TensorFlow version (use command below):2.4.0\r\n",
      "- Python version:3.6\r\n",
      "- CUDA/cuDNN version: no gpu\r\n",
      "- GPU model and memory: no gpu\r\n",
      "- CPU info: please see it below\r\n",
      "- Training config: [MultiWorkerMirroredStrategy] chief/woker: 8 instances, 8 cpus per instance, 120G memory per instance; [ParameterServerStrategy] chief:  8cpus,25G memory; worker: 30 instances, 8 cpus per instance, 25G memory per instance;ps: 8 instances, 12 cpus per instance, 25G memory per instance(which is the same with training using TF1.14)\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "\r\n",
      "I built a wide&deep based model  with about **1.3 billion trainable params** which is used for ranking items for industrial recommend ranking system.\r\n",
      "When I trained the same model using parameter server strategy on TF 1.14(asynchronous training) with about **4 billion training samples**，it only costs about 23 hours, but when I upgrade model to TF 2.4 using keras api, training becomes very slow (about 5597 hours(ETA) of MultiWorkerMirroredStrategy, at least 72 hours of ParameterServerStrategy)\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "Sorry, I can just paste model summary here because of the code complexity, you can see it beflow. I only paste wide model and input of deep model here. deep model is just a [512, 512,512, 512] sequential model. All features name are replaced for privacy so there maybe some mistake here.\r\n",
      "\r\n",
      "**Other info / logs** \r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:dist-strat\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:performance\n",
      "\n",
      "\n",
      "issue title -  Don't raise an Error on unknown kwargs\n",
      "issue body -  <em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n",
      "\r\n",
      "\r\n",
      "**System information**\r\n",
      "- TensorFlow version 2.1 - 2.3:\r\n",
      "- Are you willing to contribute it (No):\r\n",
      "\r\n",
      "\r\n",
      "**Describe the feature and the current behavior/state.**\r\n",
      "\r\n",
      "Almost all Keras objects support **kwargs but raising an TypeError if a 'Keyword argument (is) not understood'. i.e.\r\n",
      "\r\n",
      "```\r\n",
      "tf.keras.layers.Dense(8, foo='bar')\r\n",
      "```\r\n",
      "\r\n",
      "I wish a more Duck Typing behavior that either ignores unknown args or raises a warning according to the debug level.\r\n",
      "\r\n",
      "**Who will benefit with this feature?**\r\n",
      "\r\n",
      "Simplifies the parameter management for inheriting classes or adapter classes.\r\n",
      "\r\n",
      "```\r\n",
      "class MyLayer(tf.keras.layers.Layer):\r\n",
      "   def __init__(self, units=8, dims=(0,2,1), **kwargs):\r\n",
      "      super(MyLayer, self).__init__(**kwargs)\r\n",
      "      self.layer = tf.keras.layers.Dense(units, **kwargs)\r\n",
      "      self.permute = tf.keras.layers.Permute(dims, **kwargs)\r\n",
      "      [...]\r\n",
      "```\r\n",
      "\n",
      "issue labels - \n",
      "comp:keras\n",
      "stat:awaiting tensorflower\n",
      "type:feature\n",
      "\n",
      "\n",
      "issue title -  Refactor ReshapeSparseTensor into a template+class\n",
      "issue body -  This is in preparation for adding a GPU implementation.\r\n",
      "No functional change.\r\n",
      "\r\n",
      "cc @nluehr \n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:core\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  use of interpreter-->SetNumThreads. Do we need to invoke setNumThreads always to improve performance\n",
      "issue body -  In the label_image.cc example from tfLite, we are passing setNumThreads manually under the code.\r\n",
      "Does number of threads automatically picked by tfLite (or) Is it required by user to always force SetNumThreads ?\r\n",
      "\n",
      "issue labels - \n",
      "comp:lite\n",
      "stat:awaiting tensorflower\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  TFLite: iOS: Ensure we use the iOS mktemp\n",
      "issue body -  It's fairly common for iOS developers to have mktemp from coreutils\r\n",
      "installed and sitting in front of the iOS mktemp on their PATH. Since the\r\n",
      "coreutils version has different flags, trying to build tflite from source\r\n",
      "results in the following error:\r\n",
      "\r\n",
      "    ERROR: /Users/mgalgs/development/tensorflow/tensorflow/lite/ios/BUILD:49:28: Executing genrule //tensorflow/lite/ios:TensorFlowLiteC_framework failed (Exit 1): bash failed: error executing command /bin/bash -c ... (remaining 1 argument(s) skipped)\r\n",
      "    mktemp: too few X's in template 'framework'\r\n",
      "    Target //tensorflow/lite/ios:TensorFlowLiteC_framework failed to build\r\n",
      "\r\n",
      "Fix this by using the mktemp from /usr/bin/ explicitly.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:lite\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  Refactor case BuiltinOperator_GATHER_ND in flatbuffer_conversions\n",
      "issue body -  PR1 for issue #46268.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:lite\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  tensorrt not working with cuda 11.2\n",
      "issue body -  <em>Please make sure that this is a bug. As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:bug_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n",
      "- TensorFlow installed from (source or binary): source\r\n",
      "- TensorFlow version (use command below): 2.4.0\r\n",
      "- Python version: 3.9.1\r\n",
      "- Bazel version (if compiling from source): 3.7.2\r\n",
      "- GCC/Compiler version (if compiling from source):  7.5.0\r\n",
      "- CUDA/cuDNN version: 11.2 / 8.0.5\r\n",
      "- GPU model and memory:  GTX1080Ti 11GB\r\n",
      "\r\n",
      "You can collect some of this information using our environment capture\r\n",
      "[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n",
      "You can also obtain the TensorFlow version with:\r\n",
      "1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n",
      "2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n",
      "\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "tf workfs fine, but tftrt no\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "```\r\n",
      "2021-01-08 10:32:10.702646: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvrtc.so.11.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/x86_64-linux-gnu:/usr/local/cuda/lib64:/usr/local/cuda/lib64:\r\n",
      "```\r\n",
      "after add soft link\r\n",
      "\r\n",
      "```\r\n",
      "2021-01-08 10:41:23.887753: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libnvinfer.so.7'; dlerror: /usr/local/cuda/lib64/libnvrtc.so.11.1: version `libnvrtc.so.11.1' not found (required by /usr/lib/x86_64-linux-gnu/libnvinfer.so.7); LD_LIBRARY_PATH: /usr/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/local/cuda/lib64:/usr/local/cuda/lib64:\r\n",
      "```\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "any tensorrt example code\r\n",
      "\r\n",
      "\r\n",
      "**Other info / logs** Include any logs or source code that would be helpful to\r\n",
      "diagnose the problem. If including tracebacks, please include the full\r\n",
      "traceback. Large logs and files should be attached.\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:gpu:tensorrt\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Remove unneccessary define.\n",
      "issue body -  @pnikam-cad confirmed that this will no longer be needed: https://github.com/tensorflow/tensorflow/pull/46238#discussion_r553364064\n",
      "\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Have artifcat directory be different for different build_types.\n",
      "issue body -  Fixes #46261\n",
      "\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Minor improvement in _NormalizingCombiner to use MEAN_IDX and VAR_IDX\n",
      "issue body -  This is a minor improvement to `_NormalizingCombiner` to use `self.MEAN_IDX` and `self.VAR_IDX` when getting items from `accumulator`.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:keras\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Fix a minor formatting error in lite/kernels/expand_dims.cc\n",
      "issue body -  No related issue since it's a small cleanup.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:lite\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  [tf.data service] Test split provider failures with distributed and zipped datasets\n",
      "issue body -  This PR extends the test cases for `data_service_ops` by adding the following tests:\r\n",
      "- Test the failing case of creating a split provider for a `ZipDataset`\r\n",
      "- Test the failing case of creating a split provider for a `DataServiceDataset`.\r\n",
      "\r\n",
      "cc: @aaudiber\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:data\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  different TFLM builds use the same output directory.\n",
      "issue body -  @tensorflow/micro\r\n",
      "\r\n",
      "In https://github.com/tensorflow/tensorflow/pull/46242#discussion_r553049656, I was suggesting that the linker was not correctly dropping unused symbols.\r\n",
      "\r\n",
      "In fact, what was very likely happening was that I did not do a `make clean` between switching to `BUILD_TYPE=release`. And since the TFLM makefile currently uses the same directory for all `BUILD_TYPE`, only the modified files were being rebuilt with the smaller `release` build.\r\n",
      "\r\n",
      "We can reproduce this with the following sequence of commands:\r\n",
      "\r\n",
      "First check what the binary size is for the release build.\r\n",
      "```\r\n",
      "make -f tensorflow/lite/micro/tools/make/Makefile clean\r\n",
      "\r\n",
      "make -f tensorflow/lite/micro/tools/make/Makefile -j8 TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=hifimini XTENSA_CORE=mini1m1m_RG keyword_benchmark BUILD_TYPE=release\r\n",
      "\r\n",
      "xt-size tensorflow/lite/micro/tools/make/gen/xtensa_hifimini/bin/keyword_benchmark \r\n",
      "   text\t   data\t    bss\t    dec\t    hex\tfilename\r\n",
      "  46080\t  40204\t  24952\t 111236\t  1b284\ttensorflow/lite/micro/tools/make/gen/xtensa_hifimini/bin/keyword_benchmark\r\n",
      "```\r\n",
      "\r\n",
      "Next have some intermediate non-release objects and then do a release build:\r\n",
      "```\r\n",
      "make -f tensorflow/lite/micro/tools/make/Makefile clean\r\n",
      "\r\n",
      "# build non-release\r\n",
      "make -f tensorflow/lite/micro/tools/make/Makefile -j8 TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=hifimini XTENSA_CORE=mini1m1m_RG keyword_benchmark\r\n",
      "\r\n",
      "touch tensorflow/lite/micro/kernels/xtensa/fully_connected.cc\r\n",
      "\r\n",
      "#build for release\r\n",
      "make -f tensorflow/lite/micro/tools/make/Makefile -j8 TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=hifimini XTENSA_CORE=mini1m1m_RG keyword_benchmark BUILD_TYPE=release\r\n",
      "\r\n",
      "xt-size tensorflow/lite/micro/tools/make/gen/xtensa_hifimini/bin/keyword_benchmark \r\n",
      "   text\t   data\t    bss\t    dec\t    hex\tfilename\r\n",
      "  54736\t  48168\t  25032\t 127936\t  1f3c0\ttensorflow/lite/micro/tools/make/gen/xtensa_hifimini/bin/keyword_benchmark\r\n",
      "```\r\n",
      "\r\n",
      "What we really should be doing is to change the output directory based on the build type.\n",
      "issue labels - \n",
      "comp:micro\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Refactor case BuiltinOperator_EXPAND_DIMS in flatbuffer_conversions\n",
      "issue body -  PR1 for issue #46258\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:lite\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  [ROCm] Misc XLA updates for the ROCm platform - 210107\n",
      "issue body -  Some minor XLA related updates for the ROCm platform\r\n",
      "\r\n",
      "copy-pasting the individual commit messages here for convenience\r\n",
      "\r\n",
      "*  Relaxing the rtol for some fp16 subtests within dot_operation_test.cc\r\n",
      "The rtol is being given a minor bump from 5e-3 to 7e-3 (for fp16 only) to let some of the subtests pass, that were failing on the ROCm platform, because of the tighter rtol\r\n",
      "\r\n",
      "*  re-enabling a subtest withing image_ops_test.py, because it no longer fails on the ROCm platform\r\n",
      "\r\n",
      "*  Re-enabling the XLA convolution_test on the ROCm platform, after disabling a couple of failing subtests.\r\n",
      "\r\n",
      "---------------------------------------------------------------------------\r\n",
      "\r\n",
      "\r\n",
      "/cc @cheshire @chsigg @nvining-work \r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:gpu\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  [XLA] dense_layer_test.py throws internal error in fallback path\n",
      "issue body -  The failure happens in Master as well as r2.4 (these are the 2 branches that I've tested).\r\n",
      "\r\n",
      "Even with lazy_compilation turned on (via `TF_XLA_FLAGS=--tf_xla_enable_lazy_compilation=true`), the first execution always compiles as per the current implementation. If we tweak this behaviour such that the first execution doesn't compile (and uses the fallback path) or use `--tf_xla_always_defer_compilation=true` to force the fallback path, the test `tensorflow/compiler/tests/dense_layer_test.py` fails with the following signature\r\n",
      "```\r\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node cluster_3}} {{function_node cluster_3}} Trying to assign variable with wrong dtype. Expected INVALID got float\r\n",
      "     [[{{node dense/kernel/Assign}}]]\r\n",
      "     [[cluster_3_1/partitioned_call]]\r\n",
      "```\r\n",
      "\r\n",
      "There are 3 test points in the test. Based on the descriptions, they test that the dense layer node is properly compiled in jit scope. I am not sure if this test is supposed to be used for the fallback path. However, the failure is not merely a test failure but an internal error (`Trying to assign variable with wrong dtype. Expected INVALID got float`) which leads me to think there might be a bug. \r\n",
      "\r\n",
      "The error comes from handling of resource variables https://github.com/tensorflow/tensorflow/blob/13d37279f137a96ca6fa5142f20c8747103bf79e/tensorflow/core/kernels/resource_variable_ops.cc#L397-L401\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:core\n",
      "comp:xla\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Copy lite/kernels/zeros_like.cc to lite/micro/kernels/zeros_like.cc\n",
      "issue body -  PR3 for issue #46049 (kernel ZEROS_LIKE has no reference op implementation, so we are skipping PR2: refactor reference_ops.h)\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:lite\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  Saved weights as checkpoints are randomised on loading them in the model\n",
      "issue body -  <em>Please make sure that this is a bug. As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:bug_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, i have a custom U-Net model with conv1d and custom loss function for ignoring the ignore labels\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n",
      "- TensorFlow installed from (source or binary): source\r\n",
      "- TensorFlow version (use command below): 2021-01-07 16:16:22.597961: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "v2.4.0-rc4-71-g582c8d236cb 2.4.0\r\n",
      "- Python version: 3.8.3\r\n",
      "- Bazel version (if compiling from source): 3.7.2\r\n",
      "- GCC/Compiler version (if compiling from source): 9.3.0\r\n",
      "- CUDA/cuDNN version: N/A\r\n",
      "- GPU model and memory: N/A\r\n",
      "\r\n",
      "You can collect some of this information using our environment capture\r\n",
      "[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n",
      "You can also obtain the TensorFlow version with:\r\n",
      "1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n",
      "2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n",
      "\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "The checkpoints (save_weights) after successfully training the model randomise after loading them into model. The accuracy and precision plummet drastically on the same data which showed 99+ % accuracy while training.\r\n",
      "**Describe the expected behavior**\r\n",
      "The checkpoints must show the same results after loading, as while the training.\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "input_layer = l.Input((seq_len, seq_dep), name='seq')\r\n",
      "    model_unet_1d = get_unet(input_layer, labels-1, 16, dropout=0.05, batchnorm=True)\r\n",
      "    model_unet_1d.compile(optimizer=Adam(), loss=custom_loss, metrics=[custom_metrics])\r\n",
      "    \r\n",
      "    print(\"<<<< Model Compiled >>>>\")\r\n",
      "    \r\n",
      "    callbacks = [\r\n",
      "    EarlyStopping(patience=10, verbose=1),\r\n",
      "    ReduceLROnPlateau(factor=0.1, patience=5, min_lr=0.00001, verbose=1),\r\n",
      "    ModelCheckpoint('model_chkp/model-ctc_4tiles_bce_sam.h5', verbose=1, save_best_only=True, save_weights_only=True)\r\n",
      "    ]\r\n",
      "Provide a reproducible test case that is the bare minimum necessary to generate\r\n",
      "the problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n",
      "\r\n",
      "**Other info / logs** Include any logs or source code that would be helpful to\r\n",
      "diagnose the problem. If including tracebacks, please include the full\r\n",
      "traceback. Large logs and files should be attached.\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:model\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Getting TypeError: when using label smoothing in Categorical Cross entropy\n",
      "issue body -  <em>Please make sure that this is a bug. As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:bug_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Colab\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "- TensorFlow installed from (source or binary):\r\n",
      "- TensorFlow version (use command below):2.3.1\r\n",
      "- Python version:3.7\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version Colab default \r\n",
      "- GPU model and memory:V100 SXM2\r\n",
      "\r\n",
      "You can collect some of this information using our environment capture\r\n",
      "[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n",
      "You can also obtain the TensorFlow version with:\r\n",
      "1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n",
      "2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n",
      "\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "Provide a reproducible test case that is the bare minimum necessary to generate\r\n",
      "the problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n",
      "\r\n",
      "**Other info / logs** Include any logs or source code that would be helpful to\r\n",
      "diagnose the problem. If including tracebacks, please include the full\r\n",
      "traceback. Large logs and files should be attached.\r\n",
      "\r\n",
      "\r\n",
      "I am using label smoothing with Categorical Cross entropy. When I turn off label smoothing the model works fine but when I turn it The model gives the following error\r\n",
      "`    TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type float16 of argument 'x'.\r\n",
      "`\r\n",
      "I am not able to understand what is the problem with my code?. Here is My code\r\n",
      "\r\n",
      "https://drive.google.com/file/d/1lEVPMOtWOWesDmpRDgTJQPTFOQ6UTI2o/view?usp=sharing\r\n",
      "\r\n",
      "I have tried setting the dtype to float16 in Image Data Generator. But that also does not work. Can somebody Help me? I have not got any answers on stack overflow also\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.3\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  ci: create issue labeled reply\n",
      "issue body -  ref: https://github.com/tensorflow/tensorflow/issues/46249 https://github.com/tensorflow/tensorflow/issues/46230\r\n",
      "\r\n",
      "When you add `stat:awaiting response` label, the GitHub Actions will help you comment this.\n",
      "issue labels - \n",
      "cla: yes\n",
      "prtype:bugfix\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  TypeError: '<' not supported between instances of 'function' and 'str'\n",
      "issue body -  **System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab (Ubuntu)\r\n",
      "- TensorFlow version (use command below): 2.4.0\r\n",
      "- Python version: 3.6.9\r\n",
      "\r\n",
      "**Code**\r\n",
      "I've save a model using callbacks:\r\n",
      "\r\n",
      "```python\r\n",
      "mc = tf.keras.callbacks.ModelCheckpoint( filepath=save_directory + '/trial10_model.h5',\r\n",
      "                                        monitor=\"val_loss\",\r\n",
      "                                        save_freq=\"epoch\",\r\n",
      "                                        save_best_only=True,\r\n",
      "                                        save_weights_only=False)\r\n",
      "callback_list = [mc]\r\n",
      "\r\n",
      "history = model.fit_generator(get_train_set_,validation_data = get_val_set_, validation_steps=validation_steps,\r\n",
      "                              steps_per_epoch = steps_per_epoch,epochs=15, callbacks=callback_list)\r\n",
      "```\r\n",
      "\r\n",
      "While testing the model using evaluate or evaluate_generator:\r\n",
      "```python\r\n",
      "model = load_model(\"/content/gdrive/MyDrive/keras_tuner/good_trail/trial10_model.h5\", custom_objects={ 'dice_coef': dice_coef })\r\n",
      "res = model.evaluate(get_test_set_,steps=test_steps_per_epoch, verbose=1)\r\n",
      "```\r\n",
      "**Error Message**\r\n",
      "I get this error message:\r\n",
      "```python\r\n",
      "---------------------------------------------------------------------------\r\n",
      "TypeError                                 Traceback (most recent call last)\r\n",
      "<ipython-input-56-3ae064222286> in <module>()\r\n",
      "     12 callback_list = [mc]\r\n",
      "     13 \r\n",
      "---> 14 res = model.evaluate(get_test_set_,steps=test_steps_per_epoch, verbose=1, callbacks=callback_list)\r\n",
      "\r\n",
      "9 frames\r\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)\r\n",
      "    975           except Exception as e:  # pylint:disable=broad-except\r\n",
      "    976             if hasattr(e, \"ag_error_metadata\"):\r\n",
      "--> 977               raise e.ag_error_metadata.to_exception(e)\r\n",
      "    978             else:\r\n",
      "    979               raise\r\n",
      "\r\n",
      "TypeError: in user code:\r\n",
      "\r\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1233 test_function  *\r\n",
      "        return step_function(self, iterator)\r\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1224 step_function  **\r\n",
      "        outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\r\n",
      "        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\r\n",
      "        return self._call_for_each_replica(fn, args, kwargs)\r\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\r\n",
      "        return fn(*args, **kwargs)\r\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1219 run_step  **\r\n",
      "        with ops.control_dependencies(_minimum_control_deps(outputs)):\r\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:2793 _minimum_control_deps\r\n",
      "        outputs = nest.flatten(outputs, expand_composites=True)\r\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py:341 flatten\r\n",
      "        return _pywrap_utils.Flatten(structure, expand_composites)\r\n",
      "\r\n",
      "    TypeError: '<' not supported between instances of 'function' and 'str'\r\n",
      "```\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:keras\n",
      "stalled\n",
      "stat:awaiting response\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Tensorboard callback not logging learning rate when not using LearningRateSchedule (e.g. when using ReduceLROnPlateau)\n",
      "issue body -  <em>Please make sure that this is a bug. As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:bug_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code: Yes\r\n",
      "- OS Platform and Distribution: Linux Ubuntu 18.04\r\n",
      "- TensorFlow installed from: pip3\r\n",
      "- TensorFlow version: tf-nightly 2.5.0-dev20210104\r\n",
      "- Python version: 3.8.5\r\n",
      "- CUDA/cuDNN version: 11.0.3 / 8.0.4\r\n",
      "- GPU model and memory: Quadro RTX 5000 / 16384 MB\r\n",
      "\r\n",
      "**Describe the current behavior**  \r\n",
      "If there is no LearningRateSchedule (either LR is constant or is changed via callback) then [Tensorboard callback](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard) doesn't log learning rate. This is especially relevant when using [ReduceLROnPlateau](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ReduceLROnPlateau) callback.\r\n",
      "\r\n",
      "**Describe the expected behavior**  \r\n",
      "Learning rate should be always logged or at least in the case of using a callback.\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**  \r\n",
      "```\r\n",
      "import tensorflow as tf\r\n",
      "import datetime\r\n",
      "import os\r\n",
      "\r\n",
      "class SimpleModel(tf.keras.Model):\r\n",
      "    def __init__(self):\r\n",
      "        super(SimpleModel, self).__init__()\r\n",
      "        self.Dense_in = tf.keras.layers.Dense(units = 50, \r\n",
      "                                activation=tf.nn.relu, \r\n",
      "                                use_bias=True)\r\n",
      "        self.Dense_hidden = tf.keras.layers.Dense(units = 20, \r\n",
      "                                activation=tf.nn.relu, \r\n",
      "                                use_bias=True)\r\n",
      "        self.Dense_out = tf.keras.layers.Dense(units = 1, \r\n",
      "                                activation=None, \r\n",
      "                                use_bias=True)\r\n",
      "\r\n",
      "    def call(self, inputs, training = True):\r\n",
      "        X = self.Dense_in(inputs)\r\n",
      "        X = self.Dense_hidden(X)\r\n",
      "        X = self.Dense_out(X)\r\n",
      "        return X\r\n",
      "\r\n",
      "dummy_data = tf.random.normal((10000, 50))\r\n",
      "dummy_y = tf.random.uniform((10000,), minval=0, maxval=2, dtype=tf.dtypes.int32)\r\n",
      "\r\n",
      "\r\n",
      "#### LR is not logged here ####\r\n",
      "model = SimpleModel()\r\n",
      "\r\n",
      "loss = tf.keras.losses.BinaryCrossentropy(name='binary_crossentropy', from_logits=True)\r\n",
      "opt = tf.keras.optimizers.Adam(learning_rate = 0.001)\r\n",
      "model.compile(optimizer = opt, loss = loss, metrics=[\"acc\"])\r\n",
      "\r\n",
      "_datetime = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\r\n",
      "log_dir = os.path.join('dummy_logs', _datetime)\r\n",
      "tb_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\r\n",
      "\r\n",
      "model.fit(x = dummy_data, y = dummy_y, epochs = 10,\r\n",
      "        callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor='acc', \r\n",
      "                        factor=0.5, patience=10, verbose=0, mode='auto', \r\n",
      "                        min_delta=0.0001, cooldown=0, min_lr=0),\r\n",
      "                    tb_callback])\r\n",
      "\r\n",
      "#### LR is logged here ####\r\n",
      "model = SimpleModel()\r\n",
      "\r\n",
      "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\r\n",
      "    initial_learning_rate = 0.001,\r\n",
      "    decay_steps=100000,\r\n",
      "    decay_rate=0.96)\r\n",
      "\r\n",
      "loss = tf.keras.losses.BinaryCrossentropy(name='binary_crossentropy', from_logits=True)\r\n",
      "opt = tf.keras.optimizers.Adam(learning_rate = lr_schedule)\r\n",
      "model.compile(optimizer = opt, loss = loss, metrics=[\"acc\"])\r\n",
      "\r\n",
      "_datetime = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\r\n",
      "log_dir = os.path.join('dummy_logs', _datetime)\r\n",
      "tb_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\r\n",
      "\r\n",
      "model.fit(x = dummy_data, y = dummy_y, epochs = 10,\r\n",
      "        callbacks=[tb_callback])\r\n",
      "```\r\n",
      "![Screenshot from 2021-01-07 12-14-15](https://user-images.githubusercontent.com/32313554/103887434-517ee100-50e3-11eb-9597-4978bee27972.png)\r\n",
      "\r\n",
      "\r\n",
      "**Other info / logs**   \r\n",
      "I think that the problem is in tf.keras.callbacks.TensorBoard in _collect_learning_rate (see code block below). If self.model.optimizer.lr is sheduler then logs['learning_rate'] is created and LR can be seen in tensorboard. However if self.model.optimizer.lr is not an instance of learning_rate_schedule.LearningRateSchedule then logs['learning_rate'] does not exists and thus LR is not logged.\r\n",
      "```\r\n",
      "def _collect_learning_rate(self, logs):\r\n",
      "    lr_schedule = getattr(self.model.optimizer, 'lr', None)\r\n",
      "    if isinstance(lr_schedule, learning_rate_schedule.LearningRateSchedule):\r\n",
      "      logs['learning_rate'] = lr_schedule(self.model.optimizer.iterations)\r\n",
      "    return logs\r\n",
      "```\r\n",
      "\r\n",
      "A quick fix that I see here could be to add *else* or *elif* and directly log self.model.optimizer.lr. Changing as below fixed the issue for me when using ReduceLROnPlateau callback. This fix assumes that if no LearningRateSchedule is used then self.model.optimizer.lr could be logged as scalar by tensorboard callback. If this is not always possible then I guess adding some reasonable *elif* to be able to catch LR that can be logged (e.g. scalar) could resolve the problem.\r\n",
      "```\r\n",
      "class TensorBoardCallback(tf.keras.callbacks.TensorBoard):\r\n",
      "    def __init__(self, **kwargs):\r\n",
      "        super(TensorBoardCallback, self).__init__(**kwargs)\r\n",
      "\r\n",
      "    def _collect_learning_rate(self, logs):\r\n",
      "        lr_schedule = getattr(self.model.optimizer, 'lr', None)\r\n",
      "        if isinstance(lr_schedule, tf.keras.optimizers.schedules.LearningRateSchedule):\r\n",
      "            logs['learning_rate'] = lr_schedule(self.model.optimizer.iterations)\r\n",
      "        elif lr_schedule is not None:\r\n",
      "            logs['learning_rate'] = lr_schedule\r\n",
      "        return logs\r\n",
      "```\n",
      "issue labels - \n",
      "TF 2.5\n",
      "stat:awaiting response\n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  Speedup Resize{Bilinear,NearestNeighbor}\n",
      "issue body -  This PR speedup `Resize{Bilinear,NearestNeighbor}` with eigen tensor generator class. The original implementation specialize 3-channeled image with SSE, but it's still less performant compared with this PR. Below is my benchmark with `tensorflow/core/kernels/image/resize_op_benchmark_test.cc`.\r\n",
      "\r\n",
      "On ***Intel(R) Core(TM) i5-9600K CPU @ 3.70GHz***\r\n",
      "\r\n",
      "### Nearest neighbor\r\n",
      "\r\n",
      "| input_shape (N x IH x IW x C) | output_shape (OH x OW) | Old (M items/s) | New (M items/s) |\r\n",
      "| ----- | ---- | ---- | ----- |\r\n",
      "|  10x499x499x1       | 250x250       |  614        |  2849      |\r\n",
      "|  10x499x499x3       | 250x250       |  2267        |  4304      |\r\n",
      "|  10x499x499x1       | 998x998       |   58       |  368        |\r\n",
      "|  10x499x499x3       | 998x998       |   170       |  538        |\r\n",
      "\r\n",
      "where M items/s is computed as  `time / (iterations * N * IH * IW * C)`, the higher the better.\r\n",
      "\r\n",
      "### Bilinear\r\n",
      "\r\n",
      "| input_shape (N x IH x IW x C) | output_shape (OH x OW) | Old (M items/s) | New (M items/s) |\r\n",
      "| ----- | ---- | ---- | ----- |\r\n",
      "|  10x499x499x1       | 250x250       |  1282        |  2362 |\r\n",
      "|  10x499x499x3       | 250x250       |  2106        |  3275 |\r\n",
      "|  10x499x499x1       | 998x998       |  103        |  182   |\r\n",
      "|  10x499x499x3       | 998x998       |  179        |  362   |\r\n",
      "\r\n",
      "where M items/s is computed as  `time / (iterations * N * IH * IW * C)`, the higher the better.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:core\n",
      "ready to pull\n",
      "size:L\n",
      "\n",
      "\n",
      "issue title -  tf.keras Model predict slow in flask service\n",
      "issue body -  \r\n",
      "System info:\r\n",
      "tensorflow: 1.12.0\r\n",
      "flask: 1.1.2\r\n",
      "os: centos7.6\r\n",
      "use cpu\r\n",
      "\r\n",
      "problem:\r\n",
      "tf.keras Model predict slow in flask service\r\n",
      "import the follow code in python console, and run predict method directly， only cost about 800ms time;\r\n",
      "but in flask service， run predict methodin an interface, it need about 12s time.\r\n",
      "\r\n",
      "code:\r\n",
      "    def __init__(self):\r\n",
      "        self.height = 960\r\n",
      "        self.width = 960\r\n",
      "        self.channels = 3\r\n",
      "        self.model = ResNet50(include_top=False, input_shape=(self.height, self.width, self.channels))\r\n",
      "        self.graph = tf.get_default_graph()\r\n",
      "\r\n",
      "    def image_resize(self, image):\r\n",
      "        raw_height = image.shape[0]\r\n",
      "        raw_width = image.shape[1]\r\n",
      "        if raw_height > self.height or raw_width > self.width:\r\n",
      "            ratio1 = raw_height * 1.0 / self.height\r\n",
      "            ratio2 = raw_width * 1.0 / self.width\r\n",
      "            if ratio2 > ratio1:\r\n",
      "                ratio1 = ratio2\r\n",
      "            raw_height = int(raw_height / ratio1)\r\n",
      "            raw_width = int(raw_width / ratio1)\r\n",
      "            image = cv2.resize(image, (raw_width, raw_height))\r\n",
      "        new_img = np.pad(image,\r\n",
      "                         pad_width=((0, self.height - raw_height),\r\n",
      "                                    (0, self.width - raw_width),\r\n",
      "                                    (0, 0)\r\n",
      "                                    ),\r\n",
      "                         mode=\"constant\", constant_values=(0, 0))\r\n",
      "\r\n",
      "        return new_img\r\n",
      "\r\n",
      "    def predict(self):\r\n",
      "        image = cv2.imread('1.jpg')\r\n",
      "        image = self.image_resize(image)\r\n",
      "        time1 = int(round(time.time() * 1000))\r\n",
      "        with self.graph.as_default():\r\n",
      "            self.model.predict(np.array([image]))\r\n",
      "        time2 = int(round(time.time() * 1000))\r\n",
      "        cost = time2 - time1\r\n",
      "        return str(cost) + \" ms.\"\n",
      "issue labels - \n",
      "TF 1.12\n",
      "comp:keras\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Copy lite/kernels/gather.cc to lite/micro/kernels/gather.cc\n",
      "issue body -  PR3 for issue #45196\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:lite\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  Refactoring fully_connected to share code between reference and optimized kernels.\n",
      "issue body -  Summary:\r\n",
      "\r\n",
      " * Move shared structs / helper functions into fully_connected_common.cc\r\n",
      " * Clean up some of the existing code to directly call the reference implementations (made possible by the refactor of the helper functions).\r\n",
      "\r\n",
      "Also, this refactor addresses the sign flip in fully_connected: http://b/138810107\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:L\n",
      "\n",
      "\n",
      "issue title -  Refactor gather.h from TFLite reference_ops.h\n",
      "issue body -  PR2 for issue #45196\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:lite\n",
      "comp:micro\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  Refactor case BuiltinOperator_GATHER in flatbuffer_conversions\n",
      "issue body -  PR1 for issue #45196\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:lite\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  Include and build xa_nnlib with TARGET_ARCH=fusion_f1.\n",
      "issue body -   * Needed to add some defines, disable warning and exclude some sources from xa_nnlib to build everything.\r\n",
      " * The kernels are still unchanged (i.e. have a reference fallback).\r\n",
      "\r\n",
      "Manually verified that the following command downloads xa_nnlib:\r\n",
      "```\r\n",
      "make -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=fusion_f1 XTENSA_CORE=F1_190305_swupgrade test_keyword_benchmark -j8\r\n",
      "```\r\n",
      "\r\n",
      "And the latency is unchanged:\r\n",
      "```\r\n",
      "InitializeKeywordRunner() took 280862 ticks (280 ms)\r\n",
      "KeywordRunNIerations(1) took 170431 ticks (170 ms)\r\n",
      "KeywordRunNIerations(10) took 1703817 ticks (1703 ms)\r\n",
      "```\r\n",
      "\r\n",
      "And the size with the release build:\r\n",
      "```\r\n",
      "make -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=fusion_f1 XTENSA_CORE=F1_190305_swupgrade keyword_benchmark -j8 BUILD_TYPE=release\r\n",
      "xt-size tensorflow/lite/micro/tools/make/gen/xtensa_fusion_f1/bin/keyword_benchmark\r\n",
      "```\r\n",
      "\r\n",
      "is also unchanged:\r\n",
      "```\r\n",
      "   text\t   data\t    bss\t    dec\t    hex\tfilename\r\n",
      "  51168\t  40132\t  24872\t 116172\t  1c5cc\ttensorflow/lite/micro/tools/make/gen/xtensa_fusion_f1/bin/keyword_benchmark\r\n",
      "```\r\n",
      "\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  [tf.data] enhance encapsulation for IteratorResource State\n",
      "issue body -  This PR enhances the encapsulation capability of the `State` of `tensorflow:data:IteratorResource` by representing it as a `class` instead of a `struct`. Additionally, the necessary `getter` methods have been added.\r\n",
      "\r\n",
      "cc: @aaudiber this was one of the TODO items which required better encapsulation. Please let me know if this approach is suitable and if any changes are required.\r\n",
      "\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:data\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  Fix the CUDNN RNN params dim size\n",
      "issue body -  The CudnnRnnParamsDescriptor::Create() uses the RNN params size _in bytes_ as the dimension for cudnnSetFilterNdDescriptor(). This PR fixes it by dividing the params size in bytes by the byte size of data type.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "FYI. @nluehr \r\n",
      "\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "awaiting review\n",
      "cla: yes\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Tensorflow 2.4 not showing available GPU even after successful cuda and cudnn installation\n",
      "issue body -  OS - ubuntu 20.04\r\n",
      "python -3..8.5\r\n",
      "GPU - RTX 2060\r\n",
      "Cudnn - v7.6.5\r\n",
      "\r\n",
      "Output of `tf.test.is_gpu_available(cuda_only=False, min_cuda_compute_capability=None)`:\r\n",
      "`WARNING:tensorflow:From <stdin>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\r\n",
      "2021-01-07 00:08:12.313080: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n",
      "2021-01-07 00:08:12.315365: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n",
      "2021-01-07 00:08:12.403099: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\r\n",
      "2021-01-07 00:08:12.403164: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: aum-GF65-Thin-9SEXR\r\n",
      "2021-01-07 00:08:12.403176: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: aum-GF65-Thin-9SEXR\r\n",
      "2021-01-07 00:08:12.403275: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 450.80.2\r\n",
      "2021-01-07 00:08:12.403328: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 450.80.2\r\n",
      "2021-01-07 00:08:12.403345: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 450.80.2\r\n",
      "False\r\n",
      "`\r\n",
      "OUTPUT of `dpkg -l | grep cuda-toolkit`\r\n",
      "\r\n",
      "`ii  cuda-toolkit-11-0                               11.0.2-1                              amd64        CUDA Toolkit 11.0 meta-package\r\n",
      "ii  nvidia-cuda-toolkit                             10.1.243-3                            amd64        NVIDIA CUDA development toolkit\r\n",
      "`\r\n",
      "\r\n",
      "OUTPUT of `nvidia-smi` \r\n",
      "`+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce RTX 2060    Off  | 00000000:01:00.0 Off |                  N/A |\r\n",
      "| N/A   48C    P8     6W /  N/A |    306MiB /  5934MiB |      5%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A       920      G   /usr/lib/xorg/Xorg                 45MiB |\r\n",
      "|    0   N/A  N/A      1469      G   /usr/lib/xorg/Xorg                106MiB |\r\n",
      "|    0   N/A  N/A      1647      G   /usr/bin/gnome-shell              125MiB |\r\n",
      "|    0   N/A  N/A      2647      G   /usr/lib/firefox/firefox            3MiB |\r\n",
      "|    0   N/A  N/A      2690      G   /usr/lib/firefox/firefox            3MiB |\r\n",
      "|    0   N/A  N/A      2723      G   /usr/lib/firefox/firefox            3MiB |\r\n",
      "|    0   N/A  N/A      3681      G   /usr/lib/firefox/firefox            3MiB `\r\n",
      "\r\n",
      "After succesful installation of CUDA 11, nvidia-cuda-toolkit 10.1 and tensorflow 2.4 (also tried tensorflow-gpu 2.2.0 but same error) I am still getting NO GPUs FOUND.\r\n",
      "\r\n",
      "PLEASE HELP!!\r\n",
      "\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:gpu\n",
      "stat:awaiting response\n",
      "\n",
      "\n",
      "issue title -  [ROCm] Update ROCm XLA backend to use GCN Arch Name \n",
      "issue body -  Starting with ROCm 4.1(?),  TF XLA will need to correctly populate the \"target-feature\" when creating the LLVM AMDGPUTarget, in order to get optimal performance. The \"target-feature\"s supported by the underlying GPU can queried via the `hipGetDeviceProperties` API and are stored in the `hipDeviceProp_t::gcnArchName` field.\r\n",
      "\r\n",
      "This PR has 3 commits\r\n",
      "1. Add the hooks to retrieve and query the `hipDeviceProp_t::gcnArchName` field\r\n",
      "2. Update the `GpuVersion` datatype for AMDGPU in XLA code to include the `gcnArchName` string\r\n",
      "3. parse the `gcnArchName` string and map the tokens within it, to appropriate \"target-feature\" string tokens.\r\n",
      "\r\n",
      "The mapping done in the 3rd commit will be straight-forward once support for this feature gets upstreamed to the public LLVM repo. Until that happens and the TF LLVM pointer is moved beyond the upstream commit, we will need to special case the mapping. \r\n",
      "\r\n",
      "------------------------------------------------------------------------------------------\r\n",
      "\r\n",
      "/cc @cheshire @chsigg @nvining-work \r\n",
      "\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:gpu\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  tensorflow:AutoGraph could not transform <bound method WindowGenerator.split_window of Total window size: 3\n",
      "issue body -  ------------------------\r\n",
      "\r\n",
      "### System information\r\n",
      "\r\n",
      "-   **Have I written custom code (as opposed to using a stock example script\r\n",
      "    provided in TensorFlow)**:\r\n",
      "Yes, slightly modified [this](https://www.tensorflow.org/tutorials/structured_data/time_series) guide\r\n",
      "-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Manjaro Nibia 20.2 Kernel 5.4\r\n",
      "-   **TensorFlow installed from (source or binary)**: Manjaro package - [https://discover.manjaro.org/packages/python-tensorflow-cuda](https://discover.manjaro.org/packages/python-tensorflow-cuda)\r\n",
      "-   **TensorFlow version (use command below)**: 2.4.0-1\r\n",
      "-   **Python version**: Python 3.9.1\r\n",
      "-   **CUDA/cuDNN version**: bundled in manjaro package\r\n",
      "-   **GPU model and memory**: GeForce GTX 960M total memory 2GB\r\n",
      "-   **Exact command to reproduce**: \r\n",
      "Following [this](https://www.tensorflow.org/tutorials/structured_data/time_series#2_split) guide the error occurs at splitting\r\n",
      "\r\n",
      "### Describe the problem\r\n",
      "Occurred during training. Log states to report this problem.\r\n",
      "\r\n",
      "### Source code / logs\r\n",
      "This is the log output with AUTOGRAPH_VERBOSITY=10\r\n",
      "```log\r\n",
      "2021-01-06 19:56:39.014574: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "2021-01-06 19:56:40.398316: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n",
      "2021-01-06 19:56:40.399294: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n",
      "2021-01-06 19:56:40.432681: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n",
      "2021-01-06 19:56:40.433049: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\n",
      "pciBusID: 0000:03:00.0 name: GeForce GTX 960M computeCapability: 5.0\r\n",
      "coreClock: 1.176GHz coreCount: 5 deviceMemorySize: 1.96GiB deviceMemoryBandwidth: 74.65GiB/s\r\n",
      "2021-01-06 19:56:40.433100: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "2021-01-06 19:56:40.436263: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n",
      "2021-01-06 19:56:40.436390: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n",
      "2021-01-06 19:56:40.437546: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n",
      "2021-01-06 19:56:40.437861: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n",
      "2021-01-06 19:56:40.441100: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11\r\n",
      "2021-01-06 19:56:40.441937: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n",
      "2021-01-06 19:56:40.442202: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n",
      "2021-01-06 19:56:40.442373: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n",
      "2021-01-06 19:56:40.442839: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n",
      "2021-01-06 19:56:40.443141: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n",
      "2021-01-06 19:56:40.443406: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\r\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n",
      "2021-01-06 19:56:40.443944: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n",
      "2021-01-06 19:56:40.444057: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n",
      "2021-01-06 19:56:40.444410: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\n",
      "pciBusID: 0000:03:00.0 name: GeForce GTX 960M computeCapability: 5.0\r\n",
      "coreClock: 1.176GHz coreCount: 5 deviceMemorySize: 1.96GiB deviceMemoryBandwidth: 74.65GiB/s\r\n",
      "2021-01-06 19:56:40.444455: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "2021-01-06 19:56:40.444489: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n",
      "2021-01-06 19:56:40.444533: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n",
      "2021-01-06 19:56:40.444573: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n",
      "2021-01-06 19:56:40.444602: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n",
      "2021-01-06 19:56:40.444640: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11\r\n",
      "2021-01-06 19:56:40.444687: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n",
      "2021-01-06 19:56:40.444720: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n",
      "2021-01-06 19:56:40.444816: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n",
      "2021-01-06 19:56:40.445205: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n",
      "2021-01-06 19:56:40.445503: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n",
      "2021-01-06 19:56:40.445546: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "2021-01-06 19:56:40.519438: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n",
      "2021-01-06 19:56:40.519471: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \r\n",
      "2021-01-06 19:56:40.519479: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \r\n",
      "2021-01-06 19:56:40.519741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n",
      "2021-01-06 19:56:40.520226: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n",
      "2021-01-06 19:56:40.520640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n",
      "2021-01-06 19:56:40.521036: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1271 MB memory) -> physical GPU (device: 0, name: GeForce GTX 960M, pci bus id: 0000:03:00.0, compute capability: 5.0)\r\n",
      "2021-01-06 19:56:40.521335: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method WindowGenerator.split_window of Total window size: 3\r\n",
      "Input indices: [0 1]\r\n",
      "Label indices: [1 2]\r\n",
      "Label column name(s): ['price']> and will run it as-is.\r\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\n",
      "Cause: invalid syntax (tmp7x98gy_p.py, line 10)\r\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\n",
      "2021-01-06 19:56:40.756777: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n",
      "2021-01-06 19:56:40.757331: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2398910000 Hz\r\n",
      "Epoch 1/100\r\n",
      "2021-01-06 19:56:40.930451: F ./tensorflow/core/kernels/random_op_gpu.h:244] Non-OK-status: GpuLaunchKernel(FillPhiloxRandomKernelLaunch<Distribution>, num_blocks, block_size, 0, d.stream(), key, counter, gen, data, size, dist) status: Internal: no kernel image is available for execution on the device\r\n",
      "Aborted (core dumped)\r\n",
      "```\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:autograph\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  No longer compile with `-march=native`.\n",
      "issue body -  Should resolve issue reported in #45744, #45866, #44701 and #45991 as well as multiple other issues from other ecosystem places.\n",
      "issue labels - \n",
      "cla: yes\n",
      "size:S\n",
      "waiting for patch release\n",
      "\n",
      "\n",
      "issue title -  NFC - minor spelling tweaks under lite directory\n",
      "issue body -  This PR addresses minor spelling tweaks under `tensorflow/lite` directory.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:lite\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  Unable to access GPU from docker with error failed call to cuInit: CUresult(-1)\n",
      "issue body -  **System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04, Google cloud instance\r\n",
      "- Tensorflow Docker Version: tensorflow/tensorflow:1.9.0-gpu-py3\r\n",
      "- GPU model and memory: NIVIDIA-T4, 16GB\r\n",
      "\r\n",
      "We are trying to create a custom docker image to serve image classification model. Using the tensorflow/tensorflow:1.9.0-gpu-py3 as base image. The host machine has the NVIDIA drivers installed and able to run the model on GPU.  \r\n",
      "\r\n",
      "**Docker file**\r\n",
      "\r\n",
      "```\r\n",
      "FROM tensorflow/tensorflow:1.9.0-gpu-py3 as base\r\n",
      "ENV CUDA_HOME /usr/local/cuda\r\n",
      "ENV PATH=/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\r\n",
      "ENV LD_LIBRARY_PATH /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:$LD_LIBRARY_PATH\r\n",
      "RUN ln -s /usr/local/cuda/lib64/stubs/libcuda.so /usr/local/cuda/lib64/stubs/libcuda.so.1 \\\r\n",
      "     && echo \"/usr/local/cuda/lib64/stubs\" > /etc/ld.so.conf.d/z-cuda-stubs.conf \\\r\n",
      "     && ldconfig\r\n",
      "ENV NVIDIA_VISIBLE_DEVICES all\r\n",
      "ADD . /app\r\n",
      "WORKDIR /app\r\n",
      "RUN apt-get -yqq update\r\n",
      "RUN apt-get install -yqq libsm6 libxext6 libxrender-dev\r\n",
      "RUN pip install -r requirements.txt\r\n",
      "RUN python3 run_model.py\r\n",
      "```\r\n",
      "While building the image using the command `sudo nvidia-docker build -t name .` Getting the following error:\r\n",
      "\r\n",
      "```\r\n",
      "2021-01-06 17:27:26.453415: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n",
      "2021-01-06 17:27:26.476869: E tensorflow/stream_executor/cuda/cuda_driver.cc:397] failed call to cuInit: CUresult(-1)\r\n",
      "2021-01-06 17:27:26.476956: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:152] no NVIDIA GPU device is present: /dev/nvidia0 does not exist\r\n",
      "```\r\n",
      "\r\n",
      "Model is loaded on CPU instead of GPU.\r\n",
      "\r\n",
      "I have followed the instructions from:\r\n",
      "[https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#docker](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#docker\r\n",
      ")\r\n",
      "\r\n",
      "to install the nvidia-container-toolkit and when I run the following command: `sudo docker run --rm --gpus all nvidia/cuda:9.0-base nvidia-smi`\r\n",
      "I see the following output:\r\n",
      "```\r\n",
      "[+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 435.21       Driver Version: 435.21       CUDA Version: 10.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\r\n",
      "| N/A   63C    P0    32W /  70W |      0MiB / 15109MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+]\r\n",
      "```\r\n",
      "\n",
      "issue labels - \n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  Add int8 and int16x8 support for WHERE operator\n",
      "issue body -  Added int8 and int16x8 support for WHERE operator and associated tests\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:lite\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  Add int8 and int16x8 support for BROADCAST_TO operator\n",
      "issue body -  * Added support for quantized broadcast_to operator\r\n",
      "* Added tests for quantizing broadcast_to model\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:lite\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  Running multiple hexagon delegates sequentially. \n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Snapdragon 855 running Android 9\r\n",
      "- TensorFlow installed from (source or binary): source\r\n",
      "- TensorFlow version (use command below): 2.2.0\r\n",
      "- Python version: 3.8\r\n",
      "- Bazel version (if compiling from source): 2.0.0\r\n",
      "- GCC/Compiler version (if compiling from source): 7.5.0\r\n",
      "- CUDA/cuDNN version: N/A\r\n",
      "- GPU model and memory: N/A\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "\r\n",
      "Right now, I have two instances of TfLite interpreter created using the same tflite model. Some of the network is delegated to DSP using hexagon delegate. Both interpreters are initialized one after other. Then, Invoke() is called many times, on either one or the other interpreter. This works fine. \r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "\r\n",
      "What happens if I create two instances of tfliteInterpreter from different models, both using hexagon delegate? Will this work as expected? \r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.2\n",
      "comp:lite\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  [ROCm] Updating XLA custom_call_test to enable it for the ROCm platform\n",
      "issue body -  --------------------------\r\n",
      "\r\n",
      "/cc @chsigg @cheshire @nvining-work \n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:gpu\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  [ROCm] Raising the memory allocation cap for GPU unit tests from 1GB to 2GB\n",
      "issue body -  This PR/commit updates the `parallel_gpu_execute.sh` script to raise the GPU memory allocation cap from 1GB to 2GB when running unit-tests.\r\n",
      "\r\n",
      "Recently a couple of unit tests started failing on the ROCm platform because they were running out of memory\r\n",
      "\r\n",
      "```\r\n",
      "//tensorflow/python/kernel_tests:extract_image_patches_grad_test_gpu\r\n",
      "//tensorflow/python/ops/numpy_ops:np_interop_test_gpu\r\n",
      "```\r\n",
      "\r\n",
      "GPU unit tests (atleast on the ROCm platform) are run with a cap that is set and implemented as shown here :\r\n",
      "\r\n",
      "* https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/gpu_build/parallel_gpu_execute.sh#L26-L32\r\n",
      "* https://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/stream_executor_pimpl.cc#L130-L137\r\n",
      "* https://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/stream_executor_pimpl.cc#L151\r\n",
      "* https://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/stream_executor_pimpl.cc#L487-L503\r\n",
      "\r\n",
      "It does not seem that the `parallel_gpu_execute.sh` is being used on the CUDA platform (anymore...think it was in the past). There does not seem to be any reference to it in the `Invocation Details` tab of the `Linux GPU` CI job.\r\n",
      "\r\n",
      "for e.g - https://source.cloud.google.com/results/invocations/09d63e6a-f7a9-4fc6-9708-2fdd40b8b193/details\r\n",
      "\r\n",
      "It also does not seem that GPU unit tests on the CUDA platform are being subjected to the 1GB memory cap. This can be verified by looking the at `Target Log` for the `//tensorflow/python/ops/numpy_ops:np_interop_test_gpu` test in the `Linux GPU` CI job (actually any GPU unit test)\r\n",
      "\r\n",
      "for e.g. - https://source.cloud.google.com/results/invocations/09d63e6a-f7a9-4fc6-9708-2fdd40b8b193/targets/%2F%2Ftensorflow%2Fpython%2Fops%2Fnumpy_ops:np_interop_test_gpu/log\r\n",
      "\r\n",
      "On the ROCm platform, we see the following log messages which are generated as a consequence of the memory cap (when TF tried to grab the entire available GPU memory on startup)\r\n",
      "\r\n",
      "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/stream_executor_pimpl.cc#L488-L494\r\n",
      "\r\n",
      "```\r\n",
      " W tensorflow/stream_executor/stream_executor_pimpl.cc:490] Not enough memory to allocate 16133306368 on device 0 within provided limit. [used=0, limit=1073741824]\r\n",
      " W tensorflow/stream_executor/stream_executor_pimpl.cc:490] Not enough memory to allocate 14519974912 on device 0 within provided limit. [used=0, limit=1073741824]\r\n",
      " W tensorflow/stream_executor/stream_executor_pimpl.cc:490] Not enough memory to allocate 13067976704 on device 0 within provided limit. [used=0, limit=1073741824]\r\n",
      " W tensorflow/stream_executor/stream_executor_pimpl.cc:490] Not enough memory to allocate 11761178624 on device 0 within provided limit. [used=0, limit=1073741824]\r\n",
      "...\r\n",
      "...\r\n",
      "...\r\n",
      "```\r\n",
      "\r\n",
      "These messsage are not present in unit tests logs for `Linux GPU` CI job, which seems to suggest that the env var `TF_PER_DEVICE_MEMORY_LIMIT_MB` is not set when the unit tests are run. Either that or the GPU on which the tests are being run has 1GB total memory which is unlikely.\r\n",
      "\r\n",
      "--------------------------------------------------\r\n",
      "\r\n",
      "/cc @chsigg @cheshire @nvining-work \r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:gpu\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  Tensorflow Lite Micro micro_speech example suppress warning on MacOS build\n",
      "issue body -  When building for MacOS an error occurs when using the built in Apple clang (mentioned in https://github.com/tensorflow/tensorflow/issues/46218). The built in Apple clang version:\r\n",
      "\r\n",
      "```\r\n",
      "Configured with: --prefix=/Library/Developer/CommandLineTools/usr --with-gxx-include-dir=/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include/c++/4.2.1\r\n",
      "Apple clang version 12.0.0 (clang-1200.0.32.27)\r\n",
      "Target: x86_64-apple-darwin19.6.0\r\n",
      "Thread model: posix\r\n",
      "InstalledDir: /Library/Developer/CommandLineTools/usr/bin\r\n",
      "```\r\n",
      "\r\n",
      "This fix selectively suppresses the warning to work on MacOS. Alternatively, the below works on the same Clang version\r\n",
      "\r\n",
      "```\r\n",
      "AudioStreamBasicDescription recordFormat = { };\r\n",
      "```\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  [ROCm] Fix for breakage in XLA Conv Op functionality\n",
      "issue body -  The following commit breaks Conv Op functionality (in the XLA backend) for ROCm platform.\r\n",
      "\r\n",
      "https://github.com/tensorflow/tensorflow/commit/8684c6b2e95601542c6c5c006bde5dd50f589a50\r\n",
      "\r\n",
      "The cause seems to be that the `scratch_size` field in the new `GpuConvDescriptor` is not getting correctly populated in the new MLIR path. It is being used correctly in the convolution runner code.\r\n",
      "\r\n",
      "declaration:\r\n",
      "\r\n",
      "https://github.com/tensorflow/tensorflow/commit/8684c6b2e95601542c6c5c006bde5dd50f589a50#diff-6453912dbc4ee715a56da9d7b218b52795dea2aa631a482101fc6d58c573d9ccR122-R135\r\n",
      "\r\n",
      "use (get access) in conv runner:\r\n",
      "\r\n",
      "https://github.com/tensorflow/tensorflow/commit/8684c6b2e95601542c6c5c006bde5dd50f589a50#diff-a01181d08b28a9c7432f22439622f16725126184283a73822c70b2151098a8adR277\r\n",
      "\r\n",
      "set access in non-MLIR(?) based path:\r\n",
      "\r\n",
      "https://github.com/tensorflow/tensorflow/commit/8684c6b2e95601542c6c5c006bde5dd50f589a50#diff-a01181d08b28a9c7432f22439622f16725126184283a73822c70b2151098a8adR450\r\n",
      "\r\n",
      "This commit merely adds the missing \"set\" in the MLIR based path\r\n",
      "\r\n",
      "------------------------------------------\r\n",
      "\r\n",
      "thanks to @ekuznetsov139 for identifying the fix\r\n",
      "\r\n",
      "/cc @chsigg @cheshire @nvining-work \r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:gpu\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  micro_speech: Run on macOS make error\n",
      "issue body -  @tensorflow/micro\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Catalina 10.15.7\r\n",
      "- TensorFlow installed from (source or binary): binary\r\n",
      "- Tensorflow version (commit SHA if source):\r\n",
      "- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): macOS\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "Error when running\r\n",
      "```\r\n",
      "make -f tensorflow/lite/micro/tools/make/Makefile micro_speech\r\n",
      "```\r\n",
      "\r\n",
      "Error message:\r\n",
      "```\r\n",
      "tensorflow/lite/micro/examples/micro_speech/osx/audio_provider.cc:64:48: error: missing field 'mFormatID' initializer [-Werror,-Wmissing-field-initializers]\r\n",
      "  AudioStreamBasicDescription recordFormat = {0};\r\n",
      "                                               ^\r\n",
      "1 error generated.\r\n",
      "gmake: *** [tensorflow/lite/micro/tools/make/Makefile:623: tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/examples/micro_speech/osx/audio_provider.o] Error 1\r\n",
      "```\r\n",
      "\r\n",
      "**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n",
      "\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "comp:lite\n",
      "comp:micro\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Extract reference for operator LEAKY_RELU to standalone header\n",
      "issue body -  Move the reference implementation to its own header so that micro\r\n",
      "can use it without the unrelated depedencies of reference_ops.h.\r\n",
      "\r\n",
      "PR step 2 for issue #46161\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:lite\n",
      "comp:micro\n",
      "prtype:bugfix\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  Extract a function for parsing operator LEAKY_RELU\n",
      "issue body -  Extract the parsing out of a switch statement case to create a\r\n",
      "standalone function which can be called by the micro op resolver.\r\n",
      "\r\n",
      "PR step 1 for issue #46161\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:lite\n",
      "comp:micro\n",
      "prtype:bugfix\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  micro: prepare to port operator LEAKY_RELU kernel from lite with test\n",
      "issue body -  Implement skeleton (non-working) code for operator and test.\r\n",
      "Header files changed.\r\n",
      "Namespaces changed.\r\n",
      "Some original code deleted.\r\n",
      "Some original code modified.\r\n",
      "\r\n",
      "PR step 4 of the work to port operator LEAKY_RELU as tracked in Issue #46161\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:lite\n",
      "comp:micro\n",
      "prtype:bugfix\n",
      "ready to pull\n",
      "size:L\n",
      "\n",
      "\n",
      "issue title -  micro: copy operator LEAKY_RELU kernel from lite\n",
      "issue body -  This is a copy with minimal modification of the kernel and test for\r\n",
      "operator LEAKY_RELU from tensorflow/lite/kernels.\r\n",
      "Adaptations to micro and addition to the micro build to follow.\r\n",
      "\r\n",
      "PR step 3 for issue #46161\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:lite\n",
      "comp:micro\n",
      "prtype:bugfix\n",
      "ready to pull\n",
      "size:L\n",
      "\n",
      "\n",
      "issue title -  Many errors in the Example of tf.feature_column.categorical_column_with_vocabulary_file\n",
      "issue body -  ## URL(s) with the issue:\r\n",
      "\r\n",
      "Please provide a link to the documentation entry, for example:\r\n",
      "https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file\r\n",
      "\r\n",
      "## Description of issue (what needs changing):\r\n",
      "In the sentence, \r\n",
      "\r\n",
      "> Use either (but not both) of num_oov_buckets and default_value to specify how to include out-of-vocabulary values.\r\n",
      "\r\n",
      "since **either** is used, **or** should be used instead of **and**.\r\n",
      "\r\n",
      "When running the **`Example Code`**, it is resulting in the errors mentioned below:\r\n",
      "\r\n",
      "1. \r\n",
      "\r\n",
      "> NameError: name 'categorical_column_with_vocabulary_file' is not defined\r\n",
      "\r\n",
      "\r\n",
      "2. \r\n",
      "\r\n",
      "> NameError: name 'linear_model' is not defined\r\n",
      "\r\n",
      "3. \r\n",
      "\r\n",
      "> ValueError: All feature_columns must be FeatureColumn instances. Given: Ellipsis\r\n",
      "\r\n",
      "4. \r\n",
      "\r\n",
      "> NameError: name 'input_layer' is not defined\r\n",
      "\r\n",
      "The code in the documentation should be modified to fix all the above errors. \r\n",
      "\r\n",
      "Please find the [Github Gist](https://colab.research.google.com/gist/rmothukuru/74b353ebc7046dee6220cb158f25e5bc/categorical_column_with_vocabulary_file_error.ipynb) demonstrating the errors.\n",
      "issue labels - \n",
      "comp:apis\n",
      "type:docs-bug\n",
      "\n",
      "\n",
      "issue title -  Control dependency doesn't work in distribute MirroredStrategy\n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `Ubuntu 20.04 LTS` with `nvcr.io/nvidia/tensorflow:19.12-tf1-py3`\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n",
      "- TensorFlow installed from (source or binary): none\r\n",
      "- TensorFlow version (use command below): `unknown 1.15.0`\r\n",
      "- Python version: `3.6.9`\r\n",
      "- Bazel version (if compiling from source): none\r\n",
      "- GCC/Compiler version (if compiling from source): none\r\n",
      "- CUDA/cuDNN version: CUDA 10.2.89, cuDNN 7.6.5\r\n",
      "- GPU model and memory: 2x GeForce GTX 1080 Ti with 11178 MiB fb memory\r\n",
      "```\r\n",
      "        GPU0    GPU1    CPU Affinity\r\n",
      "GPU0     X      PHB     0-11\r\n",
      "GPU1    PHB      X      0-11\r\n",
      "```\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "It seems that `tf.control_dependencies` is useless in the `load_fn`, which is called by a `MirroredStrategy`.\r\n",
      "The wall times of `load` and `ret` differ greatly with each other.\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "The wall times of `load` and `ret` is comparable.\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "```python\r\n",
      "import time\r\n",
      "import tensorflow as tf\r\n",
      "\r\n",
      "SHAPE = [2048, 2048]\r\n",
      "STEP = 100\r\n",
      "WARMUP = 100\r\n",
      "\r\n",
      "_LOAD_OPS = list()\r\n",
      "\r\n",
      "\r\n",
      "def load_fn():\r\n",
      "    v = tf.get_variable('v', shape=SHAPE, initializer=tf.ones_initializer)\r\n",
      "    m = tf.matmul(v, v)\r\n",
      "    _LOAD_OPS.append(m.op)\r\n",
      "    with tf.control_dependencies([m.op]):\r\n",
      "        return tf.constant(1.0)\r\n",
      "\r\n",
      "\r\n",
      "def bench(name, session, ops):\r\n",
      "      stime = time.time()\r\n",
      "      for _ in range(STEP):\r\n",
      "          session.run(ops)\r\n",
      "      etime = time.time() - stime\r\n",
      "      print('{:6s} takes {:.3e} sec.'.format(name, etime))\r\n",
      "    \r\n",
      "\r\n",
      "def main():\r\n",
      "    strategy = tf.distribute.MirroredStrategy()\r\n",
      "    with strategy.scope():\r\n",
      "        r = strategy.experimental_run_v2(load_fn)\r\n",
      "        r = strategy.reduce(tf.distribute.ReduceOp.SUM, r)\r\n",
      "\r\n",
      "    with tf.train.MonitoredSession() as mon_sess:\r\n",
      "        for _ in range(WARMUP):\r\n",
      "            mon_sess.run([_LOAD_OPS, r, []])\r\n",
      "\r\n",
      "        bench('load', mon_sess, _LOAD_OPS)\r\n",
      "        bench('ret',  mon_sess, r)\r\n",
      "        bench('null', mon_sess, [])\r\n",
      "\r\n",
      "\r\n",
      "if __name__ == '__main__':\r\n",
      "    main()\r\n",
      "```\r\n",
      "\r\n",
      "**Other info / logs**\r\n",
      "The non-trivial output is\r\n",
      "```\r\n",
      "load   takes 8.350e-01 sec.\r\n",
      "ret    takes 3.153e-02 sec.\r\n",
      "null   takes 4.847e-03 sec.\r\n",
      "```\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:dist-strat\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Tensorflow-gpu in RTX3070\n",
      "issue body -  **System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): WIndows 10 64bit\r\n",
      "- TensorFlow installed from (source or binary): pip install tensorflow-gpu\r\n",
      "- TensorFlow version: tensorflow-gpu 2.4.0\r\n",
      "- Python version: Python 3.7\r\n",
      "- Installed using virtualenv? pip? conda?: pip\r\n",
      "- CUDA/cuDNN version: CUDA11, cuDNN 8.0.5\r\n",
      "- GPU model and memory: RTX3070 8GB OC\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "When I run the training using GPU, it raise me an error saying that my cuDNN is not initialize properly. I've no idea on how to solve it. I did tried with CUDA 11 with cuDNN 8.0.4 and 8.0.3 but keep raising the same problem.\r\n",
      "\r\n",
      "\r\n",
      "**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n",
      "`tf.config.list_physical_devices('GPU')` --> \"Physical Device GPU 0\"\r\n",
      "`tf.test.is_built_with_cuda()` --> True\r\n",
      "\r\n",
      "\r\n",
      "**Log**\r\n",
      "![Capture](https://user-images.githubusercontent.com/52826239/103755378-b1747980-5048-11eb-8bc0-f5dfc10d69c4.PNG)\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "stat:awaiting response\n",
      "subtype:windows\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  micro: copy operator ADD_N kernel from lite\n",
      "issue body -  This is a copy with minimal modification of the kernel and test for\r\n",
      "operator ADD_N from tensorflow/lite/kernels.\r\n",
      "Adaptations to micro and addition to the micro build to follow.\r\n",
      "\r\n",
      "PR step 3 for issue #46162\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:lite\n",
      "comp:micro\n",
      "prtype:bugfix\n",
      "ready to pull\n",
      "size:L\n",
      "\n",
      "\n",
      "issue title -  Update custom_gradient.py\n",
      "issue body -  fix typo in doc string\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:ops\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Many errors in the Example of tf.feature_column.categorical_column_with_identity\n",
      "issue body -  ## URL(s) with the issue:\r\n",
      "\r\n",
      "Please provide a link to the documentation entry, for example:\r\n",
      "https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_identity#linear_model\r\n",
      "\r\n",
      "## Description of issue (what needs changing):\r\n",
      "When running the **`Example Code`**, it is resulting in the errors mentioned below:\r\n",
      "\r\n",
      "1. \r\n",
      "\r\n",
      "> NameError: name 'categorical_column_with_identity' is not defined\r\n",
      "\r\n",
      "\r\n",
      "2. \r\n",
      "\r\n",
      "> NameError: name 'linear_model' is not defined\r\n",
      "\r\n",
      "3. \r\n",
      "\r\n",
      "> ValueError: All feature_columns must be FeatureColumn instances. Given: Ellipsis\r\n",
      "\r\n",
      "4. \r\n",
      "\r\n",
      "> NameError: name 'input_layer' is not defined\r\n",
      "\r\n",
      "The code in the documentation should be modified to fix all the above errors. \r\n",
      "\r\n",
      "Please find the [Github Gist](https://colab.research.google.com/gist/rmothukuru/a05819cc660a7b677e53b189affbe4d0/categorical_column_with_identity_error.ipynb) demonstrating the errors.\n",
      "issue labels - \n",
      "comp:apis\n",
      "type:docs-bug\n",
      "\n",
      "\n",
      "issue title -  Extract a function for parsing operator ADD_N\n",
      "issue body -  Extract the parsing out of a switch statement case to create a\r\n",
      "standalone function which can be called by the micro op resolver.\r\n",
      "\r\n",
      "PR step 1 for issue #46162\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:lite\n",
      "comp:micro\n",
      "prtype:bugfix\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  Many errors in the Example of tf.feature_column.categorical_column_with_hash_bucket\n",
      "issue body -  ## URL(s) with the issue:\r\n",
      "\r\n",
      "Please provide a link to the documentation entry, for example:\r\n",
      "https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_hash_bucket\r\n",
      "\r\n",
      "## Description of issue (what needs changing):\r\n",
      "When running the **`Example Code`**, it is resulting in the errors mentioned below:\r\n",
      "\r\n",
      "1. \r\n",
      "\r\n",
      "> File \"<ipython-input-1-e0419158b389>\", line 3\r\n",
      ">     keywords = categorical_column_with_hash_bucket(\"keywords\", 10K)\r\n",
      ">                                                                  ^\r\n",
      "> SyntaxError: invalid syntax\r\n",
      "\r\n",
      "2. \r\n",
      "\r\n",
      "> NameError: name 'categorical_column_with_hash_bucket' is not defined\r\n",
      "\r\n",
      "3. \r\n",
      "\r\n",
      "> NameError: name 'linear_model' is not defined\r\n",
      "\r\n",
      "4. \r\n",
      "\r\n",
      "> NameError: name 'input_layer' is not defined\r\n",
      "\r\n",
      "5. \r\n",
      "\r\n",
      "> ValueError: All feature_columns must be FeatureColumn instances. Given: Ellipsis\r\n",
      "\r\n",
      "The code in the documentation should be modified to fix all the above errors. \r\n",
      "\r\n",
      "Please find the [Github Gist](https://colab.research.google.com/gist/rmothukuru/eba74b40ed40964c730e4e667ed0c8f1/categorical_column_with_hash_bucket_error.ipynb) demonstrating the errors.\n",
      "issue labels - \n",
      "comp:apis\n",
      "type:docs-bug\n",
      "\n",
      "\n",
      "issue title -  Extract reference for operator ADD_N to standalone header\n",
      "issue body -  Move the reference implementation to its own header so that micro\r\n",
      "can use it without the unrelated depedencies of reference_ops.h.\r\n",
      "\r\n",
      "PR step 2 for issue #46162\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:lite\n",
      "comp:micro\n",
      "prtype:bugfix\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  Failed invoke tflite model in swift code(Provided data count 18181 must match the required count 4536.)\n",
      "issue body -  **System information**\r\n",
      "- OS Platform): macOS 11.1\r\n",
      "- TensorFlow version: 2.4.0\r\n",
      "\r\n",
      "Input shape of my tflite model - (18, 63) or 1134 float numbers.\r\n",
      "\r\n",
      "I get the data itself in objective-c code, and then I send it to swift code and an error already occurs there. In detail I do the following\r\n",
      "\r\n",
      "1. Receive the data in NSMutableArray. The length of the array is 1134 NSNumber\r\n",
      "2. Converting NSMutableArray to NSData\r\n",
      "```\r\n",
      "NSData *d = [NSKeyedArchiver archivedDataWithRootObject:_data];\r\n",
      "NSLog(@\"output: %@\", d);\r\n",
      "// output: {length = 18181, bytes = 0x62706c69 73743030 d4000100 02000300 ... 00000000 000034fd }\r\n",
      "```\r\n",
      "\r\n",
      "3. I send data to the swift code.\r\n",
      "```[_model predict:d];```\r\n",
      "Swift code:\r\n",
      "```\r\n",
      "@objc public func predict(_ data: NSData) {\r\n",
      "        guard\r\n",
      "          let modelPath = Bundle.main.path(forResource: \"model\", ofType: \"tflite\")\r\n",
      "        else {\r\n",
      "            return\r\n",
      "        }\r\n",
      "\r\n",
      "        do {\r\n",
      "          let interpreter = try Interpreter(modelPath: modelPath)\r\n",
      "          try interpreter.allocateTensors()\r\n",
      "          let inputData: Data = data as Data\r\n",
      "\r\n",
      "          try interpreter.copy(inputData, toInputAt: 0) // <-- an error occurs in this line\r\n",
      "\r\n",
      "          try interpreter.invoke()\r\n",
      "          let outputTensor = try interpreter.output(at: 0)\r\n",
      "        } catch {\r\n",
      "          print(error)\r\n",
      "        }\r\n",
      "}\r\n",
      "```\r\n",
      "4. I get an error in the above line of code. Error:\r\n",
      "```Provided data count 18181 must match the required count 4536.```\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:lite\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Move CMSIS downloads to a separate script.\n",
      "issue body -  This is a first step towards adding back some of the CMSIS patching needed to fix the Arduino build (http://b/175435756).\r\n",
      "\r\n",
      "Moving to a stand-alone download script is part of the cleanup described in http://b/143904317\r\n",
      "\r\n",
      "Also, this change introduces a common location for helper functions and fixes #46020\r\n",
      "\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "prtype:bugfix\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  Error in example because of Incomplete API names for tf.keras.experimental.SequenceFeatures\n",
      "issue body -  ## URL(s) with the issue:\r\n",
      "\r\n",
      "Please provide a link to the documentation entry, for example:\r\n",
      "https://www.tensorflow.org/api_docs/python/tf/keras/experimental/SequenceFeatures#example\r\n",
      "\r\n",
      "## Description of issue (what needs changing): \r\n",
      "The APIs, **`sequence_numeric_column, sequence_categorical_column_with_identity, embedding_column`**, etc.. are incomplete. Consequently, it is resulting in the error, \r\n",
      "\r\n",
      "> NameError: name 'sequence_numeric_column' is not defined\r\n",
      "\r\n",
      "It should be **`tf.feature_column.sequence_numeric_column, tf.feature_column.sequence_categorical_column_with_identity, tf.feature_column.embedding_column`**, instead.\r\n",
      "\r\n",
      "Please find the [Github Gist](https://colab.research.google.com/gist/rmothukuru/90002755d1b45f6f94709a9623259d9f/sequencefeatures_error.ipynb).\r\n",
      "\r\n",
      "There is an error in the above [Gist](https://colab.research.google.com/gist/rmothukuru/90002755d1b45f6f94709a9623259d9f/sequencefeatures_error.ipynb) because of **`Ellipsis`** and it is being tracked in #46128.\n",
      "issue labels - \n",
      "comp:apis\n",
      "type:docs-bug\n",
      "\n",
      "\n",
      "issue title -  Add `.bazelrc` to `.gitignore`\n",
      "issue body -  **System information**\r\n",
      "- TensorFlow version (you are using): 2.4\r\n",
      "- Are you willing to contribute it (Yes/No): Yes\r\n",
      "\r\n",
      "**Describe the feature and the current behavior/state.**\r\n",
      "\r\n",
      "Currently, we use `configure.py` for modifying `.bazelrc`, I think we should add `.bazelrc` to `.gitignore` and use `configure.py` for completely generating `bazelrc` from scratch. Afterward, we could add our own config to `.bazelrc` without any conflict with the upstream ( I usually add `build:asan ----copt=-fsanitize=address --linkopt=-fsanitize=address` ). Furthermore, we could generate a platform-specific `.bazelrc` using `python` ( which is good, I think ).\r\n",
      "\r\n",
      "**Will this change the current api? How?** Nothing\r\n",
      "\r\n",
      "**Who will benefit with this feature?** TensorFlow developers.\r\n",
      "\r\n",
      "/cc: @mihaimaruseac \r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "type:feature\n",
      "\n",
      "\n",
      "issue title -  micro: Add Sony Spresense board target\n",
      "issue body -  Add build terget on Sony Spresense board.\r\n",
      "To build it, Spresense SDK is required.\r\n",
      "And hello_world, micro_speech and preson_detection are added for the board.\r\n",
      "\r\n",
      "The related issue is #46240 \n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "size:L\n",
      "\n",
      "\n",
      "issue title -  Path fix for local build of openssh\n",
      "issue body -  \n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:mkl\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -   PB to Tflite\n",
      "issue body -  import tensorflow as tf\r\n",
      "\r\n",
      "path = 'E:\\\\Code\\\\PythonCode\\\\AliYunCode\\\\JS-CODE-20210104-1008-CNNX-GRU-H64-CTC-C1_0.pb'\r\n",
      "\r\n",
      "inputs = [\"input\"]\r\n",
      "\r\n",
      "outputs = [\"dense_decoded\"]\r\n",
      "\r\n",
      "converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(path, inputs, outputs)\r\n",
      "\r\n",
      "converter.post_training_quantize = True\r\n",
      "\r\n",
      "tflite_model = converter.convert()\r\n",
      "\r\n",
      "open(\"tiny_160000.tflite\", \"wb\").write(tflite_model)\r\n",
      "\r\n",
      "-----\r\n",
      "ERROR:Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, CONV_2D, DEPTHWISE_CONV_2D, FULLY_CONNECTED, GATHER, LEAKY_RELU, LESS, LOGICAL_AND, MAXIMUM, MAX_POOL_2D, MINIMUM, MUL, NOT_EQUAL, RANGE, REDUCE_ANY, RESHAPE, SELECT, SHAPE, SPARSE_TO_DENSE, STRIDED_SLICE, SUB, TANH, TILE, TRANSPOSE, ZEROS_LIKE. Here is a list of operators for which you will need custom implementations: CTC_BEAM_SEARCH_DECODER, Enter, Exit, LoopCond, Merge, Switch, TensorArrayGatherV3, TensorArrayReadV3, TensorArrayScatterV3, TensorArraySizeV3, TensorArrayV3, TensorArrayWriteV3.\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "comp:lite\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  [Dockerfile] move stubs to the end of LD_LIBRARY_PATH\n",
      "issue body -  This lands PR #44732.\n",
      "issue labels - \n",
      "cla: yes\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  Add target and pass_string parameters to the renode test script to fix #46186\n",
      "issue body -  This allows the `test_with_renode.sh` script to be called via the makefile with all the parameters needed to run on a given target, while also staying consistent with the other test scripts.\r\n",
      "\r\n",
      "As a result of this change, the makefile is passing in the following parameters to the test scripts:\r\n",
      " param 1 - test binary path\r\n",
      " param 2 - string to determine that test passes\r\n",
      " param 3 - target\r\n",
      "\r\n",
      "Parameter 3 is only used for `test_with_renode.sh`\r\n",
      "\r\n",
      "Manually tested that the following commands now pass:\r\n",
      "```\r\n",
      "make -f tensorflow/lite/micro/tools/make/Makefile TARGET=bluepill test_kernel_add_test\r\n",
      "make -f tensorflow/lite/micro/tools/make/Makefile TARGET=stm32f4 TAGS=cmsis-nn test_kernel_fully_connected_test\r\n",
      "```\r\n",
      "\r\n",
      "Fixes #46186\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  running a single test with renode is broken.\n",
      "issue body -  @tensorflow/micro\r\n",
      "\r\n",
      "While the following command passes:\r\n",
      "```\r\n",
      "make -f tensorflow/lite/micro/tools/make/Makefile TARGET=bluepill test\r\n",
      "```\r\n",
      "\r\n",
      "Running a single test with renode (for example):\r\n",
      "```bash\r\n",
      "make -f tensorflow/lite/micro/tools/make/Makefile TARGET=bluepill test_kernel_add_test\r\n",
      "```\r\n",
      "\r\n",
      "fails with:\r\n",
      "```\r\n",
      "tensorflow/lite/micro/testing/test_with_renode.sh tensorflow/lite/micro/tools/make/gen/bluepill_cortex-m3/bin/kernel_add_test '~~~ALL TESTS PASSED~~~'\r\n",
      "tensorflow/lite/micro/testing/test_with_renode.sh: line 69: $ROBOT_SCRIPT: ambiguous redirect\r\n",
      "make: *** [tensorflow/lite/micro/tools/make/Makefile:663: test_kernel_add_test] Error 1\r\n",
      "```\r\n",
      "\r\n",
      "The reason is that the changes from https://github.com/tensorflow/tensorflow/pull/45787 are incompatible with how the Makefile calls the test script when running an individual test (as opposed to `make test`).\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "comp:lite\n",
      "comp:micro\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Continue removing `-march=native`\n",
      "issue body -  \n",
      "issue labels - \n",
      "cla: yes\n",
      "\n",
      "\n",
      "issue title -  Remove `-march=native`. Testing for #45744, #45866, #44701\n",
      "issue body -  \n",
      "issue labels - \n",
      "cla: yes\n",
      "\n",
      "\n",
      "issue title -  tflite-nnapi Handle version 2 of transpose conv\n",
      "issue body -  We should support version 2 which contains INT8 instead of float\r\n",
      "in nnapi. Update the verification to version 2\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:lite\n",
      "prtype:bugfix\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Update the style checks to also include the reference kernels.\n",
      "issue body -  Prior to this change we were only looking at code within the tensorflow/lite/micro directory. However, TFLM does share the reference implementations with Lite and so we are also checking for license and formatting in those files.\r\n",
      "\r\n",
      "This should help detect errors faster, for example, PR https://github.com/tensorflow/tensorflow/pull/45814 missed adding a license to the header but that wasn't detected until the PR was imported internally.\r\n",
      "\r\n",
      "See http://b/169948621 and http://b/175315163 for more details.\r\n",
      "\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  Fix release notes\n",
      "issue body -  There were a few items left straggling in #44220, getting ready for an eventual 2.4 patch release\n",
      "issue labels - \n",
      "cla: yes\n",
      "waiting for patch release\n",
      "\n",
      "\n",
      "issue title -  Apple M1 chip - illegal hardware instruction\n",
      "issue body -  **System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Big Sur 11.0.1\r\n",
      "- TensorFlow installed from (source or binary): https://www.tensorflow.org/install/pip\r\n",
      "- TensorFlow version: Latest stable (pip)\r\n",
      "- Python version: Python 3.8.5\r\n",
      "- Installed using virtualenv? pip? conda?: exactly like in the instruction https://www.tensorflow.org/install/pip\r\n",
      "- Bazel version (if compiling from source): /\r\n",
      "- GCC/Compiler version (if compiling from source): /\r\n",
      "- CUDA/cuDNN version: /\r\n",
      "- GPU model and memory: /\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "\r\n",
      "After running the verification steps \r\n",
      "```\r\n",
      "python -c \"import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"\r\n",
      "```\r\n",
      "\r\n",
      "The following error appear\r\n",
      "```\r\n",
      "illegal hardware instruction\r\n",
      "```\r\n",
      "\r\n",
      "\r\n",
      "**Any other info / logs**\r\n",
      "This issue only appear on the Mac with the Apple M1 chip. The same setup procedure https://www.tensorflow.org/install/pip works fine on my other Mac`s.\n",
      "issue labels - \n",
      "stat:awaiting response\n",
      "subtype:macOS\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  TFX ML Metadata storage to PostgreSQL\n",
      "issue body -  I wanted to store my metadata information of ML artifacts to PostgreSQL but I couldn't find any libraries that support MLMD for PostgreSQL. Is there any API's that could support to PostgreSQL other than MySQL/SQLite?\n",
      "issue labels - \n",
      "stat:awaiting response\n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -   AttributeError: module 'tensorflow' has no attribute 'data'\n",
      "issue body -  Installing on a Windows 10 PC using CPU (no GPU)\r\n",
      "Working in an Anaconda environment\r\n",
      "\r\n",
      "Have previously used TF 1.5, but now for NLP work I needed to move to the new generation\r\n",
      "\r\n",
      "I am using Python 3.6 installed tensorflow 2.2. Received the same issue with tensorflow 2.1\r\n",
      "\r\n",
      "Running the line:\r\n",
      "text_ds = tf.data.Dataset.from_tensor_slices(train_samples).batch(128)\r\n",
      "\r\n",
      "I get:\r\n",
      "**_**Traceback (most recent call last):\r\n",
      "\r\n",
      "  File \"<ipython-input-34-3c9c34f69f7e>\", line 2, in <module>\r\n",
      "    text_ds = tf.data.Dataset.from_tensor_slices(train_samples).batch(128)\r\n",
      "\r\n",
      "AttributeError: module 'tensorflow' has no attribute 'data'**_**\r\n",
      "\r\n",
      "AttributeError: module 'tensorflow' has no attribute 'data'\r\n",
      "\r\n",
      "\r\n",
      "It looks like an installation program, so I reinstalled, using different versions. Any help much appreciated\r\n",
      "\r\n",
      "(tensorflow2) C:\\Users\\mehes>conda list\r\n",
      "# packages in environment at C:\\Users\\mehes\\Anaconda3\\envs\\tensorflow2:\r\n",
      "#\r\n",
      "# Name                    Version                   Build  Channel\r\n",
      "_tflow_select             2.2.0                     eigen\r\n",
      "absl-py                   0.11.0           py36ha15d459_0    conda-forge\r\n",
      "aiohttp                   3.7.3            py36h68aa20f_0    conda-forge\r\n",
      "alabaster                 0.7.12                     py_0    conda-forge\r\n",
      "appdirs                   1.4.4              pyh9f0ad1d_0    conda-forge\r\n",
      "argh                      0.26.2          pyh9f0ad1d_1002    conda-forge\r\n",
      "astor                     0.8.1              pyh9f0ad1d_0    conda-forge\r\n",
      "astroid                   2.4.2            py36h9f0ad1d_1    conda-forge\r\n",
      "async-timeout             3.0.1                   py_1000    conda-forge\r\n",
      "async_generator           1.10                       py_0    conda-forge\r\n",
      "atomicwrites              1.4.0              pyh9f0ad1d_0    conda-forge\r\n",
      "attrs                     20.3.0             pyhd3deb0d_0    conda-forge\r\n",
      "autopep8                  1.5.4              pyh9f0ad1d_0    conda-forge\r\n",
      "babel                     2.9.0              pyhd3deb0d_0    conda-forge\r\n",
      "backcall                  0.2.0              pyh9f0ad1d_0    conda-forge\r\n",
      "backports                 1.0                        py_2    conda-forge\r\n",
      "backports.functools_lru_cache 1.6.1                      py_0    conda-forge\r\n",
      "bcrypt                    3.2.0            py36h779f372_1    conda-forge\r\n",
      "black                     20.8b1                     py_0    conda-forge\r\n",
      "bleach                    3.2.1              pyh9f0ad1d_0    conda-forge\r\n",
      "blinker                   1.4                        py_1    conda-forge\r\n",
      "brotlipy                  0.7.0           py36hc753bc4_1001    conda-forge\r\n",
      "ca-certificates           2020.12.5            h5b45459_0    conda-forge\r\n",
      "cached-property           1.5.1                      py_0    conda-forge\r\n",
      "cachetools                4.1.1                      py_0    conda-forge\r\n",
      "certifi                   2020.12.5        py36ha15d459_0    conda-forge\r\n",
      "cffi                      1.14.4           py36he58ceb7_1    conda-forge\r\n",
      "chardet                   3.0.4           py36hd36e781_1008    conda-forge\r\n",
      "click                     7.1.2              pyh9f0ad1d_0    conda-forge\r\n",
      "cloudpickle               1.6.0                      py_0    conda-forge\r\n",
      "colorama                  0.4.4              pyh9f0ad1d_0    conda-forge\r\n",
      "cryptography              3.3.1            py36he58ceb7_0    conda-forge\r\n",
      "decorator                 4.4.2                      py_0    conda-forge\r\n",
      "defusedxml                0.6.0                      py_0    conda-forge\r\n",
      "diff-match-patch          20200713           pyh9f0ad1d_0    conda-forge\r\n",
      "docutils                  0.16             py36ha15d459_2    conda-forge\r\n",
      "entrypoints               0.3             pyhd8ed1ab_1003    conda-forge\r\n",
      "flake8                    3.8.4                      py_0    conda-forge\r\n",
      "future                    0.18.2           py36ha15d459_2    conda-forge\r\n",
      "gast                      0.2.2                      py_0    conda-forge\r\n",
      "google-auth               1.24.0             pyhd3deb0d_0    conda-forge\r\n",
      "google-auth-oauthlib      0.4.1                      py_2    conda-forge\r\n",
      "google-pasta              0.2.0              pyh8c360ce_0    conda-forge\r\n",
      "grpcio                    1.34.0           py36h4374274_0    conda-forge\r\n",
      "h5py                      3.1.0           nompi_py36hf359dfe_100    conda-forge\r\n",
      "hdf5                      1.10.6          nompi_h5268f04_1113    conda-forge\r\n",
      "helpdev                   0.7.1              pyhd8ed1ab_0    conda-forge\r\n",
      "icu                       68.1                 h0e60522_0    conda-forge\r\n",
      "idna                      2.10               pyh9f0ad1d_0    conda-forge\r\n",
      "idna_ssl                  1.1.0           py36h9f0ad1d_1001    conda-forge\r\n",
      "imagesize                 1.2.0                      py_0    conda-forge\r\n",
      "importlib-metadata        3.3.0            py36ha15d459_2    conda-forge\r\n",
      "importlib_metadata        3.3.0                hd8ed1ab_2    conda-forge\r\n",
      "intel-openmp              2020.3             h57928b3_311    conda-forge\r\n",
      "intervaltree              3.0.2                      py_0    conda-forge\r\n",
      "ipykernel                 5.4.2            py36h7b7c402_0    conda-forge\r\n",
      "ipython                   7.16.1           py36h7b2dad6_2    conda-forge\r\n",
      "ipython_genutils          0.2.0                      py_1    conda-forge\r\n",
      "isort                     5.7.0              pyhd8ed1ab_0    conda-forge\r\n",
      "jedi                      0.17.2           py36ha15d459_1    conda-forge\r\n",
      "jinja2                    2.11.2             pyh9f0ad1d_0    conda-forge\r\n",
      "jpeg                      9d                   h8ffe710_0    conda-forge\r\n",
      "jsonschema                3.2.0                      py_2    conda-forge\r\n",
      "jupyter_client            6.1.7                      py_0    conda-forge\r\n",
      "jupyter_core              4.7.0            py36ha15d459_0    conda-forge\r\n",
      "jupyterlab_pygments       0.1.2              pyh9f0ad1d_0    conda-forge\r\n",
      "keras-applications        1.0.8                      py_1    conda-forge\r\n",
      "keras-preprocessing       1.1.0                      py_0    conda-forge\r\n",
      "keyring                   21.8.0           py36ha15d459_0    conda-forge\r\n",
      "krb5                      1.17.2               hbae68bd_0    conda-forge\r\n",
      "lazy-object-proxy         1.4.3            py36h779f372_2    conda-forge\r\n",
      "libblas                   3.9.0                     6_mkl    conda-forge\r\n",
      "libcblas                  3.9.0                     6_mkl    conda-forge\r\n",
      "libclang                  11.0.0          default_h5c34c98_2    conda-forge\r\n",
      "libcurl                   7.71.1               h4b64cdc_8    conda-forge\r\n",
      "liblapack                 3.9.0                     6_mkl    conda-forge\r\n",
      "libpng                    1.6.37               h1d00b33_2    conda-forge\r\n",
      "libprotobuf               3.14.0               h7755175_0    conda-forge\r\n",
      "libsodium                 1.0.18               h8d14728_1    conda-forge\r\n",
      "libspatialindex           1.9.3                he025d50_3    conda-forge\r\n",
      "libssh2                   1.9.0                hb06d900_5    conda-forge\r\n",
      "m2w64-gcc-libgfortran     5.3.0                         6    conda-forge\r\n",
      "m2w64-gcc-libs            5.3.0                         7    conda-forge\r\n",
      "m2w64-gcc-libs-core       5.3.0                         7    conda-forge\r\n",
      "m2w64-gmp                 6.1.0                         2    conda-forge\r\n",
      "m2w64-libwinpthread-git   5.0.0.4634.697f757               2    conda-forge\r\n",
      "markdown                  3.3.3              pyh9f0ad1d_0    conda-forge\r\n",
      "markupsafe                1.1.1            py36hc753bc4_2    conda-forge\r\n",
      "mccabe                    0.6.1                      py_1    conda-forge\r\n",
      "mistune                   0.8.4           py36h68aa20f_1002    conda-forge\r\n",
      "mkl                       2020.4             hb70f87d_311    conda-forge\r\n",
      "msys2-conda-epoch         20160418                      1    conda-forge\r\n",
      "multidict                 5.1.0            py36h68aa20f_0    conda-forge\r\n",
      "mypy_extensions           0.4.3            py36ha15d459_2    conda-forge\r\n",
      "nbclient                  0.5.1                      py_0    conda-forge\r\n",
      "nbconvert                 6.0.7            py36ha15d459_3    conda-forge\r\n",
      "nbformat                  5.0.8                      py_0    conda-forge\r\n",
      "nest-asyncio              1.4.3              pyhd8ed1ab_0    conda-forge\r\n",
      "numpy                     1.14.5                   pypi_0    pypi\r\n",
      "numpydoc                  1.1.0                      py_1    conda-forge\r\n",
      "oauthlib                  3.0.1                      py_0    conda-forge\r\n",
      "openssl                   1.1.1i               h8ffe710_0    conda-forge\r\n",
      "opt_einsum                3.3.0                      py_0    conda-forge\r\n",
      "packaging                 20.8               pyhd3deb0d_0    conda-forge\r\n",
      "pandoc                    2.11.3.2             h8ffe710_0    conda-forge\r\n",
      "pandocfilters             1.4.2                      py_1    conda-forge\r\n",
      "paramiko                  2.7.2              pyh9f0ad1d_0    conda-forge\r\n",
      "parso                     0.7.0              pyh9f0ad1d_0    conda-forge\r\n",
      "pathspec                  0.8.1              pyhd3deb0d_0    conda-forge\r\n",
      "pexpect                   4.8.0              pyh9f0ad1d_2    conda-forge\r\n",
      "pickleshare               0.7.5                   py_1003    conda-forge\r\n",
      "pip                       20.3.3             pyhd8ed1ab_0    conda-forge\r\n",
      "pluggy                    0.13.1           py36hd36e781_3    conda-forge\r\n",
      "prompt-toolkit            3.0.8              pyha770c72_0    conda-forge\r\n",
      "protobuf                  3.14.0           py36he2d232f_0    conda-forge\r\n",
      "psutil                    5.8.0            py36h68aa20f_0    conda-forge\r\n",
      "ptyprocess                0.7.0              pyhd3deb0d_0    conda-forge\r\n",
      "pyasn1                    0.4.8                      py_0    conda-forge\r\n",
      "pyasn1-modules            0.2.7                      py_0    conda-forge\r\n",
      "pycodestyle               2.6.0              pyh9f0ad1d_0    conda-forge\r\n",
      "pycparser                 2.20               pyh9f0ad1d_2    conda-forge\r\n",
      "pydocstyle                5.1.1                      py_0    conda-forge\r\n",
      "pyflakes                  2.2.0              pyh9f0ad1d_0    conda-forge\r\n",
      "pygments                  2.7.3              pyhd8ed1ab_0    conda-forge\r\n",
      "pyjwt                     1.7.1                      py_0    conda-forge\r\n",
      "pylint                    2.6.0            py36h9f0ad1d_1    conda-forge\r\n",
      "pyls-black                0.4.6              pyh9f0ad1d_0    conda-forge\r\n",
      "pyls-spyder               0.3.0              pyhd8ed1ab_0    conda-forge\r\n",
      "pynacl                    1.4.0            py36h3a74357_2    conda-forge\r\n",
      "pyopenssl                 20.0.1             pyhd8ed1ab_0    conda-forge\r\n",
      "pyparsing                 2.4.7              pyh9f0ad1d_0    conda-forge\r\n",
      "pyqt                      5.12.3           py36ha15d459_6    conda-forge\r\n",
      "pyqt-impl                 5.12.3           py36he2d232f_6    conda-forge\r\n",
      "pyqt5-sip                 4.19.18          py36he2d232f_6    conda-forge\r\n",
      "pyqtchart                 5.12             py36he2d232f_6    conda-forge\r\n",
      "pyqtwebengine             5.12.1           py36he2d232f_6    conda-forge\r\n",
      "pyreadline                2.1             py36h9f0ad1d_1002    conda-forge\r\n",
      "pyrsistent                0.17.3           py36h68aa20f_1    conda-forge\r\n",
      "pysocks                   1.7.1            py36hd36e781_2    conda-forge\r\n",
      "python                    3.6.12          h39d44d4_0_cpython    conda-forge\r\n",
      "python-dateutil           2.8.1                      py_0    conda-forge\r\n",
      "python-jsonrpc-server     0.4.0              pyh9f0ad1d_0    conda-forge\r\n",
      "python-language-server    0.36.2             pyhd8ed1ab_0    conda-forge\r\n",
      "python_abi                3.6                     1_cp36m    conda-forge\r\n",
      "pytz                      2020.5             pyhd8ed1ab_0    conda-forge\r\n",
      "pywin32                   228              py36h779f372_0    conda-forge\r\n",
      "pywin32-ctypes            0.2.0           py36h9f0ad1d_1002    conda-forge\r\n",
      "pyyaml                    5.3.1            py36hc753bc4_1    conda-forge\r\n",
      "pyzmq                     20.0.0           py36hb0157bd_1    conda-forge\r\n",
      "qdarkstyle                2.8.1              pyhd8ed1ab_2    conda-forge\r\n",
      "qt                        5.12.9               h5909a2a_2    conda-forge\r\n",
      "qtawesome                 1.0.2              pyhd8ed1ab_0    conda-forge\r\n",
      "qtconsole                 5.0.1              pyhd8ed1ab_0    conda-forge\r\n",
      "qtpy                      1.9.0                      py_0    conda-forge\r\n",
      "regex                     2020.11.13       py36h68aa20f_0    conda-forge\r\n",
      "requests                  2.25.1             pyhd3deb0d_0    conda-forge\r\n",
      "requests-oauthlib         1.3.0              pyh9f0ad1d_0    conda-forge\r\n",
      "rope                      0.18.0             pyh9f0ad1d_0    conda-forge\r\n",
      "rsa                       4.6                pyh9f0ad1d_0    conda-forge\r\n",
      "rtree                     0.9.4            py36h089df06_2    conda-forge\r\n",
      "scipy                     1.5.3            py36h7ff6e69_0    conda-forge\r\n",
      "setuptools                49.6.0           py36hd36e781_2    conda-forge\r\n",
      "six                       1.15.0             pyh9f0ad1d_0    conda-forge\r\n",
      "snowballstemmer           2.0.0                      py_0    conda-forge\r\n",
      "sortedcontainers          2.3.0              pyhd8ed1ab_0    conda-forge\r\n",
      "sphinx                    3.4.2              pyhd8ed1ab_0    conda-forge\r\n",
      "sphinxcontrib-applehelp   1.0.2                      py_0    conda-forge\r\n",
      "sphinxcontrib-devhelp     1.0.2                      py_0    conda-forge\r\n",
      "sphinxcontrib-htmlhelp    1.0.3                      py_0    conda-forge\r\n",
      "sphinxcontrib-jsmath      1.0.1                      py_0    conda-forge\r\n",
      "sphinxcontrib-qthelp      1.0.3                      py_0    conda-forge\r\n",
      "sphinxcontrib-serializinghtml 1.1.4                      py_0    conda-forge\r\n",
      "spyder                    4.2.1            py36ha15d459_0    conda-forge\r\n",
      "spyder-kernels            1.10.1           py36ha15d459_0    conda-forge\r\n",
      "sqlite                    3.34.0               h8ffe710_0    conda-forge\r\n",
      "tensorboard               2.1.1                    pypi_0    pypi\r\n",
      "tensorboard-plugin-wit    1.7.0              pyh9f0ad1d_0    conda-forge\r\n",
      "tensorflow                2.1.0           eigen_py36hdbbabfe_0\r\n",
      "tensorflow-base           2.1.0           eigen_py36h49b2757_0\r\n",
      "tensorflow-estimator      2.1.0                    pypi_0    pypi\r\n",
      "termcolor                 1.1.0                      py_2    conda-forge\r\n",
      "testpath                  0.4.4                      py_0    conda-forge\r\n",
      "textdistance              4.2.0              pyhd8ed1ab_0    conda-forge\r\n",
      "three-merge               0.1.1              pyh9f0ad1d_0    conda-forge\r\n",
      "tk                        8.6.10               h8ffe710_1    conda-forge\r\n",
      "toml                      0.10.2             pyhd8ed1ab_0    conda-forge\r\n",
      "tornado                   6.1              py36h68aa20f_0    conda-forge\r\n",
      "traitlets                 4.3.3            py36h9f0ad1d_1    conda-forge\r\n",
      "typed-ast                 1.4.2            py36h68aa20f_0    conda-forge\r\n",
      "typing-extensions         3.7.4.3                       0    conda-forge\r\n",
      "typing_extensions         3.7.4.3                    py_0    conda-forge\r\n",
      "ujson                     4.0.1            py36h003fed8_1    conda-forge\r\n",
      "urllib3                   1.26.2             pyhd8ed1ab_0    conda-forge\r\n",
      "vc                        14.2                 hb210afc_2    conda-forge\r\n",
      "vs2015_runtime            14.28.29325          h5e1d092_0    conda-forge\r\n",
      "watchdog                  1.0.2            py36ha15d459_0    conda-forge\r\n",
      "wcwidth                   0.2.5              pyh9f0ad1d_2    conda-forge\r\n",
      "webencodings              0.5.1                      py_1    conda-forge\r\n",
      "werkzeug                  0.16.1                     py_0    conda-forge\r\n",
      "wheel                     0.36.2             pyhd3deb0d_0    conda-forge\r\n",
      "win_inet_pton             1.1.0            py36h9f0ad1d_1    conda-forge\r\n",
      "wincertstore              0.2             py36h9f0ad1d_1005    conda-forge\r\n",
      "wrapt                     1.11.2           py36h779f372_1    conda-forge\r\n",
      "yaml                      0.2.5                he774522_0    conda-forge\r\n",
      "yapf                      0.30.0             pyh9f0ad1d_0    conda-forge\r\n",
      "yarl                      1.6.3            py36h68aa20f_0    conda-forge\r\n",
      "zeromq                    4.3.3                h0e60522_3    conda-forge\r\n",
      "zipp                      3.4.0                      py_0    conda-forge\r\n",
      "zlib                      1.2.11            h62dcd97_1010    conda-forge\n",
      "issue labels - \n",
      "TF 2.1\n",
      "comp:data\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Failed to build tensorflow 2.3.0 from source on Nvidia Jetson Nano\n",
      "issue body -  <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Jetpack 4.4 (Ubuntu 18.04 LTS)\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n",
      "- TensorFlow installed from (source or binary): source\r\n",
      "- TensorFlow version: 2.3.0\r\n",
      "- Python version: 2.7.17 and 3.6.9\r\n",
      "- Installed using virtualenv? pip? conda?: N/A\r\n",
      "- Bazel version (if compiling from source): Build label: 3.1.0- (@non-git)\r\n",
      "- GCC/Compiler version (if compiling from source): (Ubuntu/Linaro 7.5.0-3ubuntu1~18.04) 7.5.0\r\n",
      "- CUDA/cuDNN version: 10.2.89 / 8.0.0.180\r\n",
      "- GPU model and memory: Nvidia Jetson Nano\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "\r\n",
      "ERROR: /home/minhduc/src/tensorflow-2.3.0/tensorflow/core/kernels/BUILD:6109:1: C++ compilation of rule '//tensorflow/core/kernels:training_ops' failed (Exit 4)\r\n",
      "aarch64-linux-gnu-gcc-7: internal compiler error: Killed (program cc1plus)\r\n",
      "Please submit a full bug report,\r\n",
      "with preprocessed source if appropriate.\r\n",
      "See <file:///usr/share/doc/gcc-7/README.Bugs> for instructions.\r\n",
      "Target //tensorflow:libtensorflow.so failed to build\r\n",
      "Use --verbose_failures to see the command lines of failed build steps.\r\n",
      "INFO: Elapsed time: 10499.940s, Critical Path: 654.48s\r\n",
      "INFO: 5016 processes: 5016 local.\r\n",
      "FAILED: Build did NOT complete successfully\r\n",
      "\r\n",
      "**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n",
      "I followed this [tutorial](https://jkjung-avt.github.io/build-tensorflow-2.0.0/) ([github](https://github.com/jkjung-avt/jetson_nano)). I set up my virtual screen, start the build (NOT install) script (build_libtensorflow-2.3.0.sh). Go to school and come back home with the error log above\r\n",
      "\r\n",
      "**Any other info / logs**\r\n",
      "I also install both protobuf 3.8.0 and bazel using shell scripts from the above github repo\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.3\n",
      "stat:awaiting response\n",
      "subtype: ubuntu/linux\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  Conda_Verification_Error for installing Tensor_flow 2.3.0\n",
      "issue body -  Hello Folks, \r\n",
      "I am new to Deep Learning Libraries and stuff. I tried installing tensorflow with conda in a new environment and \r\n",
      "I am getting the following error while verifying.\r\n",
      "\r\n",
      "CondaVerificationError: The package for tensorflow-base located at C:\\Users\\yuvid\\Downloads\\Softs\\Installed\\Miniconda3\\pkgs\\tensorflow-base-2.3.0-eigen_py37h17acbac_0\r\n",
      "appears to be corrupted. The path 'Lib/site-packages/tensorflow/include/external/llvm-project/mlir/_virtual_includes/AffineMemoryOpInterfacesIncGen/mlir/Dialect/Affine/IR/AffineMemoryOpInterfaces.cpp.inc'\r\n",
      "specified in the package manifest cannot be found.\r\n",
      "\r\n",
      "\r\n",
      "ClobberError: The package 'defaults/win-64::six-1.15.0-py37haa95532_0' cannot be installed due to a\r\n",
      "path collision for 'lib/site-packages/wheel/__pycache__/__init__.cpython-37.pyc'.\r\n",
      "This path already exists in the target prefix, and it won't be removed\r\n",
      "by an uninstall action in this transaction. The path is one that conda\r\n",
      "doesn't recognize. It may have been created by another package manager.\r\n",
      "\r\n",
      "\r\n",
      "ClobberError: The package 'defaults/win-64::six-1.15.0-py37haa95532_0' cannot be installed due to a\r\n",
      "path collision for 'lib/site-packages/wheel/__pycache__/bdist_wheel.cpython-37.pyc'.\r\n",
      "This path already exists in the target prefix, and it won't be removed\r\n",
      "by an uninstall action in this transaction. The path is one that conda\r\n",
      "doesn't recognize. It may have been created by another package manager.\r\n",
      "\r\n",
      "\r\n",
      "ClobberError: The package 'defaults/win-64::markdown-3.3.3-py37haa95532_0' cannot be installed due to a\r\n",
      "path collision for 'lib/site-packages/wheel/__pycache__/pkginfo.cpython-37.pyc'.\r\n",
      "This path already exists in the target prefix, and it won't be removed\r\n",
      "by an uninstall action in this transaction. The path is one that conda\r\n",
      "doesn't recognize. It may have been created by another package manager.\r\n",
      "\r\n",
      "ClobberError: This transaction has incompatible packages due to a shared path.\r\n",
      "  packages: defaults/win-64::six-1.15.0-py37haa95532_0, defaults/win-64::win_inet_pton-1.1.0-py37haa95532_0, defaults/win-64::cffi-1.14.4-py37hcd4344a_0, defaults/win-64::yarl-1.6.3-py37h2bbff1b_0, defaults/win-64::markdown-3.3.3-py37haa95532_0\r\n",
      "  path: 'lib/site-packages/wheel/__pycache__/wheelfile.cpython-37.pyc'\r\n",
      "\r\n",
      "N many more similar errors just there is change in the file name of pyc\r\n",
      "\r\n",
      "\r\n",
      "Hope anyone can help me out.\n",
      "issue labels - \n",
      "TF 2.3\n",
      "stat:awaiting response\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  Though EXTERNAL delegate is explicitly applied, the model will not be executed by the delegate\n",
      "issue body -  Apologies if this is in the incorrect category, but is more clarification that I am looking for rather than an issue with the implemented code. \r\n",
      "\r\n",
      "**System information**\r\n",
      "\r\n",
      "Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**\r\n",
      "OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 20.04.1 LTS**\r\n",
      "Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n",
      "TensorFlow installed from (source or binary): **Source**\r\n",
      "TensorFlow version (use command below): **v2.3.1**\r\n",
      "Python version: N/A\r\n",
      "Bazel version (if compiling from source): **3.1.0**\r\n",
      "GCC/Compiler version (if compiling from source): **6.3.0**\r\n",
      "CUDA/cuDNN version: N/A\r\n",
      "GPU model and memory: N/A\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "\r\n",
      "I'm currently looking into benchmarking a custom delegate against XNNPACK. I've done so through the second option specified in the tensorflow documentation (using the external delegate options within the benchmark tool). \r\n",
      "\r\n",
      "I downloaded the binary for the benchmarking tool itself here (linux aarch64) rather than building source: https://www.tensorflow.org/lite/performance/measurement#native_benchmark_binary\r\n",
      "\r\n",
      "When I run the benchmarking tool for the custom delegate, I get the info message specified in the title: \r\n",
      "_\"Though EXTERNAL delegate is explicitly applied, the model will not be executed by the delegate\"_\r\n",
      "\r\n",
      "I'm not sure what this is telling me. I've looked through the the tensorflow codebase and, from where I've looked, these output messages are no where in **_tensorflow/tensorflow/lite_**. Going up a level or two, I still can't find anything of similar. \r\n",
      "\r\n",
      "When running with xnnpack set to true, I get the output message _**\"Explicitly applied XNNPACK delegate, and the model graph will be partially executed by the delegate w/ 4 delegate kernels.\"**_. This is a little more self explanatory, and I can see that some of the code for this message was generated at tensorflow/tensorflow/lite/tools/benchmark/benchmark_tflite_model.cc:666.\r\n",
      "\r\n",
      "I added some debug information within the custom delegate and I can see that, when running the benchmark_model tool, I am creating the optimized delegate graph I expect. I am also seeing the numbers that I expect to see for the benchmarking against XNNPACK.\r\n",
      "\r\n",
      "However, the message seems to make it out as if the custom delegate has been created, but is not used during the benchmarking. Is this true? I would appreciate any clarity at your earliest convenience. Thank you!\n",
      "issue labels - \n",
      "TF 2.3\n",
      "comp:lite\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  The training job is stuck because the rpc server is not started in the evaluator\n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): centos 7\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "- TensorFlow installed from (source or binary): binary\r\n",
      "- TensorFlow version (use command below): 2.4\r\n",
      "- Python version: 3.6\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version:\r\n",
      "- GPU model and memory:\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "tf version: 2.4\r\n",
      "cluster spec: \r\n",
      "```\r\n",
      "{\r\n",
      "    \"ps\":[\r\n",
      "        \"localhost:13344\"\r\n",
      "    ],\r\n",
      "    \"chief\":[\r\n",
      "        \"localhost:15881\"\r\n",
      "    ],\r\n",
      "    \"worker\":[\r\n",
      "        \"localhost:37309\",\r\n",
      "        \"localhost:30812\"\r\n",
      "    ],\r\n",
      "    \"evaluator\":[\r\n",
      "        \"localhost:22944\"\r\n",
      "    ]\r\n",
      "}\r\n",
      "```\r\n",
      "example code:\r\n",
      "```python\r\n",
      "def main(argv):\r\n",
      "    cluster_resolver = tf.distribute.cluster_resolver.TFConfigClusterResolver()\r\n",
      "    if cluster_resolver.task_type in ('ps', 'worker'):\r\n",
      "        logging.info(\"[{}] Start {}({})...\".format(get_cur_time(), cluster_resolver.task_type, cluster_resolver.task_id))\r\n",
      "        server = tf.distribute.Server(\r\n",
      "            cluster_resolver.cluster_spec(),\r\n",
      "            job_name=cluster_resolver.task_type,\r\n",
      "            task_index=cluster_resolver.task_id,\r\n",
      "            protocol=cluster_resolver.rpc_layer or \"grpc\",\r\n",
      "            start=True)\r\n",
      "        server.join()\r\n",
      "\r\n",
      "    if cluster_resolver.task_type == 'evaluator':\r\n",
      "        ...\r\n",
      "        checkpoint = tf.train.Checkpoint(model=model)\r\n",
      "        ...\r\n",
      "\r\n",
      "    if cluster_resolver.task_type == 'chief':\r\n",
      "        logging.info(\"[{}] Start {}({})...\".format(get_cur_time(), cluster_resolver.task_type, cluster_resolver.task_id))\r\n",
      "\r\n",
      "        variable_partitioner = (\r\n",
      "            tf.distribute.experimental.partitioners.FixedShardsPartitioner(\r\n",
      "                num_shards=NUM_PS))\r\n",
      "\r\n",
      "        strategy = tf.distribute.experimental.ParameterServerStrategy(\r\n",
      "            cluster_resolver,\r\n",
      "            variable_partitioner=variable_partitioner)\r\n",
      "\r\n",
      "        ...\r\n",
      "```\r\n",
      "When i run tensorflow2.4 job with Custom Training Loop using ParameterServerStrategy on Yarn and use the same cluster spec on every process, it is stucked. \r\n",
      "Finally, I found that the GRPC Server was not started by the **evaluator** process, which caused the **coordinator** to wait all the time.\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "According to the problem described above, there are two solutions：\r\n",
      "1. in evaluator process, it start the grpc server. like the following code. Besides, it should add some related doc.\r\n",
      "```python\r\n",
      "if cluster_resolver.task_type == 'evaluator':\r\n",
      "        server = tf.distribute.Server(\r\n",
      "            cluster_resolver.cluster_spec(),\r\n",
      "            job_name=cluster_resolver.task_type,\r\n",
      "            task_index=cluster_resolver.task_id,\r\n",
      "            protocol=cluster_resolver.rpc_layer or \"grpc\",\r\n",
      "            start=True)\r\n",
      "        # dont need to call server.join()\r\n",
      "        checkpoint = tf.train.Checkpoint(model=model)\r\n",
      "        ...\r\n",
      "```\r\n",
      "2. In the PS strategy code of tf2.4, the **evaluator** type should be dynamically ignored to avoid waiting for a long time in **coordinator**\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "Provide a reproducible test case that is the bare minimum necessary to generate\r\n",
      "the problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n",
      "\r\n",
      "\r\n",
      "**Other info / logs** Include any logs or source code that would be helpful to\r\n",
      "diagnose the problem. If including tracebacks, please include the full\r\n",
      "traceback. Large logs and files should be attached.\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:dist-strat\n",
      "stat:awaiting tensorflower\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Missing header when building Android CMake TF Lite 2.4 \n",
      "issue body -  **System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.15.7\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Android\r\n",
      "- TensorFlow installed from (source or binary): source\r\n",
      "- TensorFlow version: 2.4\r\n",
      "- Python version: 3.8.3\r\n",
      "- Installed using virtualenv? pip? conda?: git/source\r\n",
      "- Bazel version (if compiling from source):3.1.0 (not used)\r\n",
      "- GCC/Compiler version (if compiling from source): clang 12.0.0\r\n",
      "- CUDA/cuDNN version: N/A\r\n",
      "- GPU model and memory: N/A, 32GB ram\r\n",
      "\r\n",
      "\r\n",
      "I am not sure but I think the tensorflow 2.4 branch is missing the file `tensorflow/lite/delegates/gpu/cl/serialization_generated.h`?\r\n",
      "\r\n",
      "Following steps from https://www.tensorflow.org/lite/guide/build_cmake I get the error messages\r\n",
      "\r\n",
      "```\r\n",
      "gmake[2]: *** [CMakeFiles/tensorflow-lite.dir/build.make:960: CMakeFiles/tensorflow-lite.dir/delegates/gpu/cl/kernels/softmax1x1.cc.o] Error 1\r\n",
      "gmake[2]: *** [CMakeFiles/tensorflow-lite.dir/build.make:895: CMakeFiles/tensorflow-lite.dir/delegates/gpu/cl/kernels/relu.cc.o] Error 1\r\n",
      "1 warning and 1 error generated.\r\n",
      "In file included from /Users/mng/Repositories/tensorflow/tensorflow/lite/delegates/gpu/cl/kernels/softmax.cc:16:\r\n",
      "In file included from /Users/mng/Repositories/tensorflow/tensorflow/lite/delegates/gpu/cl/kernels/softmax.h:20:\r\n",
      "In file included from /Users/mng/Repositories/tensorflow/tensorflow/lite/delegates/gpu/cl/kernels/gpu_operation.h:22:\r\n",
      "In file included from /Users/mng/Repositories/tensorflow/tensorflow/lite/delegates/gpu/cl/arguments.h:24:\r\n",
      "/Users/mng/Repositories/tensorflow/tensorflow/lite/delegates/gpu/cl/gpu_object.h:26:10: fatal error:\r\n",
      "      'tensorflow/lite/delegates/gpu/cl/serialization_generated.h' file not found\r\n",
      "#include \"tensorflow/lite/delegates/gpu/cl/serialization_generated.h\"\r\n",
      "         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n",
      "```\r\n",
      "\r\n",
      "And many others. On master I can see the header exist, I assume the CMakeLists file isn't downloading or generating this file on compilation.\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  micro: copy op SPACE_TO_DEPTH kernel from lite\n",
      "issue body -  Copy the kernel and test for operator SPACE_TO_DEPTH from\r\n",
      "tensorflow/lite/kernels at 49524d6 without making modifications.\r\n",
      "Adapt to micro and add to the build later.\r\n",
      "\r\n",
      "This PR is part of the work to port operator SPACE_TO_DEPTH\r\n",
      "from lite to micro, as tracked in issue #45824.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:L\n",
      "\n",
      "\n",
      "issue title -  micro: copy op DEPTH_TO_SPACE kernel from lite\n",
      "issue body -  Copy the kernel and test for operator DEPTH_TO_SPACE from\r\n",
      "tensorflow/lite/kernels at 49524d6 without making modifications.\r\n",
      "Adapt to micro and add to the build later.\r\n",
      "\r\n",
      "This PR is part of the work to port operator DEPTH_TO_SPACE\r\n",
      "from lite to micro, as tracked in issue #46025.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:L\n",
      "\n",
      "\n",
      "issue title -  Extract reference for op DEPTH_TO_SPACE to standalone header\n",
      "issue body -  Move the reference implementation to its own header so that micro\r\n",
      "can use it wtihout including unrelated dependencies via\r\n",
      "reference_ops.h.\r\n",
      "\r\n",
      "This PR is part of the work to port operator DEPTH_TO_SPACE\r\n",
      "from lite to micro, as tracked in issue #46025.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  Update SQLite to the lastest sqlite-amalgamation-3340000\n",
      "issue body -  This PR updates SQLite to the latest sqlite-amalgamation-3340000\r\n",
      "\r\n",
      "Signed-off-by: Yong Tang <yong.tang.github@outlook.com>\n",
      "issue labels - \n",
      "cla: no\n",
      "waiting for patch release\n",
      "\n",
      "\n",
      "issue title -  Update SQLite to the lastest sqlite-amalgamation-3340000\n",
      "issue body -  This PR updates SQLite to the latest sqlite-amalgamation-3340000\r\n",
      "\r\n",
      "Signed-off-by: Yong Tang <yong.tang.github@outlook.com>\n",
      "issue labels - \n",
      "cla: no\n",
      "waiting for patch release\n",
      "\n",
      "\n",
      "issue title -  Update SQLite to the lastest sqlite-amalgamation-3340000\n",
      "issue body -  This PR updates SQLite to the latest sqlite-amalgamation-3340000\r\n",
      "\r\n",
      "Signed-off-by: Yong Tang <yong.tang.github@outlook.com>\n",
      "issue labels - \n",
      "cla: no\n",
      "waiting for patch release\n",
      "\n",
      "\n",
      "issue title -  Update SQLite to the lastest sqlite-amalgamation-3340000\n",
      "issue body -  This PR updates SQLite to the latest sqlite-amalgamation-3340000\r\n",
      "\r\n",
      "Signed-off-by: Yong Tang <yong.tang.github@outlook.com>\n",
      "issue labels - \n",
      "cla: no\n",
      "waiting for patch release\n",
      "\n",
      "\n",
      "issue title -  Update SQLite to the lastest sqlite-amalgamation-3340000\n",
      "issue body -  This PR updates SQLite to the latest sqlite-amalgamation-3340000\r\n",
      "\r\n",
      "Signed-off-by: Yong Tang <yong.tang.github@outlook.com>\n",
      "issue labels - \n",
      "cla: no\n",
      "waiting for patch release\n",
      "\n",
      "\n",
      "issue title -  Update SQLite to the lastest sqlite-amalgamation-3340000\n",
      "issue body -  This PR updates SQLite to the latest sqlite-amalgamation-3340000\r\n",
      "\r\n",
      "Signed-off-by: Yong Tang <yong.tang.github@outlook.com>\n",
      "issue labels - \n",
      "cla: no\n",
      "waiting for patch release\n",
      "\n",
      "\n",
      "issue title -  [tf.data] support ~/ alias for home dir in tf.data.Dataset.list_files\n",
      "issue body -  This PR addresses the issue: https://github.com/tensorflow/tensorflow/issues/44264 by:\r\n",
      "- Adding support to handle `~/` alias for home dir in file paths while using `tf.data.Dataset.list_files` API.\r\n",
      "- Adding a test case to validate the functionality.\r\n",
      "\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:data\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  Rewrite 1D dilated convolution\n",
      "issue body -  Fixes https://github.com/tensorflow/tensorflow/issues/38638. The pattern is obtained from \r\n",
      "\r\n",
      "```python\r\n",
      "layer = tf.keras.layers.Conv1D(8, 5, 1, dilation_rate=2, padding=\"SAME\", use_bias=False)\r\n",
      "\r\n",
      "@tf.function(input_signature=(tf.TensorSpec(shape=(1, 128, 3)),))\r\n",
      "def my_func(x):\r\n",
      "  return layer(x)\r\n",
      "```\r\n",
      "\r\n",
      "Hi @haozha111, can you take a look at this when time allows? Thank you!\n",
      "issue labels - \n",
      "cla: yes\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  micro: in CONTRIBUTING.md, fix path to test script\n",
      "issue body -  Fix path to test script in CONTRIBUTING.md.\r\n",
      "\r\n",
      "Fixes #46148.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  micro: in CONTRIBUTING.md, the path a test script is wrong\n",
      "issue body -  @tensorflow/micro\r\n",
      "- Tensorflow version (commit SHA if source): 556fa126\r\n",
      " \r\n",
      "In micro's CONTRIBUTING.md, the path to a script to run tests prior to submitting a PR uses (what is now, at least) an invalid path.\r\n",
      "\r\n",
      "https://github.com/tensorflow/tensorflow/blob/7958e0cbfa55b779c3682aaaa37f6e7c55f55bc2/tensorflow/lite/micro/CONTRIBUTING.md#L204-L208\r\n",
      "\r\n",
      "The fix is trivial and on its way.\n",
      "issue labels - \n",
      "comp:lite\n",
      "comp:micro\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Failed to build wheel on MacOS Big Sur\n",
      "issue body -  Repaired `build_pip_package.sh` for enable building wheel on MacBook Air with macOS Big Sur.\r\n",
      "\r\n",
      "Solved issue #45095\r\n",
      "\r\n",
      "@mihaimaruseac\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "cla: yes\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  Pandas pct_change() function results in nan loss when training\n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n",
      "- TensorFlow installed from (source or binary): binary\r\n",
      "- TensorFlow version (use command below):  v2.4.0-rc4-71-g582c8d236cb 2.4.0\r\n",
      "- Python version: Python 3.7.6\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version: Running on CPU\r\n",
      "- GPU model and memory:\r\n",
      "\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "When training a model with data from a pandas DataFrame, the model trains fine unless I use the df.pct_change() pandas function on it. If I do so, the loss is always nan. I specifically made sure to remove any NaN or inf values from the dataset but the problem persists. I don't know if I'm just missing something obvious and the issue is on my end.\r\n",
      "\r\n",
      "I created a simplified jupyter notebook to demonstrate the issue.\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "The loss should be a real number in the last cell of the notebook (after I remove all NaN and inf values) but it is still nan.\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "Here is the notebook:\r\n",
      "https://drive.google.com/file/d/1k3dHHtFI4tGTKswF0d0TJdj9TfaqfBnX/view?usp=sharing\r\n",
      "**Other info / logs** Include any logs or source code that would be helpful to\r\n",
      "diagnose the problem. If including tracebacks, please include the full\r\n",
      "traceback. Large logs and files should be attached.\r\n",
      "\n",
      "issue labels - \n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  TensorFlowLiteSwift build error in the Bazel project\n",
      "issue body -  **System information**\r\n",
      "- OS Platform and Distribution: macOS Big Sur 11.1\r\n",
      "- TensorFlow version: 2.4.0\r\n",
      "- Bazel version: 3.7.2-homebrew\r\n",
      "- Compiler version: Apple clang 12.0\r\n",
      "- Xcode version: 12.3\r\n",
      "\r\n",
      "\r\n",
      "I want to connect to mediapipe app for ios TensorFlowLiteSwift framework.\r\n",
      "\r\n",
      "I added a bazel dependency to my project:\r\n",
      "BUILD file:\r\n",
      "```\r\n",
      "swift_library(\r\n",
      "    deps = [\r\n",
      "      \"@tensorflow_my//tensorflow/lite/swift:TensorFlowLite\",\r\n",
      "    ]\r\n",
      ")\r\n",
      "```\r\n",
      "\r\n",
      "My WORKSPACE file (fragment for tensorflow):\r\n",
      "```\r\n",
      "#Tensorflow repo should always go after the other external dependencies.\r\n",
      "# 2020-10-30\r\n",
      "_TENSORFLOW_GIT_COMMIT = \"84384703c0d8b502e33ff6fd7eefd219dca5ff8e\"\r\n",
      "_TENSORFLOW_SHA256= \"23fb322fc15a20f7a7838d9a31f8b16f60700a494ea654311a0aa8621769df98\"\r\n",
      "http_archive(\r\n",
      "    name = \"org_tensorflow\",\r\n",
      "    urls = [\r\n",
      "      \"https://github.com/tensorflow/tensorflow/archive/%s.tar.gz\" % _TENSORFLOW_GIT_COMMIT,\r\n",
      "    ],\r\n",
      "    patches = [\r\n",
      "        \"@//third_party:org_tensorflow_compatibility_fixes.diff\",\r\n",
      "    ],\r\n",
      "    patch_args = [\r\n",
      "        \"-p1\",\r\n",
      "    ],\r\n",
      "    strip_prefix = \"tensorflow-%s\" % _TENSORFLOW_GIT_COMMIT,\r\n",
      "    sha256 = _TENSORFLOW_SHA256,\r\n",
      ")\r\n",
      "\r\n",
      "load(\"@org_tensorflow//tensorflow:workspace.bzl\", \"tf_workspace\")\r\n",
      "tf_workspace(tf_repo_name = \"org_tensorflow\")\r\n",
      "\r\n",
      "local_repository(\r\n",
      "    name = \"tensorflow_my\",\r\n",
      "    path = \"third_party/tensorflow\", # < -- cloned repository\r\n",
      ")\r\n",
      "\r\n",
      "\r\n",
      "load(\"@tensorflow_my//tensorflow:workspace3.bzl\", \"workspace\")\r\n",
      "workspace()\r\n",
      "load(\"@tensorflow_my//tensorflow:workspace2.bzl\", \"workspace\")\r\n",
      "workspace()\r\n",
      "load(\"@tensorflow_my//tensorflow:workspace1.bzl\", \"workspace\")\r\n",
      "workspace()\r\n",
      "load(\"@tensorflow_my//tensorflow:workspace0.bzl\", \"workspace\")\r\n",
      "workspace()\r\n",
      "```\r\n",
      "\r\n",
      "When building a project, it outputs the following:\r\n",
      "```\r\n",
      "Showing All Messages\r\n",
      "Queuing Tulsi build...\r\n",
      "<*> Parsing options completed in 0.261 ms\r\n",
      "Running \"/usr/local/bin/bazel build --verbose_failures --bes_outerr_buffer_size=0 --apple_platform_type=ios --cpu=ios_arm64 --watchos_cpus=armv7k --announce_rc '--override_repository=tulsi=/Users/dmitry/Library/Application Support/Tulsi/0.20190814.88/Bazel' --compilation_mode=dbg --define=apple.add_debugger_entitlement=1 --define=apple.propagate_embedded_extra_outputs=1 --define=apple.experimental.tree_artifact_outputs=1 --features=debug_prefix_map_pwd_is_dot --tool_tag=tulsi:bazel_build --build_event_json_file=/Users/dmitry/Documents/mediapipeNew/Mediapipe.xcodeproj/.tulsi/8792_build_events.json --noexperimental_build_event_json_file_path_conversion --aspects @tulsi//:tulsi/tulsi_aspects.bzl%tulsi_outputs_aspect --output_groups=tulsi_outputs,default //mediapipe/examples/ios/handtrackinggpu:HandTrackingGpuApp\", patching output for workspace root at \"/Users/dmitry/Documents/mediapipeNew/mediapipe\" with project path at \"/Users/dmitry/Documents/mediapipeNew\".\r\n",
      "INFO: Options provided by the client:\r\n",
      "  Inherited 'common' options: --isatty=0 --terminal_columns=80\r\n",
      "INFO: Reading rc options for 'build' from /Users/dmitry/Documents/mediapipeNew/mediapipe/.bazelrc:\r\n",
      "  Inherited 'common' options: --experimental_repo_remote_exec\r\n",
      "INFO: Reading rc options for 'build' from /Users/dmitry/Documents/mediapipeNew/mediapipe/.bazelrc:\r\n",
      "  'build' options: --jobs 128 --define=absl=1 --enable_platform_specific_config --apple_platform_type=macos --apple_generate_dsym\r\n",
      "INFO: Found applicable config definition build:macos in file /Users/dmitry/Documents/mediapipeNew/mediapipe/.bazelrc: --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --copt=-w\r\n",
      "Loading: \r\n",
      "Loading: 0 packages loaded\r\n",
      "DEBUG: Rule 'rules_foreign_cc' indicated that a canonical reproducible form can be obtained by modifying arguments sha256 = \"3e6b0691fc57db8217d535393dcc2cf7c1d39fc87e9adb6e7d7bab1483915110\"\r\n",
      "DEBUG: Repository rules_foreign_cc instantiated at:\r\n",
      "/Users/dmitry/Documents/mediapipeNew/mediapipe/  /Users/dmitry/Documents/mediapipeNew/mediapipe/WORKSPACE:39:13: in <toplevel>\r\n",
      "Repository rule http_archive defined at:\r\n",
      "/Users/dmitry/Documents/mediapipeNew/mediapipe/  /private/var/tmp/_bazel_dmitry/02a2e3662efe5f9742ce3f7bf49d4039/external/bazel_tools/tools/build_defs/repo/http.bzl:336:31: in <toplevel>\r\n",
      "Analyzing: target //mediapipe/examples/ios/handtrackinggpu:HandTrackingGpuApp (0 packages loaded, 0 targets configured)\r\n",
      "DEBUG: Rule 'rules_cc' indicated that a canonical reproducible form can be obtained by modifying arguments sha256 = \"fc58ff069f150c81abd10231bb1d9fbff0ba9322e03c9396518db4d054d5f2e6\"\r\n",
      "DEBUG: Repository rules_cc instantiated at:\r\n",
      "/Users/dmitry/Documents/mediapipeNew/mediapipe/  /Users/dmitry/Documents/mediapipeNew/mediapipe/WORKSPACE:33:13: in <toplevel>\r\n",
      "Repository rule http_archive defined at:\r\n",
      "/Users/dmitry/Documents/mediapipeNew/mediapipe/  /private/var/tmp/_bazel_dmitry/02a2e3662efe5f9742ce3f7bf49d4039/external/bazel_tools/tools/build_defs/repo/http.bzl:336:31: in <toplevel>\r\n",
      "DEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\"\r\n",
      "DEBUG: Repository io_bazel_rules_docker instantiated at:\r\n",
      "/Users/dmitry/Documents/mediapipeNew/mediapipe/  /Users/dmitry/Documents/mediapipeNew/mediapipe/WORKSPACE:401:10: in <toplevel>\r\n",
      "/Users/dmitry/Documents/mediapipeNew/mediapipe/  /private/var/tmp/_bazel_dmitry/02a2e3662efe5f9742ce3f7bf49d4039/external/tensorflow_my/tensorflow/workspace0.bzl:65:34: in workspace\r\n",
      "/Users/dmitry/Documents/mediapipeNew/mediapipe/  /private/var/tmp/_bazel_dmitry/02a2e3662efe5f9742ce3f7bf49d4039/external/bazel_toolchains/repositories/repositories.bzl:37:23: in repositories\r\n",
      "Repository rule git_repository defined at:\r\n",
      "/Users/dmitry/Documents/mediapipeNew/mediapipe/  /private/var/tmp/_bazel_dmitry/02a2e3662efe5f9742ce3f7bf49d4039/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>\r\n",
      "INFO: Analyzed target //mediapipe/examples/ios/handtrackinggpu:HandTrackingGpuApp (1 packages loaded, 15 targets configured).\r\n",
      "INFO: Found 1 target...\r\n",
      "\r\n",
      "[0 / 7] [Prepa] BazelWorkspaceStatusAction stable-status.txt\r\n",
      "[25 / 240] Compiling tensorflow_my/tensorflow/lite/kernels/internal/mfcc_mel_filterbank.cc; 1s darwin-sandbox ... (33 actions, 4 running)\r\n",
      "[29 / 240] Compiling tensorflow_my/tensorflow/lite/kernels/internal/spectrogram.cc; 2s darwin-sandbox ... (44 actions, 4 running)\r\n",
      "[29 / 240] Compiling tensorflow_my/tensorflow/lite/kernels/internal/spectrogram.cc; 3s darwin-sandbox ... (44 actions, 4 running)\r\n",
      "[31 / 240] [Sched] Compiling tensorflow_my/tensorflow/lite/kernels/cpu_backend_gemm_eigen.cc; 4s ... (44 actions, 4 running)\r\n",
      "[34 / 240] [Sched] Compiling tensorflow_my/tensorflow/lite/experimental/resource/static_hashtable.cc; 6s ... (44 actions, 4 running)\r\n",
      "[35 / 240] [Sched] Compiling tensorflow_my/tensorflow/lite/minimal_logging_ios.cc; 7s ... (43 actions, 4 running)\r\n",
      "[35 / 240] [Sched] Compiling tensorflow_my/tensorflow/lite/minimal_logging_ios.cc; 9s ... (43 actions, 4 running)\r\n",
      "[38 / 240] [Sched] Compiling tensorflow_my/tensorflow/lite/tflite_with_xnnpack_optional.cc; 11s ... (42 actions, 4 running)\r\n",
      "[41 / 240] [Sched] Compiling tensorflow_my/tensorflow/lite/simple_memory_arena.cc; 13s ... (42 actions, 4 running)\r\n",
      "[46 / 240] [Sched] Compiling tensorflow_my/tensorflow/lite/kernels/deprecated_backends.cc; 15s ... (42 actions, 4 running)\r\n",
      "[55 / 240] [Sched] Compiling tensorflow_my/tensorflow/lite/core/api/op_resolver.cc; 17s ... (128 actions, 4 running)\r\n",
      "[57 / 240] [Sched] Compiling tensorflow_my/tensorflow/lite/delegates/utils.cc; 20s ... (128 actions, 4 running)\r\n",
      "[63 / 240] [Sched] Compiling tensorflow_my/tensorflow/lite/kernels/eigen_support.cc; 24s ... (128 actions, 4 running)\r\n",
      "[68 / 240] [Sched] Compiling tensorflow_my/tensorflow/lite/delegates/xnnpack/xnnpack_delegate.cc; 28s ... (128 actions, 4 running)\r\n",
      "INFO: From Linking external/tensorflow_my/tensorflow/lite/kernels/libcpu_backend_gemm.a:\r\n",
      "warning: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/libtool: archive library: bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/tensorflow_my/tensorflow/lite/kernels/libcpu_backend_gemm.a the table of contents is empty (no object file members in the library define global symbols)\r\n",
      "[94 / 240] [Sched] Compiling tensorflow_my/tensorflow/lite/interpreter.cc; 16s ... (124 actions, 4 running)\r\n",
      "ERROR: /private/var/tmp/_bazel_dmitry/02a2e3662efe5f9742ce3f7bf49d4039/external/tensorflow_my/tensorflow/lite/delegates/xnnpack/BUILD:22:11: C++ compilation of rule '@tensorflow_my//tensorflow/lite/delegates/xnnpack:xnnpack_delegate' failed (Exit 1): wrapped_clang failed: error executing command \r\n",
      "  (cd /private/var/tmp/_bazel_dmitry/02a2e3662efe5f9742ce3f7bf49d4039/sandbox/darwin-sandbox/2820/execroot/mediapipe && \\\r\n",
      "  exec env - \\\r\n",
      "    APPLE_SDK_PLATFORM=iPhoneOS \\\r\n",
      "    APPLE_SDK_VERSION_OVERRIDE=14.3 \\\r\n",
      "    PATH=/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin:/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/local/bin:/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/libexec:/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/usr/bin:/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/usr/local/bin:/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/usr/bin:/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/usr/local/bin:/Applications/Xcode.app/Contents/Developer/usr/bin:/Applications/Xcode.app/Contents/Developer/usr/local/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin \\\r\n",
      "    XCODE_VERSION_OVERRIDE=12.3.0.12C33 \\\r\n",
      "  external/local_config_cc/wrapped_clang '-D_FORTIFY_SOURCE=1' -fstack-protector -fcolor-diagnostics -Wall -Wthread-safety -Wself-assign -fno-omit-frame-pointer -g '-std=c++11' 'DEBUG_PREFIX_MAP_PWD=.' -g -iquote external/tensorflow_my -iquote bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/tensorflow_my -iquote external/flatbuffers -iquote bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/flatbuffers -iquote external/eigen_archive -iquote bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/eigen_archive -iquote external/FP16 -iquote bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/FP16 -iquote external/XNNPACK -iquote bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/XNNPACK -iquote external/clog -iquote bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/clog -iquote external/pthreadpool -iquote bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/pthreadpool -iquote external/FXdiv -iquote bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/FXdiv -iquote external/psimd -iquote bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/psimd -iquote external/cpuinfo -iquote bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/cpuinfo -Ibazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/flatbuffers/_virtual_includes/runtime_cc -Ibazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/FP16/_virtual_includes/FP16 -Ibazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/clog/_virtual_includes/clog -Ibazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/pthreadpool/_virtual_includes/pthreadpool -Ibazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/FXdiv/_virtual_includes/FXdiv -Ibazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/psimd/_virtual_includes/psimd -Ibazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/cpuinfo/_virtual_includes/cpuinfo -isystem external/tensorflow_my/tensorflow/lite/schema -isystem bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/tensorflow_my/tensorflow/lite/schema -isystem external/eigen_archive -isystem bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/eigen_archive -isystem external/FP16/include -isystem bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/FP16/include -isystem external/XNNPACK/include -isystem bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/XNNPACK/include -isystem external/XNNPACK/src -isystem bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/XNNPACK/src -isystem external/pthreadpool/include -isystem bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/pthreadpool/include -isystem external/FXdiv/include -isystem bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/FXdiv/include -isystem external/psimd/include -isystem bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/psimd/include -MD -MF bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/tensorflow_my/tensorflow/lite/delegates/xnnpack/_objs/xnnpack_delegate/xnnpack_delegate.d -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' '-DXNN_ENABLE_ASSEMBLY=1' '-DXNN_ENABLE_SPARSE=0' -DPTHREADPOOL_NO_DEPRECATED_API '-DXNN_ENABLE_MEMOPT=1' -DXNN_NO_QS8_OPERATORS -DXNN_NO_QU8_OPERATORS -DXNN_NO_U8_OPERATORS -DXNN_NO_X8_OPERATORS -DXNN_NO_F16_OPERATORS -DXNN_NO_X16_OPERATORS '-frandom-seed=bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/tensorflow_my/tensorflow/lite/delegates/xnnpack/_objs/xnnpack_delegate/xnnpack_delegate.o' -isysroot __BAZEL_XCODE_SDKROOT__ -F__BAZEL_XCODE_SDKROOT__/System/Library/Frameworks -F__BAZEL_XCODE_DEVELOPER_DIR__/Platforms/iPhoneOS.platform/Developer/Library/Frameworks '-miphoneos-version-min=10.0' -w '-std=c++14' -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -target arm64-apple-ios -c external/tensorflow_my/tensorflow/lite/delegates/xnnpack/xnnpack_delegate.cc -o bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/tensorflow_my/tensorflow/lite/delegates/xnnpack/_objs/xnnpack_delegate/xnnpack_delegate.o)\r\n",
      "Execution platform: @local_execution_config_platform//:platform\r\n",
      "\r\n",
      "Use --sandbox_debug to see verbose messages from the sandbox wrapped_clang failed: error executing command \r\n",
      "  (cd /private/var/tmp/_bazel_dmitry/02a2e3662efe5f9742ce3f7bf49d4039/sandbox/darwin-sandbox/2820/execroot/mediapipe && \\\r\n",
      "  exec env - \\\r\n",
      "    APPLE_SDK_PLATFORM=iPhoneOS \\\r\n",
      "    APPLE_SDK_VERSION_OVERRIDE=14.3 \\\r\n",
      "    PATH=/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin:/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/local/bin:/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/libexec:/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/usr/bin:/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/usr/local/bin:/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/usr/bin:/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/usr/local/bin:/Applications/Xcode.app/Contents/Developer/usr/bin:/Applications/Xcode.app/Contents/Developer/usr/local/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin \\\r\n",
      "    XCODE_VERSION_OVERRIDE=12.3.0.12C33 \\\r\n",
      "  external/local_config_cc/wrapped_clang '-D_FORTIFY_SOURCE=1' -fstack-protector -fcolor-diagnostics -Wall -Wthread-safety -Wself-assign -fno-omit-frame-pointer -g '-std=c++11' 'DEBUG_PREFIX_MAP_PWD=.' -g -iquote external/tensorflow_my -iquote bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/tensorflow_my -iquote external/flatbuffers -iquote bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/flatbuffers -iquote external/eigen_archive -iquote bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/eigen_archive -iquote external/FP16 -iquote bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/FP16 -iquote external/XNNPACK -iquote bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/XNNPACK -iquote external/clog -iquote bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/clog -iquote external/pthreadpool -iquote bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/pthreadpool -iquote external/FXdiv -iquote bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/FXdiv -iquote external/psimd -iquote bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/psimd -iquote external/cpuinfo -iquote bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/cpuinfo -Ibazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/flatbuffers/_virtual_includes/runtime_cc -Ibazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/FP16/_virtual_includes/FP16 -Ibazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/clog/_virtual_includes/clog -Ibazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/pthreadpool/_virtual_includes/pthreadpool -Ibazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/FXdiv/_virtual_includes/FXdiv -Ibazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/psimd/_virtual_includes/psimd -Ibazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/cpuinfo/_virtual_includes/cpuinfo -isystem external/tensorflow_my/tensorflow/lite/schema -isystem bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/tensorflow_my/tensorflow/lite/schema -isystem external/eigen_archive -isystem bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/eigen_archive -isystem external/FP16/include -isystem bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/FP16/include -isystem external/XNNPACK/include -isystem bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/XNNPACK/include -isystem external/XNNPACK/src -isystem bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/XNNPACK/src -isystem external/pthreadpool/include -isystem bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/pthreadpool/include -isystem external/FXdiv/include -isystem bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/FXdiv/include -isystem external/psimd/include -isystem bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/psimd/include -MD -MF bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/tensorflow_my/tensorflow/lite/delegates/xnnpack/_objs/xnnpack_delegate/xnnpack_delegate.d -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' '-DXNN_ENABLE_ASSEMBLY=1' '-DXNN_ENABLE_SPARSE=0' -DPTHREADPOOL_NO_DEPRECATED_API '-DXNN_ENABLE_MEMOPT=1' -DXNN_NO_QS8_OPERATORS -DXNN_NO_QU8_OPERATORS -DXNN_NO_U8_OPERATORS -DXNN_NO_X8_OPERATORS -DXNN_NO_F16_OPERATORS -DXNN_NO_X16_OPERATORS '-frandom-seed=bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/tensorflow_my/tensorflow/lite/delegates/xnnpack/_objs/xnnpack_delegate/xnnpack_delegate.o' -isysroot __BAZEL_XCODE_SDKROOT__ -F__BAZEL_XCODE_SDKROOT__/System/Library/Frameworks -F__BAZEL_XCODE_DEVELOPER_DIR__/Platforms/iPhoneOS.platform/Developer/Library/Frameworks '-miphoneos-version-min=10.0' -w '-std=c++14' -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -target arm64-apple-ios -c external/tensorflow_my/tensorflow/lite/delegates/xnnpack/xnnpack_delegate.cc -o bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/tensorflow_my/tensorflow/lite/delegates/xnnpack/_objs/xnnpack_delegate/xnnpack_delegate.o)\r\n",
      "Execution platform: @local_execution_config_platform//:platform\r\n",
      "\r\n",
      "Use --sandbox_debug to see verbose messages from the sandbox\r\n",
      "/Users/dmitry/Documents/mediapipeNew/mediapipe/external/tensorflow_my/tensorflow/lite/delegates/xnnpack/xnnpack_delegate.cc:277:49: error: use of undeclared identifier 'XNN_FLAG_SPARSE_INFERENCE'\r\n",
      "    const uint32_t flags = has_sparse_weights ? XNN_FLAG_SPARSE_INFERENCE : 0;\r\n",
      "                                                ^\r\n",
      "/Users/dmitry/Documents/mediapipeNew/mediapipe/external/tensorflow_my/tensorflow/lite/delegates/xnnpack/xnnpack_delegate.cc:1449:33: error: use of undeclared identifier 'xnn_define_depth_to_space'\r\n",
      "      const xnn_status status = xnn_define_depth_to_space(\r\n",
      "                                ^\r\n",
      "/Users/dmitry/Documents/mediapipeNew/mediapipe/external/tensorflow_my/tensorflow/lite/delegates/xnnpack/xnnpack_delegate.cc:1538:11: error: use of undeclared identifier 'xnn_define_elu'; did you mean 'xnn_define_prelu'?\r\n",
      "          xnn_define_elu(subgraph, /*alpha=*/1.0f,\r\n",
      "          ^~~~~~~~~~~~~~\r\n",
      "          xnn_define_prelu\r\n",
      "/Users/dmitry/Documents/mediapipeNew/mediapipe/external/XNNPACK/include/xnnpack.h:831:17: note: 'xnn_define_prelu' declared here\r\n",
      "enum xnn_status xnn_define_prelu(\r\n",
      "                ^\r\n",
      "3 errors generated.\r\n",
      "Aspect @tulsi//:tulsi/tulsi_aspects.bzl%tulsi_outputs_aspect of //mediapipe/examples/ios/handtrackinggpu:HandTrackingGpuApp up-to-date:\r\n",
      "  bazel-bin/mediapipe/examples/ios/handtrackinggpu/HandTrackingGpuApp.tulsiouts\r\n",
      "INFO: Elapsed time: 40,984s, Critical Path: 38,06s\r\n",
      "INFO: 205 processes: 133 internal, 72 darwin-sandbox.\r\n",
      "FAILED: Build did NOT complete successfully\r\n",
      "<*> Running Bazel completed in 41186.495 ms\r\n",
      "/Users/dmitry/Documents/mediapipeNew/Mediapipe.xcodeproj/.tulsi/Scripts/bazel_build.py:549: error: Bazel build failed with exit code 1. Please check the build log in Report Navigator (⌘9) for more information.\r\n",
      "<*> Everything completed in 41246.470 ms\r\n",
      "\r\n",
      "\r\n",
      "```\r\n",
      "\r\n",
      "Xcode highlights the following in red (a fragment from the above log):\r\n",
      "```\r\n",
      " use of undeclared identifier 'XNN_FLAG_SPARSE_INFERENCE'\r\n",
      "      const xnn_status status = xnn_define_depth_to_space(\r\n",
      "\r\n",
      " use of undeclared identifier 'xnn_define_depth_to_space'\r\n",
      "          xnn_define_elu(subgraph, /*alpha=*/1.0f,\r\n",
      "          ^~~~~~~~~~~~~~\r\n",
      "          xnn_define_prelu\r\n",
      "\r\n",
      " use of undeclared identifier 'xnn_define_elu'; did you mean 'xnn_define_prelu'?\r\n",
      "         enum xnn_status xnn_define_prelu(\r\n",
      "                        ^\r\n",
      "3 errors generated.\r\n",
      "```\r\n",
      "\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:lite\n",
      "stat:awaiting response\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  Tensorflow and Keras installation issue\n",
      "issue body -  <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "- TensorFlow installed from (source or binary):\r\n",
      "- TensorFlow version:\r\n",
      "- Python version:\r\n",
      "- Installed using virtualenv? pip? conda?:\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version:\r\n",
      "- GPU model and memory:\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "Here is the version of Tensorflow and keras installed on python 3.7 in win 10\r\n",
      "\r\n",
      ".C:\\Users\\ZS>pip show tensorflow\r\n",
      "Name: tensorflow\r\n",
      "Version: 2.4.0\r\n",
      "Summary: TensorFlow is an open source machine learning framework for everyone.\r\n",
      "Home-page: https://www.tensorflow.org/\r\n",
      "Author: Google Inc.\r\n",
      "Author-email: packages@tensorflow.org\r\n",
      "License: Apache 2.0\r\n",
      "Location: c:\\users\\zs\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\r\n",
      "Requires: termcolor, absl-py, google-pasta, typing-extensions, tensorboard, grpcio, six, wrapt, astunparse, numpy, keras-preprocessing, opt-einsum, gast, flatbuffers, tensorflow-estimator, wheel, protobuf, h5py\r\n",
      "Required-by:\r\n",
      "\r\n",
      "C:\\Users\\ZS>pip show keras\r\n",
      "Name: Keras\r\n",
      "Version: 2.4.3\r\n",
      "Summary: Deep Learning for humans\r\n",
      "Home-page: https://github.com/keras-team/keras\r\n",
      "Author: Francois Chollet\r\n",
      "Author-email: francois.chollet@gmail.com\r\n",
      "License: MIT\r\n",
      "Location: c:\\users\\zs\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\r\n",
      "Requires: numpy, h5py, scipy, pyyaml\r\n",
      "Required-by:\r\n",
      "**But..........................................\r\n",
      "******\r\n",
      "import tensorflow\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"C:\\Users\\ZS\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n",
      "    from tensorflow.python._pywrap_tensorflow_internal import *\r\n",
      "ImportError: DLL load failed: The specified module could not be found.\r\n",
      "\r\n",
      "During handling of the above exception, another exception occurred:\r\n",
      "\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"<pyshell#0>\", line 1, in <module>\r\n",
      "    import tensorflow\r\n",
      "  File \"C:\\Users\\ZS\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n",
      "    from tensorflow.python.tools import module_util as _module_util\r\n",
      "  File \"C:\\Users\\ZS\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 39, in <module>\r\n",
      "    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\r\n",
      "  File \"C:\\Users\\ZS\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 83, in <module>\r\n",
      "    raise ImportError(msg)\r\n",
      "ImportError: Traceback (most recent call last):\r\n",
      "  File \"C:\\Users\\ZS\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n",
      "    from tensorflow.python._pywrap_tensorflow_internal import *\r\n",
      "ImportError: DLL load failed: The specified module could not be found.\r\n",
      "\r\n",
      "Failed to load the native TensorFlow runtime.\r\n",
      "\r\n",
      "See https://www.tensorflow.org/install/errors\r\n",
      "\r\n",
      "for some common reasons and solutions.  Include the entire stack trace\r\n",
      "above this error message when asking for help.\r\n",
      ">>>  from tensorflow import keras\r\n",
      " \r\n",
      "SyntaxError: unexpected indent\r\n",
      ">>> \r\n",
      "\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "stalled\n",
      "stat:awaiting response\n",
      "subtype:windows\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  [tf.data service] test zipped datasets with different processing modes\n",
      "issue body -  This PR extends the `tf.data service` tests by testing the functionality on zipped datasets which have been distributed using `parallel_epochs` and `distributed_epoch` processing modes.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:data\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  [tf.data] fix the example in DispatchServer docstring\n",
      "issue body -  this PR addresses the incomplete import of `WorkerConfig` in the [DispatchServer](https://www.tensorflow.org/api_docs/python/tf/data/experimental/service/DispatchServer) docs.\r\n",
      "\r\n",
      "cc: @aaudiber \n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:data\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  Only use pthread_getname on glibc and FreeBSD\n",
      "issue body -  glibc and FreeBSD are actually the odd ones out and no other platform\r\n",
      "supports pthread_getname or equivelant (from what I can find), so let's check for those instead of checking for platforms that don't support it.\r\n",
      "\r\n",
      "This is required to get Tensorflow compiling on Musl libc.\r\n",
      "\r\n",
      "https://linux.die.net/man/3/pthread_getname_np\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:core\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  MetalDelegate.swift : Unexpected version number in 'available' attribute for non-specific platform '*'\n",
      "issue body -  Warning in line\r\n",
      " @available(*, deprecated: 2.4, renamed: \"isPrecisionLossAllowed\")\n",
      "issue labels - \n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  When activation='tanh' is used, the training gradient no drop\n",
      "issue body -  \r\n",
      "- TensorFlow installed from (source or binary): pip install tensorflow==2.4.0rc4\r\n",
      "- TensorFlow version (use command below): 2.4.0rc4\r\n",
      "- Python version: 3.8\r\n",
      "- CUDA/cuDNN version: CUDA11.0/cuDNN8.0.4\r\n",
      "- GPU model and memory: NVIDIA RTX3070 8G\r\n",
      "- OS version: windows 10\r\n",
      "\r\n",
      "ISSUE:  When activation='tanh' is used, the training gradient no drop. the follow test code from: https://www.tensorflow.org/tutorials/generative/dcgan\r\n",
      "\r\n",
      "\r\n",
      "import glob\r\n",
      "import imageio\r\n",
      "import matplotlib.pyplot as plt\r\n",
      "import numpy as np\r\n",
      "import os\r\n",
      "import PIL\r\n",
      "from tensorflow.keras import layers,Model\r\n",
      "import time\r\n",
      "\r\n",
      "from IPython import display\r\n",
      "\r\n",
      "import tensorflow as tf\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "class ResNet(Model):\r\n",
      "    def __init__(self):\r\n",
      "        super(ResNet, self).__init__()\r\n",
      "        self.d1=layers.Dense(7*7*256, use_bias=False, input_shape=(100,))\r\n",
      "        self.b1=layers.BatchNormalization()\r\n",
      "        self.a1=layers.LeakyReLU()\r\n",
      "        self.r1=layers.Reshape((7, 7, 256))\r\n",
      "        self.c1=layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False)\r\n",
      "        self.r2=layers.Reshape((7, 7, 128))\r\n",
      "        self.c2=layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False)\r\n",
      "        self.r3=layers.Reshape((14, 14, 64))\r\n",
      "        self.c3=layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False)\r\n",
      "        self.r4=layers.Reshape((28, 28, 1))\r\n",
      "\r\n",
      "    def call(self, inputs):\r\n",
      "        x=self.d1(inputs)\r\n",
      "        x=self.b1(x)\r\n",
      "        x=self.a1(x)\r\n",
      "        x=self.r1(x)\r\n",
      "        x=self.c1(x)\r\n",
      "        assert Model.output_shape == (None, 7, 7, 128)\r\n",
      "        x=self.b1(x)\r\n",
      "        x=self.a1(x)\r\n",
      "        x=self.c2(x)\r\n",
      "        \r\n",
      "        x=self.b1(x)\r\n",
      "        x=self.a1(x)\r\n",
      "        x=self.c3(x)\r\n",
      "        x=tf.math.sinh(x)/tf.math.cosh(x)\r\n",
      "        \r\n",
      "        return x\r\n",
      "\r\n",
      "def make_generator_model():\r\n",
      "    model = tf.keras.Sequential()\r\n",
      "    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))\r\n",
      "    model.add(layers.BatchNormalization())\r\n",
      "    model.add(layers.LeakyReLU())\r\n",
      "\r\n",
      "    model.add(layers.Reshape((7, 7, 256)))\r\n",
      "    assert model.output_shape == (None, 7, 7, 256) # 注意：batch size 没有限制\r\n",
      "\r\n",
      "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\r\n",
      "    assert model.output_shape == (None, 7, 7, 128)\r\n",
      "    model.add(layers.BatchNormalization())\r\n",
      "    model.add(layers.LeakyReLU())\r\n",
      "\r\n",
      "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\r\n",
      "    assert model.output_shape == (None, 14, 14, 64)\r\n",
      "    model.add(layers.BatchNormalization())\r\n",
      "    model.add(layers.LeakyReLU())\r\n",
      "\r\n",
      "    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\r\n",
      "    assert model.output_shape == (None, 28, 28, 1)\r\n",
      "\r\n",
      "    return model\r\n",
      "\r\n",
      "\r\n",
      "def make_discriminator_model():\r\n",
      "    model = tf.keras.Sequential()\r\n",
      "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\r\n",
      "                                     input_shape=[28, 28, 1]))\r\n",
      "    model.add(layers.LeakyReLU())\r\n",
      "    model.add(layers.Dropout(0.3))\r\n",
      "\r\n",
      "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\r\n",
      "    model.add(layers.LeakyReLU())\r\n",
      "    model.add(layers.Dropout(0.3))\r\n",
      "\r\n",
      "    model.add(layers.Flatten())\r\n",
      "    model.add(layers.Dense(1))\r\n",
      "\r\n",
      "    return model\r\n",
      "\r\n",
      "\r\n",
      "def discriminator_loss(real_output, fake_output):\r\n",
      "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\r\n",
      "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\r\n",
      "    total_loss = real_loss + fake_loss\r\n",
      "    return total_loss\r\n",
      "\r\n",
      "def generator_loss(fake_output):\r\n",
      "    return cross_entropy(tf.ones_like(fake_output), fake_output)\r\n",
      "\r\n",
      "\r\n",
      "@tf.function\r\n",
      "def train_step(images):\r\n",
      "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\r\n",
      "\r\n",
      "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\r\n",
      "      generated_images = generator(noise, training=True)\r\n",
      "\r\n",
      "      real_output = discriminator(images, training=True)\r\n",
      "      fake_output = discriminator(generated_images, training=True)\r\n",
      "\r\n",
      "      gen_loss = generator_loss(fake_output)\r\n",
      "      disc_loss = discriminator_loss(real_output, fake_output)\r\n",
      "\r\n",
      "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\r\n",
      "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\r\n",
      "\r\n",
      "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\r\n",
      "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\r\n",
      "\r\n",
      "\r\n",
      "def train(dataset, epochs):\r\n",
      "  for epoch in range(epochs):\r\n",
      "    start = time.time()\r\n",
      "\r\n",
      "    for image_batch in dataset:\r\n",
      "      train_step(image_batch)\r\n",
      "\r\n",
      "    display.clear_output(wait=True)\r\n",
      "    generate_and_save_images(generator,\r\n",
      "                             epoch + 1,\r\n",
      "                             seed)\r\n",
      "\r\n",
      "\r\n",
      "    if (epoch + 1) % 15 == 0:\r\n",
      "      checkpoint.save(file_prefix = checkpoint_prefix)\r\n",
      "\r\n",
      "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\r\n",
      "\r\n",
      "\r\n",
      "  display.clear_output(wait=True)\r\n",
      "  generate_and_save_images(generator,\r\n",
      "                           epochs,\r\n",
      "                           seed)\r\n",
      "\r\n",
      "def generate_and_save_images(model, epoch, test_input):\r\n",
      "\r\n",
      "  predictions = model(test_input, training=False)\r\n",
      "\r\n",
      "  fig = plt.figure(figsize=(4,4))\r\n",
      "\r\n",
      "  for i in range(predictions.shape[0]):\r\n",
      "      plt.subplot(4, 4, i+1)\r\n",
      "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\r\n",
      "      plt.axis('off')\r\n",
      "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\r\n",
      " # plt.show()\r\n",
      "\r\n",
      "def display_image(epoch_no):\r\n",
      "  return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))\r\n",
      "\r\n",
      "config = tf.compat.v1.ConfigProto()\r\n",
      "config.gpu_options.per_process_gpu_memory_fraction = 0.4\r\n",
      "session = tf.compat.v1.Session(config=config)\r\n",
      "\r\n",
      "(train_images, train_labels), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\r\n",
      "\r\n",
      "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\r\n",
      "train_images = (train_images - 127.5) / 127.5 # 将图片标准化到 [-1, 1] 区间内\r\n",
      "\r\n",
      "BUFFER_SIZE = 60000\r\n",
      "BATCH_SIZE = 256\r\n",
      "\r\n",
      "\r\n",
      "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\r\n",
      "\r\n",
      "\r\n",
      "generator =make_generator_model()\r\n",
      "\r\n",
      "noise = tf.random.normal([1, 100])\r\n",
      "generated_image = generator(noise, training=False)\r\n",
      "\r\n",
      "plt.imshow(generated_image[0, :, :, 0], cmap='gray')\r\n",
      "plt.show()\r\n",
      "\r\n",
      "\r\n",
      "discriminator = make_discriminator_model()\r\n",
      "decision = discriminator(generated_image)\r\n",
      "print (decision)\r\n",
      "\r\n",
      "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\r\n",
      "generator_optimizer = tf.keras.optimizers.Adam(1e-2)\r\n",
      "discriminator_optimizer = tf.keras.optimizers.Adam(1e-2)\r\n",
      "\r\n",
      "checkpoint_dir = './training_checkpoints'\r\n",
      "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\r\n",
      "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\r\n",
      "                                 discriminator_optimizer=discriminator_optimizer,\r\n",
      "                                 generator=generator,\r\n",
      "                                 discriminator=discriminator)\r\n",
      "\r\n",
      "EPOCHS = 50\r\n",
      "noise_dim = 100\r\n",
      "num_examples_to_generate = 16\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "seed = tf.random.normal([num_examples_to_generate, noise_dim])\r\n",
      "\r\n",
      "\r\n",
      "train(train_dataset, EPOCHS)\r\n",
      "\r\n",
      "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\r\n",
      "\r\n",
      "anim_file = 'dcgan.gif'\r\n",
      "\r\n",
      "with imageio.get_writer(anim_file, mode='I') as writer:\r\n",
      "  filenames = glob.glob('image*.png')\r\n",
      "  filenames = sorted(filenames)\r\n",
      "  last = -1\r\n",
      "  for i,filename in enumerate(filenames):\r\n",
      "    frame = 2*(i**0.5)\r\n",
      "    if round(frame) > round(last):\r\n",
      "      last = frame\r\n",
      "    else:\r\n",
      "      continue\r\n",
      "    image = imageio.imread(filename)\r\n",
      "    writer.append_data(image)\r\n",
      "  image = imageio.imread(filename)\r\n",
      "  writer.append_data(image)\r\n",
      "\r\n",
      "import IPython\r\n",
      "if IPython.version_info > (6,2,0,''):\r\n",
      "  display.Image(filename=anim_file)\r\n",
      "\r\n",
      "try:\r\n",
      "  from google.colab import files\r\n",
      "except ImportError:\r\n",
      "   pass\r\n",
      "else:\r\n",
      "  files.download(anim_file)\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:keras\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  NotFoundError: No CPU devices are available in this process\n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n",
      "- TensorFlow installed from (source or binary): source\r\n",
      "- TensorFlow version (use command below): 1.13.2\r\n",
      "- Python version: 2.7.17\r\n",
      "- Bazel version (if compiling from source): 0.19.2\r\n",
      "- GCC/Compiler version (if compiling from source): gcc 4.8\r\n",
      "\r\n",
      "\r\n",
      "Hello!\r\n",
      "I have built TF-cpu 1.13.2 from source code and I'm trying to run a benchmark [tensorflow/benchmark](https://github.com/tensorflow/benchmarks/tree/cnn_tf_v1.13_compatible/scripts/tf_cnn_benchmarks)\r\n",
      "An error (NotFoundError) occured after I run the following commands:\r\n",
      "`python tf_cnn_benchmarks.py --data_format=NHWC --num_gpus=0 --batch_size=8 --model=vgg16 --data_name=imagenet --variable_update=parameter_server --local_parameter_device=cpu --device=cpu > ps.log &`\r\n",
      "\r\n",
      "However tf can detect my CPU device correctly:\r\n",
      "```\r\n",
      "Python 2.7.17 (default, Sep 30 2020, 13:38:04)\r\n",
      "[GCC 7.5.0] on linux2\r\n",
      "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n",
      ">>> from tensorflow.python.client import device_lib\r\n",
      ">>> device_lib.list_local_devices()\r\n",
      "2021-01-04 08:35:20.085746: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F\r\n",
      "[name: \"/device:CPU:0\"\r\n",
      "device_type: \"CPU\"\r\n",
      "memory_limit: 268435456\r\n",
      "locality {\r\n",
      "}\r\n",
      "incarnation: 15041826958121108015\r\n",
      "]\r\n",
      ">>>\r\n",
      "```\r\n",
      "\r\n",
      "Any help would be appreciated. Thanks！\r\n",
      "\r\n",
      "Here is my log:\r\n",
      "\r\n",
      "```\r\n",
      "root@ip-172-31-90-169:/home/cluster/benchmarks/scripts/tf_cnn_benchmarks# python tf_cnn_benchmarks.py --data_format=NHWC --num_gpus=0 --batch_size=8 --model=vgg16 --data_name=imagenet --variable_update=parameter_server --local_parameter_device=cpu --device=cpu > ps.log &\r\n",
      "[1] 12899\r\n",
      "root@ip-172-31-90-169:/home/cluster/benchmarks/scripts/tf_cnn_benchmarks# Traceback (most recent call last):\r\n",
      "  File \"tf_cnn_benchmarks.py\", line 74, in <module>\r\n",
      "    app.run(main)  # Raises error on invalid flags, unlike tf.app.run()\r\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/absl/app.py\", line 303, in run\r\n",
      "    _run_main(main, args)\r\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/absl/app.py\", line 251, in _run_main\r\n",
      "    sys.exit(main(argv))\r\n",
      "  File \"tf_cnn_benchmarks.py\", line 63, in main\r\n",
      "    params = benchmark_cnn.setup(params)\r\n",
      "  File \"/home/cluster/benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py\", line 3503, in setup\r\n",
      "    with tf.Session(config=create_config_proto(params)) as sess:\r\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1551, in __init__\r\n",
      "    super(Session, self).__init__(target, graph, config=config)\r\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 676, in __init__\r\n",
      "    self._session = tf_session.TF_NewSessionRef(self._graph._c_graph, opts)\r\n",
      "tensorflow.python.framework.errors_impl.NotFoundError: No CPU devices are available in this process\r\n",
      "```\n",
      "issue labels - \n",
      "TF 1.13\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Disable Eager Execution for the OPs related to Graphs and Sessions in Tensorflow 2.x\n",
      "issue body -  ## URL(s) with the issue:\r\n",
      "\r\n",
      "Please provide a link to the documentation entry, for example:\r\n",
      "https://www.tensorflow.org/api_docs/python/tf/compat/v1/variable_scope\r\n",
      "\r\n",
      "## Description of issue (what needs changing): \r\n",
      "The command, **`tf.compat.v1.disable_eager_execution()`** should be added in the Code Examples of [the documentation](https://www.tensorflow.org/api_docs/python/tf/compat/v1/variable_scope) as the `Variable Scope` of `Graphs` is not applicable during `Eager Execution`.\r\n",
      "\r\n",
      "### Clear description\r\n",
      "\r\n",
      "For example, why should someone use this method? How is it useful? : For setting the **`Variable Scope`**.\r\n",
      "\r\n",
      "### Correct links\r\n",
      "\r\n",
      "Is the link to the source code correct? : Yes\r\n",
      "\r\n",
      "### Parameters defined\r\n",
      "\r\n",
      "Are all parameters defined and formatted correctly? : Yes\r\n",
      "\r\n",
      "### Returns defined\r\n",
      "\r\n",
      "Are return values defined? : NA\r\n",
      "\r\n",
      "### Raises listed and defined\r\n",
      "\r\n",
      "Are the errors defined? : Yes\r\n",
      "\r\n",
      "### Usage example\r\n",
      "\r\n",
      "Is there a usage example? : Yes\n",
      "issue labels - \n",
      "comp:apis\n",
      "type:docs-feature\n",
      "\n",
      "\n",
      "issue title -  TensorFlow\n",
      "issue body -  \n",
      "issue labels - \n",
      "stat:awaiting response\n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  ValueError: Invalid checkpoint state loaded from\n",
      "issue body -  Running Tensorflow 1.13.0  through Anaconda on Ubuntu 16.04 (Full install on external SSD)\r\n",
      "Python 3.6.11\r\n",
      "\r\n",
      "```\r\n",
      "import tensorflow as tf\r\n",
      "result = tf.train.get_checkpoint_state('')\r\n",
      "```\r\n",
      "I booted to Ubuntu 16.04 installed on external SSD from my Windows PC. When I run the above commands in my PC Internal drive path in anaconda environment, I get a warning \r\n",
      "```\r\n",
      "WARNING:tensorflow:FailedPreconditionError: checkpoint; Is a directory\r\n",
      "WARNING:tensorflow:checkpoint: Checkpoint ignored\r\n",
      "```\r\n",
      "\r\n",
      "But when I run the exact commands in my external SSD drive path in the same anaconda environment, I get a `ValueError: Invalid checkpoint state loaded from`\r\n",
      "\r\n",
      "What's the cause for this and how can I fix it?\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:apis\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  \"Table not initialized\" when loading model in Java\n",
      "issue body -  - TensorFlow version (use command below):2.3.0\r\n",
      "- Python version:3.7\r\n",
      "\r\n",
      "I am trying to use the tensorflow model in java,I convert a text classification model (with tf.lookup) to fomat .pb and want to load it in JAVA.But got \"Table not initialized\" error.\r\n",
      "```\r\n",
      "2021-01-04 14:00:10.713588: W tensorflow/core/framework/op_kernel.cc:1651] OP_REQUIRES failed at lookup_table_op.cc:809 : Failed precondition: Table not initialized.\r\n",
      "Exception in thread \"main\" java.lang.IllegalStateException: Table not initialized.\r\n",
      "\t [[{{node graph/hash_table_Lookup/LookupTableFindV2}}]]\r\n",
      "\tat org.tensorflow.Session.run(Native Method)\r\n",
      "\tat org.tensorflow.Session.access$100(Session.java:48)\r\n",
      "\tat org.tensorflow.Session$Runner.runHelper(Session.java:326)\r\n",
      "\tat org.tensorflow.Session$Runner.run(Session.java:276)\r\n",
      "\tat ctest.Ttest.predict(Ttest.java:32)\r\n",
      "\tat ctest.Ttest.main(Ttest.java:13)\r\n",
      "```\r\n",
      "\r\n",
      "**here is my code:**\r\n",
      "***In PYTHON***\r\n",
      "\r\n",
      "```\r\n",
      "import os\r\n",
      "import tensorflow.compat.v1 as tf\r\n",
      "tf.disable_v2_behavior()\r\n",
      "from tensorflow.python.framework.graph_util import convert_variables_to_constants\r\n",
      "from tensorflow.python.ops.lookup_ops import HashTable, KeyValueTensorInitializer\r\n",
      "\r\n",
      "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\r\n",
      "OUTPUT_FOLDER = ''\r\n",
      "OUTPUT_NAME = 'hash_table.pb'\r\n",
      "OUTPUT_NAMES = ['graph/output', 'init_all_tables']\r\n",
      "\r\n",
      "\r\n",
      "def build_graph():\r\n",
      "    d = {'a': 1, 'b': 2, 'c': 3, 'd': 4}\r\n",
      "    init = KeyValueTensorInitializer(list(d.keys()), list(d.values()))\r\n",
      "    hash_table = HashTable(init, default_value=-1)\r\n",
      "    data = tf.placeholder(tf.string, (None,), name='data')\r\n",
      "    values = hash_table.lookup(data)\r\n",
      "    output = tf.identity(values * 2, 'output')\r\n",
      "\r\n",
      "def freeze_graph():\r\n",
      "    with tf.Graph().as_default() as graph:\r\n",
      "        with tf.name_scope('graph'):\r\n",
      "            build_graph()\r\n",
      "\r\n",
      "        with tf.Session(graph=graph) as sess:\r\n",
      "            sess.run(tf.tables_initializer())\r\n",
      "            print(sess.run('graph/output:0', feed_dict={'graph/data:0': ['a', 'b', 'c', 'd', 'e']}))\r\n",
      "            frozen_graph = convert_variables_to_constants(sess, sess.graph_def, OUTPUT_NAMES)\r\n",
      "            tf.train.write_graph(frozen_graph, OUTPUT_FOLDER, OUTPUT_NAME, as_text=False)\r\n",
      "\r\n",
      "def load_frozen_graph():\r\n",
      "    with open(os.path.join(OUTPUT_FOLDER, OUTPUT_NAME), 'rb') as f:\r\n",
      "        output_graph_def = tf.GraphDef()\r\n",
      "        output_graph_def.ParseFromString(f.read())\r\n",
      "\r\n",
      "    with tf.Graph().as_default() as graph:\r\n",
      "        tf.import_graph_def(output_graph_def, name='')\r\n",
      "        with tf.Session(graph=graph) as sess:\r\n",
      "            try:\r\n",
      "                sess.run(graph.get_operation_by_name('init_all_tables'))\r\n",
      "            except KeyError:\r\n",
      "                pass\r\n",
      "            print(sess.run('graph/output:0', feed_dict={'graph/data:0': ['a', 'b', 'c', 'd', 'e']}))\r\n",
      "\r\n",
      "\r\n",
      "if __name__ == '__main__':\r\n",
      "    freeze_graph()\r\n",
      "    load_frozen_graph()\r\n",
      "```\r\n",
      "\r\n",
      "***In JAVA***\r\n",
      "\r\n",
      "```\r\n",
      "package ctest;\r\n",
      "\r\n",
      "import org.tensorflow.Graph;\r\n",
      "import org.tensorflow.Session;\r\n",
      "import org.tensorflow.Tensor;\r\n",
      "import java.nio.file.Files;\r\n",
      "import java.nio.file.Paths;\r\n",
      "\r\n",
      "public class Ttest {\r\n",
      "    public static void main(String[] args) throws Exception {\r\n",
      "        predict();\r\n",
      "    }\r\n",
      "    public static void predict() throws Exception {\r\n",
      "        try (Graph graph = new Graph()) {\r\n",
      "            graph.importGraphDef(Files.readAllBytes(Paths.get(\r\n",
      "                    \"/opt/resources/hash_table.pb\"\r\n",
      "            )));\r\n",
      "            try (Session sess = new Session(graph)) {\r\n",
      "                byte[][] matrix = new byte[1][];\r\n",
      "                matrix[0] = \"a\".getBytes(\"UTF-8\");\r\n",
      "                Tensor< ? > out = sess.runner()\r\n",
      "                        .feed(\"graph/data:0\", Tensor.create(matrix)).fetch(\"graph/output:0\").run().get(0);\r\n",
      "                float[][] output = new float[1][(int) out.shape()[1]];\r\n",
      "                out.copyTo(output);\r\n",
      "                for(float i:output[0])\r\n",
      "                    System.out.println(i);\r\n",
      "\r\n",
      "\r\n",
      "            }\r\n",
      "        }\r\n",
      "    }\r\n",
      "}\r\n",
      "```\r\n",
      "\r\n",
      "Any suggestions would be greatly appreciated.\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.3\n",
      "comp:apis\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  This throws ERROR: features = tf.io.parse_example(..., features=make_parse_example_spec(columns))\n",
      "issue body -  <em>Please make sure that this is a bug. As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:bug_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 20.04\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "- TensorFlow installed from (source or binary): Source\r\n",
      "- TensorFlow version (use command below): Latest from this week\r\n",
      "- Python version: 3.8.x\r\n",
      "- Bazel version (if compiling from source): 3.1.0\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version: 11.2\r\n",
      "- GPU model and memory: \r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "I was trying this code, but it throws exception\r\n",
      "\r\n",
      "```https://www.tensorflow.org/api_docs/python/tf/keras/experimental/SequenceFeatures```\r\n",
      "\r\n",
      "```return ops.EagerTensor(value, ctx.device_name, dtype)\r\n",
      "ValueError: Attempt to convert a value (Ellipsis) with an unsupported type (<class 'ellipsis'>) to a Tensor.```\r\n",
      "\r\n",
      "\r\n",
      "Here is the full code from that page,\r\n",
      "\r\n",
      "\r\n",
      "```# Behavior of some cells or feature columns may depend on whether we are in\r\n",
      "# training or inference mode, e.g. applying dropout.\r\n",
      "training = True\r\n",
      "rating = sequence_numeric_column('rating')\r\n",
      "watches = sequence_categorical_column_with_identity(\r\n",
      "    'watches', num_buckets=1000)\r\n",
      "watches_embedding = embedding_column(watches, dimension=10)\r\n",
      "columns = [rating, watches_embedding]\r\n",
      "\r\n",
      "sequence_input_layer = SequenceFeatures(columns)\r\n",
      "features = tf.io.parse_example(...,\r\n",
      "                               features=make_parse_example_spec(columns))\r\n",
      "sequence_input, sequence_length = sequence_input_layer(\r\n",
      "   features, training=training)\r\n",
      "sequence_length_mask = tf.sequence_mask(sequence_length)\r\n",
      "\r\n",
      "rnn_cell = tf.keras.layers.SimpleRNNCell(hidden_size, training=training)\r\n",
      "rnn_layer = tf.keras.layers.RNN(rnn_cell, training=training)\r\n",
      "outputs, state = rnn_layer(sequence_input, mask=sequence_length_mask)\r\n",
      "```\n",
      "issue labels - \n",
      "TF 2.5\n",
      "comp:apis\n",
      "stat:contributions welcome\n",
      "type:docs-bug\n",
      "\n",
      "\n",
      "issue title -  Installing\n",
      "issue body -  <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10 enterprise\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "- TensorFlow installed from (source or binary): pip\r\n",
      "- TensorFlow version: nightly or 2.4\r\n",
      "- Python version: 3.8.6\r\n",
      "- Installed using virtualenv? pip? conda?: normal pip\r\n",
      "- Bazel version (if compiling from source): n/a\r\n",
      "- GCC/Compiler version (if compiling from source): n/a\r\n",
      "- CUDA/cuDNN version: 11.0 (Recomemded\r\n",
      "- GPU model and memory: gtx 1060 6gb\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "\r\n",
      "I run pip install tf-nightly or pip install tensorflow and get the following error\r\n",
      " ERROR: Command errored out with exit status 1:\r\n",
      "     command: 'c:\\users\\kai\\appdata\\local\\programs\\python\\python38\\python.exe' -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\kai\\\\AppData\\\\Local\\\\Temp\\\\pip-install-w2yenss2\\\\wrapt\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\kai\\\\AppData\\\\Local\\\\Temp\\\\pip-install-w2yenss2\\\\wrapt\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base 'C:\\Users\\kai\\AppData\\Local\\Temp\\pip-pip-egg-info-bql118ug'\r\n",
      "         cwd: C:\\Users\\kai\\AppData\\Local\\Temp\\pip-install-w2yenss2\\wrapt\\\r\n",
      "    Complete output (26 lines):\r\n",
      "    Traceback (most recent call last):\r\n",
      "      File \"c:\\users\\kai\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pkg_resources\\__init__.py\", line 2866, in get_entry_map\r\n",
      "        ep_map = self._ep_map\r\n",
      "      File \"c:\\users\\kai\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pkg_resources\\__init__.py\", line 2824, in __getattr__\r\n",
      "        raise AttributeError(attr)\r\n",
      "    AttributeError: _ep_map\r\n",
      "\r\n",
      "    During handling of the above exception, another exception occurred:\r\n",
      "\r\n",
      "    Traceback (most recent call last):\r\n",
      "      File \"<string>\", line 1, in <module>\r\n",
      "      File \"C:\\Users\\kai\\AppData\\Local\\Temp\\pip-install-w2yenss2\\wrapt\\setup.py\", line 102, in <module>\r\n",
      "        run_setup(with_extensions=True)\r\n",
      "      File \"C:\\Users\\kai\\AppData\\Local\\Temp\\pip-install-w2yenss2\\wrapt\\setup.py\", line 72, in run_setup\r\n",
      "        setup(**setup_kwargs_tmp)\r\n",
      "      File \"c:\\users\\kai\\appdata\\local\\programs\\python\\python38\\lib\\distutils\\core.py\", line 108, in setup\r\n",
      "        _setup_distribution = dist = klass(attrs)\r\n",
      "      File \"c:\\users\\kai\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\setuptools\\dist.py\", line 427, in __init__\r\n",
      "        for ep in pkg_resources.iter_entry_points('distutils.setup_keywords'):\r\n",
      "      File \"c:\\users\\kai\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pkg_resources\\__init__.py\", line 655, in <genexpr>\r\n",
      "        for entry in dist.get_entry_map(group).values()\r\n",
      "      File \"c:\\users\\kai\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pkg_resources\\__init__.py\", line 2868, in get_entry_map\r\n",
      "        ep_map = self._ep_map = EntryPoint.parse_map(\r\n",
      "      File \"c:\\users\\kai\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pkg_resources\\__init__.py\", line 2549, in parse_map\r\n",
      "        raise ValueError(\"Entry points must be listed in groups\")\r\n",
      "    ValueError: Entry points must be listed in groups\r\n",
      "    ----------------------------------------\r\n",
      "ERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\r\n",
      "\r\n",
      "\r\n",
      "**Any other info / logs**\r\n",
      "Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "subtype:windows\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  Performance 5x degradation after converting numpy to tf.function equivalent\n",
      "issue body -  <em>Please make sure that this is an issue related to performance of TensorFlow.\r\n",
      "As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:performance_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Custom code\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu (Google colab) & macOs 11.1\r\n",
      "- TensorFlow installed from (source or binary): pip installation.\r\n",
      "- TensorFlow version (use command below): 2.3.1 / 2.4\r\n",
      "- Python version: 3.6+\r\n",
      "- CUDA/cuDNN version: 10.1\r\n",
      "- GPU model and memory: Varies\r\n",
      "\r\n",
      "You can collect some of this information using our environment capture\r\n",
      "[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n",
      "You can also obtain the TensorFlow version with:\r\n",
      "1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n",
      "2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "\r\n",
      "I converted this function, that performs gradient updates for a DQN agent. It takes a batch of numpy arrays, calculates target values and updates gradients using `model.fit()` for 1 epoch, it usually processes 90-100 frames per second on google colab.\r\n",
      "\r\n",
      "    def update(self, batch):\r\n",
      "        \"\"\"\r\n",
      "        Update gradients given a batch.\r\n",
      "        Args:\r\n",
      "            batch: A batch of observations in the form of\r\n",
      "                [[states], [actions], [rewards], [dones], [next states]]\r\n",
      "    \r\n",
      "        Returns:\r\n",
      "            None\r\n",
      "        \"\"\"\r\n",
      "        states, actions, rewards, dones, new_states = batch\r\n",
      "        q_states = self.main_model.predict(states)\r\n",
      "        if self.double:\r\n",
      "            new_state_actions = np.argmax(self.main_model.predict(new_states), 1)\r\n",
      "            new_state_q_values = self.target_model.predict(new_states)\r\n",
      "            new_state_values = new_state_q_values[\r\n",
      "                np.arange(self.batch_size), new_state_actions\r\n",
      "            ]\r\n",
      "        else:\r\n",
      "            new_state_values = self.target_model.predict(new_states).max(1)\r\n",
      "        new_state_values[dones] = 0\r\n",
      "        target_values = np.copy(q_states)\r\n",
      "        target_value_update = new_state_values * self.gamma ** self.n_steps + rewards\r\n",
      "        state_action_values = target_values[np.arange(self.batch_size), actions]\r\n",
      "        target_values[np.arange(self.batch_size), actions] = target_value_update\r\n",
      "        self.main_model.fit(states, target_values, verbose=0)\r\n",
      "        if self.buffer.priorities:\r\n",
      "            squared_loss = (state_action_values - target_value_update) ** 2\r\n",
      "            priorities = (\r\n",
      "                self.buffer.current_weights * squared_loss + self.buffer.priority_bias\r\n",
      "            )\r\n",
      "            self.buffer.update_priorities(priorities)\r\n",
      "\r\n",
      "**After the conversion:**\r\n",
      "\r\n",
      "    @tf.function\r\n",
      "    def get_targets(self, batch):\r\n",
      "        \"\"\"\r\n",
      "        Get target values for gradient updates.\r\n",
      "        Args:\r\n",
      "            batch: A batch of observations in the form of\r\n",
      "                [[states], [actions], [rewards], [dones], [next states]]\r\n",
      "        Returns:\r\n",
      "            None\r\n",
      "        \"\"\"\r\n",
      "        states, actions, rewards, dones, new_states = batch\r\n",
      "        q_states = self.main_model(states)\r\n",
      "        if self.double:\r\n",
      "            new_state_actions = tf.argmax(self.main_model(new_states), 1)\r\n",
      "            new_state_q_values = self.target_model(new_states)\r\n",
      "            a = self.get_action_indices(new_state_actions)\r\n",
      "            new_state_values = tf.gather_nd(new_state_q_values, a)\r\n",
      "        else:\r\n",
      "            new_state_values = tf.reduce_max(self.target_model(new_states), axis=1)\r\n",
      "        new_state_values *= tf.cast(~dones, tf.float32)\r\n",
      "        target_values = tf.identity(q_states)\r\n",
      "        target_value_update = new_state_values * self.gamma ** self.n_steps + tf.cast(\r\n",
      "            rewards, tf.float32\r\n",
      "        )\r\n",
      "        indices = self.get_action_indices(actions)\r\n",
      "        state_action_values = tf.gather_nd(target_values, indices)\r\n",
      "        tf.tensor_scatter_nd_update(target_values, indices, target_value_update)\r\n",
      "        if self.buffer.priorities:\r\n",
      "            squared_loss = (state_action_values - target_value_update) ** 2\r\n",
      "            priorities = (\r\n",
      "                self.buffer.current_weights * squared_loss + self.buffer.priority_bias\r\n",
      "            )\r\n",
      "            self.buffer.update_priorities(priorities)\r\n",
      "        return target_values\r\n",
      "\r\n",
      "And I get `model.fit()` outside the method which results in a severe performance degradation ~= 20 frames per seconds.\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "Here's the colab [notebook](https://colab.research.google.com/drive/1PNSURh-Hhd2CtQ5vn82lG0rsJjYkYPqR?usp=sharing) for the original version(numpy, no `tf.function`)\r\n",
      "\r\n",
      "Here's the less performant / worse version colab [notebook](https://colab.research.google.com/drive/1qSzJSepVSYyA3dRpdpgHo2cRWeOg2nSJ?usp=sharing)\r\n",
      "\r\n",
      "**Other info / logs** Include any logs or source code that would be helpful to\r\n",
      "diagnose the problem. If including tracebacks, please include the full\r\n",
      "traceback. Large logs and files should be attached.\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "stat:awaiting response\n",
      "type:performance\n",
      "\n",
      "\n",
      "issue title -  Unhandled Rejecting Error ~ Implicit Shape Can't Be a Fractional Number\n",
      "issue body -  Hello everyone, sos\r\n",
      "\r\n",
      "I am following an online tutorial on how to run gesture recognition using react and tensor flow. However, I am always seeing this error whenever I play around with the webcam in chrome.\r\n",
      "\r\n",
      "Here is my github for what I am working on btw. And here is the tutorial video I'm watching. I got stuck right around minute 10\r\n",
      "\r\n",
      "https://github.com/riccrdo5/help\r\n",
      "\r\n",
      "https://youtu.be/f7uBsb-0sGQ\r\n",
      "\r\n",
      "Ty and happy holidays\r\n",
      "\r\n",
      "![ErrorVSC](https://user-images.githubusercontent.com/67179440/103488631-02733c00-4dc3-11eb-81d1-70c75b9f9ffa.png)\r\n",
      "![ErrorWeb](https://user-images.githubusercontent.com/67179440/103488632-043cff80-4dc3-11eb-8446-0a80b0258b26.jpg)\r\n",
      "\n",
      "issue labels - \n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  Installation/Import issue. Help required.\n",
      "issue body -  <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n",
      "  -  OS Name:                   Microsoft Windows 10 Home Single Language\r\n",
      "  -  OS Version:                10.0.18363 N/A Build 18363\r\n",
      "  -  OS Manufacturer:           Microsoft Corporation\r\n",
      "  -  OS Configuration:          Standalone Workstation\r\n",
      "  -  OS Build Type:             Multiprocessor Free\r\n",
      "\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n",
      "- TensorFlow installed from (source or binary): pip\r\n",
      "- TensorFlow version: 2.x\r\n",
      "- Python version: 3.7.3\r\n",
      "- Installed using virtualenv? pip? conda?: pip\r\n",
      "- Bazel version (if compiling from source): NA\r\n",
      "- GCC/Compiler version (if compiling from source): NA\r\n",
      "- CUDA/cuDNN version: NA\r\n",
      "- GPU model and memory: NA\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**Installed without any issue. But when trying to import, ImportError occurs.**\r\n",
      "\r\n",
      "**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n",
      " 1. ```pip install tensorflow --verbose --user```\r\n",
      " 2. Then opened a .py file with first and only line as : ```import tensorflow```\r\n",
      " 3. Saved the .py file and opened a new cmd window in the same directory.\r\n",
      " 4. Executed ```python name_of_the_file.py```\r\n",
      "\r\n",
      "\r\n",
      "**Any other info / logs**\r\n",
      "\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"C:\\Users\\ddsme\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <\r\n",
      "module>\r\n",
      "    from tensorflow.python._pywrap_tensorflow_internal import *\r\n",
      "ImportError: DLL load failed: The specified module could not be found.\r\n",
      "\r\n",
      "During handling of the above exception, another exception occurred:\r\n",
      "\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"<stdin>\", line 1, in <module>\r\n",
      "  File \"C:\\Users\\ddsme\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n",
      "    from tensorflow.python.tools import module_util as _module_util\r\n",
      "  File \"C:\\Users\\ddsme\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\__init__.py\", line 39, in <module>\r\n",
      "    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\r\n",
      "  File \"C:\\Users\\ddsme\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 83, in <\r\n",
      "module>\r\n",
      "    raise ImportError(msg)\r\n",
      "ImportError: Traceback (most recent call last):\r\n",
      "  File \"C:\\Users\\ddsme\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <\r\n",
      "module>\r\n",
      "    from tensorflow.python._pywrap_tensorflow_internal import *\r\n",
      "ImportError: DLL load failed: The specified module could not be found.\r\n",
      "\r\n",
      "\r\n",
      "Failed to load the native TensorFlow runtime.\n",
      "issue labels - \n",
      "TF 2.4\n",
      "subtype:windows\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  Update README.md\n",
      "issue body -  modified README.md of benchmark as default value of disable_nnapi_cpu parameter changed to true\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:lite\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  always got the time out when try to download upb file. \n",
      "issue body -  Firstly try to build TP  2.3 branch in windows 10 using the following command, always got the time out when download upb file. \r\n",
      "can I manully download the file or disable the download operation? or modify the connect_time_out option?\r\n",
      "bazel build //tensorflow/tools/pip_package:build_pip_package\r\n",
      "ERROR: no such package '@upb//bazel': java.io.IOException: Error downloading [https://github.com/protocolbuffers/upb/archive/9effcbcb27f0a665f9f345030188c0b291e32482.tar.gz] to C:/users/86178/_bazel_86178/26orbg4z/external/upb/9effcbcb27f0a665f9f345030188c0b291e32482.tar.gz: connect timed out\r\n",
      "INFO: Elapsed time: 69.014s\r\n",
      "INFO: 0 processes.\r\n",
      "FAILED: Build did NOT complete successfully (0 packages loaded)\n",
      "issue labels - \n",
      "TF 2.3\n",
      "subtype:windows\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  Bazel out of memory at checking cache actions when building for ARMv8\n",
      "issue body -  <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n",
      "\r\n",
      "### System information\r\n",
      "- OS Platform and Distribution: Ubuntu 20.04\r\n",
      "- TensorFlow installed from (source or binary): source\r\n",
      "- TensorFlow version: v2.4.0\r\n",
      "- Python version: 3.8\r\n",
      "- Installed using virtualenv? pip? conda?: pip\r\n",
      "- Bazel version (if compiling from source): 3.7.2\r\n",
      "- GCC/Compiler version (if compiling from source): 9.3.0\r\n",
      "- CUDA/cuDNN version: None\r\n",
      "- GPU model and memory: None\r\n",
      "\r\n",
      "### Describe the problem\r\n",
      "\r\n",
      "**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n",
      "\r\n",
      "Hello.\r\n",
      "\r\n",
      "I am tried to build tensorflow v2.4.0 from source for aarch64 / ARMv8 devices. I did this on a quad core A53 single board with 1GB RAM.\r\n",
      "\r\n",
      "The process was always terminated at the time when about 3000 targets were done. At that moment, Bazel showed **Checking cache actions**, stuck for a while and raise java out of memory error.\r\n",
      "\r\n",
      "I thought that the RAM and swap might be not enough, so I picked a 8GB USB drive and use it as swap. But the building process still crashed.\r\n",
      "\r\n",
      "So, how much memory does it need when building tensorflow? Should I at least double the swap size, or any step is wrong?\r\n",
      "\r\n",
      "Thank you.\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "subtype: ubuntu/linux\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  Update version_check.bzl\n",
      "issue body -  Update the version\n",
      "issue labels - \n",
      "cla: no\n",
      "invalid\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  How to convert this numpy to tf.function compatible code?\n",
      "issue body -  I'm trying to convert numpy to tensorflow equivalent code to be compatible with `tf.function` ...\r\n",
      "\r\n",
      "Given have a `(32, 6)` numpy array `target_values` that looks like this:\r\n",
      "\r\n",
      "    array([[-0.01656106,  0.04762066,  0.05735449, -0.0284767 , -0.02237438,\r\n",
      "            -0.00042562],\r\n",
      "           [-0.01420249,  0.0477839 ,  0.0563598 , -0.02971786, -0.02367548,\r\n",
      "             0.00001262],\r\n",
      "           [-0.01695916,  0.04826669,  0.05893629, -0.03067053, -0.02261235,\r\n",
      "             0.00345904],\r\n",
      "           [-0.01953977,  0.04540274,  0.05829531, -0.02759781, -0.02390759,\r\n",
      "            -0.00487727],\r\n",
      "           [-0.01708016,  0.04894669,  0.0606699 , -0.02576046, -0.02461138,\r\n",
      "            -0.00068538],\r\n",
      "           [-0.01604217,  0.04770135,  0.05761468, -0.02858265, -0.02624938,\r\n",
      "            -0.00084356],\r\n",
      "           [-0.01527106,  0.04699571,  0.05959677, -0.02956396, -0.02510098,\r\n",
      "            -0.00223234],\r\n",
      "           [-0.01448676,  0.04620824,  0.05775366, -0.03008122, -0.02655901,\r\n",
      "            -0.00159649],\r\n",
      "           [-0.0172577 ,  0.04814827,  0.05807308, -0.02916523, -0.02367857,\r\n",
      "            -0.00100602],\r\n",
      "           [-0.01690523,  0.0484785 ,  0.05807881, -0.02960616, -0.02560546,\r\n",
      "            -0.00065042],\r\n",
      "           [-0.0166171 ,  0.0488232 ,  0.05776291, -0.03231864, -0.02132723,\r\n",
      "            -0.00033605],\r\n",
      "           [-0.01541627,  0.04840397,  0.0580376 , -0.02927143, -0.02461101,\r\n",
      "             0.00121263],\r\n",
      "           [-0.01685588,  0.047661  ,  0.05873172, -0.02989979, -0.02574112,\r\n",
      "            -0.00126612],\r\n",
      "           [-0.01333553,  0.05043796,  0.05915743, -0.02990219, -0.02657976,\r\n",
      "            -0.0007656 ],\r\n",
      "           [-0.01531163,  0.04781894,  0.05637252, -0.02968849, -0.02225551,\r\n",
      "            -0.00151382],\r\n",
      "           [-0.01357749,  0.04807179,  0.05955081, -0.02748637, -0.02498721,\r\n",
      "            -0.00040934],\r\n",
      "           [-0.01606943,  0.04768877,  0.05455931, -0.03136749, -0.02475093,\r\n",
      "             0.00245846],\r\n",
      "           [-0.01609829,  0.04687681,  0.05982678, -0.02886578, -0.02608151,\r\n",
      "             0.00015348],\r\n",
      "           [-0.01503662,  0.04740106,  0.05958583, -0.03141545, -0.02522127,\r\n",
      "            -0.00063602],\r\n",
      "           [-0.01697148,  0.04910276,  0.05744712, -0.02858391, -0.02481578,\r\n",
      "            -0.00072039],\r\n",
      "           [-0.01503395,  0.04843756,  0.05773868, -0.03061879, -0.02586869,\r\n",
      "            -0.00025573],\r\n",
      "           [-0.0152991 ,  0.04847359,  0.05739099, -0.0299796 , -0.02552593,\r\n",
      "            -0.00334571],\r\n",
      "           [-0.01324895,  0.04529134,  0.05534273, -0.03109139, -0.02304241,\r\n",
      "            -0.00143186],\r\n",
      "           [-0.01280282,  0.05004944,  0.05856398, -0.0314032 , -0.02394999,\r\n",
      "            -0.00030306],\r\n",
      "           [-0.01677033,  0.04876196,  0.05794405, -0.02888608, -0.02658239,\r\n",
      "            -0.00015171],\r\n",
      "           [-0.01572544,  0.04779808,  0.05939355, -0.03048976, -0.02896303,\r\n",
      "            -0.00090334],\r\n",
      "           [-0.01542805,  0.04709881,  0.05839922, -0.02894112, -0.02240603,\r\n",
      "            -0.00188624],\r\n",
      "           [-0.01493233,  0.0476524 ,  0.0581631 , -0.0297201 , -0.02485022,\r\n",
      "            -0.00087418],\r\n",
      "           [-0.01804641,  0.04739738,  0.06070606, -0.02981704, -0.02543145,\r\n",
      "            -0.00115484],\r\n",
      "           [-0.01518638,  0.04843838,  0.05744548, -0.02980216, -0.02420005,\r\n",
      "             0.00036349],\r\n",
      "           [-0.01442349,  0.04673778,  0.05804737, -0.03062913, -0.02476445,\r\n",
      "            -0.00066772],\r\n",
      "           [-0.01598305,  0.04622466,  0.0588723 , -0.03096713, -0.02364032,\r\n",
      "            -0.00005574]])\r\n",
      "\r\n",
      "Given another `(32,)` array of indices `actions` with values being in range(5) inclusive:\r\n",
      "\r\n",
      "    array([0, 2, 5, 5, 1, 1, 3, 4, 0, 5, 4, 3, 4, 5, 1, 0, 3, 0, 0, 2, 2, 2,\r\n",
      "           0, 1, 4, 1, 4, 4, 0, 4, 1, 0])\r\n",
      "\r\n",
      "I'm expecting this result:\r\n",
      "\r\n",
      "    array([-0.01656106,  0.0563598 ,  0.00345904, -0.00487727,  0.04894669,\r\n",
      "            0.04770135, -0.02956396, -0.02655901, -0.0172577 , -0.00065042,\r\n",
      "           -0.02132723, -0.02927143, -0.02574112, -0.0007656 ,  0.04781894,\r\n",
      "           -0.01357749, -0.03136749, -0.01609829, -0.01503662,  0.05744712,\r\n",
      "            0.05773868,  0.05739099, -0.01324895,  0.05004944, -0.02658239,\r\n",
      "            0.04779808, -0.02240603, -0.02485022, -0.01804641, -0.02420005,\r\n",
      "            0.04673778, -0.01598305], dtype=float32)\r\n",
      "\r\n",
      "For `self.batch_size == 32`, I'm able to achieve what I need in numpy using:\r\n",
      "\r\n",
      "    state_action_values = target_values[np.arange(self.batch_size), actions]\r\n",
      "\r\n",
      "For `target_value_update` being another `(32,)` array of new values, I will need to assign the new values to this slice using:\r\n",
      "\r\n",
      "    target_values[np.arange(self.batch_size), actions] = target_value_update\r\n",
      "\r\n",
      "However in tensorflow under `tf.function`, this is not possible and I get the following error:\r\n",
      "\r\n",
      "    TypeError: Only integers, slices (`:`), ellipsis (`...`), tf.newaxis (`None`) and scalar tf.int32/tf.int64 tensors are valid indices, got array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\r\n",
      "           17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])\r\n",
      "\r\n",
      "So I try:\r\n",
      "\r\n",
      "    target_values = tf.Variable(target_values)\r\n",
      "    state_action_values = tf.gather(target_values, actions, axis=1)\r\n",
      "\r\n",
      "However here's the value of `state_action_values` which should be `(32,)` not `(32, 32)`\r\n",
      "\r\n",
      "    Tensor(\"GatherV2:0\", shape=(32, 32), dtype=float32)\r\n",
      "\r\n",
      "\r\n",
      "  [1]: https://www.tensorflow.org/api_docs/python/tf/function\r\n",
      "\n",
      "issue labels - \n",
      "comp:autograph\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  RPi Zero(ARMv6l) _FusedConv2D <no registered kernels>\n",
      "issue body -  **System information**\r\n",
      "- Using Keras MobileNet model\r\n",
      "- Platform: RPi Zero (armv6l)\r\n",
      "- Tested on binaries: v1.14.0 from piwheel / v2.4.0 from [https://github.com/lhelontra/tensorflow-on-arm/releases/tag/v2.4.0](url)\r\n",
      "- Python version: 3.7\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "Error when attempting model.predict on RPi zero using keras.apps MobileNet model\r\n",
      "Attempting _FusedConv2D calculation via a non existing kernel\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "Attempted unfused conv2D + relu since armv6l doesnt support it\r\n",
      "Successful forward pass calculation of model\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "https://pastebin.com/4syM43S1\r\n",
      "\r\n",
      "Reproducable when applied on RPi Zero \r\n",
      "libs:\r\n",
      "- atlasbase\r\n",
      "- hdf5\r\n",
      "- openjp2\r\n",
      "- tiff5\r\n",
      "\r\n",
      "pip: \r\n",
      "- tf\r\n",
      "- keras\r\n",
      "- h5py\r\n",
      "- pillow\r\n",
      "\r\n",
      "**Other info / logs**\r\n",
      "Trace (TF v2.4.0): https://pastebin.com/QNFsJjkJ\r\n",
      "\r\n",
      "Having same issue as open:  https://github.com/tensorflow/tensorflow/issues/24732\r\n",
      "\r\n",
      "Could a possible work around for this issue be to enter the TFLite path?\r\n",
      "Or can FusedConv2D be disabled before building binary?\r\n",
      "\r\n",
      "Thank you.\r\n",
      "\r\n",
      "**EDIT**: TFLite path was great \r\n",
      "https://github.com/google-coral/pycoral/issues/7\r\n",
      "https://github.com/google-coral/edgetpu/issues/229\r\n",
      "https://drive.google.com/file/d/1mW8QGmhfk4kRYXqUTEH1ckPUNIsl7S3d/view\r\n",
      "https://www.tensorflow.org/lite/guide/inference#load_and_run_a_model_in_python\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:lite\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  ImageDataGenerator().batch_index starts from zero even though manually setting it\n",
      "issue body -  <em>Please make sure that this is a bug. As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:bug_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n",
      "- TensorFlow installed from (source or binary): Binary\r\n",
      "- TensorFlow version (use command below): 2.4.0\r\n",
      "- Python version: 3.7.9\r\n",
      "- Bazel version (if compiling from source): n/a\r\n",
      "- GCC/Compiler version (if compiling from source): n/a\r\n",
      "- CUDA/cuDNN version: Cuda 11.0 update 1 + cudnn 8.0.5\r\n",
      "- GPU model and memory: RTX 3080 10GB\r\n",
      "\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "I define my datagenerator by `ImageDataGenerator(preprocessing_function=preprocess_input)` then I define the generator:        \r\n",
      "\r\n",
      "```\r\n",
      " generator = datagen.flow_from_directory(\r\n",
      "            directory=data_dir,\r\n",
      "            target_size=(112, 112),\r\n",
      "            color_mode=\"rgb\",\r\n",
      "            class_mode=None,\r\n",
      "            batch_size=batch_size,\r\n",
      "            shuffle=False,\r\n",
      "        )\r\n",
      "```\r\n",
      "I want to **start at a particular batch_index**, so I set the `generator.batch_index = 500`. When I loop for the first time over the generator, it will ignore this set batch_index and start at zero:  \r\n",
      "```\r\n",
      "for batch, label in generator:\r\n",
      "    pred = model.predict(batch, verbose=0)\r\n",
      "    features.append(scipy.sparse.csr_matrix(pred))\r\n",
      "```\r\n",
      "\r\n",
      "But if just loop over it once with \r\n",
      "```\r\n",
      "for batch, label in generator:\r\n",
      "    break\r\n",
      "```\r\n",
      " and then set the `generator.batch_index = 500`, and then loop over it again with the previous code it will start at the correct index.\r\n",
      "\r\n",
      "I don't know if this is a feature or not. But if it is, it's not very practical to loop over it once before being able to adjust the batch_index.\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "I should be able to change the batch_index before looping over the generator. \r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "Provide a reproducible test case that is the bare minimum necessary to generate\r\n",
      "the problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:keras\n",
      "stat:awaiting tensorflower\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  what CUDA 11 and cuDNN i have to install to use TF 2.4?\n",
      "issue body -  Hello i just want know what CUDA 11 and cuDNN i have to install to use TF 2.4\r\n",
      "I'ts not clear in doc\r\n",
      "![image](https://user-images.githubusercontent.com/73416709/103465568-4885bc80-4d3d-11eb-8438-4ccf6f940133.png)\r\n",
      "Thanks\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  tensorflow lite model hangs in tflite.run in android emulator\n",
      "issue body -  Please go to Stack Overflow for help and support:\r\n",
      "\r\n",
      "https://stackoverflow.com/questions/tagged/tensorflow\r\n",
      "\r\n",
      "If you open a GitHub issue, here is our policy:\r\n",
      "\r\n",
      "1.  It must be a bug, a feature request, or a significant problem with the\r\n",
      "    documentation (for small docs fixes please send a PR instead).\r\n",
      "2.  The form below must be filled out.\r\n",
      "3.  It shouldn't be a TensorBoard issue. Those go\r\n",
      "    [here](https://github.com/tensorflow/tensorboard/issues).\r\n",
      "\r\n",
      "**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n",
      "\r\n",
      "------------------------\r\n",
      "\r\n",
      "### System information\r\n",
      "\r\n",
      "-   **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n",
      "     \r\n",
      "-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: \r\n",
      "     Linux Ubuntu 20.04\r\n",
      "-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on a mobile device**:\r\n",
      "     Android emulator.\r\n",
      "-   **TensorFlow installed from (source or binary)**:\r\n",
      "    Installed from source\r\n",
      "-   **TensorFlow version (use command below)**:\r\n",
      "    Tensorflow version used to convert tf model to tflite: 2.5.0\r\n",
      "    Tensorflow version and tf op version:  2.3.0\r\n",
      "\r\n",
      "-   **Python version**: 3.8.0\r\n",
      "-   **Bazel version (if compiling from source)**:\r\n",
      "-   **GCC/Compiler version (if compiling from source)**:\r\n",
      "-   **CUDA/cuDNN version**:\r\n",
      "-   **GPU model and memory**:\r\n",
      "-   **Exact command to reproduce**:\r\n",
      "\r\n",
      "You can collect some of this information using our environment capture script:\r\n",
      "\r\n",
      "https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n",
      "\r\n",
      "You can obtain the TensorFlow version with:\r\n",
      "\r\n",
      "```bash\r\n",
      "python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n",
      "```\r\n",
      "\r\n",
      "### Describe the problem\r\n",
      "Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n",
      "\r\n",
      "### Source code / logs\r\n",
      "Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.3\n",
      "comp:lite\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Illegal instruction (core dumped) using tensorflow==2.4.0 with AVX instruction when importing\n",
      "issue body -  **System information**\r\n",
      "- OS Platform and Distribution: Linux Ubuntu 18.04)\r\n",
      "- TensorFlow installed from (source or binary): pip\r\n",
      "- TensorFlow version (use command below): 2.4.0\r\n",
      "- Python version: 3.7.9\r\n",
      "- CUDA/cuDNN version: 11.0/8.0.5\r\n",
      "- GPU model and memory: GTX 2080Ti\r\n",
      "\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "\r\n",
      "Attempting to import tensorflow produces an \"Illegal instruction(core dumped)\" error.\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "\r\n",
      "Import tensorflow without error.\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "```\r\n",
      "conda create -n test-tf2.4 python=3.7\r\n",
      "conda activate test-tf2.4\r\n",
      "pip install tensorflow==2.4.0\r\n",
      "```\r\n",
      "The following command exits with an \"Illegal instruction\" error:\r\n",
      "```\r\n",
      "python -c \"import tensorflow as tf\"\r\n",
      "```\r\n",
      "\r\n",
      "**Other info / logs** \r\n",
      "This will be not an issue with the last development version of 2.5.0 (tf-nightly 2.5.0.dev20210102) from tf-nightly, installed with:\r\n",
      "```\r\n",
      "pip install tf-nightly\r\n",
      "```\r\n",
      "\r\n",
      "MOST IMPORTANTLY, My machine has AVX instructions. So this is not the same as the previous \"old cpu\", while it is THE SAME AS the [#44668 ](https://github.com/tensorflow/tensorflow/issues/44668)problem  which your team has NOT SOLVED.\n",
      "issue labels - \n",
      "TF 2.4\n",
      "stat:awaiting tensorflower\n",
      "subtype: ubuntu/linux\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  Fix MobileNetV3 Preprocess\n",
      "issue body -  I measured Top1-Acc of ImageNet pretrained MobileNetV3Large model in `tensorflow/tensorflow/python/keras/applications/mobilenet_v3.py` , but 65% is measured.\r\n",
      "\r\n",
      "After changing preprocess layer\r\n",
      "```python\r\n",
      "x = layers.Rescaling(1. / 255.)(x)\r\n",
      "```\r\n",
      "to\r\n",
      "```python\r\n",
      "x = layers.Rescaling(scale=1./127.5, offset=-1.)(x)\r\n",
      "```\r\n",
      "can measure 72.5% Top1-Acc.\r\n",
      "\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:keras\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Hello guys , does any one know how to solve this issue ? Thanks in advance!\n",
      "issue body -  <em>Please make sure that this is a bug. As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:bug_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "- TensorFlow installed from (source or binary):\r\n",
      "- TensorFlow version (use command below):\r\n",
      "- Python version:\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version:\r\n",
      "- GPU model and memory:\r\n",
      "\r\n",
      "You can collect some of this information using our environment capture\r\n",
      "[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n",
      "You can also obtain the TensorFlow version with:\r\n",
      "1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n",
      "2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n",
      "\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "Provide a reproducible test case that is the bare minimum necessary to generate\r\n",
      "the problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n",
      "\r\n",
      "**Other info / logs** Include any logs or source code that would be helpful to\r\n",
      "diagnose the problem. If including tracebacks, please include the full\r\n",
      "traceback. Large logs and files should be attached.\r\n",
      "\n",
      "issue labels - \n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  ProfilerNotRunningError: Cannot stop profiling. No profiler is running. raised using profiler in tf.function\n",
      "issue body -  Hello. I'm trying to use profiling in my custom tf.function, but I'm getting the following error calling `tf.summary.trace_export` \r\n",
      "\r\n",
      "`tensorflow.python.eager.profiler.ProfilerNotRunningError: Cannot stop profiling. No profiler is running.`\r\n",
      "\r\n",
      "I followed the tutorial in [https://www.tensorflow.org/tensorboard/graphs](https://www.tensorflow.org/tensorboard/graphs) and I realized the error is raised in the tutorial too in line `tf.summary.trace_export(\r\n",
      "      name=\"my_func_trace\",\r\n",
      "      step=0,\r\n",
      "      profiler_outdir=logdir)`. I ran it using google colab in remote enviroment so I guess is a bug in version 2.4\r\n",
      "\r\n",
      "Does anybody another way to achieve it?\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:tensorboard\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  ImportError: cannot import name 'function_pb2' from 'tensorflow.core.framework' (unknown location)\n",
      "issue body -  **System information**\r\n",
      "- ubuntu20.04\r\n",
      "- build tf from source\r\n",
      "- TensorFlow version (use command below):2.5\r\n",
      "- Python version:3.8.5\r\n",
      "- Bazel version (if compiling from source):3.7.2\r\n",
      "- GCC/Compiler version (if compiling from source):gcc-9\r\n",
      "- CUDA/cuDNN version:11.1\r\n",
      "\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"<stdin>\", line 1, in <module>\r\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py\", line 41, in <module>\r\n",
      "    from tensorflow.python.tools import module_util as _module_util\r\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py\", line 40, in <module>\r\n",
      "    from tensorflow.python.eager import context\r\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py\", line 35, in <module>\r\n",
      "    from tensorflow.python import pywrap_tfe\r\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/pywrap_tfe.py\", line 28, in <module>\r\n",
      "    from tensorflow.python import pywrap_tensorflow\r\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 83, in <module>\r\n",
      "    raise ImportError(msg)\r\n",
      "ImportError: Traceback (most recent call last):\r\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 64, in <module>\r\n",
      "    from tensorflow.python._pywrap_tensorflow_internal import *\r\n",
      "ImportError: libtensorflow_framework.so.2: cannot open shared object file: No such file or directory\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.5\n",
      "subtype: ubuntu/linux\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  Building TF 2.3.1 with CUDA 11.1 fails with undefined protobuf symbol\n",
      "issue body -  <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- OS Platform and Distribution: Ubuntu 20.04 LTS\r\n",
      "- TensorFlow installed from: Trying to build from source\r\n",
      "- TensorFlow version: 2.3.1\r\n",
      "- Python version: 3.8 with pip\r\n",
      "- Bazel version: 3.1.0 (also tried with 3.7.2)\r\n",
      "- GCC/Compiler version: 7.5.0 (also tried with 8.x, 9.x)\r\n",
      "- CUDA/cuDNN version: CUDA 11.1 / cuDNN 8 / TensorRT 7\r\n",
      "- GPU model and memory: RTX 3070 8GB\r\n",
      "<br>\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "I've tried to build Tensorflow 2.3.1 with CUDA 11.1 to support RTX 3070,\r\n",
      "but it always fails because of the problem related to undefined symbol in protobuf.\r\n",
      "(I need to use 2.3.1 instead of 2.4.0 or 2.5.0 because of the compatibility with other framework now.)\r\n",
      "<br>\r\n",
      "\r\n",
      "**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n",
      "These are my commands which were used to build TF.\r\n",
      "```\r\n",
      "$ cd ~/utils/tensorflow\r\n",
      "$ bazel clean\r\n",
      "$ ./configure\r\n",
      "You have bazel 3.1.0 installed.\r\n",
      "Please specify the location of python. [Default is /usr/bin/python3]: \r\n",
      "\r\n",
      "\r\n",
      "Found possible Python library paths:\r\n",
      "  /usr/local/lib/python3.8/dist-packages\r\n",
      "  /usr/lib/python3.8/dist-packages\r\n",
      "  /home/sjlee/utils/tvm/python\r\n",
      "  /usr/lib/python3/dist-packages\r\n",
      "Please input the desired Python library path to use.  Default is [/usr/local/lib/python3.8/dist-packages]\r\n",
      "\r\n",
      "Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n\r\n",
      "No OpenCL SYCL support will be enabled for TensorFlow.\r\n",
      "\r\n",
      "Do you wish to build TensorFlow with ROCm support? [y/N]: n\r\n",
      "No ROCm support will be enabled for TensorFlow.\r\n",
      "\r\n",
      "Do you wish to build TensorFlow with CUDA support? [y/N]: y\r\n",
      "CUDA support will be enabled for TensorFlow.\r\n",
      "\r\n",
      "Do you wish to build TensorFlow with TensorRT support? [y/N]: y\r\n",
      "TensorRT support will be enabled for TensorFlow.\r\n",
      "\r\n",
      "Found CUDA 11.1 in:\r\n",
      "    /usr/local/cuda-11.1/targets/x86_64-linux/lib\r\n",
      "    /usr/local/cuda-11.1/targets/x86_64-linux/include\r\n",
      "Found cuDNN 8 in:\r\n",
      "    /usr/lib/x86_64-linux-gnu\r\n",
      "    /usr/include\r\n",
      "Found TensorRT 7 in:\r\n",
      "    /usr/lib/x86_64-linux-gnu\r\n",
      "    /usr/include/x86_64-linux-gnu\r\n",
      "\r\n",
      "\r\n",
      "Please specify a list of comma-separated CUDA compute capabilities you want to build with.\r\n",
      "You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as \"x.y\" or \"compute_xy\" to include both virtual and binary GPU code, or as \"sm_xy\" to only include the binary code.\r\n",
      "Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 8.6]: 8.6,7.5,6.1\r\n",
      "\r\n",
      "\r\n",
      "Do you want to use clang as CUDA compiler? [y/N]: n\r\n",
      "nvcc will be used as CUDA compiler.\r\n",
      "\r\n",
      "Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \r\n",
      "\r\n",
      "\r\n",
      "Please specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]: \r\n",
      "\r\n",
      "\r\n",
      "Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n\r\n",
      "Not configuring the WORKSPACE for Android builds.\r\n",
      "\r\n",
      "Preconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n",
      "        --config=mkl            # Build with MKL support.\r\n",
      "        --config=monolithic     # Config for mostly static monolithic build.\r\n",
      "        --config=ngraph         # Build with Intel nGraph support.\r\n",
      "        --config=numa           # Build with NUMA support.\r\n",
      "        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\n",
      "        --config=v2             # Build TensorFlow 2.x instead of 1.x.\r\n",
      "Preconfigured Bazel build configs to DISABLE default on features:\r\n",
      "        --config=noaws          # Disable AWS S3 filesystem support.\r\n",
      "        --config=nogcp          # Disable GCP support.\r\n",
      "        --config=nohdfs         # Disable HDFS support.\r\n",
      "        --config=nonccl         # Disable NVIDIA NCCL support.\r\n",
      "Configuration finished\r\n",
      "\r\n",
      "$ bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --copt=-msse4.2 //tensorflow/tools/pip_package:build_pip_package\r\n",
      "```\r\n",
      "<br>\r\n",
      "\r\n",
      "**Any other info / logs**\r\n",
      "Build fails with these error messages. It says there are undefined symbol named \"fixed_address_empty_string\".\r\n",
      "I've referenced other issues related to my case, but nothing helped for me.\r\n",
      "I think sources for protobuf were already compiled before tensorflow kernels, so I cannot understand why the error occurs.\r\n",
      "```\r\n",
      "ERROR: /home/sjlee/utils/tensorflow/tensorflow/python/BUILD:2823:29: Executing genrule //tensorflow/python:clustering_ops_pygenrule failed (Exit 127): bash failed: error executing command /bin/bash bazel-out/k8-opt/bin/tensorflow/python/clustering_ops_pygenrule.genrule_script.sh\r\n",
      "bazel-out/k8-opt-exec-50AE0418/bin/tensorflow/python/gen_clustering_ops_py_wrappers_cc: symbol lookup error: bazel-out/k8-opt-exec-50AE0418/bin/tensorflow/python/gen_clustering_ops_py_wrappers_cc: undefined symbol: _ZN6google8protobuf8internal26fixed_address_empty_stringB5cxx11E\r\n",
      "Target //tensorflow/tools/pip_package:build_pip_package failed to build\r\n",
      "Use --verbose_failures to see the command lines of failed build steps.\r\n",
      "INFO: Elapsed time: 2049.987s, Critical Path: 94.77s\r\n",
      "INFO: 26147 processes: 11918 internal, 14229 local.\r\n",
      "FAILED: Build did NOT complete successfully\r\n",
      "```\r\n",
      "<br>\r\n",
      "\r\n",
      "These info messages also can be found during build process.\r\n",
      "Could these ones be related to the error above?\r\n",
      "```\r\n",
      "INFO: From ProtoCompile tensorflow/core/protobuf/saved_object_graph.pb.h [for host]:\r\n",
      "bazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\n",
      "INFO: From ProtoCompile tensorflow/core/protobuf/queue_runner.pb.h [for host]:\r\n",
      "bazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\n",
      "INFO: From ProtoCompile tensorflow/core/protobuf/graph_debug_info.pb.h [for host]:\r\n",
      "bazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\n",
      "INFO: From ProtoCompile tensorflow/core/protobuf/tensor_bundle.pb.h [for host]:\r\n",
      "bazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\n",
      "INFO: From ProtoCompile tensorflow/core/protobuf/saver.pb.h [for host]:\r\n",
      "bazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\n",
      "INFO: From ProtoCompile tensorflow/core/protobuf/trackable_object_graph.pb.h [for host]:\r\n",
      "bazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\n",
      "INFO: From ProtoCompile tensorflow/core/protobuf/transport_options.pb.h [for host]:\r\n",
      "bazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\n",
      "INFO: From ProtoCompile tensorflow/core/grappler/costs/op_performance_data.pb.h:\r\n",
      "bazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\n",
      "INFO: From ProtoCompile tensorflow/core/protobuf/device_filters.pb.h [for host]:\r\n",
      "bazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\n",
      "INFO: From ProtoCompile tensorflow/core/framework/node_def.pb.h [for host]:\r\n",
      "bazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\n",
      "```\r\n",
      "<br>\r\n",
      "\r\n",
      "Thank you :)\n",
      "issue labels - \n",
      "TF 2.3\n",
      "subtype: ubuntu/linux\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  quant transpose convolution layer is not supported in nnapi acceleration\n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n",
      "- No\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n",
      "  Not related\r\n",
      "- TensorFlow installed from (source or binary):\r\n",
      "- TensorFlow version (use command below):\r\n",
      "  gtihub master branch, but this problem exist on v2.3.0 v2.2.0 etc.\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "Quant transpose convolution layer not supported in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/nnapi/nnapi_delegate.cc#L2228\r\n",
      "The layer version of INT8 transpose convolution layer had a version of 2 which failed the above version checking. See below:\r\n",
      "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/versioning/op_version.cc#L261\r\n",
      "\r\n",
      "i.e. All deep learning model (esp. segmentation) using quant transpose convolution will not get nnapi acceleration on  all Android phone when they are using tensorflow lite to do the inferencing instead of Qualcomm SNPE DL engine to do the inference.\r\n",
      "\r\n",
      "Related to https://github.com/tensorflow/tensorflow/issues/46084\n",
      "issue labels - \n",
      "TF 2.3\n",
      "comp:lite\n",
      "stat:awaiting tensorflower\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  [INTEL MKL] Change order for remapper\n",
      "issue body -  This PR is to test if there will be any regression if the order of remapper in grappler meta_optimizer is moved before arithmetic_optimizer.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:grappler\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Bert Preprocess Model not working on windows 10\n",
      "issue body -  I have the same issue described here [error-with-using-bert-model-from-tensorflow](https://stackoverflow.com/questions/65298391/error-with-using-bert-model-from-tensorflow)\r\n",
      "\r\n",
      "I get this exception when i try to use the bert preprocessor on windows 10\r\n",
      "\r\n",
      "`Trying to access resource using the wrong type. Expected class tensorflow::lookup::LookupInterface got class tensorflow::lookup::LookupInterface`\r\n",
      "\r\n",
      "**Stack trace**\r\n",
      "```\r\n",
      "File \"C:\\work\\vpython\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1100, in fit\r\n",
      "    tmp_logs = self.train_function(iterator)\r\n",
      "  File \"C:\\work\\vpython\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 828, in __call__\r\n",
      "    result = self._call(*args, **kwds)\r\n",
      "  File \"C:\\work\\vpython\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 888, in _call\r\n",
      "    return self._stateless_fn(*args, **kwds)\r\n",
      "  File \"C:\\work\\vpython\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2942, in __call__\r\n",
      "    return graph_function._call_flat(\r\n",
      "  File \"C:\\work\\vpython\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1918, in _call_flat\r\n",
      "    return self._build_call_outputs(self._inference_function.call(\r\n",
      "  File \"C:\\work\\vpython\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 555, in call\r\n",
      "    outputs = execute.execute(\r\n",
      "  File \"C:\\work\\vpython\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 59, in quick_execute\r\n",
      "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError:  Trying to access resource using the wrong type. Expected class tensorflow::lookup::LookupInterface got class tensorflow::lookup::LookupInterface\r\n",
      "\t [[{{node prediction/keras_layer_1/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall/bert_tokenizer/StatefulPartitionedCall/WordpieceTokenizeWithOffsets/WordpieceTokenizeWithOffsets/WordpieceTokenizeWithOffsets}}]] [Op:__inference_train_function_52076]\r\n",
      "Function call stack:\r\n",
      "train_function\r\n",
      "```\n",
      "issue labels - \n",
      "TF 2.4\n",
      "stalled\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Bump tensorflow from 2.0.0-beta1 to 2.4.0 in /tensorflow/lite/micro/examples/magic_wand/train\n",
      "issue body -  Bumps [tensorflow](https://github.com/tensorflow/tensorflow) from 2.0.0-beta1 to 2.4.0.\n",
      "<details>\n",
      "<summary>Release notes</summary>\n",
      "<p><em>Sourced from <a href=\"https://github.com/tensorflow/tensorflow/releases\">tensorflow's releases</a>.</em></p>\n",
      "<blockquote>\n",
      "<h2>TensorFlow 2.4.0</h2>\n",
      "<h1>Release 2.4.0</h1>\n",
      "<h2>Major Features and Improvements</h2>\n",
      "<ul>\n",
      "<li>\n",
      "<p><code>tf.distribute</code> introduces experimental support for asynchronous training of models via the <a href=\"https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/ParameterServerStrategy\"><code>tf.distribute.experimental.ParameterServerStrategy</code></a> API. Please see the <a href=\"https://www.tensorflow.org/tutorials/distribute/parameter_server_training\">tutorial</a> to learn more.</p>\n",
      "</li>\n",
      "<li>\n",
      "<p><a href=\"https://www.tensorflow.org/api_docs/python/tf/distribute/MultiWorkerMirroredStrategy\"><code>MultiWorkerMirroredStrategy</code></a> is now a stable API and is no longer considered experimental. Some of the major improvements involve handling peer failure and many bug fixes. Please check out the detailed tutorial on <a href=\"https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras\">Multi-worker training with Keras</a>.</p>\n",
      "</li>\n",
      "<li>\n",
      "<p>Introduces experimental support for a new module named <a href=\"https://www.tensorflow.org/api_docs/python/tf/experimental/numpy\"><code>tf.experimental.numpy</code></a> which is a NumPy-compatible API for writing TF programs. See the <a href=\"https://www.tensorflow.org/guide/tf_numpy\">detailed guide</a> to learn more. Additional details below.</p>\n",
      "</li>\n",
      "<li>\n",
      "<p>Adds Support for\n",
      "<a href=\"https://blogs.nvidia.com/blog/2020/05/14/tensorfloat-32-precision-format/\">TensorFloat-32</a> on Ampere based GPUs. TensorFloat-32, or TF32 for short, is a math mode for NVIDIA Ampere based GPUs and is enabled by default.</p>\n",
      "</li>\n",
      "<li>\n",
      "<p>A major refactoring of the internals of the Keras Functional API has been completed, that should improve the reliability, stability, and performance of constructing Functional models.</p>\n",
      "</li>\n",
      "<li>\n",
      "<p>Keras mixed precision API <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/mixed_precision?version=nightly\"><code>tf.keras.mixed_precision</code></a> is no longer experimental and allows the use of 16-bit floating point formats during training, improving performance by up to 3x on GPUs and 60% on TPUs. Please see below for additional details.</p>\n",
      "</li>\n",
      "<li>\n",
      "<p>TensorFlow Profiler now supports profiling <code>MultiWorkerMirroredStrategy</code> and tracing multiple workers using the <a href=\"https://www.tensorflow.org/guide/profiler#profiling_apis\">sampling mode API</a>.</p>\n",
      "</li>\n",
      "<li>\n",
      "<p>TFLite Profiler for Android is available. See the detailed <a href=\"https://www.tensorflow.org/lite/performance/measurement#trace_tensorflow_lite_internals_in_android\">guide</a> to learn more.</p>\n",
      "</li>\n",
      "<li>\n",
      "<p>TensorFlow pip packages are now built with CUDA11 and cuDNN 8.0.2.</p>\n",
      "</li>\n",
      "</ul>\n",
      "<h2>Breaking Changes</h2>\n",
      "<ul>\n",
      "<li>\n",
      "<p>TF Core:</p>\n",
      "<ul>\n",
      "<li>Certain float32 ops run in lower precsion on Ampere based GPUs, including  matmuls and convolutions, due to the use of <a href=\"https://blogs.nvidia.com/blog/2020/05/14/tensorfloat-32-precision-format/\">TensorFloat-32</a>. Specifically, inputs to such ops are rounded from 23 bits of precision to 10\n",
      "bits of precision. This is unlikely to cause issues in practice for deep learning models. In some cases, TensorFloat-32 is also used for complex64 ops.\n",
      "TensorFloat-32 can be disabled by running <code>tf.config.experimental.enable_tensor_float_32_execution(False)</code>.</li>\n",
      "<li>The byte layout for string tensors across the C-API has been updated to match TF Core/C++; i.e., a contiguous array of <code>tensorflow::tstring</code>/<code>TF_TString</code>s.</li>\n",
      "<li>C-API functions <code>TF_StringDecode</code>, <code>TF_StringEncode</code>, and <code>TF_StringEncodedSize</code> are no longer relevant and have been removed; see <code>core/platform/ctstring.h</code> for  string access/modification in C.</li>\n",
      "<li><code>tensorflow.python</code>, <code>tensorflow.core</code> and <code>tensorflow.compiler</code> modules are now hidden. These modules are not part of TensorFlow public API.</li>\n",
      "<li><code>tf.raw_ops.Max</code> and <code>tf.raw_ops.Min</code> no longer accept inputs of type <code>tf.complex64</code> or <code>tf.complex128</code>, because the behavior of these ops is not well defined for complex types.</li>\n",
      "<li>XLA:CPU and XLA:GPU devices are no longer registered by default. Use <code>TF_XLA_FLAGS=--tf_xla_enable_xla_devices</code> if you really need them, but this flag will eventually be removed in subsequent releases.</li>\n",
      "</ul>\n",
      "</li>\n",
      "<li>\n",
      "<p><code>tf.keras</code>:</p>\n",
      "<ul>\n",
      "<li>The <code>steps_per_execution</code> argument in <code>model.compile()</code> is no longer experimental; if you were passing <code>experimental_steps_per_execution</code>, rename it to <code>steps_per_execution</code> in your code. This argument controls the number of batches to run during each <code>tf.function</code> call when calling <code>model.fit()</code>. Running multiple batches inside a single <code>tf.function</code> call can greatly improve performance on TPUs or small models with a large Python overhead.</li>\n",
      "<li>A <strong>major refactoring</strong> of the internals of the Keras Functional API may affect code that\n",
      "is relying on certain internal details:\n",
      "<ul>\n",
      "<li>Code that uses <code>isinstance(x, tf.Tensor)</code> instead of <code>tf.is_tensor</code> when checking Keras symbolic inputs/outputs should switch to using <code>tf.is_tensor</code>.</li>\n",
      "<li>Code that is overly dependent on the exact names attached to symbolic tensors (e.g. assumes there will be &quot;:0&quot; at the end of the inputs, treats names as unique identifiers instead of using <code>tensor.ref()</code>, etc.) may break.</li>\n",
      "<li>Code that uses full path for <code>get_concrete_function</code> to trace Keras symbolic inputs directly should switch to building matching <code>tf.TensorSpec</code>s directly and tracing the <code>TensorSpec</code> objects.</li>\n",
      "<li>Code that relies on the exact number and names of the op layers that TensorFlow operations  were converted into may have changed.</li>\n",
      "<li>Code that uses <code>tf.map_fn</code>/<code>tf.cond</code>/<code>tf.while_loop</code>/control flow as op layers and  happens to work before TF 2.4. These will explicitly be unsupported now. Converting these ops to Functional API op layers was unreliable before TF 2.4, and prone to erroring incomprehensibly  or being silently buggy.</li>\n",
      "<li>Code that directly asserts on a Keras symbolic value in cases where ops like <code>tf.rank</code> used to  return a static or symbolic value depending on if the input had a fully static shape or not. Now these ops always return symbolic values.</li>\n",
      "<li>Code already susceptible to leaking tensors outside of graphs becomes slightly more likely to do so now.</li>\n",
      "<li>Code that tries directly getting gradients with respect to symbolic Keras inputs/outputs. Use <code>GradientTape</code> on the actual Tensors passed to the already-constructed model instead.</li>\n",
      "<li>Code that requires very tricky shape manipulation via converted op layers in order to work, where the Keras symbolic shape inference proves insufficient.</li>\n",
      "<li>Code that tries manually walking a <code>tf.keras.Model</code> layer by layer and assumes layers only ever have one positional argument. This assumption doesn't hold       true before TF 2.4 either, but is more likely to cause issues now.</li>\n",
      "</ul>\n",
      "</li>\n",
      "</ul>\n",
      "</li>\n",
      "</ul>\n",
      "<!-- raw HTML omitted -->\n",
      "</blockquote>\n",
      "<p>... (truncated)</p>\n",
      "</details>\n",
      "<details>\n",
      "<summary>Changelog</summary>\n",
      "<p><em>Sourced from <a href=\"https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md\">tensorflow's changelog</a>.</em></p>\n",
      "<blockquote>\n",
      "<h1>Release 2.4.0</h1>\n",
      "<h2>Major Features and Improvements</h2>\n",
      "<ul>\n",
      "<li>\n",
      "<p><code>tf.distribute</code> introduces experimental support for asynchronous training of\n",
      "models via the [<code>tf.distribute.experimental.ParameterServerStrategy</code>]\n",
      "(<a href=\"https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/ParameterServerStrategy\">https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/ParameterServerStrategy</a>)\n",
      "API. Please see the <a href=\"https://www.tensorflow.org/tutorials/distribute/parameter_server_training\">tutorial</a>\n",
      "to learn more.</p>\n",
      "</li>\n",
      "<li>\n",
      "<p><a href=\"https://www.tensorflow.org/api_docs/python/tf/distribute/MultiWorkerMirroredStrategy\"><code>MultiWorkerMirroredStrategy</code></a>\n",
      "is now a stable API and is no longer considered experimental. Some of the\n",
      "major improvements involve handling peer failure and many bug fixes. Please\n",
      "check out the detailed tutorial on [Multi-worker training with Keras]\n",
      "(<a href=\"https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras\">https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras</a>).</p>\n",
      "</li>\n",
      "<li>\n",
      "<p>Introduces experimental support for a new module named [<code>tf.experimental.numpy</code>]\n",
      "(<a href=\"https://www.tensorflow.org/api_docs/python/tf/experimental/numpy\">https://www.tensorflow.org/api_docs/python/tf/experimental/numpy</a>) which is a\n",
      "NumPy-compatible API for writing TF programs. See the [detailed guide]\n",
      "(<a href=\"https://www.tensorflow.org/guide/tf_numpy\">https://www.tensorflow.org/guide/tf_numpy</a>) to learn more. Additional details below.</p>\n",
      "</li>\n",
      "<li>\n",
      "<p>Adds Support for\n",
      "<a href=\"https://blogs.nvidia.com/blog/2020/05/14/tensorfloat-32-precision-format/\">TensorFloat-32</a>\n",
      "on Ampere based GPUs. TensorFloat-32, or TF32 for short, is a math mode for\n",
      "NVIDIA Ampere based GPUs and is enabled by default.</p>\n",
      "</li>\n",
      "<li>\n",
      "<p>A major refactoring of the internals of the Keras Functional API has been\n",
      "completed, that should improve the reliability, stability, and performance of\n",
      "constructing Functional models.</p>\n",
      "</li>\n",
      "<li>\n",
      "<p>Keras mixed precision API [<code>tf.keras.mixed_precision</code>]\n",
      "(<a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/mixed_precision?version=nightly\">https://www.tensorflow.org/api_docs/python/tf/keras/mixed_precision?version=nightly</a>)\n",
      "is no longer experimental and allows the use of 16-bit floating point formats\n",
      "during training, improving performance by up to 3x on GPUs and 60% on TPUs.\n",
      "Please see below for additional details.</p>\n",
      "</li>\n",
      "<li>\n",
      "<p>TensorFlow Profiler now supports profiling <code>MultiWorkerMirroredStrategy</code> and\n",
      "tracing multiple workers using the [sampling mode API]\n",
      "(<a href=\"https://www.tensorflow.org/guide/profiler#profiling_apis\">https://www.tensorflow.org/guide/profiler#profiling_apis</a>).</p>\n",
      "</li>\n",
      "<li>\n",
      "<p>TFLite Profiler for Android is available. See the detailed [guide]\n",
      "(<a href=\"https://www.tensorflow.org/lite/performance/measurement#trace_tensorflow_lite_internals_in_android\">https://www.tensorflow.org/lite/performance/measurement#trace_tensorflow_lite_internals_in_android</a>)\n",
      "to learn more.</p>\n",
      "</li>\n",
      "<li>\n",
      "<p>TensorFlow pip packages are now built with CUDA11 and cuDNN 8.0.2.</p>\n",
      "</li>\n",
      "</ul>\n",
      "<h2>Breaking Changes</h2>\n",
      "<ul>\n",
      "<li>TF Core:\n",
      "<ul>\n",
      "<li>Certain float32 ops run in lower precsion on Ampere based GPUs, including</li>\n",
      "</ul>\n",
      "</li>\n",
      "</ul>\n",
      "<!-- raw HTML omitted -->\n",
      "</blockquote>\n",
      "<p>... (truncated)</p>\n",
      "</details>\n",
      "<details>\n",
      "<summary>Commits</summary>\n",
      "<ul>\n",
      "<li><a href=\"https://github.com/tensorflow/tensorflow/commit/582c8d236cb079023657287c318ff26adb239002\"><code>582c8d2</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/44220\">#44220</a> from tensorflow-jenkins/relnotes-2.4.0rc0-18048</li>\n",
      "<li><a href=\"https://github.com/tensorflow/tensorflow/commit/c16387f692bf46a95100adc91a68922414b53d4c\"><code>c16387f</code></a> Update RELEASE.md</li>\n",
      "<li><a href=\"https://github.com/tensorflow/tensorflow/commit/4cf406c8a617392864efa7d8f50510a2c95e049a\"><code>4cf406c</code></a> Update RELEASE.md</li>\n",
      "<li><a href=\"https://github.com/tensorflow/tensorflow/commit/3f35ef2452dc0f27797e8a295371065834335944\"><code>3f35ef2</code></a> Update RELEASE.md</li>\n",
      "<li><a href=\"https://github.com/tensorflow/tensorflow/commit/3647e8ec38de7891887ca72c9777ab92a2c09ad2\"><code>3647e8e</code></a> Update RELEASE.md</li>\n",
      "<li><a href=\"https://github.com/tensorflow/tensorflow/commit/281c7d540508ebf92d5c8f602e52b37836c9f55c\"><code>281c7d5</code></a> Update RELEASE.md</li>\n",
      "<li><a href=\"https://github.com/tensorflow/tensorflow/commit/91ec75f872634537d95485b66d8c82c2bb61a497\"><code>91ec75f</code></a> Update RELEASE.md</li>\n",
      "<li><a href=\"https://github.com/tensorflow/tensorflow/commit/ed5ad82e763f0842d96762cf3ae214ea5c6eadc8\"><code>ed5ad82</code></a> Update RELEASE.md</li>\n",
      "<li><a href=\"https://github.com/tensorflow/tensorflow/commit/1267bba74887f0a4ae47a8758b24b11165daf928\"><code>1267bba</code></a> Update RELEASE.md</li>\n",
      "<li><a href=\"https://github.com/tensorflow/tensorflow/commit/13a4067a164648411cb7ef9cb579e2a4d5844260\"><code>13a4067</code></a> Update RELEASE.md</li>\n",
      "<li>Additional commits viewable in <a href=\"https://github.com/tensorflow/tensorflow/compare/v2.0.0-beta1...v2.4.0\">compare view</a></li>\n",
      "</ul>\n",
      "</details>\n",
      "<br />\n",
      "\n",
      "\n",
      "[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=tensorflow&package-manager=pip&previous-version=2.0.0-beta1&new-version=2.4.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n",
      "\n",
      "Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n",
      "\n",
      "[//]: # (dependabot-automerge-start)\n",
      "[//]: # (dependabot-automerge-end)\n",
      "\n",
      "---\n",
      "\n",
      "<details>\n",
      "<summary>Dependabot commands and options</summary>\n",
      "<br />\n",
      "\n",
      "You can trigger Dependabot actions by commenting on this PR:\n",
      "- `@dependabot rebase` will rebase this PR\n",
      "- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n",
      "- `@dependabot merge` will merge this PR after your CI passes on it\n",
      "- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n",
      "- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n",
      "- `@dependabot reopen` will reopen this PR if it is closed\n",
      "- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n",
      "- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n",
      "- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n",
      "- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n",
      "- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language\n",
      "- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language\n",
      "- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language\n",
      "- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language\n",
      "\n",
      "You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/tensorflow/tensorflow/network/alerts).\n",
      "\n",
      "</details>\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "dependencies\n",
      "python\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  cuda_driver.cc:175] Check failed: err == cudaSuccess || err == cudaErrorInvalidValue Unexpected CUDA error: out of memory\n",
      "issue body -  hello,\r\n",
      "\r\n",
      "I'm trying to run inference on a tensorrt converted graph (tf-trt) and received the error in the title.\r\n",
      "I've followed instructions for \"TF-TRT 2.0 Workflow With A SavedModel\" at https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html.\r\n",
      "but i'm afraid i have not been able to reach the part of the code the performs the inference, I get the OOM error a after loading the saved model , during preparation of the input data. \r\n",
      "\r\n",
      "I'm running tensorflow 2.0.0 (eager execution). on ubuntu 16.04, cuda 10.0, geforce gtx 1050 ( 4gb ram )\r\n",
      "\r\n",
      "here is my conversion code \r\n",
      "from tensorflow.python.compiler.tensorrt import trt_convert as trt\r\n",
      "ConvParams = trt.DEFAULT_TRT_CONVERSION_PARAMS\r\n",
      "ConvParams = ConvParams._replace(max_workspace_size_bytes=1*1024*1024*1024)\r\n",
      "ConvParams = ConvParams._replace(precision_mode=\"FP32\")\r\n",
      "ConvParams = ConvParams._replace(max_batch_size=1)\r\n",
      "converter = trt.TrtGraphConverterV2(input_saved_model_dir=args.InputToConvert, conversion_params=ConvParams)\r\n",
      "\r\n",
      "TensorRT_graph = converter.convert()\r\n",
      "\r\n",
      "print('convert completed, building')\r\n",
      "def my_input_fn():\r\n",
      "  import numpy as np\r\n",
      "  Inp1 = np.random.normal(size=(1,250, 250, 3)).astype(np.uint8)\r\n",
      "  yield [Inp1]\r\n",
      "converter.build(input_fn=my_input_fn)\r\n",
      "\r\n",
      "print('build completed, saveing results to {}'.format(args.Output))\r\n",
      "converter.save(args.Output)\r\n",
      "\r\n",
      "\r\n",
      "the conversion process generates warnings but succeeds. \r\n",
      "\r\n",
      "**in another program/script/\"execution session\" I perform the following:**\r\n",
      "load the saved model using the following code\r\n",
      "\r\n",
      "saved_model_loaded = tf.saved_model.load('my saved model directory name',tags=[tf.compat.v1.saved_model.tag_constants.SERVING])\r\n",
      "    graph_func = saved_model_loaded.signatures[tf.compat.v1.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY]\r\n",
      "    from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\r\n",
      "    frozen_func = convert_variables_to_constants_v2(graph_func)\r\n",
      "    print('created concreate function')\r\n",
      "\r\n",
      "\r\n",
      "try to load an image for inference \r\n",
      "\r\n",
      "ReadFile = tf.io.read_file('/home/omerbrandis/ffrobotics/AppleMask/mask_rcnn/test_images/img3095_RECT2_2.jpg')\r\n",
      "print('after read file')\r\n",
      "DecodedImg = tf.io.decode_jpeg(ReadFile)\r\n",
      "print('after decode ')\r\n",
      "image_expanded = tf.expand_dims(DecodedImg, axis=0)\r\n",
      "print('read file')\r\n",
      "\r\n",
      "\r\n",
      "**at runtime the code does not reach \"read file\" , only \"after decode\" is written.**\r\n",
      "\r\n",
      "if i comment out the first section of the program that deals with reading the graph , the part that reads the file works without any problem.\r\n",
      "\r\n",
      "as stated before, i have 4GB of ram on my gpu , and the conversion process should have limited the graphs usage to 1gb thus the OOM error is surprising. ( the image is only 210X210 pixels).\r\n",
      "\r\n",
      "please advise\r\n",
      "Omer.\r\n",
      "\r\n",
      " \r\n",
      "\n",
      "issue labels - \n",
      "TF 2.0\n",
      "comp:gpu:tensorrt\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Wrong output dimension calculation of StrideSlice operation on TensorFlow Lite.\n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 8\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n",
      "- TensorFlow installed from (source or binary): Both pip and source build\r\n",
      "- TensorFlow version (use command below): `v2.4.0-rc4-71-g582c8d236cb 2.4.0`\r\n",
      "- Python version: `3.6.8`\r\n",
      "- Bazel version (if compiling from source): `3.7.1`\r\n",
      "- GCC/Compiler version (if compiling from source): `8.3.1`\r\n",
      "- CUDA/cuDNN version: CUDA 11.1 / cuDNN 8\r\n",
      "- GPU model and memory: RTX3090 24GB\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "The `StridedSlice` operation on TensorFlow Lite calculates output dimension incorrectly.\r\n",
      "\r\n",
      "I'm developing some custom operations for both TensorFlow and TensorFlow Lite.\r\n",
      "While I debugging my custom operation, inference fails only on TensorFlow Lite.\r\n",
      "I found that `StridedSlice` operation of TensorFlow Lite calculates output dimension incorrectly.\r\n",
      "I added some `printf` function to [`tflite::ops::builtin::strided_slice::Eval`](https://github.com/tensorflow/tensorflow/blob/582c8d236cb079023657287c318ff26adb239002/tensorflow/lite/kernels/strided_slice.cc#L185):\r\n",
      "```c++\r\n",
      "TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\r\n",
      "  StridedSliceContext op_context(context, node);\r\n",
      "\r\n",
      "  printf(\"**** [STRIDED_SLICE] Input [\");\r\n",
      "  for (int i = 0; i < NumDimensions(op_context.input); i++) {\r\n",
      "    printf(\"%d, \", SizeOfDimension(op_context.input, i));\r\n",
      "  }\r\n",
      "  printf(\"]\\n**** [STRIDED_SLICE] Begin: \");\r\n",
      "  for (int i = 0; i < SizeOfDimension(op_context.begin, 0); i++) {\r\n",
      "    printf(\"%d, \", GetTensorData<int32_t>(op_context.begin)[i]);\r\n",
      "  }\r\n",
      "  printf(\"\\n**** [STRIDED_SLICE] End: \");\r\n",
      "  for (int i = 0; i < SizeOfDimension(op_context.end, 0); i++) {\r\n",
      "    printf(\"%d, \", GetTensorData<int32_t>(op_context.end)[i]);\r\n",
      "  }\r\n",
      "  printf(\"\\n**** [STRIDED_SLICE] Strides: \");\r\n",
      "  for (int i = 0; i < SizeOfDimension(op_context.strides, 0); i++) {\r\n",
      "    printf(\"%d, \", GetTensorData<int32_t>(op_context.strides)[i]);\r\n",
      "  }\r\n",
      "  printf(\"\\n\");\r\n",
      "\r\n",
      "  if (IsDynamicTensor(op_context.output)) {\r\n",
      "    TF_LITE_ENSURE_OK(context, ResizeOutputTensor(context, &op_context));\r\n",
      "  }\r\n",
      "  StridedSliceParams op_params = BuildStridedSliceParams(&op_context);\r\n",
      "\r\n",
      "  printf(\"**** [STRIDED_SLICE] Output [\");\r\n",
      "  for (int i = 0; i < NumDimensions(op_context.output); i++) {\r\n",
      "    printf(\"%d, \", SizeOfDimension(op_context.output, i));\r\n",
      "  }\r\n",
      "  printf(\"]\\n\");\r\n",
      "```\r\n",
      "\r\n",
      "And I got following logs:\r\n",
      "```\r\n",
      "**** [STRIDED_SLICE] Input [1656, 8, 32, ]\r\n",
      "**** [STRIDED_SLICE] Begin: 0, 0, 0, \r\n",
      "**** [STRIDED_SLICE] End: 893, \r\n",
      "**** [STRIDED_SLICE] Strides: 1, 1, 1, \r\n",
      "**** [STRIDED_SLICE] Output [893, 8, 0, ]\r\n",
      "```\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "After strided slicing, output dimension should be `[893, 8, 32, ]` in above example.\r\n",
      "\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "I wrote a smallest [reproducible code](https://github.com/kukdh1/tflite_buf_report/blob/master/test_strided_slice.py).\r\n",
      "The `SimpleLayer` keras layer creates read-only `Tensor` with size of [128, 8, 32].\r\n",
      "After ten `model.call`s, I saved the model into tflite model file.\r\n",
      "\r\n",
      "On TensorFlow (python, no code modification -- installed by pip `tensorflow-2.4.0-cp36-cp36m-manylinux2010_x86_64.whl`) it calculates output dimension correctly.\r\n",
      "```\r\n",
      "TEST: Input: [128, 8, 32] SiliceTo: 53 Output: (53, 8, 32)\r\n",
      "TEST: Input: [128, 8, 32] SiliceTo: 38 Output: (38, 8, 32)\r\n",
      "TEST: Input: [128, 8, 32] SiliceTo: 64 Output: (64, 8, 32)\r\n",
      "TEST: Input: [128, 8, 32] SiliceTo: 100 Output: (100, 8, 32)\r\n",
      "TEST: Input: [128, 8, 32] SiliceTo: 106 Output: (106, 8, 32)\r\n",
      "TEST: Input: [128, 8, 32] SiliceTo: 90 Output: (90, 8, 32)\r\n",
      "TEST: Input: [128, 8, 32] SiliceTo: 126 Output: (126, 8, 32)\r\n",
      "TEST: Input: [128, 8, 32] SiliceTo: 122 Output: (122, 8, 32)\r\n",
      "TEST: Input: [128, 8, 32] SiliceTo: 62 Output: (62, 8, 32)\r\n",
      "TEST: Input: [128, 8, 32] SiliceTo: 99 Output: (99, 8, 32)\r\n",
      "```\r\n",
      "\r\n",
      "I checked the stored model with [netron](https://netron.app/):\r\n",
      "![image](https://user-images.githubusercontent.com/6904750/103407030-52be8400-4ba0-11eb-8344-2e2d1b85e0d2.png)\r\n",
      "And it shows that `StridedSlice` has input `Tensor` of size `[128, 8, 32]` and output `Tensor` has size `[Unknown, 8, 32]`.\r\n",
      "\r\n",
      "To test on TensorFlow Lite, I wrote [simple inference code](https://github.com/kukdh1/tflite_buf_report/blob/master/strided_slice.cc) with TensorFlow Lite C++ API.\r\n",
      "It randomly selects input `Tensor` (slice index) and print the output dimension.\r\n",
      "\r\n",
      "I used the source code (582c8d23 == v2.4.0) with above `StrideSlice` `printf` modification.\r\n",
      "I built `libtensorflowlite.so` with following command:\r\n",
      "`bazel build --verbose_failures -c opt --define=no_aws_support=true --define=no_gcp_support=true --define=no_hdfs_support=true --define=no_nccl_support=true --define=build_with_mkl=false --config=monolithic //tensorflow/lite:libtensorflowlite.so`\r\n",
      "And the content of `.tf_configure.bazelrc` is:\r\n",
      "```\r\n",
      "build --action_env PYTHON_BIN_PATH=\"/home/kukdh1/.virtualenvs/tf_develop/bin/python3\"\r\n",
      "build --action_env PYTHON_LIB_PATH=\"/home/kukdh1/.virtualenvs/tf_develop/lib/python3.6/site-packages\"\r\n",
      "build --python_path=\"/home/kukdh1/.virtualenvs/tf_develop/bin/python3\"\r\n",
      "build --config=xla\r\n",
      "build --action_env CUDA_TOOLKIT_PATH=\"/usr/local/cuda-11.1\"\r\n",
      "build --action_env TF_CUDA_COMPUTE_CAPABILITIES=\"7.5,8.6\"\r\n",
      "build --action_env LD_LIBRARY_PATH=\"/usr/local/cuda-11.1/lib64:\"\r\n",
      "build --action_env GCC_HOST_COMPILER_PATH=\"/usr/bin/gcc\"\r\n",
      "build --config=cuda\r\n",
      "build:opt --copt=-march=native\r\n",
      "build:opt --copt=-Wno-sign-compare\r\n",
      "build:opt --host_copt=-march=native\r\n",
      "build:opt --define with_default_optimizations=true\r\n",
      "test --flaky_test_attempts=3\r\n",
      "test --test_size_filters=small,medium\r\n",
      "test --test_env=LD_LIBRARY_PATH\r\n",
      "test:v1 --test_tag_filters=-benchmark-test,-no_oss,-no_gpu,-oss_serial\r\n",
      "test:v1 --build_tag_filters=-benchmark-test,-no_oss,-no_gpu\r\n",
      "test:v2 --test_tag_filters=-benchmark-test,-no_oss,-no_gpu,-oss_serial,-v1only\r\n",
      "test:v2 --build_tag_filters=-benchmark-test,-no_oss,-no_gpu,-v1only\r\n",
      "build --action_env TF_CONFIGURE_IOS=\"0\"\r\n",
      "```\r\n",
      "\r\n",
      "I built test program with `mkdir build && cmake -DTF_SOURCE_DIR=<source directory> ..` ([CMakeLists.txt](https://github.com/kukdh1/tflite_buf_report/blob/master/CMakeLists.txt))\r\n",
      "When I ran the test program, I got following result:\r\n",
      "```\r\n",
      "$ CUDA_VISIBLE_DEVICES=-1 ./tflite-strided-slice strided_slice.tflite \r\n",
      "2020-12-31 19:47:18.088431: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "**** [STRIDED_SLICE] Input [128, 8, 32, ]\r\n",
      "**** [STRIDED_SLICE] Begin: 0, 0, 0, \r\n",
      "**** [STRIDED_SLICE] End: 103, \r\n",
      "**** [STRIDED_SLICE] Strides: 1, 1, 1, \r\n",
      "**** [STRIDED_SLICE] Output [103, 0, 32, ]\r\n",
      "Input: 103\r\n",
      "Output: [103 0 32 ]\r\n",
      "########## PRINT INTERPRETER STATE BEGIN ##########\r\n",
      "Interpreter has 6 tensors and 2 nodes\r\n",
      "Inputs: 0\r\n",
      "Outputs: 5\r\n",
      "\r\n",
      "Tensor   0 input_1              kTfLiteInt32   kTfLiteCustom          4 bytes ( 0.0 MB) \r\n",
      "Tensor   1 simple_layer/ReadVariableOp/resource kTfLiteFloat32   kTfLiteMmapRo     131072 bytes ( 0.1 MB)  128 8 32\r\n",
      "Tensor   2 simple_layer/strided_slice kTfLiteInt32   kTfLiteMmapRo         12 bytes ( 0.0 MB)  3\r\n",
      "Tensor   3 simple_layer/strided_slice1 kTfLiteInt32   kTfLiteMmapRo         12 bytes ( 0.0 MB)  3\r\n",
      "Tensor   4 simple_layer/strided_slice/stack_1 kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1\r\n",
      "Tensor   5 Identity             kTfLiteFloat32  kTfLiteDynamic          0 bytes ( 0.0 MB)  103 0 32\r\n",
      "\r\n",
      "Node   0 Operator Builtin Code  83 PACK\r\n",
      "  Inputs: 0\r\n",
      "  Outputs: 4\r\n",
      "Node   1 Operator Builtin Code  45 STRIDED_SLICE\r\n",
      "  Inputs: 1 2 4 3\r\n",
      "  Outputs: 5\r\n",
      "########### PRINT INTERPRETER STATE END ###########\r\n",
      "```\r\n",
      "\r\n",
      "The output Tensor has size of `[103, 0, 32]` not `[103, 8, 32]`.\r\n",
      "\r\n",
      "I think this is a bug on [`tflite::ops::builtin::strided_slice::ResizeOutputTensor`](https://github.com/tensorflow/tensorflow/blob/582c8d236cb079023657287c318ff26adb239002/tensorflow/lite/kernels/strided_slice.cc#L101).\r\n",
      "Please let me know if I did something wrong.\r\n",
      "\r\n",
      "**Other info / logs**\r\n",
      "You can find all the code snippets I used to reproduce the problem at [here](https://github.com/kukdh1/tflite_buf_report).\r\n",
      "(Download all dependencies by running `./download_dependencies.sh` at `tensorflow/lite/tools/make`)\r\n",
      "You can find tflite model too.\r\n",
      "\r\n",
      "P.S. The dimension error is quite random. Sometimes it calculates correctly. Sometimes innermost (axis=2) dimension becomes zero. Sometimes middle (axis=1) dimension becoms zero, Sometimes both (axis=1, axis=2) dimensions become zero.\r\n",
      "\r\n",
      "P.S. 2. I double checked with unmodified tensorflow source downloaded with:\r\n",
      "`wget https://github.com/tensorflow/tensorflow/archive/v2.4.0.tar.gz`\r\n",
      "on Ubuntu 18.04.5 (different machine) with GCC 7.5.0, bazel 3.7.2 and Python 3.6.9.\r\n",
      "Same command used to build `libtensorflowlite.so` and `.tf_configure.bazelrc` is:\r\n",
      "```\r\n",
      "build --action_env PYTHON_BIN_PATH=\"/home/kukdh1/.virtualenvs/tensorflow/bin/python3\"\r\n",
      "build --action_env PYTHON_LIB_PATH=\"/home/kukdh1/.virtualenvs/tensorflow/lib/python3.6/site-packages\"\r\n",
      "build --python_path=\"/home/kukdh1/.virtualenvs/tensorflow/bin/python3\"\r\n",
      "build --config=xla\r\n",
      "build:opt --copt=-march=native\r\n",
      "build:opt --copt=-Wno-sign-compare\r\n",
      "build:opt --host_copt=-march=native\r\n",
      "build:opt --define with_default_optimizations=true\r\n",
      "test --flaky_test_attempts=3\r\n",
      "test --test_size_filters=small,medium\r\n",
      "test:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial\r\n",
      "test:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu\r\n",
      "test:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only\r\n",
      "test:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only\r\n",
      "build --action_env TF_CONFIGURE_IOS=\"0\"\r\n",
      "```\r\n",
      "\r\n",
      "The output on Ubuntu machine was (one example):\r\n",
      "```\r\n",
      "Input: 41\r\n",
      "Output: [41 0 0 ]\r\n",
      "########## PRINT INTERPRETER STATE BEGIN ##########\r\n",
      "Interpreter has 6 tensors and 2 nodes\r\n",
      "Inputs: 0\r\n",
      "Outputs: 5\r\n",
      "\r\n",
      "Tensor   0 input_1              kTfLiteInt32   kTfLiteCustom          4 bytes ( 0.0 MB)\r\n",
      "Tensor   1 simple_layer/ReadVariableOp/resource kTfLiteFloat32   kTfLiteMmapRo     131072 bytes ( 0.1 MB)  128 8 32\r\n",
      "Tensor   2 simple_layer/strided_slice kTfLiteInt32   kTfLiteMmapRo         12 bytes ( 0.0 MB)  3\r\n",
      "Tensor   3 simple_layer/strided_slice1 kTfLiteInt32   kTfLiteMmapRo         12 bytes ( 0.0 MB)  3\r\n",
      "Tensor   4 simple_layer/strided_slice/stack_1 kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1\r\n",
      "Tensor   5 Identity             kTfLiteFloat32  kTfLiteDynamic          0 bytes ( 0.0 MB)  41 0 0\r\n",
      "\r\n",
      "Node   0 Operator Builtin Code  83 PACK\r\n",
      "  Inputs: 0\r\n",
      "  Outputs: 4\r\n",
      "Node   1 Operator Builtin Code  45 STRIDED_SLICE\r\n",
      "  Inputs: 1 2 4 3\r\n",
      "  Outputs: 5\r\n",
      "########### PRINT INTERPRETER STATE END ###########\r\n",
      "```\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:lite\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Missing functions from TF 2.4 vs TF 2.3 (tf.keras.backend)\n",
      "issue body -  **System information**\r\n",
      "- Used code from another repository : https://github.com/LeonLok/Deep-SORT-YOLOv4\r\n",
      "- Windows 10 Pro\r\n",
      "- pip install via anaconda\r\n",
      "- TensorFlow v2.4.0-rc4-71-g582c8d236cb 2.4.0\r\n",
      "- Python 3.6\r\n",
      "- CUDA 11.0 cuDNN 8.04\r\n",
      "- GeForce RTX 3060 Ti 8192 MB\r\n",
      "\r\n",
      "I am trying to run the repository stated above on my 3060Ti and made some modifications to run it on tf 2.4. I noticed that alot of functions from tf.keras.backend for tf 2.4 (https://www.tensorflow.org/api_docs/python/tf/keras/backend) are missing compared to tf 2.3 (https://www.tensorflow.org/versions/r2.3/api_docs/python/tf/keras/backend) in the API docs . I tried to look for alternatives and managed to replace some of them. However, I am not able to find some of the functions mentioned below. Are they obsolete or are they any other alternatives.\r\n",
      "\r\n",
      "K.arange\r\n",
      "K.concatenate \r\n",
      "K.dtype\r\n",
      "K.min\r\n",
      "K,gather \r\n",
      "K.sum\r\n",
      "K.switch \r\n",
      "K.max\r\n",
      "K.control_flow_ops.while_loop\r\n",
      "K.learning_phase\r\n",
      "\r\n",
      "**Code**\r\n",
      "https://github.com/LeonLok/Deep-SORT-YOLOv4/blob/master/tensorflow2.0/deep-sort-yolov4/yolo.py\r\n",
      "https://github.com/LeonLok/Deep-SORT-YOLOv4/blob/master/tensorflow2.0/deep-sort-yolov4/yolo4/model.py\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:keras\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\n",
      "issue body -  <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n",
      "- TensorFlow installed from (source or binary): binary\r\n",
      "- TensorFlow version: 2.4.0\r\n",
      "- Python version: 3.7.0\r\n",
      "- Installed using virtualenv? pip? conda?: pip\r\n",
      "- Bazel version (if compiling from source): N/A\r\n",
      "- GCC/Compiler version (if compiling from source): N/A\r\n",
      "- CUDA/cuDNN version: N/A\r\n",
      "- GPU model and memory: GeForce GTX 1060 6GB\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "Whenever I build or use tensorflow it gives me an Import error, I tried searching the web for solutions but I can't find any, I even tried reinstalling tensorflow just to be greeted  with the same error.\r\n",
      "\r\n",
      "**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n",
      "import tensorflow as tf\r\n",
      "\r\n",
      "https://user-images.githubusercontent.com/49057329/103400118-ec753980-4b7e-11eb-81f1-29a2e2ab7da2.mp4\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**Any other info / logs**\r\n",
      "```\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"C:\\Users\\LEE_FAMILY\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n",
      "    from tensorflow.python._pywrap_tensorflow_internal import *\r\n",
      "ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n",
      "\r\n",
      "During handling of the above exception, another exception occurred:\r\n",
      "\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"<stdin>\", line 1, in <module>\r\n",
      "  File \"C:\\Users\\LEE_FAMILY\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n",
      "    from tensorflow.python.tools import module_util as _module_util\r\n",
      "  File \"C:\\Users\\LEE_FAMILY\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 39, in <module>\r\n",
      "    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\r\n",
      "  File \"C:\\Users\\LEE_FAMILY\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 83, in <module>\r\n",
      "    raise ImportError(msg)\r\n",
      "ImportError: Traceback (most recent call last):\r\n",
      "  File \"C:\\Users\\LEE_FAMILY\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n",
      "    from tensorflow.python._pywrap_tensorflow_internal import *\r\n",
      "ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n",
      "```\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "stalled\n",
      "stat:awaiting response\n",
      "subtype:windows\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  Newline after printing ConfigError in configuration\n",
      "issue body -  Fix error message printed on configuration exception to have a newline suffix\r\n",
      "in CUDA configuration, without this the two lines run into each other.\r\n",
      "\r\n",
      "For some reason I didn't investigate further `print(e, file=sys.stderr)` didn't\r\n",
      "display output at all in the configuration process (print monkeypatched?),\r\n",
      "so I just tacked a newline on. Mostly just didn't want to leave the way it was\r\n",
      "after I had seen it happen on the console. :-)\r\n",
      "\r\n",
      "Before:\r\n",
      "\r\n",
      "```\r\n",
      "Please specify the comma-separated list of base paths to look for CUDA libraries and headers. [Leave empty to use the default]:\r\n",
      "\r\n",
      "Inconsistent CUDA toolkit path: /usr vs /usr/libAsking for detailed CUDA configuration...\r\n",
      "```\r\n",
      "\r\n",
      "After:\r\n",
      "\r\n",
      "```\r\n",
      "Please specify the comma-separated list of base paths to look for CUDA libraries and headers. [Leave empty to use the default]:\r\n",
      "\r\n",
      "Inconsistent CUDA toolkit path: /usr vs /usr/lib\r\n",
      "Asking for detailed CUDA configuration...\r\n",
      "```\n",
      "issue labels - \n",
      "cla: yes\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  dlerror: cusparse64_11.dll not found. Cannot detetct GPU.\n",
      "issue body -  **System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows10 64bit\r\n",
      "- TensorFlow version: tensorflow-gpu=2.4.0\r\n",
      "- Python version: 3.8.5\r\n",
      "- Installed using pip \r\n",
      "- CUDA/cuDNN version: CUDA 11.1 | cuDNN v8.0.4 for CUDA 11.1\r\n",
      "- GPU model and memory: NVIDIA GeForce GTX 1050\r\n",
      "\r\n",
      "**I try to use NVIDIA GeForce GTX 1050 by installing all the ablove mnetioned specs. Cannot find the cusparse64_11.dll. The GPU is not being detected for running deep learning models. \r\n",
      "\r\n",
      "\r\n",
      "Could not load dynamic library 'cusparse64_11.dll'; **dlerror: cusparse64_11.dll not found**\r\n",
      "\r\n",
      "Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\n",
      "Skipping registering GPU devices... **\r\n",
      "\r\n",
      "![image](https://user-images.githubusercontent.com/48597846/103396793-2c2a2a00-4b5b-11eb-9499-8b66759fba37.png)\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "stat:awaiting response\n",
      "subtype:windows\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  Cherry-pick: Lazy load estimator instead of using virtual pip package\n",
      "issue body -  …//github.com/tensorflow/community/pull/182\r\n",
      "\r\n",
      "PiperOrigin-RevId: 294847967\r\n",
      "Change-Id: I327d075a2065e2ccf8ad5317882ebde14e3dc3d6\r\n",
      "\r\n",
      "Hello, I try to use Tensorflow v1.15.0 with vscode.\r\n",
      "But, there is the same issue with #32982.\r\n",
      "That issue was fixed according to https://github.com/tensorflow/tensorflow/issues/32982#issuecomment-589767901, but, only  >= 2.0.0...\r\n",
      "\r\n",
      "So, I just cherry-pick the commit 5c00e79.\n",
      "issue labels - \n",
      "cla: yes\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  u-net always not able to run in a single delegate\n",
      "issue body -  \r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n",
      "  No, just use  prebuilt  android_aarch64_benchmark_model\r\n",
      "- OS Platform and Distribution: \r\n",
      "  ested on Android \r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "  Qualcomm dragon 855\r\n",
      "- TensorFlow installed from (source or binary):\r\n",
      "  google tensorflow prebuilt android_aarch64_benchmark_model\r\n",
      "\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "I tried many u-net model for semantic segmentation, none of those can be run on nnapi delegate fully. and most likely the transpose conv will be fallback on CPU. Though the underlying nnapi accel supports transport conv.\r\n",
      "\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "It should run the model fully on one single delegate without fallback to CPU which extremely slow. Or have some kinds of indication  why t is not  running fully in nnapi delegate and fallback to CPU. The TRANSPOSE_CONV and CONV2D are clearly supported by the underly nnapi acceleration. But it just fallback to CPU for unknown reason. \r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "Download android_aarch64_benchmark_model from https://www.tensorflow.org/lite/performance/measurement\r\n",
      "adb push that into /data/local/tmp/\r\n",
      "created one using unet with post quant.\r\n",
      "Run \r\n",
      "/android_aarch64_benchmark_model  --graph=model.tflite  --use_nnapi=true  --nnapi_accelerator_name=qti-gpu --enable_op_profiling=true                                                               \r\n",
      "\r\n",
      "Expected:\r\n",
      "Explicitly applied NNAPI delegate, and the model graph will be completely executed by the delegate.\r\n",
      "\r\n",
      "Current:\r\n",
      "Explicitly applied NNAPI delegate, and the model graph will be partially executed by the delegate w/ 4 delegate kernels.\r\n",
      "\r\n",
      "**Other info / logs** Include any logs or source code that would be helpful to\r\n",
      "diagnose the problem. If including tracebacks, please include the full\r\n",
      "traceback. Large logs and files should be attached.\r\n",
      "STARTING!\r\n",
      "Log parameter values verbosely: [0]\r\n",
      "Graph: [model.tflite]\r\n",
      "Enable op profiling: [1]\r\n",
      "Use NNAPI: [1]\r\n",
      "NNAPI accelerator name: [qti-gpu]\r\n",
      "NNAPI accelerators available: [qti-default,qti-dsp,qti-gpu,qti-hta,nnapi-reference]\r\n",
      "Loaded model model.tflite\r\n",
      "INFO: Initialized TensorFlow Lite runtime.\r\n",
      "INFO: Created TensorFlow Lite delegate for NNAPI.\r\n",
      "Explicitly applied NNAPI delegate, and the model graph will be partially executed by the delegate w/ 4 delegate kernels.\r\n",
      "The input model file size (MB): 0.398212\r\n",
      "Initialized session in 185.23ms.\r\n",
      "Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\r\n",
      "count=10 first=72646 curr=55109 min=43832 max=72646 avg=50987.8 std=7966\r\n",
      "\r\n",
      "Running benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.\r\n",
      "count=50 first=57407 curr=57830 min=39546 max=63652 avg=54400.3 std=5091\r\n",
      "\r\n",
      "Inference timings in us: Init: 185230, First inference: 72646, Warmup (avg): 50987.8, Inference (avg): 54400.3\r\n",
      "Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.\r\n",
      "Peak memory footprint (MB): init=3.77344 overall=14.918\r\n",
      "Profiling Info for Benchmark Initialization:\r\n",
      "============================== Run Order ==============================\r\n",
      "\t             [node type]\t          [start]\t  [first]\t [avg ms]\t     [%]\t  [cdf%]\t  [mem KB]\t[times called]\t[Name]\r\n",
      "\t ModifyGraphWithDelegate\t            0.000\t  184.511\t  184.511\t 50.938%\t 50.938%\t  3864.000\t        1\tModifyGraphWithDelegate/0\r\n",
      "\t         AllocateTensors\t           95.737\t  177.694\t   88.858\t 49.062%\t100.000%\t     0.000\t        2\tAllocateTensors/0\r\n",
      "\r\n",
      "============================== Top by Computation Time ==============================\r\n",
      "\t             [node type]\t          [start]\t  [first]\t [avg ms]\t     [%]\t  [cdf%]\t  [mem KB]\t[times called]\t[Name]\r\n",
      "\t ModifyGraphWithDelegate\t            0.000\t  184.511\t  184.511\t 50.938%\t 50.938%\t  3864.000\t        1\tModifyGraphWithDelegate/0\r\n",
      "\t         AllocateTensors\t           95.737\t  177.694\t   88.858\t 49.062%\t100.000%\t     0.000\t        2\tAllocateTensors/0\r\n",
      "\r\n",
      "Number of nodes executed: 2\r\n",
      "============================== Summary by node type ==============================\r\n",
      "\t             [Node type]\t  [count]\t  [avg ms]\t    [avg %]\t    [cdf %]\t  [mem KB]\t[times called]\r\n",
      "\t ModifyGraphWithDelegate\t        1\t   184.511\t    50.938%\t    50.938%\t  3864.000\t        1\r\n",
      "\t         AllocateTensors\t        1\t   177.715\t    49.062%\t   100.000%\t     0.000\t        2\r\n",
      "\r\n",
      "Timings (microseconds): count=1 curr=362226\r\n",
      "Memory (bytes): count=0\r\n",
      "2 nodes observed\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "Operator-wise Profiling Info for Regular Benchmark Runs:\r\n",
      "============================== Run Order ==============================\r\n",
      "\t             [node type]\t          [start]\t  [first]\t [avg ms]\t     [%]\t  [cdf%]\t  [mem KB]\t[times called]\t[Name]\r\n",
      "\t     TfLiteNnapiDelegate\t            0.000\t    7.375\t    6.549\t 12.047%\t 12.047%\t     0.000\t        1\t[unet/Relu_1;StatefulPartitionedCall/unet/Relu_1;unet/batch_normalization_1/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_1/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_1/BiasAdd/ReadVariableOp;unet/conv2d_1/BiasAdd;StatefulPartitionedCall/unet/conv2d_1/BiasAdd;unet/batch_normalization_15/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_15/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_15/BiasAdd/ReadVariableOp;unet/conv2d_15/BiasAdd;StatefulPartitionedCall/unet/conv2d_15/BiasAdd;StatefulPartitionedCall/unet/conv2d_15/Conv2D/ReadVariableOp;unet/conv2d_15/Conv2D;StatefulPartitionedCall/unet/conv2d_15/Conv2D;StatefulPartitionedCall/unet/conv2d_1/Conv2D/ReadVariableOp;unet/conv2d_1/Conv2D;StatefulPartitionedCall/unet/conv2d_1/Conv2D1, unet/Relu_3;StatefulPartitionedCall/unet/Relu_3;unet/batch_normalization_3/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_3/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_3/BiasAdd/ReadVariableOp;unet/conv2d_3/BiasAdd;StatefulPartitionedCall/unet/conv2d_3/BiasAdd;unet/batch_normalization_13/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_13/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_13/BiasAdd/ReadVariableOp;unet/conv2d_13/BiasAdd;StatefulPartitionedCall/unet/conv2d_13/BiasAdd;StatefulPartitionedCall/unet/conv2d_13/Conv2D/ReadVariableOp;unet/conv2d_13/Conv2D;StatefulPartitionedCall/unet/conv2d_13/Conv2D;StatefulPartitionedCall/unet/conv2d_3/Conv2D/ReadVariableOp;unet/conv2d_3/Conv2D;StatefulPartitionedCall/unet/conv2d_3/Conv2D1, unet/batch_normalization_7/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_7/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_7/BiasAdd/ReadVariableOp;unet/conv2d_7/BiasAdd;StatefulPartitionedCall/unet/conv2d_7/BiasAdd;unet/batch_normalization_11/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_11/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_11/BiasAdd/ReadVariableOp;unet/conv2d_11/BiasAdd;StatefulPartitionedCall/unet/conv2d_11/BiasAdd;StatefulPartitionedCall/unet/conv2d_11/Conv2D/ReadVariableOp;unet/conv2d_11/Conv2D;StatefulPartitionedCall/unet/conv2d_11/Conv2D;StatefulPartitionedCall/unet/conv2d_7/Conv2D/ReadVariableOp;unet/conv2d_7/Conv2D;StatefulPartitionedCall/unet/conv2d_7/Conv2D1]:25\r\n",
      "\t                 CONV_2D\t            6.554\t   11.632\t    6.679\t 12.286%\t 24.333%\t     0.000\t        1\t[unet/batch_normalization_8/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_8/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_8/BiasAdd/ReadVariableOp;unet/conv2d_8/BiasAdd;StatefulPartitionedCall/unet/conv2d_8/BiasAdd;unet/batch_normalization_11/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_11/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_11/BiasAdd/ReadVariableOp;unet/conv2d_11/BiasAdd;StatefulPartitionedCall/unet/conv2d_11/BiasAdd;StatefulPartitionedCall/unet/conv2d_11/Conv2D/ReadVariableOp;unet/conv2d_11/Conv2D;StatefulPartitionedCall/unet/conv2d_11/Conv2D;StatefulPartitionedCall/unet/conv2d_8/Conv2D/ReadVariableOp;unet/conv2d_8/Conv2D;StatefulPartitionedCall/unet/conv2d_8/Conv2D1]:10\r\n",
      "\t                 CONV_2D\t           13.236\t    7.296\t    6.370\t 11.717%\t 36.050%\t     0.000\t        1\t[unet/Relu_7;StatefulPartitionedCall/unet/Relu_7;unet/batch_normalization_9/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_9/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_9/BiasAdd/ReadVariableOp;unet/conv2d_9/BiasAdd;StatefulPartitionedCall/unet/conv2d_9/BiasAdd;unet/batch_normalization_11/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_11/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_11/BiasAdd/ReadVariableOp;unet/conv2d_11/BiasAdd;StatefulPartitionedCall/unet/conv2d_11/BiasAdd;StatefulPartitionedCall/unet/conv2d_11/Conv2D/ReadVariableOp;unet/conv2d_11/Conv2D;StatefulPartitionedCall/unet/conv2d_11/Conv2D;StatefulPartitionedCall/unet/conv2d_9/Conv2D/ReadVariableOp;unet/conv2d_9/Conv2D;StatefulPartitionedCall/unet/conv2d_9/Conv2D1]:11\r\n",
      "\t                 CONV_2D\t           19.609\t    4.624\t    5.985\t 11.009%\t 47.059%\t     0.000\t        1\t[unet/Relu_8;StatefulPartitionedCall/unet/Relu_8;unet/batch_normalization_10/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_10/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_10/BiasAdd/ReadVariableOp;unet/conv2d_10/BiasAdd;StatefulPartitionedCall/unet/conv2d_10/BiasAdd;unet/batch_normalization_11/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_11/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_11/BiasAdd/ReadVariableOp;unet/conv2d_11/BiasAdd;StatefulPartitionedCall/unet/conv2d_11/BiasAdd;StatefulPartitionedCall/unet/conv2d_11/Conv2D/ReadVariableOp;unet/conv2d_11/Conv2D;StatefulPartitionedCall/unet/conv2d_11/Conv2D;StatefulPartitionedCall/unet/conv2d_10/Conv2D/ReadVariableOp;unet/conv2d_10/Conv2D;StatefulPartitionedCall/unet/conv2d_10/Conv2D1]:12\r\n",
      "\t     TfLiteNnapiDelegate\t           25.596\t    3.368\t    3.671\t  6.753%\t 53.812%\t     0.000\t        1\t[unet/Relu_10;StatefulPartitionedCall/unet/Relu_10;unet/batch_normalization_12/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_12/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_12/BiasAdd/ReadVariableOp;unet/conv2d_12/BiasAdd;StatefulPartitionedCall/unet/conv2d_12/BiasAdd;unet/batch_normalization_13/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_13/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_13/BiasAdd/ReadVariableOp;unet/conv2d_13/BiasAdd;StatefulPartitionedCall/unet/conv2d_13/BiasAdd;StatefulPartitionedCall/unet/conv2d_13/Conv2D/ReadVariableOp;unet/conv2d_13/Conv2D;StatefulPartitionedCall/unet/conv2d_13/Conv2D;StatefulPartitionedCall/unet/conv2d_12/Conv2D/ReadVariableOp;unet/conv2d_12/Conv2D;StatefulPartitionedCall/unet/conv2d_12/Conv2D1]:26\r\n",
      "\t          TRANSPOSE_CONV\t           29.271\t    3.924\t    4.570\t  8.407%\t 62.218%\t     0.000\t        1\t[unet/conv2d_transpose/conv2d_transpose;StatefulPartitionedCall/unet/conv2d_transpose/conv2d_transpose1]:15\r\n",
      "\t                     ADD\t           33.843\t    0.238\t    0.267\t  0.490%\t 62.709%\t     0.000\t        1\t[unet/conv2d_transpose/BiasAdd;StatefulPartitionedCall/unet/conv2d_transpose/BiasAdd]:16\r\n",
      "\t     TfLiteNnapiDelegate\t           34.111\t    4.639\t    4.690\t  8.627%\t 71.335%\t     0.000\t        1\t[unet/Relu_12;StatefulPartitionedCall/unet/Relu_12;unet/batch_normalization_14/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_14/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_14/BiasAdd/ReadVariableOp;unet/conv2d_14/BiasAdd;StatefulPartitionedCall/unet/conv2d_14/BiasAdd;unet/batch_normalization_15/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_15/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_15/BiasAdd/ReadVariableOp;unet/conv2d_15/BiasAdd;StatefulPartitionedCall/unet/conv2d_15/BiasAdd;StatefulPartitionedCall/unet/conv2d_15/Conv2D/ReadVariableOp;unet/conv2d_15/Conv2D;StatefulPartitionedCall/unet/conv2d_15/Conv2D;StatefulPartitionedCall/unet/conv2d_14/Conv2D/ReadVariableOp;unet/conv2d_14/Conv2D;StatefulPartitionedCall/unet/conv2d_14/Conv2D1]:27\r\n",
      "\t          TRANSPOSE_CONV\t           38.804\t    7.393\t    8.092\t 14.884%\t 86.219%\t     0.000\t        1\t[unet/conv2d_transpose_1/conv2d_transpose;StatefulPartitionedCall/unet/conv2d_transpose_1/conv2d_transpose1]:20\r\n",
      "\t                     ADD\t           46.899\t    0.915\t    1.073\t  1.974%\t 88.194%\t     0.000\t        1\t[unet/conv2d_transpose_1/BiasAdd;StatefulPartitionedCall/unet/conv2d_transpose_1/BiasAdd]:21\r\n",
      "\t     TfLiteNnapiDelegate\t           47.974\t    5.968\t    6.418\t 11.806%\t100.000%\t     0.000\t        1\t[Identity]:28\r\n",
      "\r\n",
      "============================== Top by Computation Time ==============================\r\n",
      "\t             [node type]\t          [start]\t  [first]\t [avg ms]\t     [%]\t  [cdf%]\t  [mem KB]\t[times called]\t[Name]\r\n",
      "\t          TRANSPOSE_CONV\t           38.804\t    7.393\t    8.092\t 14.884%\t 14.884%\t     0.000\t        1\t[unet/conv2d_transpose_1/conv2d_transpose;StatefulPartitionedCall/unet/conv2d_transpose_1/conv2d_transpose1]:20\r\n",
      "\t                 CONV_2D\t            6.554\t   11.632\t    6.679\t 12.286%\t 27.170%\t     0.000\t        1\t[unet/batch_normalization_8/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_8/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_8/BiasAdd/ReadVariableOp;unet/conv2d_8/BiasAdd;StatefulPartitionedCall/unet/conv2d_8/BiasAdd;unet/batch_normalization_11/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_11/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_11/BiasAdd/ReadVariableOp;unet/conv2d_11/BiasAdd;StatefulPartitionedCall/unet/conv2d_11/BiasAdd;StatefulPartitionedCall/unet/conv2d_11/Conv2D/ReadVariableOp;unet/conv2d_11/Conv2D;StatefulPartitionedCall/unet/conv2d_11/Conv2D;StatefulPartitionedCall/unet/conv2d_8/Conv2D/ReadVariableOp;unet/conv2d_8/Conv2D;StatefulPartitionedCall/unet/conv2d_8/Conv2D1]:10\r\n",
      "\t     TfLiteNnapiDelegate\t            0.000\t    7.375\t    6.549\t 12.047%\t 39.217%\t     0.000\t        1\t[unet/Relu_1;StatefulPartitionedCall/unet/Relu_1;unet/batch_normalization_1/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_1/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_1/BiasAdd/ReadVariableOp;unet/conv2d_1/BiasAdd;StatefulPartitionedCall/unet/conv2d_1/BiasAdd;unet/batch_normalization_15/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_15/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_15/BiasAdd/ReadVariableOp;unet/conv2d_15/BiasAdd;StatefulPartitionedCall/unet/conv2d_15/BiasAdd;StatefulPartitionedCall/unet/conv2d_15/Conv2D/ReadVariableOp;unet/conv2d_15/Conv2D;StatefulPartitionedCall/unet/conv2d_15/Conv2D;StatefulPartitionedCall/unet/conv2d_1/Conv2D/ReadVariableOp;unet/conv2d_1/Conv2D;StatefulPartitionedCall/unet/conv2d_1/Conv2D1, unet/Relu_3;StatefulPartitionedCall/unet/Relu_3;unet/batch_normalization_3/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_3/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_3/BiasAdd/ReadVariableOp;unet/conv2d_3/BiasAdd;StatefulPartitionedCall/unet/conv2d_3/BiasAdd;unet/batch_normalization_13/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_13/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_13/BiasAdd/ReadVariableOp;unet/conv2d_13/BiasAdd;StatefulPartitionedCall/unet/conv2d_13/BiasAdd;StatefulPartitionedCall/unet/conv2d_13/Conv2D/ReadVariableOp;unet/conv2d_13/Conv2D;StatefulPartitionedCall/unet/conv2d_13/Conv2D;StatefulPartitionedCall/unet/conv2d_3/Conv2D/ReadVariableOp;unet/conv2d_3/Conv2D;StatefulPartitionedCall/unet/conv2d_3/Conv2D1, unet/batch_normalization_7/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_7/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_7/BiasAdd/ReadVariableOp;unet/conv2d_7/BiasAdd;StatefulPartitionedCall/unet/conv2d_7/BiasAdd;unet/batch_normalization_11/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_11/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_11/BiasAdd/ReadVariableOp;unet/conv2d_11/BiasAdd;StatefulPartitionedCall/unet/conv2d_11/BiasAdd;StatefulPartitionedCall/unet/conv2d_11/Conv2D/ReadVariableOp;unet/conv2d_11/Conv2D;StatefulPartitionedCall/unet/conv2d_11/Conv2D;StatefulPartitionedCall/unet/conv2d_7/Conv2D/ReadVariableOp;unet/conv2d_7/Conv2D;StatefulPartitionedCall/unet/conv2d_7/Conv2D1]:25\r\n",
      "\t     TfLiteNnapiDelegate\t           47.974\t    5.968\t    6.418\t 11.806%\t 51.023%\t     0.000\t        1\t[Identity]:28\r\n",
      "\t                 CONV_2D\t           13.236\t    7.296\t    6.370\t 11.717%\t 62.740%\t     0.000\t        1\t[unet/Relu_7;StatefulPartitionedCall/unet/Relu_7;unet/batch_normalization_9/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_9/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_9/BiasAdd/ReadVariableOp;unet/conv2d_9/BiasAdd;StatefulPartitionedCall/unet/conv2d_9/BiasAdd;unet/batch_normalization_11/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_11/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_11/BiasAdd/ReadVariableOp;unet/conv2d_11/BiasAdd;StatefulPartitionedCall/unet/conv2d_11/BiasAdd;StatefulPartitionedCall/unet/conv2d_11/Conv2D/ReadVariableOp;unet/conv2d_11/Conv2D;StatefulPartitionedCall/unet/conv2d_11/Conv2D;StatefulPartitionedCall/unet/conv2d_9/Conv2D/ReadVariableOp;unet/conv2d_9/Conv2D;StatefulPartitionedCall/unet/conv2d_9/Conv2D1]:11\r\n",
      "\t                 CONV_2D\t           19.609\t    4.624\t    5.985\t 11.009%\t 73.749%\t     0.000\t        1\t[unet/Relu_8;StatefulPartitionedCall/unet/Relu_8;unet/batch_normalization_10/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_10/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_10/BiasAdd/ReadVariableOp;unet/conv2d_10/BiasAdd;StatefulPartitionedCall/unet/conv2d_10/BiasAdd;unet/batch_normalization_11/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_11/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_11/BiasAdd/ReadVariableOp;unet/conv2d_11/BiasAdd;StatefulPartitionedCall/unet/conv2d_11/BiasAdd;StatefulPartitionedCall/unet/conv2d_11/Conv2D/ReadVariableOp;unet/conv2d_11/Conv2D;StatefulPartitionedCall/unet/conv2d_11/Conv2D;StatefulPartitionedCall/unet/conv2d_10/Conv2D/ReadVariableOp;unet/conv2d_10/Conv2D;StatefulPartitionedCall/unet/conv2d_10/Conv2D1]:12\r\n",
      "\t     TfLiteNnapiDelegate\t           34.111\t    4.639\t    4.690\t  8.627%\t 82.376%\t     0.000\t        1\t[unet/Relu_12;StatefulPartitionedCall/unet/Relu_12;unet/batch_normalization_14/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_14/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_14/BiasAdd/ReadVariableOp;unet/conv2d_14/BiasAdd;StatefulPartitionedCall/unet/conv2d_14/BiasAdd;unet/batch_normalization_15/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_15/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_15/BiasAdd/ReadVariableOp;unet/conv2d_15/BiasAdd;StatefulPartitionedCall/unet/conv2d_15/BiasAdd;StatefulPartitionedCall/unet/conv2d_15/Conv2D/ReadVariableOp;unet/conv2d_15/Conv2D;StatefulPartitionedCall/unet/conv2d_15/Conv2D;StatefulPartitionedCall/unet/conv2d_14/Conv2D/ReadVariableOp;unet/conv2d_14/Conv2D;StatefulPartitionedCall/unet/conv2d_14/Conv2D1]:27\r\n",
      "\t          TRANSPOSE_CONV\t           29.271\t    3.924\t    4.570\t  8.407%\t 90.783%\t     0.000\t        1\t[unet/conv2d_transpose/conv2d_transpose;StatefulPartitionedCall/unet/conv2d_transpose/conv2d_transpose1]:15\r\n",
      "\t     TfLiteNnapiDelegate\t           25.596\t    3.368\t    3.671\t  6.753%\t 97.535%\t     0.000\t        1\t[unet/Relu_10;StatefulPartitionedCall/unet/Relu_10;unet/batch_normalization_12/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_12/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_12/BiasAdd/ReadVariableOp;unet/conv2d_12/BiasAdd;StatefulPartitionedCall/unet/conv2d_12/BiasAdd;unet/batch_normalization_13/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_13/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_13/BiasAdd/ReadVariableOp;unet/conv2d_13/BiasAdd;StatefulPartitionedCall/unet/conv2d_13/BiasAdd;StatefulPartitionedCall/unet/conv2d_13/Conv2D/ReadVariableOp;unet/conv2d_13/Conv2D;StatefulPartitionedCall/unet/conv2d_13/Conv2D;StatefulPartitionedCall/unet/conv2d_12/Conv2D/ReadVariableOp;unet/conv2d_12/Conv2D;StatefulPartitionedCall/unet/conv2d_12/Conv2D1]:26\r\n",
      "\t                     ADD\t           46.899\t    0.915\t    1.073\t  1.974%\t 99.510%\t     0.000\t        1\t[unet/conv2d_transpose_1/BiasAdd;StatefulPartitionedCall/unet/conv2d_transpose_1/BiasAdd]:21\r\n",
      "\r\n",
      "Number of nodes executed: 11\r\n",
      "============================== Summary by node type ==============================\r\n",
      "\t             [Node type]\t  [count]\t  [avg ms]\t    [avg %]\t    [cdf %]\t  [mem KB]\t[times called]\r\n",
      "\t     TfLiteNnapiDelegate\t        4\t    21.327\t    39.233%\t    39.233%\t     0.000\t        4\r\n",
      "\t                 CONV_2D\t        3\t    19.033\t    35.013%\t    74.246%\t     0.000\t        3\r\n",
      "\t          TRANSPOSE_CONV\t        2\t    12.661\t    23.291%\t    97.537%\t     0.000\t        2\r\n",
      "\t                     ADD\t        2\t     1.339\t     2.463%\t   100.000%\t     0.000\t        2\r\n",
      "\r\n",
      "Timings (microseconds): count=50 first=57372 curr=57796 min=39517 max=63611 avg=54364.5 std=5088\r\n",
      "Memory (bytes): count=0\r\n",
      "11 nodes observed\r\n",
      "\n",
      "issue labels - \n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  undefined symbol: _ZN6tflite12tensor_utils27NeonSymmetricQuantizeFloatsEPKfiPaPfS4_S4_\n",
      "issue body -  when I run .tflite file in my Raspberry Pi 3B+ ,some errors happened.\r\n",
      "Python version: 3.7.3\r\n",
      "Tensorflow version: 1.13.1\r\n",
      "GCC: 8.3.0\r\n",
      "Description:    Raspbian GNU/Linux 10 (buster)\r\n",
      "Release:        10\r\n",
      "Codename:       buster\r\n",
      "I have changed tf version but it did not worK. \r\n",
      "Please help!!\r\n",
      "here is the error:\r\n",
      "`Traceback (most recent call last):\r\n",
      "  File \"/home/pi/project/test.py\", line 62, in <module>\r\n",
      "    interpreter = tf.lite.Interpreter(model_path=tflite_path)\r\n",
      "  File \"/home/pi/.local/lib/python3.7/site-packages/tensorflow_core/lite/python/interpreter.py\", line 154, in __init__\r\n",
      "    _interpreter_wrapper.InterpreterWrapper_CreateWrapperCPPFromFile(\r\n",
      "  File \"/home/pi/.local/lib/python3.7/site-packages/tensorflow_core/python/util/lazy_loader.py\", line 62, in __getattr__\r\n",
      "    module = self._load()\r\n",
      "  File \"/home/pi/.local/lib/python3.7/site-packages/tensorflow_core/python/util/lazy_loader.py\", line 45, in _load\r\n",
      "    module = importlib.import_module(self.__name__)\r\n",
      "  File \"/usr/lib/python3.7/importlib/__init__.py\", line 127, in import_module\r\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\r\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n",
      "  File \"/home/pi/.local/lib/python3.7/site-packages/tensorflow_core/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py\", line 28, in <module>\r\n",
      "    _tensorflow_wrap_interpreter_wrapper = swig_import_helper()\r\n",
      "  File \"/home/pi/.local/lib/python3.7/site-packages/tensorflow_core/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py\", line 24, in swig_import_helper\r\n",
      "    _mod = imp.load_module('_tensorflow_wrap_interpreter_wrapper', fp, pathname, description)\r\n",
      "  File \"/usr/lib/python3.7/imp.py\", line 242, in load_module\r\n",
      "    return load_dynamic(name, filename, file)\r\n",
      "  File \"/usr/lib/python3.7/imp.py\", line 342, in load_dynamic\r\n",
      "    return _load(spec)\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 696, in _load\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 670, in _load_unlocked\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 583, in module_from_spec\r\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1043, in create_module\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n",
      "ImportError: /home/pi/.local/lib/python3.7/site-packages/tensorflow_core/lite/python/interpreter_wrapper/_tensorflow_wrap_interpreter_wrapper.so: undefined symbol: _ZN6tflite12tensor_utils24NeonVectorScalarMultiplyEPKaifPf\r\n",
      "`\r\n",
      "\n",
      "issue labels - \n",
      "TF 1.13\n",
      "comp:lite\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Is there have API to automatically calculate #flops (e.g. like flop_counter in pytorch) in tensorflow2?\n",
      "issue body -  This template is for miscellaneous issues not covered by the other issue categories.\r\n",
      "***\r\n",
      "Environment: python3.6+tensorflow2.1, \r\n",
      "Question:  Is there have API to automatically calculate #flops (e.g.  flop_counter) in tensorflow2?\r\n",
      "Note: in Pytorch, this could be easily realized by calling API, e.g. thop.profile(),  flop_counter， etc.\r\n",
      "***\r\n",
      "\r\n",
      "For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n",
      "\r\n",
      "If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n",
      "\r\n",
      "For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n",
      "\n",
      "issue labels - \n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  Receiving the error: \"ValueError: Input 0 of layer sequential_8 is incompatible with the layer: expected axis -1 of input shape to have value 8 but received input with shape (None, 7, 169)\"\n",
      "issue body -  Hello,\r\n",
      "\r\n",
      "I would like to apologize if this is posted in the wrong location.  If that is the case, any guidance as to where I can ask this question would be greatly appreciated.  I am new to coding and GitHub, but have found many answers to posts on this forum very helpful to me.  I am trying to setup a code to utilize the Keras model for deep learning with a 3d dataset, and am uncertain how to deal with the error I'm receiving (in the title).  I have found a similar error reported on this forum, but the response was that it would be fixed in TensorFlow v 2.1.  I am using v2.4.0, so I expect I'm probably doing something wrong here.  The code I am using is as follows:\r\n",
      "\r\n",
      "```\r\n",
      "# define the keras model\r\n",
      "model = Sequential()\r\n",
      "model.add(Dense(12, input_dim=7, activation='relu'))\r\n",
      "model.add(Dense(8, activation='relu'))\r\n",
      "model.add(Dense(1, activation='sigmoid'))\r\n",
      "\r\n",
      "# compile the keras model\r\n",
      "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
      "\r\n",
      "# separate the data\r\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\r\n",
      "\r\n",
      "# fit the keras model on the dataset\r\n",
      "model.fit(X_train, y_train, epochs=150, batch_size=10)\r\n",
      "\r\n",
      "# evaluate the keras model\r\n",
      "_, accuracy = model.evaluate(X_test, y_test)\r\n",
      "print('Accuracy: %.2f' % (accuracy*100))\r\n",
      "```\r\n",
      "\r\n",
      "Information on the variables as follows:\r\n",
      "_X is an array of float64, with size (152,7,169)\r\n",
      "Y is an array of int32, with size (152,)_\r\n",
      "\r\n",
      "Information on versions as follows:\r\n",
      "_Python version 3.8.5\r\n",
      "Keras version 2.4.3\r\n",
      "TensorFlow version 2.4.0\r\n",
      "Windows 10\r\n",
      "Intel(R) Core(TM) i5-8350U CPU @ 1.70GHz   1.90 GHz  (AVX capable)\r\n",
      "Spyder version 4.2.0\r\n",
      " _\r\n",
      "\r\n",
      "The full error I am getting is as follows:\r\n",
      "\r\n",
      "> Epoch 1/150\r\n",
      "> Traceback (most recent call last):\r\n",
      "> \r\n",
      ">   File \"C:\\Users\\pdeol\\Documents\\CodingPractice\\KerasCombined.py\", line 171, in <module>\r\n",
      ">     model.fit(X_train, y_train, epochs=150, batch_size=10)\r\n",
      "> \r\n",
      ">   File \"C:\\Users\\pdeol\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1100, in fit\r\n",
      ">     tmp_logs = self.train_function(iterator)\r\n",
      "> \r\n",
      ">   File \"C:\\Users\\pdeol\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 828, in __call__\r\n",
      ">     result = self._call(*args, **kwds)\r\n",
      "> \r\n",
      ">   File \"C:\\Users\\pdeol\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 871, in _call\r\n",
      ">     self._initialize(args, kwds, add_initializers_to=initializers)\r\n",
      "> \r\n",
      ">   File \"C:\\Users\\pdeol\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 725, in _initialize\r\n",
      ">     self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n",
      "> \r\n",
      ">   File \"C:\\Users\\pdeol\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2969, in _get_concrete_function_internal_garbage_collected\r\n",
      ">     graph_function, _ = self._maybe_define_function(args, kwargs)\r\n",
      "> \r\n",
      ">   File \"C:\\Users\\pdeol\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 3361, in _maybe_define_function\r\n",
      ">     graph_function = self._create_graph_function(args, kwargs)\r\n",
      "> \r\n",
      ">   File \"C:\\Users\\pdeol\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 3196, in _create_graph_function\r\n",
      ">     func_graph_module.func_graph_from_py_func(\r\n",
      "> \r\n",
      ">   File \"C:\\Users\\pdeol\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\", line 990, in func_graph_from_py_func\r\n",
      ">     func_outputs = python_func(*func_args, **func_kwargs)\r\n",
      "> \r\n",
      ">   File \"C:\\Users\\pdeol\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 634, in wrapped_fn\r\n",
      ">     out = weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n",
      "> \r\n",
      ">   File \"C:\\Users\\pdeol\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\", line 977, in wrapper\r\n",
      ">     raise e.ag_error_metadata.to_exception(e)\r\n",
      "> \r\n",
      "> ValueError: in user code:\r\n",
      "> \r\n",
      ">     C:\\Users\\pdeol\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:805 train_function  *\r\n",
      ">         return step_function(self, iterator)\r\n",
      ">     C:\\Users\\pdeol\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:795 step_function  **\r\n",
      ">         outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n",
      ">     C:\\Users\\pdeol\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1259 run\r\n",
      ">         return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n",
      ">     C:\\Users\\pdeol\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2730 call_for_each_replica\r\n",
      ">         return self._call_for_each_replica(fn, args, kwargs)\r\n",
      ">     C:\\Users\\pdeol\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3417 _call_for_each_replica\r\n",
      ">         return fn(*args, **kwargs)\r\n",
      ">     C:\\Users\\pdeol\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:788 run_step  **\r\n",
      ">         outputs = model.train_step(data)\r\n",
      ">     C:\\Users\\pdeol\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:754 train_step\r\n",
      ">         y_pred = self(x, training=True)\r\n",
      ">     C:\\Users\\pdeol\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:998 __call__\r\n",
      ">         input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\r\n",
      ">     C:\\Users\\pdeol\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:255 assert_input_compatibility\r\n",
      ">         raise ValueError(\r\n",
      "> \r\n",
      ">     ValueError: Input 0 of layer sequential is incompatible with the layer: expected axis -1 of input shape to have value 7 but received input with shape (None, 7, 169)\r\n",
      "\r\n",
      "The above error is what I receive every time I run this code.  I just restarted the software and ran it again, and in addition to the above error, I received the following error immediately below:\r\n",
      "\r\n",
      "> 2020-12-30 10:17:26.556667: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\r\n",
      "> 2020-12-30 10:17:26.557141: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n",
      "> 2020-12-30 10:26:08.613023: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n",
      "> 2020-12-30 10:26:08.616994: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'nvcuda.dll'; dlerror: nvcuda.dll not found\r\n",
      "> 2020-12-30 10:26:08.617895: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\r\n",
      "> 2020-12-30 10:26:08.628902: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: LT-B831B58344E3\r\n",
      "> 2020-12-30 10:26:08.629094: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: LT-B831B58344E3\r\n",
      "> 2020-12-30 10:26:08.634489: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\r\n",
      "> To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n",
      "> 2020-12-30 10:26:08.636834: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n",
      "> 2020-12-30 10:26:08.863911: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n",
      "\r\n",
      "I'm not sure if this is related, or a completely separate issue.  Any information or guidance you can provide would be greatly appreciated.\n",
      "issue labels - \n",
      "comp:keras\n",
      "stat:awaiting response\n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  fix static path for find_cuda_config.py script, so tensorflow with cuda can be installed outside master directory.\n",
      "issue body -  Master Tensorflow repo is getting used in another tensorflow related projects ( for example [s4tf](https://github.com/tensorflow/swift-apis/blob/main/Documentation/Development.md) )\r\n",
      "\r\n",
      "As building in other project are done from upper directory static path for find_cuda_config.py was breaking installation this pull request fix that.\n",
      "issue labels - \n",
      "cla: yes\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  Are there any excellent github projects on action recognition implemented by TF2?\n",
      "issue body -  I have been searching for official or non-official awesome code projects of action recognition written by TF2 for a long time, but unfortunately fail. I'd like to integrate video classification with other TF2 elements more conviniently. The most majority of video-relevant resources are under the community of Pytorch, though only a few projects were done by TF1, which seem fairly passe and only attract very few stars. Since the TF 2.X version has been released for over 2 years, why does its current ecosystem yet appear to be frustrated? Does anybody else cope with similar questions?\n",
      "issue labels - \n",
      "stat:awaiting response\n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  installing Keras package with tensorflow 2.3.1\n",
      "issue body -  I am a beginner in this, I want to make OpenCV and TensorFlow project and I have some issues\r\n",
      "\r\n",
      "I downloaded python 3.7.6 with pip 19.2.3\r\n",
      "\r\n",
      "after that, I used CMD to download Tensorflow 2.3.1 and I made the path in a python project  where I am coding \r\n",
      "_C:\\Users\\Desktop\\PycharmProjects\\SudokuSolver\\venv\\Lib\\site-packages\\tensorflow>_**pip install tensorflow==2.3.1**\r\n",
      "\r\n",
      "now when I am importing the libraries:\r\n",
      "\r\n",
      "import TensorFlow\r\n",
      "from TensorFlow.Keras.models import load_model\r\n",
      "\r\n",
      "That gives me an error\r\n",
      "**\"ModuleNotFoundError: No module named 'tensorflow.keras'\"**\r\n",
      "\r\n",
      "So what should I do and where is the wrong how to fix it?\r\n",
      "Many thanks.\n",
      "issue labels - \n",
      "TF 2.3\n",
      "comp:keras\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  No matching distribution found during installation suing dockerfile\n",
      "issue body -  <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- OS Platform and Distribution : Ubuntu 18.04 (Docker)\r\n",
      "- TensorFlow installed from (source or binary): binary\r\n",
      "- TensorFlow version:1.12\r\n",
      "- Python version:2.7\r\n",
      "- Installed using virtualenv? pip? conda?: virtualenv\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "Hi, i am building this data analytics project in which i need to use tensorflow using dockerfile running on raspberry pi 4 python 2.7. I did not encounter this issue when i was running the dockerfile in amd64 architecture however in raspberry pi 4 python 2.7 it gives out error no matching distribution found for this platform. P\r\n",
      "![tensorflow2](https://user-images.githubusercontent.com/76122954/103345254-d9556180-4acb-11eb-90d7-09a6229fe3be.png)\r\n",
      "lease do help me figure this out. Thank you in advance. \r\n",
      "\r\n",
      "**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n",
      "\r\n",
      "RUN pip install --no-cache-dir pandas sklearn keras tensorflow \\\r\n",
      "    && apt-get autoremove -y \\\r\n",
      "    && apt-get clean \\\r\n",
      "    && rm -rf /var/lib/apt/lists/*\r\n",
      "\r\n",
      "**Any other info / logs**\r\n",
      "Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n",
      "\n",
      "issue labels - \n",
      "TF 1.12\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  Error when build plugin\n",
      "issue body -  When I try to compile TF with a plugin, I run the following commands:\r\n",
      "c++ -O3 -rdynamic -std=c++11 -I/usr/local/lib/python2.7/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -Wall -c plugin.cc -o plugin.o\r\n",
      "c++ -shared -fPIC -Wl,-soname,libplugin.so.1 -L/usr/local/lib/python2.7/dist-packages/tensorflow -ltensorflow_framework -o libplugin.so plugin.o\r\n",
      "\r\n",
      "However, when I try to tensorflow.load_library('libplugin.so'), error occurs: \r\n",
      "tensorflow.python.framework.errors_impl.NotFoundError: ./libplugin.so: undefined symbol: _ZTIN10tensorflow8OpKernelE\r\n",
      "\r\n",
      "I use the following command: ldd libconfig.so, but no libtensorflow_framework.so is included:\r\n",
      "\tlinux-vdso.so.1 (0x00007ffceaa5e000)\r\n",
      "\tlibstdc++.so.6 => /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007fd2cc4f2000)\r\n",
      "\tlibgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007fd2cc2da000)\r\n",
      "\tlibc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007fd2cbee9000)\r\n",
      "\tlibm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007fd2cbb4b000)\r\n",
      "\t/lib64/ld-linux-x86-64.so.2 (0x00007fd2ccaa7000)\r\n",
      "I find libtensorflow_framework.so in /usr/local/lib/python2.7/dist-packages/tensorflow.\r\n",
      "So what's wrong with the compilation process?\r\n",
      "\r\n",
      "Any help would be appreciated! Thanks\r\n",
      "\r\n",
      "The system information: ubuntu18.04, cuda10.0, cudnn7, tf1.13.2, gcc 4.8.5\n",
      "issue labels - \n",
      "TF 1.13\n",
      "stat:awaiting response\n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  Data input pipeline does not implement Batch for \"from_tensor_slices(dict(df))\"\n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "- TensorFlow installed from (source or binary): latest from source\r\n",
      "- TensorFlow version (use command below): latest from source\r\n",
      "- Python version: 3.8\r\n",
      "- Bazel version (if compiling from source): 3.6.0\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version:\r\n",
      "- GPU model and memory: 2070\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "```https://www.tensorflow.org/guide/data#consuming_csv_data```\r\n",
      "\r\n",
      "```titanic_slices = tf.data.Dataset.from_tensor_slices(dict(df))```\r\n",
      "\r\n",
      "That code will have dictionary object slice.\r\n",
      "element_spec returns {dict:XX} This throws exception if we use Batch from the same documentation,\r\n",
      "\r\n",
      "```def make_window_dataset(ds, window_size=5, shift=1, stride=1):\r\n",
      "  windows = ds.window(window_size, shift=shift, stride=stride)\r\n",
      "\r\n",
      "  def sub_to_batch(sub):\r\n",
      "    return sub.batch(window_size, drop_remainder=True)\r\n",
      "\r\n",
      "  windows = windows.flat_map(sub_to_batch)\r\n",
      "  return windows\r\n",
      "```\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "comp:data\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Input pipe line with consuming CSV data (dictionary object) does not implement Batch\n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "- TensorFlow installed from (source or binary): latest from source\r\n",
      "- TensorFlow version (use command below): latest from source\r\n",
      "- Python version: 3.8\r\n",
      "- Bazel version (if compiling from source): 3.6.0\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version:\r\n",
      "- GPU model and memory: 2070\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "```https://www.tensorflow.org/guide/data#consuming_csv_data```\r\n",
      "\r\n",
      "titanic_slices = tf.data.Dataset.from_tensor_slices(dict(df))\r\n",
      "That code will have dictionary object slice.\r\n",
      "element_spec returns {dict:XX} This throws exception if we use Batch from the same documentation,\r\n",
      "\r\n",
      "def make_window_dataset(ds, window_size=5, shift=1, stride=1):\r\n",
      "  windows = ds.window(window_size, shift=shift, stride=stride)\r\n",
      "\r\n",
      "  def sub_to_batch(sub):\r\n",
      "    return sub.batch(window_size, drop_remainder=True)\r\n",
      "\r\n",
      "  windows = windows.flat_map(sub_to_batch)\r\n",
      "  return windows\r\n",
      "\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:data\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Wrong casting (mixed precision) in attention layers\n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n",
      "- TensorFlow installed from (source or binary): Google Colab\r\n",
      "- TensorFlow version (use command below): v2.4.0-0-g582c8d236cb 2.4.0\r\n",
      "- Python version: 3\r\n",
      "- Bazel version (if compiling from source): No\r\n",
      "- GCC/Compiler version (if compiling from source): No\r\n",
      "- CUDA/cuDNN version: No\r\n",
      "- GPU model and memory: No\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "When using Attention or AdditiveAttention with mixed precision policy issue occurred due to wrong casting (mask casted to floatx but should be casted to scores.dtype)\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "Layers should work without issues with mixed_fp16\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "https://colab.research.google.com/drive/1R2MKXAIrmYBBGkij11m9aG7hLm2AEHjF?usp=sharing\r\n",
      "\r\n",
      "**Other info / logs** Include any logs or source code that would be helpful to\r\n",
      "diagnose the problem. If including tracebacks, please include the full\r\n",
      "traceback. Large logs and files should be attached.\r\n",
      "The issue comes from here https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/keras/layers/dense_attention.py#L129\r\n",
      "Should be this:\r\n",
      "```python\r\n",
      "scores -= 1.e9 * math_ops.cast(padding_mask, dtype=scores.dtype)\r\n",
      "```\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:keras\n",
      "stat:awaiting tensorflower\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Fix conv layer for partially unknown spatial shape\n",
      "issue body -  Fixes #44092. Though I think it's better to handle this in `tf.nn.conv*`. Is it possible to expose [keras.utils.conv_utils.conv_output_length](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/utils/conv_utils.py#L90) to tf core?\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:keras\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  Propagate optional argv to benchmarks_main\n",
      "issue body -  Allows the following:\r\n",
      "\r\n",
      "```python\r\n",
      "import tensorflow as tf\r\n",
      "\r\n",
      "class BasicBenchmark(tf.test.Benchmark):\r\n",
      "  def benchmark_foo(self):\r\n",
      "    print(\"Ran foo\")\r\n",
      " \r\n",
      "if __name__ == '__main__':\r\n",
      "  tf.test.main([__file__, '--benchmarks=.*'])\n",
      "issue labels - \n",
      "cla: yes\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Refactor flatbuffer_conversions for operators POW and ZEROS_LIKE\n",
      "issue body -  PR1 for issues #46058 and #46049 \n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:lite\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  micro: port op POW from Lite\n",
      "issue body -  @tensorflow/micro\r\n",
      "\r\n",
      "**System information**\r\n",
      "Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n",
      "TensorFlow installed from (source or binary): source\r\n",
      "Tensorflow version (commit SHA if source): master\r\n",
      "Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Sparkfun Edge\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "I am about to port The TF Lite kernel op POW to TF Lite Micro.\r\n",
      "\r\n",
      "**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n",
      "PR 1: refactor flatbuffer_conversions parsing function\r\n",
      "PR 2: refactor reference implementation from lite/kernels/internal/reference/reference_ops.h into its own header without making any changes.\r\n",
      "PR 3: copy the reference kernel from lite to micro without making any changes. At this point the kernel is in micro but it is not part of the build.\r\n",
      "PR 4: delete all the extra code from the lite reference kernel (but from the new location in micro. Extra code includes the integer ops, generic optimized calls, and other cleanup.\r\n",
      "PR 5: make the necessary changes to the reference kernel, reference header and test to make it work with micro. This is where we add it into the TFLM build, and make all the necessary changes to make the code ready for embedded.\r\n",
      "\n",
      "issue labels - \n",
      "comp:micro\n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  Refactor flatbuffer_conversions for operators WHERE and ZEROS_LIKE\n",
      "issue body -  PR1 (refactor flatbuffer_conversions parsing function) for issues #46048 (to port TFL kernel WHERE to TFLM) and #46049 (to port TFL kernel ZEROS_LIKE to TFLM).\n",
      "issue labels - \n",
      "cla: yes\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  micro: port op ZEROS_LIKE from lite\n",
      "issue body -  @tensorflow/micro\r\n",
      "\r\n",
      "**System information**\r\n",
      "Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n",
      "TensorFlow installed from (source or binary): source\r\n",
      "Tensorflow version (commit SHA if source): master\r\n",
      "Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Sparkfun Edge\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "I am about to port The TF Lite kernel op ZEROS_LIKE to TF Lite Micro.\r\n",
      "\r\n",
      "**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n",
      "PR 1: refactor flatbuffer_conversions parsing function\r\n",
      "PR 2: refactor reference implementation from lite/kernels/internal/reference/reference_ops.h into its own header without making any changes (? zeros_like doesn't seem to need a reference implementation).\r\n",
      "PR 3: copy the reference kernel from lite to micro without making any changes. At this point the kernel is in micro but it is not part of the build.\r\n",
      "PR 4: delete all the extra code from the lite reference kernel (but from the new location in micro. Extra code includes the integer ops, generic optimized calls, and other cleanup.\r\n",
      "PR 5: make the necessary changes to the reference kernel, reference header and test to make it work with micro. This is where we add it into the TFLM build, and make all the necessary changes to make the code ready for embedded.\n",
      "issue labels - \n",
      "comp:lite\n",
      "comp:micro\n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  micro: port op WHERE from lite\n",
      "issue body -  @tensorflow/micro\r\n",
      "\r\n",
      "**System information**\r\n",
      "Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n",
      "TensorFlow installed from (source or binary): source\r\n",
      "Tensorflow version (commit SHA if source): master\r\n",
      "Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Sparkfun Edge\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "I am about to port The TF Lite kernel op WHERE to TF Lite Micro.\r\n",
      "\r\n",
      "**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n",
      "PR 1: refactor flatbuffer_conversions parsing function\r\n",
      "PR 2: refactor reference implementation from lite/kernels/internal/reference/reference_ops.h into its own header without making any changes.\r\n",
      "PR 3: copy the reference kernel from lite to micro without making any changes. At this point the kernel is in micro but it is not part of the build.\r\n",
      "PR 4: delete all the extra code from the lite reference kernel (but from the new location in micro. Extra code includes the integer ops, generic optimized calls, and other cleanup.\r\n",
      "PR 5: make the necessary changes to the reference kernel, reference header and test to make it work with micro. This is where we add it into the TFLM build, and make all the necessary changes to make the code ready for embedded.\n",
      "issue labels - \n",
      "comp:lite\n",
      "comp:micro\n",
      "\n",
      "\n",
      "issue title -      pointclouds_pl = tf.placeholder(tf.float32, shape=(batch_size, num_point, 3)) AttributeError: module 'tensorflow' has no attribute 'placeholder'\n",
      "issue body -  **Describe the current behavior**\r\n",
      "```\r\n",
      "[3306:3298 0:1022] 01:57:24 Tue Dec 29 [mona@goku:pts/0 +1] ~/research/code/DJ-RN/pointnet\r\n",
      "$ python train.py \r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"train.py\", line 260, in <module>\r\n",
      "    train()\r\n",
      "  File \"train.py\", line 96, in train\r\n",
      "    pointclouds_pl, labels_pl = MODEL.placeholder_inputs(BATCH_SIZE, NUM_POINT)\r\n",
      "  File \"/home/mona/research/code/DJ-RN/pointnet/models/pointnet_cls.py\", line 13, in placeholder_inputs\r\n",
      "    pointclouds_pl = tf.placeholder(tf.float32, shape=(batch_size, num_point, 3))\r\n",
      "AttributeError: module 'tensorflow' has no attribute 'placeholder'\r\n",
      "```\r\n",
      "\r\n",
      "Also:\r\n",
      "\r\n",
      "```\r\n",
      "[3306:3298 0:1023] 01:57:31 Tue Dec 29 [mona@goku:pts/0 +1] ~/research/code/DJ-RN/pointnet\r\n",
      "$ python\r\n",
      "Python 3.8.5 (default, Sep  4 2020, 07:30:14) \r\n",
      "[GCC 7.3.0] :: Anaconda, Inc. on linux\r\n",
      "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n",
      ">>> import tensorflow as tf\r\n",
      ">>> tf.__version__\r\n",
      "'2.2.0'\r\n",
      ">>> quit()\r\n",
      "12149/31772MB\r\n",
      "[3306:3298 0:1024] 01:59:05 Tue Dec 29 [mona@goku:pts/0 +1] ~/research/code/DJ-RN/pointnet\r\n",
      "$ nvcc --version\r\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\r\n",
      "Copyright (c) 2005-2019 NVIDIA Corporation\r\n",
      "Built on Sun_Jul_28_19:07:16_PDT_2019\r\n",
      "Cuda compilation tools, release 10.1, V10.1.243\r\n",
      "12149/31772MB\r\n",
      "\r\n",
      "$ lsb_release -a\r\n",
      "LSB Version:\tcore-11.1.0ubuntu2-noarch:security-11.1.0ubuntu2-noarch\r\n",
      "Distributor ID:\tUbuntu\r\n",
      "Description:\tUbuntu 20.04.1 LTS\r\n",
      "Release:\t20.04\r\n",
      "Codename:\tfocal\r\n",
      "\r\n",
      "```\r\n",
      "\r\n",
      "\r\n",
      "Please also check https://github.com/charlesq34/pointnet/issues/265 for the code repo\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "- TensorFlow installed from (source or binary): installed using\r\n",
      "`conda install tensorflow-gpu cudatoolkit=10.1`\r\n",
      "\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version:\r\n",
      "```\r\n",
      "$ python \r\n",
      "Python 3.8.5 (default, Sep  4 2020, 07:30:14) \r\n",
      "[GCC 7.3.0] :: Anaconda, Inc. on linux\r\n",
      "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n",
      ">>> from tensorflow.python.platform import build_info as tf_build_info\r\n",
      ">>> tf_build_info.cudnn_version_number\r\n",
      "'7.6'\r\n",
      "```\r\n",
      "\r\n",
      "- GPU model and memory: `NVIDIA GeForce 1650 Ti :  4G`\r\n",
      "\r\n",
      "You can collect some of this information using our environment capture\r\n",
      "[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n",
      "You can also obtain the TensorFlow version with:\r\n",
      "TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n",
      "```\r\n",
      "$ python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n",
      "\r\n",
      "unknown 2.2.0\r\n",
      "```\r\n",
      "\r\n",
      "\r\n",
      "--------------------------------------------------\r\n",
      "\r\n",
      "$ ./tf_env_collect.sh \r\n",
      "Collecting system information...\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/tmp/check_os.py\", line 18, in <module>\r\n",
      "    platform.linux_distribution(),\r\n",
      "AttributeError: module 'platform' has no attribute 'linux_distribution'\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/tmp/check_tf.py\", line 5, in <module>\r\n",
      "    with tf.Session() as sess:\r\n",
      "AttributeError: module 'tensorflow' has no attribute 'Session'\r\n",
      "./tf_env_collect.sh: line 168: bazel: command not found\r\n",
      "Wrote environment to tf_env.txt. You can review the contents of that file.\r\n",
      "and use it to populate the fields in the github issue template.\r\n",
      "\r\n",
      "cat tf_env.txt\r\n",
      "\r\n",
      "12515/31772MB\r\n",
      "[8547:3298 0:1043] 02:08:05 Tue Dec 29 [mona@goku:pts/1 +1] ~/Downloads\r\n",
      "$ cat tf_env.txt\r\n",
      "\r\n",
      "== check python ===================================================\r\n",
      "python version: 3.8.5\r\n",
      "python branch: \r\n",
      "python build version: ('default', 'Sep  4 2020 07:30:14')\r\n",
      "python compiler version: GCC 7.3.0\r\n",
      "python implementation: CPython\r\n",
      "\r\n",
      "\r\n",
      "== check os platform ===============================================\r\n",
      "\r\n",
      "== are we in docker =============================================\r\n",
      "No\r\n",
      "\r\n",
      "== compiler =====================================================\r\n",
      "c++ (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0\r\n",
      "Copyright (C) 2019 Free Software Foundation, Inc.\r\n",
      "This is free software; see the source for copying conditions.  There is NO\r\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n",
      "\r\n",
      "\r\n",
      "== check pips ===================================================\r\n",
      "numpy                              1.19.2\r\n",
      "numpydoc                           1.1.0\r\n",
      "protobuf                           3.14.0\r\n",
      "tensorflow                         2.2.0\r\n",
      "tensorflow-estimator               2.4.0\r\n",
      "\r\n",
      "== check for virtualenv =========================================\r\n",
      "False\r\n",
      "\r\n",
      "== tensorflow import ============================================\r\n",
      "tf.version.VERSION = 2.2.0\r\n",
      "tf.version.GIT_VERSION = unknown\r\n",
      "tf.version.COMPILER_VERSION = 5.4.0\r\n",
      "      9014:\tfind library=libpthread.so.0 [0]; searching\r\n",
      "      9014:\t search path=/home/mona/anaconda3/bin/../lib/tls/haswell/x86_64:/home/mona/anaconda3/bin/../lib/tls/haswell:/home/mona/anaconda3/bin/../lib/tls/x86_64:/home/mona/anaconda3/bin/../lib/tls:/home/mona/anaconda3/bin/../lib/haswell/x86_64:/home/mona/anaconda3/bin/../lib/haswell:/home/mona/anaconda3/bin/../lib/x86_64:/home/mona/anaconda3/bin/../lib(RPATH from file /home/mona/anaconda3/bin/python)\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/bin/../lib/tls/haswell/x86_64/libpthread.so.0\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/bin/../lib/tls/haswell/libpthread.so.0\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/bin/../lib/tls/x86_64/libpthread.so.0\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/bin/../lib/tls/libpthread.so.0\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/bin/../lib/haswell/x86_64/libpthread.so.0\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/bin/../lib/haswell/libpthread.so.0\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/bin/../lib/x86_64/libpthread.so.0\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/bin/../lib/libpthread.so.0\r\n",
      "      9014:\t search cache=/etc/ld.so.cache\r\n",
      "      9014:\t  trying file=/lib/x86_64-linux-gnu/libpthread.so.0\r\n",
      "      9014:\t\r\n",
      "      9014:\tfind library=libc.so.6 [0]; searching\r\n",
      "      9014:\t search path=/home/mona/anaconda3/bin/../lib\t\t(RPATH from file /home/mona/anaconda3/bin/python)\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/bin/../lib/libc.so.6\r\n",
      "      9014:\t search cache=/etc/ld.so.cache\r\n",
      "      9014:\t  trying file=/lib/x86_64-linux-gnu/libc.so.6\r\n",
      "      9014:\t\r\n",
      "      9014:\tfind library=libdl.so.2 [0]; searching\r\n",
      "      9014:\t search path=/home/mona/anaconda3/bin/../lib\t\t(RPATH from file /home/mona/anaconda3/bin/python)\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/bin/../lib/libdl.so.2\r\n",
      "      9014:\t search cache=/etc/ld.so.cache\r\n",
      "      9014:\t  trying file=/lib/x86_64-linux-gnu/libdl.so.2\r\n",
      "      9014:\t\r\n",
      "      9014:\tfind library=libutil.so.1 [0]; searching\r\n",
      "      9014:\t search path=/home/mona/anaconda3/bin/../lib\t\t(RPATH from file /home/mona/anaconda3/bin/python)\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/bin/../lib/libutil.so.1\r\n",
      "      9014:\t search cache=/etc/ld.so.cache\r\n",
      "      9014:\t  trying file=/lib/x86_64-linux-gnu/libutil.so.1\r\n",
      "      9014:\t\r\n",
      "      9014:\tfind library=librt.so.1 [0]; searching\r\n",
      "      9014:\t search path=/home/mona/anaconda3/bin/../lib\t\t(RPATH from file /home/mona/anaconda3/bin/python)\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/bin/../lib/librt.so.1\r\n",
      "      9014:\t search cache=/etc/ld.so.cache\r\n",
      "      9014:\t  trying file=/lib/x86_64-linux-gnu/librt.so.1\r\n",
      "      9014:\t\r\n",
      "      9014:\tfind library=libm.so.6 [0]; searching\r\n",
      "      9014:\t search path=/home/mona/anaconda3/bin/../lib\t\t(RPATH from file /home/mona/anaconda3/bin/python)\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/bin/../lib/libm.so.6\r\n",
      "      9014:\t search cache=/etc/ld.so.cache\r\n",
      "      9014:\t  trying file=/lib/x86_64-linux-gnu/libm.so.6\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /lib/x86_64-linux-gnu/libpthread.so.0\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /lib/x86_64-linux-gnu/libc.so.6\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /lib/x86_64-linux-gnu/libm.so.6\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /lib/x86_64-linux-gnu/librt.so.1\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /lib/x86_64-linux-gnu/libutil.so.1\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /lib/x86_64-linux-gnu/libdl.so.2\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tinitialize program: /home/mona/anaconda3/bin/python\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\ttransferring control: /home/mona/anaconda3/bin/python\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/_heapq.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/_opcode.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/_struct.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\tfind library=libffi.so.7 [0]; searching\r\n",
      "      9014:\t search path=/home/mona/anaconda3/lib/python3.8/lib-dynload/../../tls/haswell/x86_64:/home/mona/anaconda3/lib/python3.8/lib-dynload/../../tls/haswell:/home/mona/anaconda3/lib/python3.8/lib-dynload/../../tls/x86_64:/home/mona/anaconda3/lib/python3.8/lib-dynload/../../tls:/home/mona/anaconda3/lib/python3.8/lib-dynload/../../haswell/x86_64:/home/mona/anaconda3/lib/python3.8/lib-dynload/../../haswell:/home/mona/anaconda3/lib/python3.8/lib-dynload/../../x86_64:/home/mona/anaconda3/lib/python3.8/lib-dynload/../..\t\t(RPATH from file /home/mona/anaconda3/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so)\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/lib-dynload/../../tls/haswell/x86_64/libffi.so.7\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/lib-dynload/../../tls/haswell/libffi.so.7\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/lib-dynload/../../tls/x86_64/libffi.so.7\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/lib-dynload/../../tls/libffi.so.7\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/lib-dynload/../../haswell/x86_64/libffi.so.7\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/lib-dynload/../../haswell/libffi.so.7\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/lib-dynload/../../x86_64/libffi.so.7\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/lib-dynload/../../libffi.so.7\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/../../libffi.so.7\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\tfind library=libmkl_rt.so [0]; searching\r\n",
      "      9014:\t search path=/home/mona/anaconda3/lib/python3.8/site-packages/mkl/../../../tls/haswell/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/mkl/../../../tls/haswell:/home/mona/anaconda3/lib/python3.8/site-packages/mkl/../../../tls/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/mkl/../../../tls:/home/mona/anaconda3/lib/python3.8/site-packages/mkl/../../../haswell/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/mkl/../../../haswell:/home/mona/anaconda3/lib/python3.8/site-packages/mkl/../../../x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/mkl/../../..(RPATH from file /home/mona/anaconda3/lib/python3.8/site-packages/mkl/_mklinit.cpython-38-x86_64-linux-gnu.so)\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/mkl/../../../tls/haswell/x86_64/libmkl_rt.so\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/mkl/../../../tls/haswell/libmkl_rt.so\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/mkl/../../../tls/x86_64/libmkl_rt.so\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/mkl/../../../tls/libmkl_rt.so\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/mkl/../../../haswell/x86_64/libmkl_rt.so\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/mkl/../../../haswell/libmkl_rt.so\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/mkl/../../../x86_64/libmkl_rt.so\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/mkl/../../../libmkl_rt.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/mkl/../../../libmkl_rt.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/mkl/_mklinit.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t/home/mona/anaconda3/lib/python3.8/site-packages/mkl/_mklinit.cpython-38-x86_64-linux-gnu.so: error: symbol lookup error: undefined symbol: omp_get_num_threads (fatal)\r\n",
      "      9014:\tfind library=libiomp5.so [0]; searching\r\n",
      "      9014:\t search path=/home/mona/anaconda3/lib/python3.8/site-packages/mkl/../../..\t\t(RPATH from file /home/mona/anaconda3/lib/python3.8/site-packages/mkl/_mklinit.cpython-38-x86_64-linux-gnu.so)\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/mkl/../../../libiomp5.so\r\n",
      "      9014:\t\r\n",
      "      9014:\tfind library=libgcc_s.so.1 [0]; searching\r\n",
      "      9014:\t search path=/home/mona/anaconda3/bin/../lib\t\t(RPATH from file /home/mona/anaconda3/bin/python)\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/bin/../lib/libgcc_s.so.1\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/bin/../lib/libgcc_s.so.1\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/mkl/../../../libiomp5.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/mkl/_py_mkl_service.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/math.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/_datetime.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/_pickle.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/numpy/core/_multiarray_tests.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/numpy/linalg/lapack_lite.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/numpy/linalg/_umath_linalg.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\tfind library=libz.so.1 [0]; searching\r\n",
      "      9014:\t search path=/home/mona/anaconda3/lib/python3.8/lib-dynload/../..\t\t(RPATH from file /home/mona/anaconda3/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so)\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/lib-dynload/../../libz.so.1\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/../../libz.so.1\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/zlib.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/_bz2.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\tfind library=liblzma.so.5 [0]; searching\r\n",
      "      9014:\t search path=/home/mona/anaconda3/lib/python3.8/lib-dynload/../..\t\t(RPATH from file /home/mona/anaconda3/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so)\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/lib-dynload/../../liblzma.so.5\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/../../liblzma.so.5\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/_lzma.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/grp.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/_decimal.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/numpy/fft/_pocketfft_internal.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/numpy/random/mtrand.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/numpy/random/bit_generator.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/numpy/random/_common.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/binascii.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\tfind library=libcrypto.so.1.1 [0]; searching\r\n",
      "      9014:\t search path=/home/mona/anaconda3/lib/python3.8/lib-dynload/../..\t\t(RPATH from file /home/mona/anaconda3/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so)\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/lib-dynload/../../libcrypto.so.1.1\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/../../libcrypto.so.1.1\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/_hashlib.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/_blake2.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/_sha3.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/_bisect.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/_sha512.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/_random.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/numpy/random/_bounded_integers.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/numpy/random/_mt19937.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/numpy/random/_philox.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/numpy/random/_pcg64.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/numpy/random/_sfc64.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/numpy/random/_generator.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/mkl/../../../libmkl_core.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/mkl/../../../libmkl_intel_thread.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/mkl/../../../libmkl_intel_lp64.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/mkl/../../../libmkl_avx2.so\r\n",
      "      9014:\t\r\n",
      "      9014:\tfind library=libtensorflow_framework.so.2 [0]; searching\r\n",
      "      9014:\t search path=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/tls/haswell/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/tls/haswell:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/tls/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/tls:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/haswell/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/haswell:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/tls/haswell/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/tls/haswell:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/tls/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/tls:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/haswell/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/haswell:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../tls/haswell/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../tls/haswell:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../tls/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../tls:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../haswell/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../haswell:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/..:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../../../tls/haswell/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../../../tls/haswell:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../../../tls/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../../../tls:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../../../haswell/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../../../haswell:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../../../x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../../..\t\t(RPATH from file /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/tls/haswell/x86_64/libtensorflow_framework.so.2\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/tls/haswell/libtensorflow_framework.so.2\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/tls/x86_64/libtensorflow_framework.so.2\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/tls/libtensorflow_framework.so.2\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/haswell/x86_64/libtensorflow_framework.so.2\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/haswell/libtensorflow_framework.so.2\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/x86_64/libtensorflow_framework.so.2\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/libtensorflow_framework.so.2\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/tls/haswell/x86_64/libtensorflow_framework.so.2\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/tls/haswell/libtensorflow_framework.so.2\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/tls/x86_64/libtensorflow_framework.so.2\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/tls/libtensorflow_framework.so.2\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/haswell/x86_64/libtensorflow_framework.so.2\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/haswell/libtensorflow_framework.so.2\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/x86_64/libtensorflow_framework.so.2\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/libtensorflow_framework.so.2\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../tls/haswell/x86_64/libtensorflow_framework.so.2\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../tls/haswell/libtensorflow_framework.so.2\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../tls/x86_64/libtensorflow_framework.so.2\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../tls/libtensorflow_framework.so.2\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../haswell/x86_64/libtensorflow_framework.so.2\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../haswell/libtensorflow_framework.so.2\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../x86_64/libtensorflow_framework.so.2\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../libtensorflow_framework.so.2\r\n",
      "      9014:\t\r\n",
      "      9014:\tfind library=libstdc++.so.6 [0]; searching\r\n",
      "      9014:\t search path=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/..:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../../../tls/haswell/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../../../tls/haswell:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../../../tls/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../../../tls:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../../../haswell/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../../../haswell:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../../../x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../../..\t\t(RPATH from file /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/libstdc++.so.6\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../libstdc++.so.6\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../../../tls/haswell/x86_64/libstdc++.so.6\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../../../tls/haswell/libstdc++.so.6\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../../../tls/x86_64/libstdc++.so.6\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../../../tls/libstdc++.so.6\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../../../haswell/x86_64/libstdc++.so.6\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../../../haswell/libstdc++.so.6\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../../../x86_64/libstdc++.so.6\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../../../libstdc++.so.6\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../../../libstdc++.so.6\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../libtensorflow_framework.so.2\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/pyexpat.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/_socket.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/select.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tfind library=libprotobuf.so.24 [0]; searching\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_tf_stack.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_tfe.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_tf_session.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/termios.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/_csv.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/fcntl.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_py_exception_registry.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_utils.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/wrapt/_wrappers.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_bfloat16.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_dtypes.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/fast_tensor_util.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_op_def_registry.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_python_op_gen.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/_posixsubprocess.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_file_io.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_py_func.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/_queue.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_events_writer.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/profiler/internal/_pywrap_profiler.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_record_io.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_checkpoint_reader.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/_json.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\tfind library=libssl.so.1.1 [0]; searching\r\n",
      "      9014:\t search path=/home/mona/anaconda3/lib/python3.8/lib-dynload/../..\t\t(RPATH from file /home/mona/anaconda3/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so)\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/lib-dynload/../../libssl.so.1.1\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/../../libssl.so.1.1\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/_ssl.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/_contextvars.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/_asyncio.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_device_lib.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_tf_optimizer.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_tf_cluster.so\r\n",
      "      9014:\t\r\n",
      "      9014:\tfind library=libhdf5.so.103 [0]; searching\r\n",
      "      9014:\t search path=/home/mona/anaconda3/lib/python3.8/site-packages/h5py/../../../tls/haswell/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/h5py/../../../tls/haswell:/home/mona/anaconda3/lib/python3.8/site-packages/h5py/../../../tls/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/h5py/../../../tls:/home/mona/anaconda3/lib/python3.8/site-packages/h5py/../../../haswell/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/h5py/../../../haswell:/home/mona/anaconda3/lib/python3.8/site-packages/h5py/../../../x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/h5py/../../..\t\t(RPATH from file /home/mona/anaconda3/lib/python3.8/site-packages/h5py/_errors.cpython-38-x86_64-linux-gnu.so)\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/h5py/../../../tls/haswell/x86_64/libhdf5.so.103\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/h5py/../../../tls/haswell/libhdf5.so.103\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/h5py/../../../tls/x86_64/libhdf5.so.103\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/h5py/../../../tls/libhdf5.so.103\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/h5py/../../../haswell/x86_64/libhdf5.so.103\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/h5py/../../../haswell/libhdf5.so.103\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/h5py/../../../x86_64/libhdf5.so.103\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/h5py/../../../libhdf5.so.103\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/../../../libhdf5.so.103\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/_errors.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\tfind library=libhdf5_hl.so.100 [0]; searching\r\n",
      "      9014:\t search path=/home/mona/anaconda3/lib/python3.8/site-packages/h5py/../../..\t\t(RPATH from file /home/mona/anaconda3/lib/python3.8/site-packages/h5py/_errors.cpython-38-x86_64-linux-gnu.so)\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/h5py/../../../libhdf5_hl.so.100\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/../../../libhdf5_hl.so.100\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/defs.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/_objects.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/_conv.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5r.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5t.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/utils.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5z.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5a.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5s.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5p.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5ac.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/_proxy.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5d.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/array.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5ds.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5f.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5g.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5i.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5fd.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5pl.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5o.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5l.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/brotli/_brotli.abi3.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/_cffi_backend.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/unicodedata.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/_multiprocessing.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\tfind library=libyaml-0.so.2 [0]; searching\r\n",
      "      9014:\t search path=/home/mona/anaconda3/lib/python3.8/site-packages/../../tls/haswell/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/../../tls/haswell:/home/mona/anaconda3/lib/python3.8/site-packages/../../tls/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/../../tls:/home/mona/anaconda3/lib/python3.8/site-packages/../../haswell/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/../../haswell:/home/mona/anaconda3/lib/python3.8/site-packages/../../x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/../..\t\t(RPATH from file /home/mona/anaconda3/lib/python3.8/site-packages/_yaml.cpython-38-x86_64-linux-gnu.so)\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/../../tls/haswell/x86_64/libyaml-0.so.2\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/../../tls/haswell/libyaml-0.so.2\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/../../tls/x86_64/libyaml-0.so.2\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/../../tls/libyaml-0.so.2\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/../../haswell/x86_64/libyaml-0.so.2\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/../../haswell/libyaml-0.so.2\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/../../x86_64/libyaml-0.so.2\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/../../libyaml-0.so.2\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/../../libyaml-0.so.2\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/_yaml.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/_lib/_ccallback_c.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/_lib/_uarray/_uarray.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/fft/_pocketfft/pypocketfft.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/sparse/_sparsetools.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/sparse/_csparsetools.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/sparse/csgraph/_shortest_path.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/sparse/csgraph/_tools.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/sparse/csgraph/_traversal.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/sparse/csgraph/_min_spanning_tree.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/sparse/csgraph/_flow.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/sparse/csgraph/_matching.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/sparse/csgraph/_reordering.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/interval.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/hashtable.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/missing.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/dtypes.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/conversion.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/base.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/nattype.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/np_datetime.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/timezones.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/tzconversion.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/ccalendar.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/parsing.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/offsets.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/timedeltas.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/timestamps.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/fields.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/strptime.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/properties.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/period.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/vectorized.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/ops_dispatch.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/algos.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/lib.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslib.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/hashing.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/ops.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/bottleneck/reduce.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/bottleneck/nonreduce.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/bottleneck/nonreduce_axis.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/bottleneck/move.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/index.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/join.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/sparse.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/reshape.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/indexing.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/writers.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/internals.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/mmap.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/window/aggregations.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/window/indexers.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/groupby.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/reduction.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/parsers.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/json.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/testing.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/profiler/internal/_pywrap_traceme.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_tfprof.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_quantize_training.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_stacktrace_handler.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_util_port.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_debug_events_writer.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_mlir.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/lite/experimental/microfrontend/python/ops/_audio_microfrontend_op.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_toco_api.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/_elementtree.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\tfind library=libjpeg.so.9 [0]; searching\r\n",
      "      9014:\t search path=/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../../tls/haswell/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../../tls/haswell:/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../../tls/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../../tls:/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../../haswell/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../../haswell:/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../../x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../..(RPATH from file /home/mona/anaconda3/lib/python3.8/site-packages/PIL/_imaging.cpython-38-x86_64-linux-gnu.so)\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../../tls/haswell/x86_64/libjpeg.so.9\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../../tls/haswell/libjpeg.so.9\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../../tls/x86_64/libjpeg.so.9\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../../tls/libjpeg.so.9\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../../haswell/x86_64/libjpeg.so.9\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../../haswell/libjpeg.so.9\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../../x86_64/libjpeg.so.9\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../../libjpeg.so.9\r\n",
      "      9014:\t\r\n",
      "      9014:\tfind library=libtiff.so.5 [0]; searching\r\n",
      "      9014:\t search path=/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../..\t\t(RPATH from file /home/mona/anaconda3/lib/python3.8/site-packages/PIL/_imaging.cpython-38-x86_64-linux-gnu.so)\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../../libtiff.so.5\r\n",
      "      9014:\t\r\n",
      "      9014:\tfind library=libzstd.so.1 [0]; searching\r\n",
      "      9014:\t search path=/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../.././tls/haswell/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../.././tls/haswell:/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../.././tls/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../.././tls:/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../.././haswell/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../.././haswell:/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../.././x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../../.\t\t(RPATH from file /home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../../libtiff.so.5)\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../.././tls/haswell/x86_64/libzstd.so.1\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../.././tls/haswell/libzstd.so.1\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../.././tls/x86_64/libzstd.so.1\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../.././tls/libzstd.so.1\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../.././haswell/x86_64/libzstd.so.1\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../.././haswell/libzstd.so.1\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../.././x86_64/libzstd.so.1\r\n",
      "      9014:\t  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../.././libzstd.so.1\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../.././libzstd.so.1\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../../libjpeg.so.9\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../../libtiff.so.5\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/PIL/_imaging.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/ndimage/_nd_image.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/ndimage/_ni_label.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/linalg/_fblas.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/linalg/_flapack.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/linalg/_flinalg.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/linalg/_solve_toeplitz.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/linalg/_decomp_update.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/linalg/cython_blas.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling init: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/linalg/cython_lapack.cpython-38-x86_64-linux-gnu.so\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/bin/python [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /lib/x86_64-linux-gnu/libutil.so.1 [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/_heapq.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/_opcode.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/_struct.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/mkl/_mklinit.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/mkl/../../../libiomp5.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/mkl/_py_mkl_service.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/math.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/_datetime.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/_pickle.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/numpy/core/_multiarray_tests.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/numpy/linalg/lapack_lite.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/numpy/linalg/_umath_linalg.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/zlib.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/_bz2.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/_lzma.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/grp.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/_decimal.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/numpy/fft/_pocketfft_internal.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/numpy/random/mtrand.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/numpy/random/bit_generator.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/numpy/random/_common.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/binascii.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/_hashlib.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/_blake2.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/_sha3.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/_bisect.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/_sha512.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/_random.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/numpy/random/_bounded_integers.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/numpy/random/_mt19937.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/numpy/random/_philox.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/numpy/random/_pcg64.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/numpy/random/_sfc64.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/numpy/random/_generator.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/mkl/../../../libmkl_avx2.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/mkl/../../../libmkl_intel_lp64.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/mkl/../../../libmkl_intel_thread.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/mkl/../../../libmkl_core.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/pyexpat.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/_socket.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/select.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_tf_stack.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_tfe.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_tf_session.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/termios.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/_csv.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/fcntl.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_py_exception_registry.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_utils.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/wrapt/_wrappers.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_bfloat16.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_dtypes.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/fast_tensor_util.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_op_def_registry.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_python_op_gen.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/_posixsubprocess.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_file_io.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_py_func.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/_queue.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_events_writer.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/profiler/internal/_pywrap_profiler.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_record_io.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_checkpoint_reader.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/_json.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/_ssl.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/../../libssl.so.1.1 [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/../../libcrypto.so.1.1 [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/_contextvars.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/_asyncio.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_device_lib.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_tf_optimizer.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_tf_cluster.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/_errors.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/defs.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/../../../libhdf5_hl.so.100 [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/_objects.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/_conv.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5r.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5t.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/utils.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5z.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5a.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5s.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5p.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5ac.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/_proxy.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5d.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/array.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5ds.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5f.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5g.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5i.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5fd.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/../../../libhdf5.so.103 [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5pl.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5o.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5l.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/brotli/_brotli.abi3.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/_cffi_backend.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/../../libffi.so.7 [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/unicodedata.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/_multiprocessing.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/_yaml.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/../../libyaml-0.so.2 [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/_lib/_ccallback_c.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/_lib/_uarray/_uarray.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/fft/_pocketfft/pypocketfft.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/sparse/_sparsetools.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/sparse/_csparsetools.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/sparse/csgraph/_shortest_path.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/sparse/csgraph/_tools.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/sparse/csgraph/_traversal.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/sparse/csgraph/_min_spanning_tree.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/sparse/csgraph/_flow.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/sparse/csgraph/_matching.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/sparse/csgraph/_reordering.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/interval.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/hashtable.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/missing.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/dtypes.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/conversion.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/base.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/nattype.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/np_datetime.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/timezones.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/tzconversion.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/ccalendar.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/parsing.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/offsets.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/timedeltas.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/timestamps.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/fields.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/strptime.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/properties.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/period.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/vectorized.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/ops_dispatch.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/algos.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/lib.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslib.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/hashing.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/ops.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/bottleneck/reduce.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/bottleneck/nonreduce.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/bottleneck/nonreduce_axis.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/bottleneck/move.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/index.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/join.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/sparse.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/reshape.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/indexing.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/writers.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/internals.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/mmap.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/window/aggregations.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/window/indexers.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/groupby.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/reduction.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/parsers.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/json.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/testing.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/profiler/internal/_pywrap_traceme.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_tfprof.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_quantize_training.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_stacktrace_handler.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_util_port.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_debug_events_writer.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_mlir.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/lite/experimental/microfrontend/python/ops/_audio_microfrontend_op.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_toco_api.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../libtensorflow_framework.so.2 [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../../../libstdc++.so.6 [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/bin/../lib/libgcc_s.so.1 [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/_elementtree.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/PIL/_imaging.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../../libtiff.so.5 [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../../libjpeg.so.9 [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/../../liblzma.so.5 [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /lib/x86_64-linux-gnu/librt.so.1 [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/../../libz.so.1 [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /lib/x86_64-linux-gnu/libm.so.6 [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../.././libzstd.so.1 [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/ndimage/_nd_image.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/ndimage/_ni_label.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/linalg/_fblas.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/linalg/_flapack.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/linalg/_flinalg.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/linalg/_solve_toeplitz.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/linalg/_decomp_update.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /lib/x86_64-linux-gnu/libpthread.so.0 [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/linalg/cython_blas.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/linalg/cython_lapack.cpython-38-x86_64-linux-gnu.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /home/mona/anaconda3/lib/python3.8/site-packages/mkl/../../../libmkl_rt.so [0]\r\n",
      "      9014:\t\r\n",
      "      9014:\t\r\n",
      "      9014:\tcalling fini: /lib/x86_64-linux-gnu/libdl.so.2 [0]\r\n",
      "      9014:\t\r\n",
      "\r\n",
      "== env ==========================================================\r\n",
      "LD_LIBRARY_PATH is unset\r\n",
      "DYLD_LIBRARY_PATH is unset\r\n",
      "\r\n",
      "== nvidia-smi ===================================================\r\n",
      "Tue Dec 29 14:08:04 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 455.38       Driver Version: 455.38       CUDA Version: 11.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce GTX 165...  Off  | 00000000:01:00.0 Off |                  N/A |\r\n",
      "| N/A   43C    P8     4W /  N/A |    690MiB /  3911MiB |      6%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A       981      G   /usr/lib/xorg/Xorg                133MiB |\r\n",
      "|    0   N/A  N/A      1735      G   /usr/lib/xorg/Xorg                400MiB |\r\n",
      "|    0   N/A  N/A      1906      G   /usr/bin/gnome-shell              122MiB |\r\n",
      "|    0   N/A  N/A      2264      G   ...gAAAAAAAAA --shared-files        9MiB |\r\n",
      "|    0   N/A  N/A      3148      G   /usr/lib/firefox/firefox            1MiB |\r\n",
      "|    0   N/A  N/A      3724      G   /usr/lib/firefox/firefox            1MiB |\r\n",
      "|    0   N/A  N/A      4056      G   /usr/lib/firefox/firefox            1MiB |\r\n",
      "|    0   N/A  N/A      4209      G   /usr/lib/firefox/firefox            1MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "\r\n",
      "== cuda libs  ===================================================\r\n",
      "\r\n",
      "== tensorflow installed from info ==================\r\n",
      "Name: tensorflow\r\n",
      "Version: 2.2.0\r\n",
      "Summary: TensorFlow is an open source machine learning framework for everyone.\r\n",
      "Home-page: https://www.tensorflow.org/\r\n",
      "Author-email: packages@tensorflow.org\r\n",
      "License: Apache 2.0\r\n",
      "Location: /home/mona/anaconda3/lib/python3.8/site-packages\r\n",
      "Required-by: \r\n",
      "\r\n",
      "== python version  ==============================================\r\n",
      "(major, minor, micro, releaselevel, serial)\r\n",
      "(3, 8, 5, 'final', 0)\r\n",
      "\r\n",
      "== bazel version  ===============================================\r\n",
      "12476/31772MB\r\n",
      "[8547:3298 0:1044] 02:08:12 Tue Dec 29 [mona@goku:pts/1 +1] ~/Downloads\r\n",
      "$ \r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.2\n",
      "comp:ops\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Pylint upgrade 2.6.2\n",
      "issue body -  Re-submit https://github.com/tensorflow/tensorflow/pull/43040\r\n",
      "\r\n",
      "Fixes https://github.com/tensorflow/tensorflow/issues/43038\r\n",
      "\r\n",
      "/cc @mihaimaruseac Please discuss what we want as in  https://github.com/tensorflow/tensorflow/issues/43038#issuecomment-752186887\n",
      "issue labels - \n",
      "cla: yes\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  [r2.4 cherry-pick]: fix `modular_filesystem` call itself\n",
      "issue body -  **NOTE:** This PR is for r2.4 branch\r\n",
      "\r\n",
      "This PR is a cherry-pick of PR #45682\n",
      "issue labels - \n",
      "cla: yes\n",
      "size:S\n",
      "waiting for patch release\n",
      "\n",
      "\n",
      "issue title -  RTX 3060 Ti is approximately x1.5 slower compared to RTX 2080 Super\n",
      "issue body -  **System information**\r\n",
      "- Tested using a simple script and sample code from Tensorflow Object Detection API:\r\n",
      "- Windows 10 Pro\r\n",
      "- Installed using pip in Anaconda\r\n",
      "- v2.4.0-rc4-71-g582c8d236cb 2.4.0 (3060 Ti)/v2.2.0-rc4-8-g2b96f3662b 2.2.0 (2080 Super)\r\n",
      "- Python version: 3.6\r\n",
      "- Bazel version (if compiling from source): -\r\n",
      "- GCC/Compiler version (if compiling from source): -\r\n",
      "- CUDA 11.0 cuDNN 8.04 (3060 Ti)/ CUDA 10.1 cuDNN 7.6.5 (2080 Super)\r\n",
      "- GeForce RTX 3060 Ti 8192 MB/GeForce RTX 2080 Super 8192 MB\r\n",
      "\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "Tested the performance of my 3060 Ti by training a simple model and running it with Tensorflow Object Detection tutorial\r\n",
      "Training 10 iteration of simple model: 30 seconds (3060 Ti)/ 18 seconds (2080 Super)\r\n",
      "Inference speed of the 2 sample image provided: 7.15 s (3060Ti)/5.54 s (2080 Super))\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "Performance of +-15% in terms of speed\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "Custom script:\r\n",
      "```\r\n",
      "import tensorflow as tf\r\n",
      "import time\r\n",
      "\r\n",
      "if tf.test.gpu_device_name():\r\n",
      "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\r\n",
      "else:\r\n",
      "    print(\"Please install GPU version of TF\")\r\n",
      "\r\n",
      "from tensorflow.python.client import device_lib\r\n",
      "print(device_lib.list_local_devices())\r\n",
      "\r\n",
      "mnist = tf.keras.datasets.mnist\r\n",
      "\r\n",
      "(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\n",
      "x_train, x_test = x_train / 255.0, x_test / 255.0\r\n",
      "\r\n",
      "start = time.time()\r\n",
      "\r\n",
      "model = tf.keras.models.Sequential([\r\n",
      "  tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n",
      "  tf.keras.layers.Dense(128, activation='relu'),\r\n",
      "  tf.keras.layers.Dropout(0.2),\r\n",
      "  tf.keras.layers.Dense(10)\r\n",
      "])\r\n",
      "\r\n",
      "end = time.time()\r\n",
      "print(\"Time delta: \", end - start)\r\n",
      "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\n",
      "model.compile(optimizer='adam',\r\n",
      "              loss=loss_fn,\r\n",
      "              metrics=['accuracy'])\r\n",
      "\r\n",
      "start = time.time()\r\n",
      "model.fit(x_train, y_train, epochs=10)\r\n",
      "end = time.time()\r\n",
      "print(\"Time delta: \", end - start)\r\n",
      "```\r\n",
      "\r\n",
      "Tensorflow Object Detection API:\r\n",
      "https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/object_detection_tutorial.ipynb\r\n",
      "\r\n",
      "**Attempts done**\r\n",
      "Initially had the problem indicated in this issue https://github.com/tensorflow/tensorflow/issues/45170 and followed the suggestions but no improvement to the speed.\r\n",
      "\r\n",
      "**Other info / logs**\r\n",
      "**logs from terminal when running on object detection api (3060 Ti):**\r\n",
      "```\r\n",
      "2020-12-30 00:23:16.972492: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n",
      "2020-12-30 00:23:18.909521: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\r\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n",
      "2020-12-30 00:23:18.912363: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n",
      "2020-12-30 00:23:18.932425: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:\r\n",
      "pciBusID: 0000:01:00.0 name: GeForce RTX 3060 Ti computeCapability: 8.6\r\n",
      "coreClock: 1.665GHz coreCount: 38 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n",
      "2020-12-30 00:23:18.932532: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n",
      "2020-12-30 00:23:18.944202: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n",
      "2020-12-30 00:23:18.944282: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n",
      "2020-12-30 00:23:18.947437: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n",
      "2020-12-30 00:23:18.948443: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n",
      "2020-12-30 00:23:18.955239: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n",
      "2020-12-30 00:23:18.957428: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n",
      "2020-12-30 00:23:18.958003: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n",
      "2020-12-30 00:23:18.958093: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n",
      "2020-12-30 00:23:19.358911: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n",
      "2020-12-30 00:23:19.358988: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0\r\n",
      "2020-12-30 00:23:19.359579: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N\r\n",
      "2020-12-30 00:23:19.359887: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6553 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6)\r\n",
      "2020-12-30 00:23:19.360476: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n",
      "2020-12-30 00:23:20.842690: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n",
      "2020-12-30 00:23:20.842856: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:\r\n",
      "pciBusID: 0000:01:00.0 name: GeForce RTX 3060 Ti computeCapability: 8.6\r\n",
      "coreClock: 1.665GHz coreCount: 38 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n",
      "2020-12-30 00:23:20.843420: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n",
      "2020-12-30 00:23:20.843686: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n",
      "2020-12-30 00:23:20.843868: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n",
      "2020-12-30 00:23:20.844089: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n",
      "2020-12-30 00:23:20.844257: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n",
      "2020-12-30 00:23:20.844440: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n",
      "2020-12-30 00:23:20.844608: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n",
      "2020-12-30 00:23:20.844782: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n",
      "2020-12-30 00:23:20.844981: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n",
      "2020-12-30 00:23:20.845492: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:\r\n",
      "pciBusID: 0000:01:00.0 name: GeForce RTX 3060 Ti computeCapability: 8.6\r\n",
      "coreClock: 1.665GHz coreCount: 38 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n",
      "2020-12-30 00:23:20.845542: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n",
      "2020-12-30 00:23:20.845717: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n",
      "2020-12-30 00:23:20.845888: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n",
      "2020-12-30 00:23:20.846081: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n",
      "2020-12-30 00:23:20.846310: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n",
      "2020-12-30 00:23:20.846483: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n",
      "2020-12-30 00:23:20.846768: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n",
      "2020-12-30 00:23:20.846957: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n",
      "2020-12-30 00:23:20.847157: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n",
      "2020-12-30 00:23:20.847318: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n",
      "2020-12-30 00:23:20.847521: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0\r\n",
      "2020-12-30 00:23:20.847710: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N\r\n",
      "2020-12-30 00:23:20.847788: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6553 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6)\r\n",
      "2020-12-30 00:23:20.847924: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n",
      "2020-12-30 00:23:20.880594: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n",
      "2020-12-30 00:23:26.283777: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n",
      "2020-12-30 00:23:27.288825: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 0\r\n",
      "\r\n",
      "2020-12-30 00:23:27.321549: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 0\r\n",
      "\r\n",
      "2020-12-30 00:23:27.335160: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n",
      "2020-12-30 00:23:27.915665: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n",
      "```\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**logs from terminal when running on object detection api (2080 Super):**\r\n",
      "```\r\n",
      "2020-12-30 00:16:36.575415: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n",
      "2020-12-30 00:16:38.228703: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n",
      "2020-12-30 00:16:38.237835: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x271c5ab6f50 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n",
      "2020-12-30 00:16:38.240279: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n",
      "2020-12-30 00:16:38.242462: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n",
      "2020-12-30 00:16:38.270196: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:\r\n",
      "pciBusID: 0000:01:00.0 name: GeForce RTX 2080 SUPER computeCapability: 7.5\r\n",
      "coreClock: 1.815GHz coreCount: 48 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 462.00GiB/s\r\n",
      "2020-12-30 00:16:38.274723: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n",
      "2020-12-30 00:16:38.279122: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n",
      "2020-12-30 00:16:38.283598: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n",
      "2020-12-30 00:16:38.286135: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n",
      "2020-12-30 00:16:38.292216: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n",
      "2020-12-30 00:16:38.295924: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n",
      "2020-12-30 00:16:38.303861: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n",
      "2020-12-30 00:16:38.305815: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n",
      "2020-12-30 00:16:38.752621: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n",
      "2020-12-30 00:16:38.755254: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0\r\n",
      "2020-12-30 00:16:38.756368: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N\r\n",
      "2020-12-30 00:16:38.757586: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6553 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n",
      "2020-12-30 00:16:38.763118: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x271890480c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n",
      "2020-12-30 00:16:38.765334: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2080 SUPER, Compute Capability 7.5\r\n",
      "2020-12-30 00:16:40.654793: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:\r\n",
      "pciBusID: 0000:01:00.0 name: GeForce RTX 2080 SUPER computeCapability: 7.5\r\n",
      "coreClock: 1.815GHz coreCount: 48 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 462.00GiB/s\r\n",
      "2020-12-30 00:16:40.658737: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n",
      "2020-12-30 00:16:40.660559: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n",
      "2020-12-30 00:16:40.662372: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n",
      "2020-12-30 00:16:40.664757: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n",
      "2020-12-30 00:16:40.666563: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n",
      "2020-12-30 00:16:40.668383: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n",
      "2020-12-30 00:16:40.670203: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n",
      "2020-12-30 00:16:40.672337: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n",
      "2020-12-30 00:16:40.674537: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:\r\n",
      "pciBusID: 0000:01:00.0 name: GeForce RTX 2080 SUPER computeCapability: 7.5\r\n",
      "coreClock: 1.815GHz coreCount: 48 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 462.00GiB/s\r\n",
      "2020-12-30 00:16:40.677940: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n",
      "2020-12-30 00:16:40.679743: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n",
      "2020-12-30 00:16:40.682034: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n",
      "2020-12-30 00:16:40.683834: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n",
      "2020-12-30 00:16:40.685635: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n",
      "2020-12-30 00:16:40.687685: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n",
      "2020-12-30 00:16:40.689888: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n",
      "2020-12-30 00:16:40.692332: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n",
      "2020-12-30 00:16:40.693697: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n",
      "2020-12-30 00:16:40.695596: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0\r\n",
      "2020-12-30 00:16:40.696726: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N\r\n",
      "2020-12-30 00:16:40.697905: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6553 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n",
      "```\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:gpu\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:performance\n",
      "\n",
      "\n",
      "issue title -  Tensorflow related error when deploying shiny app on shinyapps.io\n",
      "issue body -  I am trying to deploy shiny app, that uses reticulate and keras packages. I do not have any problem to run it locally, but real troubles appear, when I try to deploy it to shinyapps.io. My app.r file is as follows:\r\n",
      "```\r\n",
      "virtualenv_dir = Sys.getenv(\"VIRTUALENV_NAME\")\r\n",
      "python_path = Sys.getenv(\"PYTHON_PATH\")\r\n",
      "reticulate::virtualenv_create(envname = virtualenv_dir, python = python_path)\r\n",
      "reticulate::virtualenv_install(virtualenv_dir, packages = c(\"numpy\", \"h5py\", \"scipy\", \"scikit-image\", \"pyyaml\", \"pillow\"), ignore_installed = TRUE)\r\n",
      "reticulate::use_virtualenv(virtualenv = virtualenv_dir)\r\n",
      "\r\n",
      "library(shiny)\r\n",
      "library(keras)\r\n",
      "library(reticulate)\r\n",
      "library(magick)\r\n",
      "library(raster)\r\n",
      "library(EBImage)\r\n",
      "library(rdrop2)\r\n",
      "library(plotly)\r\n",
      "\r\n",
      "np <- import(\"numpy\", convert=FALSE)\r\n",
      "ndi <- import(\"scipy.ndimage\", convert=FALSE)\r\n",
      "segment <- import(\"skimage.segmentation\", convert=FALSE)\r\n",
      "feature <- import(\"skimage.feature\", convert=FALSE)\r\n",
      "\r\n",
      "model = load_model_hdf5(\"model_v02122020.h5\")\r\n",
      "\r\n",
      "ui <- \r\n",
      "tagList(\r\n",
      "    fluidPage(\r\n",
      "        sidebarLayout(sidebarPanel(\r\n",
      "            fileInput(\"upload\", \"Choose a file\", accept = c('image/png', 'image/jpeg')),\r\n",
      "            actionButton('click', 'Start')\r\n",
      "        ),\r\n",
      "        mainPanel(\r\n",
      "            tabsetPanel(type=\"tabs\",\r\n",
      "                tabPanel(\"Input image\", plotOutput(\"InputImagePlot\", height=\"100%\")),\r\n",
      "                tabPanel(\"Output image\", plotOutput(\"OutputImagePlot\", height=\"100%\")),\r\n",
      "            )\r\n",
      "        )\r\n",
      "        )\r\n",
      "    )\r\n",
      ")\r\n",
      "server <- \r\n",
      "function(input, output, session) {\r\n",
      "\r\n",
      "    observeEvent(input$click, {\r\n",
      "## some code for image processing\r\n",
      "})\r\n",
      "}\r\n",
      "shinyApp(ui = ui, server = server)\r\n",
      "```\r\n",
      "My .Rprofile file is as follows (credit to [this source](https://github.com/ranikay/shiny-reticulate-app/blob/master/.Rprofile)):\r\n",
      "```\r\n",
      "VIRTUALENV_NAME = \"virt_tf\"\r\n",
      "\r\n",
      "if (Sys.info()[[\"user\"]] == \"shiny\"){\r\n",
      "  \r\n",
      "  # Running on shinyapps.io\r\n",
      "  Sys.setenv(PYTHON_PATH = 'python3')\r\n",
      "  Sys.setenv(VIRTUALENV_NAME = VIRTUALENV_NAME) # Installs into default shiny virtualenvs dir\r\n",
      "  Sys.setenv(RETICULATE_PYTHON = paste0('/home/shiny/.virtualenvs/', VIRTUALENV_NAME, '/bin/python'))\r\n",
      "  \r\n",
      "} else if (Sys.info()[[\"user\"]] == \"rstudio-connect\"){\r\n",
      "  \r\n",
      "  # Running on remote server\r\n",
      "  Sys.setenv(PYTHON_PATH = '/opt/python/3.7.6/bin/python')\r\n",
      "  Sys.setenv(VIRTUALENV_NAME = paste0(VIRTUALENV_NAME, '/')) # include '/' => installs into rstudio-connect/apps/\r\n",
      "  Sys.setenv(RETICULATE_PYTHON = paste0(VIRTUALENV_NAME, '/bin/python'))\r\n",
      "  \r\n",
      "} else {\r\n",
      "  \r\n",
      "  # Running locally\r\n",
      "  options(shiny.port = 7450)\r\n",
      "  Sys.setenv(PYTHON_PATH = 'python 3.6.12')\r\n",
      "  Sys.setenv(VIRTUALENV_NAME = VIRTUALENV_NAME) # exclude '/' => installs into ~/.virtualenvs/\r\n",
      "  # RETICULATE_PYTHON is not required locally, RStudio infers it based on the ~/.virtualenvs path\r\n",
      "}\r\n",
      "```\r\n",
      "The deployment process seems to run completely according to R log:\r\n",
      "```\r\n",
      "rsconnect::deployApp()\r\n",
      "Preparing to deploy application...Update application currently deployed at\r\n",
      "https://name.shinyapps.io/appname/? [Y/n] y\r\n",
      "DONE\r\n",
      "Uploading bundle for application: 3428026...DONE\r\n",
      "Deploying bundle: 4035381 for application: 3428026 ...\r\n",
      "Waiting for task: 846214175\r\n",
      "  building: Parsing manifest\r\n",
      "  building: Building image: 4594673\r\n",
      "  building: Installing system dependencies\r\n",
      "  building: Fetching packages\r\n",
      "  building: Installing packages\r\n",
      "  building: Installing files\r\n",
      "  building: Pushing image: 4594673\r\n",
      "  deploying: Starting instances\r\n",
      "  terminating: Stopping old instances\r\n",
      "Application successfully deployed to https://name.shinyapps.io/appname/\r\n",
      "```\r\n",
      "The error I get from the bottom of the log:\r\n",
      "\r\n",
      "> Error in value[[3L]](cond) : Installation of TensorFlow not found.\r\n",
      "> Python environments searched for 'tensorflow' package:\r\n",
      "> You can install TensorFlow using the install_tensorflow() function.\r\n",
      "> /home/shiny/.virtualenvs/virt_tf/bin/python3\r\n",
      "> Calls: local ... tryCatch -> tryCatchList -> tryCatchOne -> <Anonymous>\r\n",
      "> Execution halted\r\n",
      "\r\n",
      "When I try to include `tensorflow` into the list of packages required to be installed into my virtualenvironment I get the following error message:\r\n",
      "\r\n",
      "> Downloading tensorflow-2.3.1-cp35-cp35m-manylinux2010_x86_64.whl (320.4 MB)\r\n",
      "> Collecting tensorflow\r\n",
      "> Killed\r\n",
      "> Calls: local ... tryCatch -> tryCatchList -> tryCatchOne -> <Anonymous>\r\n",
      "> Error in value[[3L]](cond) : \r\n",
      "> Error installing package(s): 'numpy', 'h5py', 'scipy', 'scikit-image', 'pyyaml', 'pillow', 'tensorflow'\r\n",
      "> Execution halted\r\n",
      "> Out of memory!\r\n",
      "\r\n",
      "As far as I understand, shinyapps.io pushes me to install tensorflow package into virtualenvironment. However, I guess, it should be in the list of available packages. But how to force using it?\n",
      "issue labels - \n",
      "stat:awaiting response\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  keras can't save model when useing featurecolumns embding\n",
      "issue body -  `# -*- coding: utf-8 -*-\r\n",
      "import tensorflow as tf\r\n",
      "from tensorflow import keras\r\n",
      "from tensorflow.keras.layers import Dense\r\n",
      "from tensorflow.keras import Input, Model\r\n",
      "from tensorflow.keras import regularizers\r\n",
      "from matplotlib import pyplot as plt\r\n",
      "import numpy as np\r\n",
      "import pandas as pd\r\n",
      "from tensorflow.keras.layers import TimeDistributed\r\n",
      "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau, LearningRateScheduler\r\n",
      "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\r\n",
      "from tensorflow.keras.layers import Input, Dense, Multiply\r\n",
      "from tensorflow.keras.layers import DenseFeatures\r\n",
      "\r\n",
      "from tensorflow.keras.layers import BatchNormalization\r\n",
      "from tensorflow.keras.models import *\r\n",
      "import numpy as np\r\n",
      "import os\r\n",
      "\r\n",
      "# 模型细节有关的设置\r\n",
      "dnn_activation = 'tanh'\r\n",
      "dropout_rate = 0.5\r\n",
      "dnn_l1 = 0.01\r\n",
      "dnn_l2 = 0.01\r\n",
      "data_use_num = 1\r\n",
      "data_2pre_num = 1\r\n",
      "pred_label = 'pred_d'\r\n",
      "weights_path = 'model_dir/jarvis_super'\r\n",
      "batch_size = 1\r\n",
      "lr = 0.01\r\n",
      "\r\n",
      "use_feature_columns = True\r\n",
      "\r\n",
      "\r\n",
      "def lrs(epoch):\r\n",
      "    if epoch < 5:\r\n",
      "        learning_rate = lr\r\n",
      "    elif epoch < 10:\r\n",
      "        learning_rate = lr / 3\r\n",
      "    elif epoch < 20:\r\n",
      "        learning_rate = lr / 10\r\n",
      "    else:\r\n",
      "        learning_rate = lr / 100\r\n",
      "    print('learning rate : {}'.format(learning_rate))\r\n",
      "    return learning_rate\r\n",
      "\r\n",
      "\r\n",
      "def get_model(shape_deep, shape_times, pred_num):\r\n",
      "    ##############################################################\r\n",
      "    if use_feature_columns:\r\n",
      "\r\n",
      "        feature_columns, feature_layer_inputs = feature_columns_list(\r\n",
      "            numeric_column_list=['30day_press_dnn', '60day_press_dnn', '5day_press', ], buckets_column_list=[])\r\n",
      "\r\n",
      "        feature_columns2, feature_layer_inputs2 = feature_columns_list(\r\n",
      "            numeric_column_list=['30day_press', '60day_press', ], buckets_column_list=[])\r\n",
      "\r\n",
      "        feature_layer = DenseFeatures(feature_columns)\r\n",
      "        input_deep = feature_layer(feature_layer_inputs)\r\n",
      "        ##############################################################\r\n",
      "        ###############input_layer\r\n",
      "        feature_layer = DenseFeatures(feature_columns2)\r\n",
      "        input_times = feature_layer(feature_layer_inputs2)\r\n",
      "\r\n",
      "    else:\r\n",
      "        input_deep = tf.keras.Input(shape=(3), name='input_deep_list')\r\n",
      "        input_times = tf.keras.Input(shape=(2), name='input_times_list')\r\n",
      "\r\n",
      "    dnn1 = (keras.layers.Dense(units=500, activation=dnn_activation, kernel_regularizer=regularizers.l2(dnn_l2),\r\n",
      "                               activity_regularizer=regularizers.l1(dnn_l1))(input_deep))\r\n",
      "    # dnn1 = keras.layers.LayerNormalization(axis=1)(dnn1)\r\n",
      "    dnn1 = tf.keras.layers.Dropout(dropout_rate)(dnn1)\r\n",
      "\r\n",
      "    #######################\r\n",
      "    dnn2 = (keras.layers.Dense(units=400, activation=dnn_activation, kernel_regularizer=regularizers.l2(dnn_l2),\r\n",
      "                               activity_regularizer=regularizers.l1(dnn_l1))(dnn1))\r\n",
      "    # dnn2 = keras.layers.LayerNormalization(axis=1)(dnn2)\r\n",
      "    dnn2 = tf.keras.layers.Dropout(dropout_rate)(dnn2)\r\n",
      "\r\n",
      "    ###############################\r\n",
      "    dnn3 = (keras.layers.Dense(units=200, activation=dnn_activation, kernel_regularizer=regularizers.l2(dnn_l2),\r\n",
      "                               activity_regularizer=regularizers.l1(dnn_l1))(dnn2))\r\n",
      "    concat = (keras.layers.concatenate([input_deep, dnn3]))\r\n",
      "    # dnn3 = keras.layers.BatchNormalization(axis=1)(concat)\r\n",
      "    dnn3 = tf.keras.layers.Dropout(dropout_rate)(concat)\r\n",
      "\r\n",
      "    ###########################################\r\n",
      "    dnn4 = (keras.layers.Dense(units=150, activation=dnn_activation, kernel_regularizer=regularizers.l2(dnn_l2),\r\n",
      "                               activity_regularizer=regularizers.l1(dnn_l1), name='dnn4')(dnn3))\r\n",
      "\r\n",
      "    ################时间变量结构\r\n",
      "\r\n",
      "    attention_probs_time = (keras.layers.Dense(units=input_deep.shape[1], activation='softmax', )(input_times))\r\n",
      "    dnn_time1 = Multiply()([input_deep, attention_probs_time])\r\n",
      "\r\n",
      "    dnn_time1 = (keras.layers.Dense(units=500, activation=dnn_activation, kernel_regularizer=regularizers.l2(dnn_l2),\r\n",
      "                                    activity_regularizer=regularizers.l1(dnn_l1))(dnn_time1))\r\n",
      "    # dnn_time1 = keras.layers.LayerNormalization(axis=1)(dnn_time1)\r\n",
      "    dnn_time1 = tf.keras.layers.Dropout(dropout_rate)(dnn_time1)\r\n",
      "\r\n",
      "    #######################\r\n",
      "    dnn_time2 = (keras.layers.Dense(units=400, activation=dnn_activation, kernel_regularizer=regularizers.l2(dnn_l2),\r\n",
      "                                    activity_regularizer=regularizers.l1(dnn_l1))(dnn_time1))\r\n",
      "    # dnn_time2 = keras.layers.LayerNormalization(axis=1)(dnn_time2)\r\n",
      "    dnn_time2 = tf.keras.layers.Dropout(dropout_rate)(dnn_time2)\r\n",
      "\r\n",
      "    ###############################\r\n",
      "    dnn_time3 = (keras.layers.Dense(units=200, activation=dnn_activation, kernel_regularizer=regularizers.l2(dnn_l2),\r\n",
      "                                    activity_regularizer=regularizers.l1(dnn_l1))(dnn_time2))\r\n",
      "    concat = (keras.layers.concatenate([input_deep, dnn_time3]))\r\n",
      "    # dnn_time3 = keras.layers.LayerNormalization(axis=1)(concat)\r\n",
      "    dnn_time3 = tf.keras.layers.Dropout(dropout_rate)(concat)\r\n",
      "\r\n",
      "    ###########################################\r\n",
      "    # tf.keras.layers.SpatialDropout3D(\r\n",
      "    dnn_time4 = (keras.layers.Dense(units=150, activation=dnn_activation, kernel_regularizer=regularizers.l2(dnn_l2),\r\n",
      "                                    activity_regularizer=regularizers.l1(dnn_l1))(dnn_time3))\r\n",
      "\r\n",
      "    ############################################\r\n",
      "    finall_concat = keras.layers.concatenate([dnn4, dnn_time4])\r\n",
      "\r\n",
      "    x = Dense(128, activation=\"relu\")(finall_concat)\r\n",
      "    x = BatchNormalization()(x)\r\n",
      "\r\n",
      "    outputs = Dense(1, name='outputs')(x)\r\n",
      "\r\n",
      "    input_deep_list = []\r\n",
      "    if use_feature_columns:\r\n",
      "        for v in feature_layer_inputs.values():\r\n",
      "            print(v)\r\n",
      "            input_deep_list.append(v)\r\n",
      "\r\n",
      "        input_times_list = []\r\n",
      "        for v in feature_layer_inputs2.values():\r\n",
      "            input_times_list.append(v)\r\n",
      "        print(input_deep_list, input_times_list)\r\n",
      "    else:\r\n",
      "        input_times_list = input_times\r\n",
      "        input_deep_list = input_deep\r\n",
      "    model = Model(inputs=[input_deep_list, input_times_list], outputs=outputs)\r\n",
      "    return model\r\n",
      "\r\n",
      "\r\n",
      "def feature_columns_list(numeric_column_list, buckets_column_list):\r\n",
      "    feature_columns = []\r\n",
      "    feature_layer_inputs = {}\r\n",
      "\r\n",
      "    for header in numeric_column_list:\r\n",
      "        print(header)\r\n",
      "        day_press=tf.feature_column.numeric_column(header)\r\n",
      "        day_press_buket = tf.feature_column.bucketized_column(day_press, [1,100,1000])#这里因为没有嵌入dnn 所已要去掉\r\n",
      "        day_press_emb = tf.feature_column.embedding_column(categorical_column=day_press_buket,dimension=8)\r\n",
      "        feature_columns.append(day_press_emb)\r\n",
      "\r\n",
      "        feature_layer_inputs[header] = tf.keras.Input(shape=(1,), name=header)  ######所有单个的都用这个\r\n",
      "\r\n",
      "    return feature_columns, feature_layer_inputs\r\n",
      "\r\n",
      "\r\n",
      "def feature_columns_list2(numeric_column_list, buckets_column_list):\r\n",
      "    feature_columns = []\r\n",
      "    feature_layer_inputs = {}\r\n",
      "\r\n",
      "    for header in numeric_column_list:\r\n",
      "        print(header)\r\n",
      "        numeric_column = tf.feature_column.numeric_column(header)\r\n",
      "        feature_columns.append(numeric_column)\r\n",
      "        feature_layer_inputs[header] = tf.keras.Input(shape=(1,), name=(header + 'time'))  ######所有单个的都用这个\r\n",
      "    return feature_columns, feature_layer_inputs\r\n",
      "\r\n",
      "\r\n",
      "df1 = np.random.random((100, 3))\r\n",
      "df2 = np.random.random((100, 2))\r\n",
      "y_train = np.random.random((100, 1))\r\n",
      "\r\n",
      "list1 = ['30day_press_dnn', '60day_press_dnn',  '5day_press', ]\r\n",
      "list2 = ['30day_press', '60day_press', ]\r\n",
      "df1 = pd.DataFrame(df1, columns=list1)\r\n",
      "df2 = pd.DataFrame(df2, columns=list2)\r\n",
      "y_train = pd.DataFrame(y_train, columns=['pred'])\r\n",
      "\r\n",
      "dataset1 = tf.data.Dataset.from_tensor_slices((dict(df1), dict(df2)))\r\n",
      "dataset2 = tf.data.Dataset.from_tensor_slices(y_train)\r\n",
      "train_dataset = tf.data.Dataset.zip((dataset1, dataset2))\r\n",
      "train_dataset = train_dataset.batch(96, drop_remainder=True)\r\n",
      "\r\n",
      "if __name__ == '__main__':\r\n",
      "    model = get_model(shape_deep=(1,), shape_times=(1,), pred_num=1)\r\n",
      "    # model.summary()\r\n",
      "\r\n",
      "    plateau = LearningRateScheduler(lrs)\r\n",
      "\r\n",
      "    checkpoint = ModelCheckpoint(weights_path,\r\n",
      "                                 monitor='val_loss',\r\n",
      "                                 verbose=0,\r\n",
      "                                 mode='max',\r\n",
      "                                 save_best_only=True)\r\n",
      "    early_stopping = EarlyStopping(monitor='val_loss', patience=15, mode='max')\r\n",
      "    opt = Adam(lr=lr)\r\n",
      "    model.compile(\r\n",
      "        loss='mse',\r\n",
      "        optimizer=opt,\r\n",
      "    )  # metrics='loss'\r\n",
      "    model.fit(train_dataset, epochs=10, callbacks=[checkpoint, plateau],  # #plateau   early_stopping\r\n",
      "              shuffle=False,  # 时间序列为什么要打乱  是写错里吗？ #另外x是否应该为38000*300 行？\r\n",
      "              batch_size=1, verbose=2)\r\n",
      "    model.save('model.h5')\r\n",
      "\r\n",
      "<em>Please` make sure that this is a bug. As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:bug_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):no \r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04\r\n",
      "- TensorFlow installed from (source or binary):conda install\r\n",
      "- TensorFlow version (use command below):tensorflow 2.3.0-gpu\r\n",
      "- Python version:python 3.6\r\n",
      "- Bazel version (if compiling from source): \r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version:10.1\r\n",
      "- GPU model and memory: gpu 2080s   8g \r\n",
      "\r\n",
      "When I use keras to save the model, if I don't use embedding in feature columns_ The column model can be saved normally, but if embedding is used_ The model after column training cannot be saved. The error is as follows:\r\n",
      "\r\n",
      "\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/zy/Project_save/BTC_project/code_program/keras_bug.py\", line 212, in <module>\r\n",
      "    model.save('model.h5')\r\n",
      "  File \"/home/zy/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 1979, in save\r\n",
      "    signatures, options)\r\n",
      "  File \"/home/zy/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/saving/save.py\", line 131, in save_model\r\n",
      "    model, filepath, overwrite, include_optimizer)\r\n",
      "  File \"/home/zy/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/saving/hdf5_format.py\", line 119, in save_model_to_hdf5\r\n",
      "    save_weights_to_hdf5_group(model_weights_group, model_layers)\r\n",
      "  File \"/home/zy/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/saving/hdf5_format.py\", line 640, in save_weights_to_hdf5_group\r\n",
      "    param_dset = g.create_dataset(name, val.shape, dtype=val.dtype)\r\n",
      "  File \"/home/zy/anaconda3/envs/tf2/lib/python3.6/site-packages/h5py/_hl/group.py\", line 139, in create_dataset\r\n",
      "    self[name] = dset\r\n",
      "  File \"/home/zy/anaconda3/envs/tf2/lib/python3.6/site-packages/h5py/_hl/group.py\", line 373, in __setitem__\r\n",
      "    h5o.link(obj.id, self.id, name, lcpl=lcpl, lapl=self._lapl)\r\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\r\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\r\n",
      "  File \"h5py/h5o.pyx\", line 202, in h5py.h5o.link\r\n",
      "RuntimeError: Unable to create link (name already exists)\r\n",
      "\r\n",
      "\r\n",
      "You can collect some of this information using our environment capture\r\n",
      "[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n",
      "You can also obtain the TensorFlow version with:\r\n",
      "1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n",
      "2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n",
      "\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "Provide a reproducible test case that is the bare minimum necessary to generate\r\n",
      "the problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n",
      "\r\n",
      "**Other info / logs** Include any logs or source code that would be helpful to\r\n",
      "diagnose the problem. If including tracebacks, please include the full\r\n",
      "traceback. Large logs and files should be attached.\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.3\n",
      "comp:keras\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Remove extraneous expand_nonconcat_dims from doc string of tf.sparse.concat\n",
      "issue body -  This PR tries to address the issue raised in #45778 where the doc string\r\n",
      "of  tf.sparse.concat has an extraneous expand_nonconcat_dims arg.\r\n",
      "The expand_nonconcat_dims was the leftover of tf.v1.sparse.concat.\r\n",
      "\r\n",
      "This PR fixes #45778.\r\n",
      "\r\n",
      "Signed-off-by: Yong Tang <yong.tang.github@outlook.com>\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:ops\n",
      "prtype:bugfix\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Try to upgrade gast to 0.4.0\n",
      "issue body -  \n",
      "issue labels - \n",
      "cla: yes\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  Error retrieving driver version: Unimplemented: kernel reported driver version not implemented on Windows\n",
      "issue body -  After installing tensorflow2.4rc4 version on windows10, cudnn initialization error appears when running the training code. \r\n",
      "error info: Unimplemented: kernel reported driver version not implemented on Windows\r\n",
      "\r\n",
      "tensorflow version:  tensorflow 2.4.0rc4\r\n",
      "os :  windows 10\r\n",
      "cuda version: 11.0\r\n",
      "python version: 3.8\r\n",
      "GPU: RTX3070 \r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "2020-12-29 18:23:45.222526: E tensorflow/stream_executor/cuda/cuda_dnn.cc:336] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\r\n",
      "2020-12-29 18:23:45.222972: E tensorflow/stream_executor/cuda/cuda_dnn.cc:340] Error retrieving driver version: Unimplemented: kernel reported driver version not implemented on Windows\r\n",
      "2020-12-29 18:23:45.225297: E tensorflow/stream_executor/cuda/cuda_dnn.cc:336] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\r\n",
      "2020-12-29 18:23:45.225401: E tensorflow/stream_executor/cuda/cuda_dnn.cc:340] Error retrieving driver version: Unimplemented: kernel reported driver version not implemented on Windows\r\n",
      "2020-12-29 18:23:45.225508: F tensorflow/core/kernels/conv_grad_input_ops.cc:1173] Check failed: stream->parent()->GetConvolveBackwardDataAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo<T>(stream->parent()), &algorithms)\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  tensorflow 1.15.4 contrib.slim.python.slim.nets\n",
      "issue body -  i use tensorflow 1.15.4<br>\r\n",
      "`from tensorflow.contrib.slim.python.slim.nets import resnet_v2\r\n",
      "ImportError: No module named contrib.slim.python.slim.nets`\n",
      "issue labels - \n",
      "contrib\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Error when build TF 1.13.2 from the source\n",
      "issue body -  <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- OS Platform and Distribution: Linux Ubuntu 16.04/18.04\r\n",
      "- TensorFlow installed from source\r\n",
      "- TensorFlow version: 1.13.2\r\n",
      "- Python version: 2.7\r\n",
      "- Installed using: docker\r\n",
      "- Bazel version: 0.19.2\r\n",
      "- GCC/Compiler version: 4.9\r\n",
      "- CUDA/cuDNN version: 10.0, 7\r\n",
      "- GPU model and memory: T4, 32GB\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "I tried to install TF from the source following instructions here, https://github.com/tensorflow/docs/blob/r1.13/site/en/install/source.md#build-from-source. \r\n",
      "However some error occurs: ERROR: /home/cluster/tensorflow/tensorflow/core/kernels/BUILD:593:1: C++ compilation of rule '//tensorflow/core/kernels:eigen_contraction_kernel' failed (Exit 1). \r\n",
      "Any help would be appreciated! Thanks!\r\n",
      "\n",
      "issue labels - \n",
      "TF 1.13\n",
      "subtype: ubuntu/linux\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'is_fully_defined'\n",
      "issue body -  <em>Please make sure that this is a bug. As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:bug_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):windows10\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "- TensorFlow installed from (source or binary):binary\r\n",
      "- TensorFlow version (use command below):2.4.0\r\n",
      "- Python version:3.7.6\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version:11.0\r\n",
      "- GPU model and memory:24GB\r\n",
      "\r\n",
      "You can collect some of this information using our environment capture\r\n",
      "[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n",
      "You can also obtain the TensorFlow version with:\r\n",
      "1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n",
      "2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n",
      "\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "Can not reshapes SparseTensor\r\n",
      "**Describe the expected behavior**\r\n",
      "Reshapes a SparseTensor\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "Provide a reproducible test case that is the bare minimum necessary to generate\r\n",
      "the problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n",
      "```\r\n",
      "import tensorflow as tf\r\n",
      "\r\n",
      "sp = tf.SparseTensor([[1,1],[2,2]],[1.,2.],[4,4])\r\n",
      "new_sp = tf.sparse.reshape(sp,[8,-1])\r\n",
      "```\r\n",
      "**Other info / logs** Include any logs or source code that would be helpful to\r\n",
      "diagnose the problem. If including tracebacks, please include the full\r\n",
      "traceback. Large logs and files should be attached.\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"C:\\Program Files\\Python37\\lib\\site-packages\\tensorflow\\python\\ops\\sparse_ops.py\", line 902, in sparse_reshape\r\n",
      "    and sp_input.shape.is_fully_defined()):\r\n",
      "AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'is_fully_defined'\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:ops\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  dataset_fn() must return a tf.data.Dataset when using a tf.distribute.Strategy.\n",
      "issue body -  Please go to Stack Overflow for help and support:\r\n",
      "\r\n",
      "https://stackoverflow.com/questions/tagged/tensorflow\r\n",
      "\r\n",
      "If you open a GitHub issue, here is our policy:\r\n",
      "\r\n",
      "1.  It must be a bug, a feature request, or a significant problem with the\r\n",
      "    documentation (for small docs fixes please send a PR instead).\r\n",
      "2.  The form below must be filled out.\r\n",
      "3.  It shouldn't be a TensorBoard issue. Those go\r\n",
      "    [here](https://github.com/tensorflow/tensorboard/issues).\r\n",
      "\r\n",
      "**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n",
      "\r\n",
      "------------------------\r\n",
      "\r\n",
      "### System information\r\n",
      "\r\n",
      "-   **Have I written custom code (as opposed to using a stock example script\r\n",
      "    provided in TensorFlow)**:\r\n",
      "-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n",
      "-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n",
      "    happens on a mobile device**:\r\n",
      "-   **TensorFlow installed from (source or binary)**:\r\n",
      "-   **TensorFlow version (use command below)**:\r\n",
      "-   **Python version**:\r\n",
      "-   **Bazel version (if compiling from source)**:\r\n",
      "-   **GCC/Compiler version (if compiling from source)**:\r\n",
      "-   **CUDA/cuDNN version**:\r\n",
      "-   **GPU model and memory**:\r\n",
      "-   **Exact command to reproduce**:\r\n",
      "\r\n",
      "You can collect some of this information using our environment capture script:\r\n",
      "\r\n",
      "https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n",
      "\r\n",
      "You can obtain the TensorFlow version with:\r\n",
      "\r\n",
      "```bash\r\n",
      "python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n",
      "```\r\n",
      "\r\n",
      "### Describe the problem\r\n",
      "Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n",
      "\r\n",
      "### Source code / logs\r\n",
      "Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n",
      "\n",
      "issue labels - \n",
      "comp:data\n",
      "comp:dist-strat\n",
      "stalled\n",
      "stat:awaiting response\n",
      "\n",
      "\n",
      "issue title -   java.lang.NoClassDefFoundError: Failed resolution of: Lorg/tensorflow/lite/schema/Model;\n",
      "issue body -  java.lang.NoClassDefFoundError: Failed resolution of: Lorg/tensorflow/lite/schema/Model;\r\n",
      "        at org.tensorflow.lite.support.metadata.ModelInfo.assertTFLiteModel(ModelInfo.java:222)\r\n",
      "        at org.tensorflow.lite.support.metadata.ModelInfo.<init>(ModelInfo.java:64)\r\n",
      "        at org.tensorflow.lite.support.metadata.MetadataExtractor.<init>(MetadataExtractor.java:75)\r\n",
      "        at com.gc.testt.tflite.TFLiteObjectDetectionAPIModel.create(TFLiteObjectDetectionAPIModel.java:129)\r\n",
      "        at com.gc.testt.ADetectorActivity.onPreviewSizeChosen(ADetectorActivity.java:84)\r\n",
      "        at com.gc.testt.ACameraActivity.onPreviewFrame(ACameraActivity.java:147)\r\n",
      "        at android.hardware.Camera$EventHandler.handleMessage(Camera.java:1512)\r\n",
      "        at android.os.Handler.dispatchMessage(Handler.java:106)\r\n",
      "        at android.os.Looper.loop(Looper.java:192)\r\n",
      "        at android.app.ActivityThread.main(ActivityThread.java:6738)\r\n",
      "        at java.lang.reflect.Method.invoke(Native Method)\r\n",
      "        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:525)\r\n",
      "        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:825)\r\n",
      "     Caused by: java.lang.ClassNotFoundException: org.tensorflow.lite.schema.Model\r\n",
      "        at java.lang.VMClassLoader.findLoadedClass(Native Method)\r\n",
      "        at java.lang.ClassLoader.findLoadedClass(ClassLoader.java:738)\r\n",
      "        at java.lang.ClassLoader.loadClass(ClassLoader.java:363)\r\n",
      "        at java.lang.ClassLoader.loadClass(ClassLoader.java:312)\r\n",
      "        at org.tensorflow.lite.support.metadata.ModelInfo.assertTFLiteModel(ModelInfo.java:222) \r\n",
      "        at org.tensorflow.lite.support.metadata.ModelInfo.<init>(ModelInfo.java:64) \r\n",
      "        at org.tensorflow.lite.support.metadata.MetadataExtractor.<init>(MetadataExtractor.java:75) \r\n",
      "        at com.gc.testt.tflite.TFLiteObjectDetectionAPIModel.create(TFLiteObjectDetectionAPIModel.java:129) \r\n",
      "        at com.gc.testt.ADetectorActivity.onPreviewSizeChosen(ADetectorActivity.java:84) \r\n",
      "        at com.gc.testt.ACameraActivity.onPreviewFrame(ACameraActivity.java:147) \r\n",
      "        at android.hardware.Camera$EventHandler.handleMessage(Camera.java:1512) \r\n",
      "        at android.os.Handler.dispatchMessage(Handler.java:106) \r\n",
      "        at android.os.Looper.loop(Looper.java:192) \r\n",
      "        at android.app.ActivityThread.main(ActivityThread.java:6738) \r\n",
      "        at java.lang.reflect.Method.invoke(Native Method) \r\n",
      "        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:525) \r\n",
      "        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:825) \r\n",
      "     Caused by: java.lang.NoClassDefFoundError: Failed resolution of: Lcom/google/flatbuffers/Table;\r\n",
      "        at org.tensorflow.lite.support.metadata.ModelInfo.assertTFLiteModel(ModelInfo.java:222) \r\n",
      "        at org.tensorflow.lite.support.metadata.ModelInfo.<init>(ModelInfo.java:64) \r\n",
      "        at org.tensorflow.lite.support.metadata.MetadataExtractor.<init>(MetadataExtractor.java:75) \r\n",
      "        at com.gc.testt.tflite.TFLiteObjectDetectionAPIModel.create(TFLiteObjectDetectionAPIModel.java:129) \r\n",
      "        at com.gc.testt.ADetectorActivity.onPreviewSizeChosen(ADetectorActivity.java:84) \r\n",
      "        at com.gc.testt.ACameraActivity.onPreviewFrame(ACameraActivity.java:147) \r\n",
      "        at android.hardware.Camera$EventHandler.handleMessage(Camera.java:1512) \r\n",
      "        at android.os.Handler.dispatchMessage(Handler.java:106) \r\n",
      "        at android.os.Looper.loop(Looper.java:192) \r\n",
      "        at android.app.ActivityThread.main(ActivityThread.java:6738) \r\n",
      "        at java.lang.reflect.Method.invoke(Native Method) \r\n",
      "        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:525) \r\n",
      "        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:825) \r\n",
      "     Caused by: java.lang.ClassNotFoundException: Didn't find class \"com.google.flatbuffers.Table\" on path: DexPathList[[zip file \"/data/app/com.gc.testt-RmxHcdzbPpW7fPFnmeS8Rw==/base.apk\"],nativeLibraryDirectories=[/data/app/com.gc.testt-RmxHcdzbPpW7fPFnmeS8Rw==/lib/arm64, /data/app/com.gc.testt-RmxHcdzbPpW7fPFnmeS8Rw==/base.apk!/lib/arm64-v8a, /system/lib64, /system/vendor/lib64, /system/vendor/lib64/hw]]\r\n",
      "        at dalvik.system.BaseDexClassLoader.findClass(BaseDexClassLoader.java:125)\r\n",
      "        at java.lang.ClassLoader.loadClass(ClassLoader.java:379)\r\n",
      "        at java.lang.ClassLoader.loadClass(ClassLoader.java:312)\r\n",
      "        at org.tensorflow.lite.support.metadata.ModelInfo.assertTFLiteModel(ModelInfo.java:222) \r\n",
      "        at org.tensorflow.lite.support.metadata.ModelInfo.<init>(ModelInfo.java:64) \r\n",
      "        at org.tensorflow.lite.support.metadata.MetadataExtractor.<init>(MetadataExtractor.java:75) \r\n",
      "        at com.gc.testt.tflite.TFLiteObjectDetectionAPIModel.create(TFLiteObjectDetectionAPIModel.java:129) \r\n",
      "        at com.gc.testt.ADetectorActivity.onPreviewSizeChosen(ADetectorActivity.java:84) \r\n",
      "        at com.gc.testt.ACameraActivity.onPreviewFrame(ACameraActivity.java:147) \r\n",
      "        at android.hardware.Camera$EventHandler.handleMessage(Camera.java:1512) \r\n",
      "        at android.os.Handler.dispatchMessage(Handler.java:106) \r\n",
      "        at android.os.Looper.loop(Looper.java:192) \r\n",
      "        at android.app.ActivityThread.main(ActivityThread.java:6738) \r\n",
      "        at java.lang.reflect.Method.invoke(Native Method) \r\n",
      "        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:525) \r\n",
      "        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:825) \r\n",
      "\r\n",
      "Androidstudio using the Tensorflow-lite-Metadata - 0.0.0.0-Nightly JAR file, the Table file could not be found；\n",
      "issue labels - \n",
      "comp:lite\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  When use  activation='tanh' , the training program will crash\n",
      "issue body -  def make_generator_model():\r\n",
      "    model = tf.keras.Sequential()\r\n",
      "    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))\r\n",
      "    model.add(layers.BatchNormalization())\r\n",
      "    model.add(layers.LeakyReLU())\r\n",
      "\r\n",
      "    model.add(layers.Reshape((7, 7, 256)))\r\n",
      "    assert model.output_shape == (None, 7, 7, 256) \r\n",
      "\r\n",
      "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\r\n",
      "    assert model.output_shape == (None, 7, 7, 128)\r\n",
      "    model.add(layers.BatchNormalization())\r\n",
      "    model.add(layers.LeakyReLU())\r\n",
      "\r\n",
      "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\r\n",
      "    assert model.output_shape == (None, 14, 14, 64)\r\n",
      "    model.add(layers.BatchNormalization())\r\n",
      "    model.add(layers.LeakyReLU())\r\n",
      "\r\n",
      "    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\r\n",
      "    model.add(layers.LeakyReLU())\r\n",
      "    assert model.output_shape == (None, 28, 28, 1)\r\n",
      "\r\n",
      "    return model\r\n",
      "\r\n",
      "generator =make_generator_model()\r\n",
      "\r\n",
      "noise = tf.random.normal([1, 100])\r\n",
      "generated_image = generator(noise, training=False)\r\n",
      "\r\n",
      "###################\r\n",
      "issue:  when use activation='tanh' , the training program will crash, use other activation function is normal.\r\n",
      "tensorflow version:   from tf-nightly2.5.0.dev20201217 to tf-nightly2.5.0.dev20201228\r\n",
      "cuda version:  11.0\r\n",
      "python version: 3.8\r\n",
      "os:  windows 10\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:keras\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  gpu out of mempry\n",
      "issue body -  i difined a simple model use tensorflow2.0, but it run slowly. then i code gpustat in terminal, i found gpu out of memory. i run the same code use tensorflow tensorflow1.15, it runs normaly, and gpu memory cost correctly. the model defined as follow:\r\n",
      "\r\n",
      "model = keras.Sequential([\r\n",
      "    layers.Reshape(target_shape=(28 * 28,), input_shape=(28, 28)),\r\n",
      "    layers.Dense(100, activation='relu'),\r\n",
      "    layers.Dense(100, activation='relu'),\r\n",
      "    layers.Dense(10)])\r\n",
      "\r\n",
      "what should i do?\n",
      "issue labels - \n",
      "comp:gpu\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Extract a function for parsing op DEPTH_TO_SPACE\n",
      "issue body -  Extract the parsing of op parameters from the flatbuffer out of a\r\n",
      "switch statement case, into a standalone function which can be\r\n",
      "called by the micro op resolver.\r\n",
      "\r\n",
      "This PR is part of the work to port operator DEPTH_TO_SPACE\r\n",
      "from lite to micro, as tracked in issue #46025.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  Extract reference for op SPACE_TO_DEPTH to standalone header\n",
      "issue body -  Move the reference implementation to its own header so that micro\r\n",
      "can use it without including unrelated depedencies via\r\n",
      "reference_ops.h.\r\n",
      "\r\n",
      "This PR is part of the work to port operator SPACE_TO_DEPTH\r\n",
      "from lite to micro, as tracked in issue #45824.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  AttributeError: module 'tensorflow_probability.python.bijectors' has no attribute 'Shift'\n",
      "issue body -  \r\n",
      "On MacOS 10.13.6 (High Sierra), I get:\r\n",
      "\r\n",
      "AttributeError: module 'tensorflow_probability.python.bijectors' has no attribute 'Shift'\r\n",
      "\r\n",
      "\r\n",
      "% python bg1.py\r\n",
      "\r\n",
      "2020-12-28 18:32:50.699423: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX\r\n",
      "o enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\r\n",
      "2020-12-28 18:32:50.699890: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 4. Tune using inter_op_parallelism_threads for best performance.\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"bg1.py\", line 65, in <module>\r\n",
      "    chol_precision_tril=true_chol_precision)\r\n",
      "  File \"<decorator-gen-351>\", line 2, in __init__\r\n",
      "  File \"/Users/ram/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_probability/python/distributions/distribution.py\", line 276, in wrapped_init\r\n",
      "    default_init(self_, *args, **kwargs)\r\n",
      "  File \"bg1.py\", line 36, in __init__\r\n",
      "    tfb.Shift(shift=loc),\r\n",
      "**AttributeError: module 'tensorflow_probability.python.bijectors' has no attribute 'Shift'**\r\n",
      "\r\n",
      "====================================================================\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**Versions**\r\n",
      "\r\n",
      "MacOS 10.13.6 (High Sierra)\r\n",
      "\r\n",
      "tensorflow                2.0.0           mkl_py37hda344b4_0  \r\n",
      "tensorflow-base           2.0.0           mkl_py37h66b1bf0_0  \r\n",
      "tensorflow-estimator      2.0.0              pyh2649769_0  \r\n",
      "tensorflow-probability    0.8.0                      py_0    conda-forge\r\n",
      "jupyter_client            6.1.7                      py_0  \r\n",
      "jupyter_core              4.7.0            py37hecd8cb5_0  \r\n",
      "jupyterlab_pygments       0.1.2                      py_0  \r\n",
      "ipython                   7.19.0           py37h01d92e1_0  \r\n",
      "ipython_genutils          0.2.0              pyhd3eb1b0_1  \r\n",
      "python                    3.7.9                h26836e1_0  \r\n",
      "python-dateutil           2.8.1                      py_0  \r\n",
      "python_abi                3.7                     1_cp37m    conda-forge\r\n",
      "\r\n",
      "====================================================================\r\n",
      "\r\n",
      "**The source-code:**\r\n",
      "\r\n",
      "```\r\n",
      "\r\n",
      "import os\r\n",
      "os.environ['KMP_DUPLICATE_LIB_OK']='True'\r\n",
      "\r\n",
      "from pprint import pprint\r\n",
      "import matplotlib.pyplot as plt\r\n",
      "import numpy as np\r\n",
      "import seaborn as sns\r\n",
      "\r\n",
      "#import tensorflow as tf\r\n",
      "#print(tf.__version__)\r\n",
      "\r\n",
      "import tensorflow.compat.v2 as tf\r\n",
      "tf.enable_v2_behavior()\r\n",
      "\r\n",
      "import tensorflow_probability as tfp\r\n",
      "\r\n",
      "sns.reset_defaults()\r\n",
      "sns.set_context(context='talk',font_scale=0.7)\r\n",
      "plt.rcParams['image.cmap'] = 'viridis'\r\n",
      "\r\n",
      "#%matplotlib inline\r\n",
      "\r\n",
      "tfd = tfp.distributions\r\n",
      "tfb = tfp.bijectors\r\n",
      "\r\n",
      "class MVNCholPrecisionTriL(tfd.TransformedDistribution):\r\n",
      "  \"\"\"MVN from loc and (Cholesky) precision matrix.\"\"\"\r\n",
      "\r\n",
      "  def __init__(self, loc, chol_precision_tril, name=None):\r\n",
      "    super(MVNCholPrecisionTriL, self).__init__(\r\n",
      "        distribution=tfd.Independent(tfd.Normal(tf.zeros_like(loc),\r\n",
      "                                                scale=tf.ones_like(loc)),\r\n",
      "                                     reinterpreted_batch_ndims=1),\r\n",
      "        bijector=tfb.Chain([\r\n",
      "            tfb.Shift(shift=loc),\r\n",
      "            tfb.Invert(tfb.ScaleMatvecTriL(scale_tril=chol_precision_tril,\r\n",
      "                                           adjoint=True)),\r\n",
      "        ]),\r\n",
      "        name=name)\r\n",
      "\r\n",
      "def compute_sample_stats(d, seed=42, n=int(1e6)):\r\n",
      "  x = d.sample(n, seed=seed)\r\n",
      "  sample_mean = tf.reduce_mean(x, axis=0, keepdims=True)\r\n",
      "  s = x - sample_mean\r\n",
      "  sample_cov = tf.linalg.matmul(s, s, adjoint_a=True) / tf.cast(n, s.dtype)\r\n",
      "  sample_scale = tf.linalg.cholesky(sample_cov)\r\n",
      "  sample_mean = sample_mean[0]\r\n",
      "  return [\r\n",
      "      sample_mean,\r\n",
      "      sample_cov,\r\n",
      "      sample_scale,\r\n",
      "  ]\r\n",
      "\r\n",
      "dtype = np.float32\r\n",
      "true_loc = np.array([1., -1.], dtype=dtype)\r\n",
      "true_chol_precision = np.array([[1., 0.],\r\n",
      "                                [2., 8.]],\r\n",
      "                               dtype=dtype)\r\n",
      "true_precision = np.matmul(true_chol_precision, true_chol_precision.T)\r\n",
      "true_cov = np.linalg.inv(true_precision)\r\n",
      "\r\n",
      "d = MVNCholPrecisionTriL(\r\n",
      "    loc=true_loc,\r\n",
      "    chol_precision_tril=true_chol_precision)\r\n",
      "\r\n",
      "[sample_mean, sample_cov, sample_scale] = [\r\n",
      "    t.numpy() for t in compute_sample_stats(d)]\r\n",
      "\r\n",
      "print('true mean:', true_loc)\r\n",
      "print('sample mean:', sample_mean)\r\n",
      "print('true cov:\\n', true_cov)\r\n",
      "print('sample cov:\\n', sample_cov)\r\n",
      "\r\n",
      "```\r\n",
      "\n",
      "issue labels - \n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Extract a function for parsing op SPACE_TO_DEPTH\n",
      "issue body -  Extract the parsing of op parameters from the flatbuffer out of a\r\n",
      "switch statement case, into a standalone function which can be\r\n",
      "called by the micro op resolver.\r\n",
      "\r\n",
      "This PR is part of the work to port operator SPACE_TO_DEPTH\r\n",
      "from lite to micro, as tracked in issue #45824.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  Common location for portable bash helper functions / aliases\n",
      "issue body -  @tensorflow/micro\r\n",
      "\r\n",
      "PR #46011 fixes the use of `md5sum` to be compatible with a Mac. We are already using `md5sum` in additional places too.\r\n",
      "\r\n",
      "While we should get #46011 merged, it would be better to have a common location for these helper functions / aliases.\r\n",
      "\r\n",
      "I can imagine collecting these into a common helper script (for example `micro/tools/bash_helpers.sh`) and have something like:\r\n",
      "\r\n",
      "```bash\r\n",
      "\r\n",
      "UNAME_S=`uname -s`\r\n",
      "\r\n",
      "if [ UNAME_S == Linux]; then\r\n",
      "  alias tflm_md5sum='md5sum'\r\n",
      "else if [ UNAME_S == Darwin ]; then\r\n",
      "  alias tflm_md5sum='md5 -r'\r\n",
      "fi\r\n",
      "\r\n",
      "```\r\n",
      "\r\n",
      "We would then need to change the different download scripts to determine the directory in which the script lives, something like: https://github.com/tensorflow/tensorflow/blob/59f5abfbc8dc5559c361f80f4fa4a006db825e40/tensorflow/lite/micro/tools/ci_build/test_bluepill.sh#L21-L23\r\n",
      "\r\n",
      "and then have\r\n",
      "```bash\r\n",
      "source tensorflow/lite/micro/tools/bash_helper.sh\r\n",
      "```\n",
      "issue labels - \n",
      "comp:lite\n",
      "comp:micro\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Upgrade grpcio to 1.34.0\n",
      "issue body -  \n",
      "issue labels - \n",
      "cla: yes\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Update Hello World README to point to new model\n",
      "issue body -  \n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  TypeError: sample_chain() got an unexpected keyword argument 'seed'\n",
      "issue body -  Error with Tensorflow 2.0 using MCMC on MacOS 10.13.6\r\n",
      "\r\n",
      "**The error on the console:**\r\n",
      "\r\n",
      "```\r\n",
      "2020-12-27 22:06:48.253835: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX\r\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\r\n",
      "2020-12-27 22:06:48.254353: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 4. Tune using inter_op_parallelism_threads for best performance.\r\n",
      "objc[69111]: Class zmAppHelper is implemented in both /Library/ScriptingAdditions/zOLPluginInjection.osax/Contents/MacOS/zOLPluginInjection (0x1a48eaf4f0) and /Library/Application Support/Microsoft/ZoomOutlookPlugin/zOutlookPlugin64.bundle/Contents/MacOS/zOutlookPlugin64 (0x1a490e0518). One of the two will be used. Which one is undefined.\r\n",
      "objc[69111]: class `ERCalendarEventEditorWindowController' not linked into application\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"dc7.py\", line 131, in <module>\r\n",
      "    chains, kernel_results = run_chain(initial_state)\r\n",
      "  File \"/Users/ram/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 457, in __call__\r\n",
      "    result = self._call(*args, **kwds)\r\n",
      "  File \"/Users/ram/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 503, in _call\r\n",
      "    self._initialize(args, kwds, add_initializers_to=initializer_map)\r\n",
      "  File \"/Users/ram/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 408, in _initialize\r\n",
      "    *args, **kwds))\r\n",
      "  File \"/Users/ram/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1848, in _get_concrete_function_internal_garbage_collected\r\n",
      "    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n",
      "  File \"/Users/ram/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2150, in _maybe_define_function\r\n",
      "    graph_function = self._create_graph_function(args, kwargs)\r\n",
      "  File \"/Users/ram/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2041, in _create_graph_function\r\n",
      "    capture_by_value=self._capture_by_value),\r\n",
      "  File \"/Users/ram/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 915, in func_graph_from_py_func\r\n",
      "    func_outputs = python_func(*func_args, **func_kwargs)\r\n",
      "  File \"/Users/ram/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 358, in wrapped_fn\r\n",
      "    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n",
      "  File \"/Users/ram/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 905, in wrapper\r\n",
      "    raise e.ag_error_metadata.to_exception(e)\r\n",
      "TypeError: in converted code:\r\n",
      "\r\n",
      "    dc7.py:117 run_chain  *\r\n",
      "        return tfp.mcmc.sample_chain(\r\n",
      "\r\n",
      "    **TypeError: sample_chain() got an unexpected keyword argument 'seed'**\r\n",
      "```\r\n",
      "\r\n",
      "**Versions**\r\n",
      "\r\n",
      "```\r\n",
      "MacOS 10.13.6 High Sierra\r\n",
      "\r\n",
      "tensorflow                2.0.0           mkl_py37hda344b4_0  \r\n",
      "tensorflow-base           2.0.0           mkl_py37h66b1bf0_0  \r\n",
      "tensorflow-estimator      2.0.0              pyh2649769_0  \r\n",
      "tensorflow-probability    0.8.0                      py_0    conda-forge\r\n",
      "jupyter_client            6.1.7                      py_0  \r\n",
      "jupyter_core              4.7.0            py37hecd8cb5_0  \r\n",
      "jupyterlab_pygments       0.1.2                      py_0  \r\n",
      "ipython                   7.19.0           py37h01d92e1_0  \r\n",
      "ipython_genutils          0.2.0              pyhd3eb1b0_1  \r\n",
      "python                    3.7.9                h26836e1_0  \r\n",
      "python-dateutil           2.8.1                      py_0  \r\n",
      "python_abi                3.7                     1_cp37m    conda-forge\r\n",
      "```\r\n",
      "\r\n",
      "**The source-code:**\r\n",
      "\r\n",
      "\r\n",
      "```\r\n",
      "import os\r\n",
      "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\r\n",
      "\r\n",
      "from pprint import pprint\r\n",
      "import matplotlib.pyplot as plt\r\n",
      "import numpy as np\r\n",
      "import seaborn as sns\r\n",
      "\r\n",
      "#import tensorflow as tf\r\n",
      "#print(tf.__version__)\r\n",
      "\r\n",
      "import tensorflow.compat.v2 as tf\r\n",
      "tf.enable_v2_behavior()\r\n",
      "\r\n",
      "import tensorflow_probability as tfp\r\n",
      "\r\n",
      "sns.reset_defaults()\r\n",
      "sns.set_context(context = 'talk', font_scale = 0.7)\r\n",
      "plt.rcParams['image.cmap'] = 'viridis'\r\n",
      "\r\n",
      "#%matplotlib inline\r\n",
      "\r\n",
      "tfd = tfp.distributions\r\n",
      "tfb = tfp.bijectors\r\n",
      "\r\n",
      "\r\n",
      "#### ============================================\r\n",
      "\r\n",
      "#@title Utils { display-mode: \"form\" }\r\n",
      "def print_subclasses_from_module(module, base_class, maxwidth=80):\r\n",
      "  import functools, inspect, sys\r\n",
      "  subclasses = [name for name, obj in inspect.getmembers(module)\r\n",
      "                if inspect.isclass(obj) and issubclass(obj, base_class)]\r\n",
      "  def red(acc, x):\r\n",
      "    if not acc or len(acc[-1]) + len(x) + 2 > maxwidth:\r\n",
      "      acc.append(x)\r\n",
      "    else:\r\n",
      "      acc[-1] += \", \" + x\r\n",
      "    return acc\r\n",
      "  print('\\n'.join(functools.reduce(red, subclasses, [])))\r\n",
      "\r\n",
      "# Generate some data\r\n",
      "def f(x, w):\r\n",
      "  # Pad x with 1's so we can add bias via matmul\r\n",
      "  x = tf.pad(x, [[1, 0], [0, 0]], constant_values=1)\r\n",
      "  linop = tf.linalg.LinearOperatorFullMatrix(w[..., np.newaxis])\r\n",
      "  result = linop.matmul(x, adjoint=True)\r\n",
      "  return result[..., 0, :]\r\n",
      "\r\n",
      "num_features = 2\r\n",
      "num_examples = 50\r\n",
      "noise_scale = .5\r\n",
      "true_w = np.array([-1., 2., 3.])\r\n",
      "\r\n",
      "xs = np.random.uniform(-1., 1., [num_features, num_examples])\r\n",
      "ys = f(xs, true_w) + np.random.normal(0., noise_scale, size=num_examples)\r\n",
      "\r\n",
      "# Visualize the data set\r\n",
      "plt.scatter(*xs, c=ys, s=100, linewidths=0)\r\n",
      "\r\n",
      "grid = np.meshgrid(*([np.linspace(-1, 1, 100)] * 2))\r\n",
      "xs_grid = np.stack(grid, axis=0)\r\n",
      "fs_grid = f(xs_grid.reshape([num_features, -1]), true_w)\r\n",
      "fs_grid = np.reshape(fs_grid, [100, 100])\r\n",
      "plt.colorbar()\r\n",
      "plt.contour(xs_grid[0, ...], xs_grid[1, ...], fs_grid, 20, linewidths=1)\r\n",
      "plt.show()\r\n",
      "\r\n",
      "### Sampling the noise scale\r\n",
      "\r\n",
      "# Define the joint_log_prob function, and our unnormalized posterior.\r\n",
      "def joint_log_prob(w, sigma, x, y):\r\n",
      "  # Our model in maths is\r\n",
      "  #   w ~ MVN([0, 0, 0], diag([1, 1, 1]))\r\n",
      "  #   y_i ~ Normal(w @ x_i, noise_scale),  i=1..N\r\n",
      "\r\n",
      "  rv_w = tfd.MultivariateNormalDiag(\r\n",
      "    loc=np.zeros(num_features + 1),\r\n",
      "    scale_diag=np.ones(num_features + 1))\r\n",
      "  \r\n",
      "  rv_sigma = tfd.LogNormal(np.float64(1.), np.float64(5.))\r\n",
      "\r\n",
      "  rv_y = tfd.Normal(f(x, w), sigma[..., np.newaxis])\r\n",
      "  return (rv_w.log_prob(w) +\r\n",
      "          rv_sigma.log_prob(sigma) +\r\n",
      "          tf.reduce_sum(rv_y.log_prob(y), axis=-1))\r\n",
      "\r\n",
      "# Create our unnormalized target density by currying x and y from the joint.\r\n",
      "def unnormalized_posterior(w, sigma):\r\n",
      "  return joint_log_prob(w, sigma, xs, ys)\r\n",
      "\r\n",
      "\r\n",
      "# Create an HMC TransitionKernel\r\n",
      "hmc_kernel = tfp.mcmc.HamiltonianMonteCarlo(\r\n",
      "  target_log_prob_fn=unnormalized_posterior,\r\n",
      "  step_size=np.float64(.1),\r\n",
      "  num_leapfrog_steps=4)\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "# Create a TransformedTransitionKernl\r\n",
      "transformed_kernel = tfp.mcmc.TransformedTransitionKernel(\r\n",
      "    inner_kernel=hmc_kernel,\r\n",
      "    bijector=[tfb.Identity(),    # w\r\n",
      "              tfb.Invert(tfb.Softplus())])   # sigma\r\n",
      "\r\n",
      "\r\n",
      "# Apply a simple step size adaptation during burnin\r\n",
      "@tf.function\r\n",
      "def run_chain(initial_state, num_results=1000, num_burnin_steps=500):\r\n",
      "  adaptive_kernel = tfp.mcmc.SimpleStepSizeAdaptation(\r\n",
      "      transformed_kernel,\r\n",
      "      num_adaptation_steps=int(.8 * num_burnin_steps),\r\n",
      "      target_accept_prob=np.float64(.75))\r\n",
      "\r\n",
      "  return tfp.mcmc.sample_chain(\r\n",
      "    num_results=num_results,\r\n",
      "    num_burnin_steps=num_burnin_steps,\r\n",
      "    current_state=initial_state,\r\n",
      "    kernel=adaptive_kernel,\r\n",
      "    seed=(0, 1),\r\n",
      "    trace_fn=lambda cs, kr: kr)\r\n",
      "\r\n",
      "\r\n",
      "# Instead of a single set of initial w's, we create a batch of 8.\r\n",
      "num_chains = 8\r\n",
      "initial_state = [np.zeros([num_chains, num_features + 1]),\r\n",
      "                 .54 * np.ones([num_chains], dtype=np.float64)]\r\n",
      "\r\n",
      "chains, kernel_results = run_chain(initial_state)\r\n",
      "\r\n",
      "r_hat = tfp.mcmc.potential_scale_reduction(chains)\r\n",
      "print(\"Acceptance rate:\", kernel_results.inner_results.inner_results.is_accepted.numpy().mean())\r\n",
      "print(\"R-hat diagnostic (per w variable):\", r_hat[0].numpy())\r\n",
      "print(\"R-hat diagnostic (sigma):\", r_hat[1].numpy())\r\n",
      "\r\n",
      "w_chains, sigma_chains = chains\r\n",
      "```\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.0\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  How do I pass an array of bytes instead of an array of shorts\n",
      "issue body -  Good evening, I started to deal with this example https://github.com/tensorflow/examples/tree/master/lite/examples/speech_commands/android but ran into a problem. Help me please, I want to pass an array of bytes, not shorts.\r\n",
      "```\r\n",
      "class MainActivity : AppCompatActivity() {\r\n",
      "\r\n",
      "    companion object {\r\n",
      "\r\n",
      "        private const val TAG = \"MainActivity\"\r\n",
      "        private const val SAMPLE_RATE = 16_000\r\n",
      "        private const val BUFFER_SIZE_SECONDS = 0.3F\r\n",
      "        private const val DETECTION_THRESHOLD = 0.50F\r\n",
      "        private const val SUPPRESSION_MS = 1500\r\n",
      "        private const val MINIMUM_COUNT = 3\r\n",
      "        private const val MINIMUM_TIME_BETWEEN_SAMPLES_MS = 30L\r\n",
      "        private const val AVERAGE_WINDOW_DURATION_MS = 1_000L\r\n",
      "        private const val REQUEST_AUDIO_RECORD_PERMISSION = 200\r\n",
      "    }\r\n",
      "\r\n",
      "    private val labels = listOf(\"_silence_\", \"_unknown_\", \"yes\", \"no\", \"up\", \"down\", \"left\", \"right\", \"on\", \"off\", \"stop\", \"go\")\r\n",
      "    private val tfLiteOptions = Interpreter.Options()\r\n",
      "    private val recordingBufferLock = ReentrantLock()\r\n",
      "\r\n",
      "    private var recordingOffset = 0\r\n",
      "    private var shouldContinue = true\r\n",
      "    private var recordingThread: Thread? = null\r\n",
      "    private var shouldContinueRecognition = true\r\n",
      "    private var recognitionThread: Thread? = null\r\n",
      "    private var recordingBuffer = ByteArray(SAMPLE_RATE)\r\n",
      "\r\n",
      "    private lateinit var tfLite: Interpreter\r\n",
      "    private lateinit var tfLiteModel: MappedByteBuffer\r\n",
      "    private lateinit var recognizeCommands: RecognizeCommands\r\n",
      "\r\n",
      "    override fun onCreate(savedInstanceState: Bundle?) {\r\n",
      "        super.onCreate(savedInstanceState)\r\n",
      "        setContentView(R.layout.activity_main)\r\n",
      "\r\n",
      "        recognizeCommands = RecognizeCommands(\r\n",
      "            labels,\r\n",
      "            AVERAGE_WINDOW_DURATION_MS,\r\n",
      "            DETECTION_THRESHOLD,\r\n",
      "            SUPPRESSION_MS,\r\n",
      "            MINIMUM_COUNT,\r\n",
      "            MINIMUM_TIME_BETWEEN_SAMPLES_MS\r\n",
      "        )\r\n",
      "\r\n",
      "        if (ContextCompat.checkSelfPermission(this, Manifest.permission.RECORD_AUDIO) == PermissionChecker.PERMISSION_GRANTED) {\r\n",
      "            initTfLite()\r\n",
      "        } else {\r\n",
      "            ActivityCompat.requestPermissions(this, arrayOf(Manifest.permission.RECORD_AUDIO), REQUEST_AUDIO_RECORD_PERMISSION)\r\n",
      "        }\r\n",
      "    }\r\n",
      "\r\n",
      "    private fun initTfLite() {\r\n",
      "        try {\r\n",
      "            tfLiteModel = loadModelFile()\r\n",
      "            tfLite = Interpreter(tfLiteModel, tfLiteOptions)\r\n",
      "\r\n",
      "            tfLite.resizeInput(0, intArrayOf(SAMPLE_RATE, 1))\r\n",
      "            tfLite.resizeInput(1, intArrayOf(1))\r\n",
      "\r\n",
      "            startRecording()\r\n",
      "            startRecognition()\r\n",
      "        } catch (exc: IOException) {\r\n",
      "            Log.e(TAG, \"Error: ${exc.message}\")\r\n",
      "        }\r\n",
      "    }\r\n",
      "\r\n",
      "    private fun startRecording() {\r\n",
      "        if (recordingThread == null) {\r\n",
      "            shouldContinue = true\r\n",
      "            recordingThread = Thread { record() }\r\n",
      "            recordingThread?.start()\r\n",
      "        }\r\n",
      "    }\r\n",
      "\r\n",
      "    private fun record() {\r\n",
      "        Process.setThreadPriority(Process.THREAD_PRIORITY_AUDIO)\r\n",
      "\r\n",
      "        val bufferSize = (SAMPLE_RATE.toFloat() * BUFFER_SIZE_SECONDS).roundToInt() * 2\r\n",
      "        val record = AudioRecord(\r\n",
      "            MediaRecorder.AudioSource.DEFAULT,\r\n",
      "            SAMPLE_RATE,\r\n",
      "            AudioFormat.CHANNEL_IN_MONO,\r\n",
      "            AudioFormat.ENCODING_PCM_16BIT,\r\n",
      "            bufferSize\r\n",
      "        )\r\n",
      "\r\n",
      "        if (record.state != AudioRecord.STATE_INITIALIZED) {\r\n",
      "            Log.e(TAG,\"Audio Record can't initialize!\")\r\n",
      "            return\r\n",
      "        }\r\n",
      "\r\n",
      "        record.startRecording()\r\n",
      "\r\n",
      "        while (shouldContinue) {\r\n",
      "            val audioBuffer = ByteArray(bufferSize)\r\n",
      "            val numberRead = record.read(audioBuffer, 0, audioBuffer.size)\r\n",
      "            val newRecordingOffset = recordingOffset + numberRead\r\n",
      "            val secondCopyLength = Math.max(0, newRecordingOffset - recordingBuffer.size)\r\n",
      "            val firstCopyLength = numberRead - secondCopyLength\r\n",
      "\r\n",
      "            recordingBufferLock.lock()\r\n",
      "            try {\r\n",
      "                System.arraycopy(audioBuffer, 0, recordingBuffer, recordingOffset, firstCopyLength)\r\n",
      "                System.arraycopy(audioBuffer, firstCopyLength, recordingBuffer, 0, secondCopyLength)\r\n",
      "                recordingOffset = newRecordingOffset % recordingBuffer.size\r\n",
      "            } finally {\r\n",
      "                recordingBufferLock.unlock()\r\n",
      "            }\r\n",
      "        }\r\n",
      "\r\n",
      "        record.stop()\r\n",
      "        record.release()\r\n",
      "    }\r\n",
      "\r\n",
      "    private fun startRecognition() {\r\n",
      "        if (recognitionThread == null) {\r\n",
      "            shouldContinueRecognition = true\r\n",
      "            recognitionThread = Thread { recognize() }\r\n",
      "            recognitionThread?.start()\r\n",
      "        }\r\n",
      "    }\r\n",
      "\r\n",
      "    private fun recognize() {\r\n",
      "        val inputBuffer = ByteArray(SAMPLE_RATE)\r\n",
      "        val floatInputBuffer = Array(SAMPLE_RATE) { FloatArray(1) }\r\n",
      "        val outputScores = Array(1) { FloatArray(labels.size) }\r\n",
      "        val sampleRateList = intArrayOf(SAMPLE_RATE)\r\n",
      "\r\n",
      "        while (shouldContinueRecognition) {\r\n",
      "            recordingBufferLock.lock()\r\n",
      "\r\n",
      "            try {\r\n",
      "                val maxLength = recordingBuffer.size\r\n",
      "                val firstCopyLength = maxLength - recordingOffset\r\n",
      "                val secondCopyLength = recordingOffset\r\n",
      "                System.arraycopy(recordingBuffer, recordingOffset, inputBuffer, 0, firstCopyLength)\r\n",
      "                System.arraycopy(recordingBuffer, 0, inputBuffer, firstCopyLength, secondCopyLength)\r\n",
      "            } finally {\r\n",
      "                recordingBufferLock.unlock()\r\n",
      "            }\r\n",
      "\r\n",
      "            for (i in 0 until SAMPLE_RATE) {\r\n",
      "                floatInputBuffer[i][0] = inputBuffer[i] / Byte.MAX_VALUE.toFloat()\r\n",
      "            }\r\n",
      "\r\n",
      "            val inputArray = arrayOf<Any>(floatInputBuffer, sampleRateList)\r\n",
      "            val outputMap: MutableMap<Int, Any> = HashMap()\r\n",
      "            outputMap[0] = outputScores\r\n",
      "\r\n",
      "            tfLite.runForMultipleInputsOutputs(inputArray, outputMap)\r\n",
      "\r\n",
      "            val result = recognizeCommands.processLatestResults(outputScores[0], System.currentTimeMillis())\r\n",
      "\r\n",
      "            if (!result.foundCommand.startsWith(\"_\") && result.isNewCommand) {\r\n",
      "                Log.d(TAG, \"Command: ${result.foundCommand} (${result.score})\")\r\n",
      "            }\r\n",
      "\r\n",
      "            try {\r\n",
      "                Thread.sleep(MINIMUM_TIME_BETWEEN_SAMPLES_MS)\r\n",
      "            } catch (exc: InterruptedException) {\r\n",
      "                Log.d(TAG, \"Error: ${exc.message}\")\r\n",
      "            }\r\n",
      "        }\r\n",
      "    }\r\n",
      "\r\n",
      "    @Throws(IOException::class)\r\n",
      "    private fun loadModelFile(): MappedByteBuffer {\r\n",
      "        val fileDescriptor = assets.openFd(\"conv_actions_frozen.tflite\")\r\n",
      "        val inputStream = FileInputStream(fileDescriptor.fileDescriptor)\r\n",
      "        val fileChannel = inputStream.channel\r\n",
      "        val startOffset = fileDescriptor.startOffset\r\n",
      "        val declaredLength = fileDescriptor.declaredLength\r\n",
      "        return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength)\r\n",
      "    }\r\n",
      "\r\n",
      "    override fun onRequestPermissionsResult(\r\n",
      "        requestCode: Int,\r\n",
      "        permissions: Array<out String>,\r\n",
      "        grantResults: IntArray\r\n",
      "    ) {\r\n",
      "        super.onRequestPermissionsResult(requestCode, permissions, grantResults)\r\n",
      "        if (requestCode == REQUEST_AUDIO_RECORD_PERMISSION) {\r\n",
      "            initTfLite()\r\n",
      "        } else {\r\n",
      "            Toast.makeText(this, \"Необходимо предоставить разрешение\", Toast.LENGTH_LONG).show()\r\n",
      "            finish()\r\n",
      "        }\r\n",
      "    }\r\n",
      "}\r\n",
      "```\n",
      "issue labels - \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comp:lite\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Adjust error message of tf.debugging.assert_type\n",
      "issue body -  This PR tries to address the issue raised in #45975 where\r\n",
      "the error message of tf.debugging.assert_type could be\r\n",
      "misleading when the tf_type is not passed with a DType.\r\n",
      "\r\n",
      "This PR adds additional check so that tf_type arg can be guarded\r\n",
      "if non-DType value (e.g., list, tuple etc) is passed.\r\n",
      "\r\n",
      "This PR fixes #45975.\r\n",
      "\r\n",
      "Signed-off-by: Yong Tang <yong.tang.github@outlook.com>\n",
      "issue labels - \n",
      "cla: yes\n",
      "prtype:bugfix\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  tf.function retracing\n",
      "issue body -  **System information**\r\n",
      "- OS: Ubuntu 18.04.5 LTS (Google Colab)\r\n",
      "- tf version: 2.4.0\r\n",
      "- tf git version: v2.4.0-0-g582c8d236cb\r\n",
      "\r\n",
      "Code:\r\n",
      "```\r\n",
      "import tensorflow as tf\r\n",
      "import numpy as np\r\n",
      "from keras import *\r\n",
      "from keras.layers import *\r\n",
      "\r\n",
      "X = np.random.uniform(-1, 1, size = (1, 1000))\r\n",
      "y = np.array([[0.7974]])\r\n",
      "\r\n",
      "for _ in range(6):\r\n",
      "  model = Sequential([\r\n",
      "    Input(shape = 1000),\r\n",
      "    Dense(1, activation = 'sigmoid'),\r\n",
      "  ])\r\n",
      "\r\n",
      "  model.compile(loss = 'mse', optimizer = 'adam')\r\n",
      "  model.fit(X, y, batch_size = 1, epochs = 100, verbose = 0)\r\n",
      "  print(model.predict(X))\r\n",
      "```\r\n",
      "\r\n",
      "Output:\r\n",
      "```\r\n",
      "[[0.7982576]]\r\n",
      "[[0.7960699]]\r\n",
      "[[0.7987139]]\r\n",
      "[[0.79762185]]\r\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f38dc45f0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\r\n",
      "[[0.79733586]]\r\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f38dbbf3510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\r\n",
      "[[0.79833543]]\r\n",
      "```\r\n",
      "\r\n",
      "Is this warning a bug, or am I doing something wrong?\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:keras\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Fix md5sum command not found issue on mac\n",
      "issue body -  The PR fixes and issue where some systems viz., mac do not have `md5sum` command.\r\n",
      "It uses alias `md5 -r` for the same which bash doesn't expand.\r\n",
      "\r\n",
      "Check for the same and use `md5 -r` instead.\r\n",
      "\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Internal error: Failed to run on the given Interpreter: tensorflow/lite/kernels/reduce.cc:534 \n",
      "issue body -  **Descriptions**\r\n",
      "We successfully converted the Matterpot Mask RCNN object detection model (using hdf5 weight file) into tflite format using tensorflow (version 2.3.0). We got the prediction from the converted tflite model using python API of tf-nightly () due to Flex delegate issue raised when using tensorflow version 2.3.0.\r\n",
      "\r\n",
      "**Converted tflite model link** : https://drive.google.com/file/d/1kJjQnuf5FYdn0DcEAIkeMoWFXZFuM2eb/view?usp=sharing\r\n",
      "\r\n",
      "When we tried to get the prediction using our converted tflite model using the android application (https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android) but the following error is raised at **runForMultipleInputsOutputs()** function.\r\n",
      "\r\n",
      "**Error raised in android :**\r\n",
      "```\r\n",
      "2020-12-28 11:30:01.263 6838-6857/org.tensorflow.lite.examples.detection E/AndroidRuntime: FATAL EXCEPTION: inference\r\n",
      "    Process: org.tensorflow.lite.examples.detection, PID: 6838\r\n",
      "    java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: tensorflow/lite/kernels/reduce.cc:534 reference_ops::ReduceGeneric<T>( GetTensorData<T>(op_context->input), op_context->input->dims->data, op_context->input->dims->size, GetTensorData<T>(op_context->output), op_context->output->dims->data, op_context->output->dims->size, GetTensorData<int>(op_context->axis), num_axis, op_context->params->keep_dims, GetTensorData<int>(temp_index), GetTensorData<int>(resolved_axis), init_value, reducer) was not true.\r\n",
      "    No\r\n",
      "        at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)\r\n",
      "        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:158)\r\n",
      "        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:343)\r\n",
      "        at org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.recognizeImage(TFLiteObjectDetectionAPIModel.java:334)\r\n",
      "        at org.tensorflow.lite.examples.detection.DetectorActivity$2.run(DetectorActivity.java:182)\r\n",
      "        at android.os.Handler.handleCallback(Handler.java:761)\r\n",
      "        at android.os.Handler.dispatchMessage(Handler.java:98)\r\n",
      "        at android.os.Looper.loop(Looper.java:156)\r\n",
      "        at android.os.HandlerThread.run(HandlerThread.java:61)\r\n",
      "```\r\n",
      "\r\n",
      "**Used python code used for converting the model into tflite format**\r\n",
      "```\r\n",
      "model = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR, config=config)\r\n",
      "keras_model = `model.keras_model`\r\n",
      "converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\r\n",
      "converter.allow_custom_ops = True\r\n",
      "converter.target_spec.supported_ops = [\r\n",
      "    tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\r\n",
      "    tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\r\n",
      "]\r\n",
      "converter.optimizations = [ tf.lite.Optimize.DEFAULT ]\r\n",
      "tflite_model = converter.convert()\r\n",
      "```\r\n",
      "\r\n",
      "\r\n",
      "**Java code used in android studio to the get the prediction**\r\n",
      "We tested the code for single image and set input tensors accordly (images, img_metas and anchors).\r\n",
      "\r\n",
      "```\r\n",
      "// File:  org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel\r\n",
      "\r\n",
      "Interpreter.Options options = new Interpreter.Options();\r\n",
      "\r\n",
      "// Model file is loaded from assets.\r\n",
      "Interpreter Tflite = new Interpreter(modelFile, options);\r\n",
      "\r\n",
      "float[][][][] images = new float[1][1024][1024][3];\r\n",
      "float[][] img_metas = new float[1][14];\r\n",
      "float[][][] anchors = new float[1][261888][4];\r\n",
      "// above arrays are populated. \r\n",
      "\r\n",
      "Object[] inputArray = {images, img_metas, anchors};\r\n",
      "tensor0 = new float[1][1][1];\r\n",
      "tensor1 = new float[1][1000][2][4];\r\n",
      "tensor2 = new float[1][1000][2];\r\n",
      "tensor3 = new float[1][100][6];\r\n",
      "tensor4 = new float[1][100][28][28][2];\r\n",
      "tensor5 = new float[1][1][4];\r\n",
      "tensor6 = new float[1][1][2];\r\n",
      "Map<Integer, Object> outputMap = new HashMap<>();\r\n",
      "    outputMap.put(0, tensor0);\r\n",
      "    outputMap.put(1, tensor1);\r\n",
      "    outputMap.put(2, tensor2);\r\n",
      "    outputMap.put(3, tensor3);\r\n",
      "    outputMap.put(4, tensor4);\r\n",
      "    outputMap.put(5, tensor5);\r\n",
      "    outputMap.put(6, tensor6);\r\n",
      "tfLite.runForMultipleInputsOutputs(inputArray, outputMap);\r\n",
      "```\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.3\n",
      "comp:lite\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  ADDING DARK MODE TO TensorFlow WEBSITE\n",
      "issue body -  **   FEATURE REQUEST FOR DARK MODE ON WEBSITE **\r\n",
      "\r\n",
      "As the bright interphase of our website affects our eyes i would request to put an dark mode feature to our TensorFlow website.\r\n",
      "  \n",
      "issue labels - \n",
      "invalid\n",
      "type:docs-feature\n",
      "\n",
      "\n",
      "issue title -  SparseTensor mul. broadcasting gradient fails\n",
      "issue body -  ## System information\r\n",
      "- Have I written custom code: yes\r\n",
      "- OS Platform and Distribution: Linux Ubuntu 18.04\r\n",
      "- TensorFlow installed from: binary\r\n",
      "- TensorFlow version: v1.12.1-47912-gec43aacb56f 2.5.0-dev20201219\r\n",
      "- Python version: 3.7\r\n",
      "\r\n",
      "## Current behavior\r\n",
      "\r\n",
      "Computing gradients of scaled `tf.SparseTensor`s fails. This is resolved be adding dimensions to scalar.\r\n",
      "\r\n",
      "## Expected behavior\r\n",
      "\r\n",
      "Gradient computation compatible with automatic dimension adding when broadcasting leading dimensions.\r\n",
      "\r\n",
      "## Standalone code to reproduce the issue\r\n",
      "\r\n",
      "[Notebook](https://colab.research.google.com/drive/1R-HV0570iNzbY3yUGXSuqnikPWZd6aDO?usp=sharing)\r\n",
      "\r\n",
      "Code copied below for convenience\r\n",
      "\r\n",
      "```python\r\n",
      "import tensorflow as tf\r\n",
      "\r\n",
      "n = 5\r\n",
      "values = tf.Variable(tf.random.uniform((n,)))\r\n",
      "indices = tf.sparse.eye(n).indices\r\n",
      "with tf.GradientTape() as tape:\r\n",
      "    tape.watch(values)\r\n",
      "    st = tf.SparseTensor(indices, values, (n, n))\r\n",
      "    st = st * 2.                        # doesn't work\r\n",
      "    # st = st * tf.reshape(2., (1, 1))  # works\r\n",
      "    loss = tf.sparse.reduce_sum(st)\r\n",
      "\r\n",
      "grad = tape.gradient(loss, values)\r\n",
      "print(grad)\r\n",
      "```\r\n",
      "\r\n",
      "## Stack Trace\r\n",
      "\r\n",
      "```txt\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"main.py\", line 13, in <module>\r\n",
      "    grad = tape.gradient(loss, values)\r\n",
      "  File \"/home/jackd/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py\", line 1073, in gradient\r\n",
      "    unconnected_gradients=unconnected_gradients)\r\n",
      "  File \"/home/jackd/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/eager/imperative_grad.py\", line 77, in imperative_grad\r\n",
      "    compat.as_str(unconnected_gradients.value))\r\n",
      "  File \"/home/jackd/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py\", line 162, in _gradient_function\r\n",
      "    return grad_fn(mock_op, *out_grads)\r\n",
      "  File \"/home/jackd/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/ops/sparse_grad.py\", line 247, in _SparseDenseCwiseMulGrad\r\n",
      "    return _SparseDenseCwiseMulOrDivGrad(op, grad, True)\r\n",
      "  File \"/home/jackd/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/ops/sparse_grad.py\", line 227, in _SparseDenseCwiseMulOrDivGrad\r\n",
      "    dense_vals = array_ops.gather_nd(y, scaled_indices)\r\n",
      "  File \"/home/jackd/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\", line 206, in wrapper\r\n",
      "    return target(*args, **kwargs)\r\n",
      "  File \"/home/jackd/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\", line 5348, in gather_nd\r\n",
      "    return gen_array_ops.gather_nd(params, indices, name=name)\r\n",
      "  File \"/home/jackd/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 3695, in gather_nd\r\n",
      "    _ops.raise_from_not_ok_status(e, name)\r\n",
      "  File \"/home/jackd/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 6870, in raise_from_not_ok_status\r\n",
      "    six.raise_from(core._status_to_exception(e.code, message), None)\r\n",
      "  File \"<string>\", line 3, in raise_from\r\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: params must be at least a vector [Op:GatherNd]\r\n",
      "```\n",
      "issue labels - \n",
      "TF 1.12\n",
      "comp:ops\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  ConverterError: <unknown>:0: error: loc(\"lstm_bias_lstm_17\"): is not immutable (RNN)\n",
      "issue body -  **System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n",
      "- TensorFlow installed from (source or binary): 2.4.0\r\n",
      "\r\n",
      "\r\n",
      "**Command used to run the converter or code if you’re using the Python API**\r\n",
      "If possible, please share a link to Colab/Jupyter/any notebook.\r\n",
      "`import tensorflow as tf\r\n",
      "converter = tf.lite.TFLiteConverter.from_saved_model(model_path)\r\n",
      "converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n",
      "converter.target_spec.supported_types = [tf.float16]\r\n",
      "converter.experimental_new_converter = True\r\n",
      "converter.target_spec.supported_ops = [\r\n",
      "  tf.lite.OpsSet.TFLITE_BUILTINS,\r\n",
      "  tf.lite.OpsSet.SELECT_TF_OPS]\r\n",
      "tflite_model = converter.convert()`\r\n",
      "\r\n",
      "**The output from the converter invocation**\r\n",
      "`---------------------------------------------------------------------------\r\n",
      "Exception                                 Traceback (most recent call last)\r\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n",
      "    212                                                  debug_info_str,\r\n",
      "--> 213                                                  enable_mlir_converter)\r\n",
      "    214       return model_str\r\n",
      "\r\n",
      "4 frames\r\n",
      "Exception: <unknown>:0: error: loc(\"lstm_bias_lstm_17\"): is not immutable, try running tf-saved-model-optimize-global-tensors to prove tensors are immutable\r\n",
      "\r\n",
      "\r\n",
      "During handling of the above exception, another exception occurred:\r\n",
      "\r\n",
      "ConverterError                            Traceback (most recent call last)\r\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n",
      "    214       return model_str\r\n",
      "    215     except Exception as e:\r\n",
      "--> 216       raise ConverterError(str(e))\r\n",
      "    217 \r\n",
      "    218   if distutils.spawn.find_executable(_toco_from_proto_bin) is None:\r\n",
      "\r\n",
      "ConverterError: <unknown>:0: error: loc(\"lstm_bias_lstm_17\"): is not immutable, try running tf-saved-model-optimize-global-tensors to prove tensors are immutable`\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "TFLiteConverter\n",
      "comp:lite\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  windows 10 build failed (ERROR: An error occurred during the fetch of repository 'local_config_cuda':)\n",
      "issue body -  I've tried to install tensorflow step by step from [source_windows](https://www.tensorflow.org/install/source_windows)\r\n",
      "but got error\r\n",
      "**System information**\r\n",
      "- OS Platform and Distribution : windows10 x64\r\n",
      "- TensorFlow installed from (source or binary): source\r\n",
      "- TensorFlow version: 2.4.0\r\n",
      "- Python version: 3.7.7\r\n",
      "- Bazel version : 3.7.2\r\n",
      "- GCC/Compiler version (if compiling from source): visual studio 2019\r\n",
      "- CUDA/cuDNN version: 10.2\r\n",
      "- GPU model and memory: 960m, 4G\r\n",
      "\r\n",
      "output of `python ./configure.py`:\r\n",
      "```\r\n",
      "You have bazel 3.7.2 installed.\r\n",
      "Please specify the location of python. [Default is C:\\Users\\127051\\AppData\\Local\\Programs\\Python\\Python37\\python.exe]:\r\n",
      "\r\n",
      "\r\n",
      "Found possible Python library paths:\r\n",
      "  C:\\Users\\127051\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\r\n",
      "Please input the desired Python library path to use.  Default is [C:\\Users\\127051\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages]\r\n",
      "\r\n",
      "Do you wish to build TensorFlow with ROCm support? [y/N]: N\r\n",
      "No ROCm support will be enabled for TensorFlow.\r\n",
      "\r\n",
      "Do you wish to build TensorFlow with CUDA support? [y/N]: y\r\n",
      "CUDA support will be enabled for TensorFlow.\r\n",
      "\r\n",
      "Found CUDA 10.2 in:\r\n",
      "    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2/lib/x64\r\n",
      "    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2/include\r\n",
      "Found cuDNN 8 in:\r\n",
      "    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2/lib/x64\r\n",
      "    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2/include\r\n",
      "\r\n",
      "\r\n",
      "Please specify a list of comma-separated CUDA compute capabilities you want to build with.\r\n",
      "You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as \"x.y\" or \"compute_xy\" to include both virtual and binary GPU code, or as \"sm_xy\" to only include the binary code.\r\n",
      "Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 5.0\r\n",
      "\r\n",
      "\r\n",
      "Please specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]:\r\n",
      "\r\n",
      "\r\n",
      "Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]: Y\r\n",
      "Eigen strong inline overridden.\r\n",
      "\r\n",
      "Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: N\r\n",
      "Not configuring the WORKSPACE for Android builds.\r\n",
      "\r\n",
      "Preconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n",
      "        --config=mkl            # Build with MKL support.\r\n",
      "        --config=mkl_aarch64    # Build with oneDNN support for Aarch64.\r\n",
      "        --config=monolithic     # Config for mostly static monolithic build.\r\n",
      "        --config=ngraph         # Build with Intel nGraph support.\r\n",
      "        --config=numa           # Build with NUMA support.\r\n",
      "        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\n",
      "        --config=v2             # Build TensorFlow 2.x instead of 1.x.\r\n",
      "Preconfigured Bazel build configs to DISABLE default on features:\r\n",
      "        --config=noaws          # Disable AWS S3 filesystem support.\r\n",
      "        --config=nogcp          # Disable GCP support.\r\n",
      "        --config=nohdfs         # Disable HDFS support.\r\n",
      "        --config=nonccl         # Disable NVIDIA NCCL support.\r\n",
      "```\r\n",
      "Install error:\r\n",
      "```\r\n",
      "bazel build //tensorflow/tools/pip_package:build_pip_package\r\n",
      "INFO: Options provided by the client:\r\n",
      "  Inherited 'common' options: --isatty=1 --terminal_columns=164\r\n",
      "INFO: Reading rc options for 'build' from d:\\softwareinstaltion\\tensorflow-2.4.0\\.bazelrc:\r\n",
      "  Inherited 'common' options: --experimental_repo_remote_exec\r\n",
      "INFO: Options provided by the client:\r\n",
      "  'build' options: --python_path=C:/Users/127051/AppData/Local/Programs/Python/Python37/python.exe\r\n",
      "INFO: Reading rc options for 'build' from d:\\softwareinstaltion\\tensorflow-2.4.0\\.bazelrc:\r\n",
      "  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2\r\n",
      "INFO: Reading rc options for 'build' from d:\\softwareinstaltion\\tensorflow-2.4.0\\.tf_configure.bazelrc:\r\n",
      "  'build' options: --action_env PYTHON_BIN_PATH=C:/Users/127051/AppData/Local/Programs/Python/Python37/python.exe --action_env PYTHON_LIB_PATH=C:/Users/127051/AppData/Local/Programs/Python/Python37/lib/site-packages --python_path=C:/Users/127051/AppData/Local/Programs/Python/Python37/python.exe --config=xla --action_env CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2 --action_env TF_CUDA_COMPUTE_CAPABILITIES=5.0 --config=cuda --define=override_eigen_strong_inline=true --action_env TF_CONFIGURE_IOS=0\r\n",
      "INFO: Found applicable config definition build:short_logs in file d:\\softwareinstaltion\\tensorflow-2.4.0\\.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\n",
      "INFO: Found applicable config definition build:v2 in file d:\\softwareinstaltion\\tensorflow-2.4.0\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\n",
      "INFO: Found applicable config definition build:xla in file d:\\softwareinstaltion\\tensorflow-2.4.0\\.bazelrc: --define=with_xla_support=true\r\n",
      "INFO: Found applicable config definition build:cuda in file d:\\softwareinstaltion\\tensorflow-2.4.0\\.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\n",
      "INFO: Found applicable config definition build:using_cuda in file d:\\softwareinstaltion\\tensorflow-2.4.0\\.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1\r\n",
      "INFO: Found applicable config definition build:windows in file d:\\softwareinstaltion\\tensorflow-2.4.0\\.bazelrc: --copt=/W0 --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --experimental_strict_action_env=true --verbose_failures --distinct_host_configuration=false\r\n",
      "INFO: Found applicable config definition build:monolithic in file d:\\softwareinstaltion\\tensorflow-2.4.0\\.bazelrc: --define framework_shared_object=false\r\n",
      "INFO: Repository local_config_cuda instantiated at:\r\n",
      "  D:/softwareinstaltion/tensorflow-2.4.0/WORKSPACE:19:16: in <toplevel>\r\n",
      "  D:/softwareinstaltion/tensorflow-2.4.0/tensorflow/workspace.bzl:96:19: in tf_repositories\r\n",
      "Repository rule cuda_configure defined at:\r\n",
      "  D:/softwareinstaltion/tensorflow-2.4.0/third_party/gpus/cuda_configure.bzl:1430:33: in <toplevel>\r\n",
      "ERROR: An error occurred during the fetch of repository 'local_config_cuda':\r\n",
      "   Traceback (most recent call last):\r\n",
      "        File \"D:/softwareinstaltion/tensorflow-2.4.0/third_party/gpus/cuda_configure.bzl\", line 1400, column 38, in _cuda_autoconf_impl\r\n",
      "                _create_local_cuda_repository(repository_ctx)\r\n",
      "        File \"D:/softwareinstaltion/tensorflow-2.4.0/third_party/gpus/cuda_configure.bzl\", line 1244, column 56, in _create_local_cuda_repository\r\n",
      "                host_compiler_includes + _cuda_include_path(\r\n",
      "        File \"D:/softwareinstaltion/tensorflow-2.4.0/third_party/gpus/cuda_configure.bzl\", line 364, column 32, in _cuda_include_path\r\n",
      "                inc_entries.append(realpath(repository_ctx, cuda_config.cuda_toolkit_path + \"/include\"))\r\n",
      "        File \"D:/softwareinstaltion/tensorflow-2.4.0/third_party/remote_config/common.bzl\", line 277, column 19, in realpath\r\n",
      "                return execute(repository_ctx, [bash_bin, \"-c\", \"realpath \\\"%s\\\"\" % path]).stdout.strip()\r\n",
      "        File \"D:/softwareinstaltion/tensorflow-2.4.0/third_party/remote_config/common.bzl\", line 217, column 13, in execute\r\n",
      "                fail(\r\n",
      "Error in fail: Repository command failed\r\n",
      "/usr/bin/realpath: missing operand\r\n",
      "Try '/usr/bin/realpath --help' for more information.\r\n",
      "ERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': no such package '@local_config_cuda//cuda': Repository command failed\r\n",
      "/usr/bin/realpath: missing operand\r\n",
      "Try '/usr/bin/realpath --help' for more information.\r\n",
      "WARNING: Target pattern parsing failed.\r\n",
      "ERROR: no such package '@local_config_cuda//cuda': Repository command failed\r\n",
      "/usr/bin/realpath: missing operand\r\n",
      "Try '/usr/bin/realpath --help' for more information.\r\n",
      "INFO: Elapsed time: 1.781s\r\n",
      "INFO: 0 processes.\r\n",
      "FAILED: Build did NOT complete successfully (0 packages loaded)\r\n",
      "    currently loading: tensorflow/tools/pip_package\r\n",
      "\r\n",
      "```\n",
      "issue labels - \n",
      "TF 2.4\n",
      "stalled\n",
      "stat:awaiting response\n",
      "subtype:windows\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  Normalizing checkpoint path for Windows File System in Saver.py\n",
      "issue body -  https://github.com/keras-team/keras-tuner/issues/198\r\n",
      "\r\n",
      "The hardcoded forward slash value for _SHARDED_SUFFIX broke on windows since the windows file system does not except forward slashes.\r\n",
      "\r\n",
      "Solution:\r\n",
      "Using os.path.normpath is going to normalize path relative to the OS \n",
      "issue labels - \n",
      "cla: yes\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  keras.Model.fit error with 2.4 and TPUv3\n",
      "issue body -  My code is worked in 2.3 and 2.4 with GPU. And it is failed when doing \"keras.Model.fit\" in Cloud TPU with 2.4, but worked in 2.3.1 and nightly.\r\n",
      "\r\n",
      "In the past, this error is only encountered when the TensorFlow version in TPU does not fully matched the version in VM.\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:keras\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Add VERSION_INFO\n",
      "issue body -  Fixes https://github.com/tensorflow/tensorflow/issues/39795\r\n",
      "\r\n",
      "/cc @mihaimaruseac \n",
      "issue labels - \n",
      "cla: yes\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  from tensorflow.contrib.layers.python.layers.initializers import variance_scaling_initializer\n",
      "issue body -  `from tensorflow.contrib.layers.python.layers.initializers import variance_scaling_initializer`<br>\r\n",
      "ImportError: No module named slim.layers.python.layers.initializers<br>\r\n",
      "is there a new way to do this ? \n",
      "issue labels - \n",
      "contrib\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Tensorflow 2.4 requires BMI2 CPU extension (AVX is not enough)\n",
      "issue body -  **System information**\r\n",
      "- OS Platform and Distribution : Debian GNU/Linux 10 (buster) (and Ubuntu)\r\n",
      "- TensorFlow installed from (source or binary): binary\r\n",
      "\r\n",
      "```\r\n",
      "== check python ===================================================\r\n",
      "python version: 3.6.9\r\n",
      "python branch: \r\n",
      "python build version: ('default', 'Oct  8 2020 12:12:24')\r\n",
      "python compiler version: GCC 8.4.0\r\n",
      "python implementation: CPython\r\n",
      "\r\n",
      "== check os platform ===============================================\r\n",
      "os: Linux\r\n",
      "os kernel version: #1 SMP Debian 4.19.160-2 (2020-11-28)\r\n",
      "os release version: 4.19.0-13-amd64\r\n",
      "os platform: Linux-4.19.0-13-amd64-x86_64-with-Ubuntu-18.04-bionic\r\n",
      "linux distribution: ('Ubuntu', '18.04', 'bionic')\r\n",
      "linux os distribution: ('Ubuntu', '18.04', 'bionic')\r\n",
      "mac version: ('', ('', '', ''), '')\r\n",
      "uname: uname_result(system='Linux', node='90e48bf24072', release='4.19.0-13-amd64', version='#1 SMP Debian 4.19.160-2 (2020-11-28)', machine='x86_64', processor='x86_64')\r\n",
      "architecture: ('64bit', 'ELF')\r\n",
      "machine: x86_64\r\n",
      "\r\n",
      "== are we in docker =============================================\r\n",
      "Yes\r\n",
      "\r\n",
      "== compiler =====================================================\r\n",
      "c++ (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\r\n",
      "Copyright (C) 2017 Free Software Foundation, Inc.\r\n",
      "This is free software; see the source for copying conditions.  There is NO\r\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n",
      "\r\n",
      "== check pips ===================================================\r\n",
      "numpy                  1.19.4\r\n",
      "protobuf               3.14.0\r\n",
      "tensorflow             2.4.0\r\n",
      "tensorflow-estimator   2.4.0rc0\r\n",
      "\r\n",
      "== check for virtualenv =========================================\r\n",
      "False\r\n",
      "\r\n",
      "== tensorflow installed from info ==================\r\n",
      "Name: tensorflow\r\n",
      "Version: 2.4.0\r\n",
      "Summary: TensorFlow is an open source machine learning framework for everyone.\r\n",
      "Home-page: https://www.tensorflow.org/\r\n",
      "Author-email: packages@tensorflow.org\r\n",
      "License: Apache 2.0\r\n",
      "Location: /usr/local/lib/python3.6/dist-packages\r\n",
      "Required-by: \r\n",
      "\r\n",
      "== python version  ==============================================\r\n",
      "(major, minor, micro, releaselevel, serial)\r\n",
      "(3, 6, 9, 'final', 0)\r\n",
      "```\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "import tensorflow fails with \"Illegal Instruction\" on a CPU with AVX\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "according to the documentation (https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements), all CPU with AVX are supported, so the import should not fail\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "python3\r\n",
      "`>>> import tensorflow`\r\n",
      "\r\n",
      "**Other info / logs**\r\n",
      "The problem comes from an instruction from the \"BMI2\" extension. Output from GDB : \r\n",
      "```\r\n",
      "Program received signal SIGILL, Illegal instruction.\r\n",
      "0x00007fb5fe658c48 in tensorflow::UnaryVariantOpRegistry::RegisterDeviceCopyFn(tensorflow::VariantDeviceCopyDirection, tensorflow::TypeIndex const&, std::function<tensorflow::Status (tensorflow::Variant const&, tensorflow::Variant*, std::function<tensorflow::Status (tensorflow::Tensor const&, tensorflow::Tensor*)>)> const&) () from /usr/local/lib/python3.6/dist-packages/tensorflow/python/../libtensorflow_framework.so.2\r\n",
      "```\r\n",
      "\r\n",
      "Which points to instruction **SHLX**.\r\n",
      "\r\n",
      "Some older CPUs do have AVX, but not BMI2 : \r\n",
      "- Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz\r\n",
      "- Intel(R) Core(TM) i3-2120 CPU @ 3.30GHz\r\n",
      "\r\n",
      "example output of lscpu : \r\n",
      "`Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm cpuid_fault epb pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase smep erms xsaveopt dtherm ida arat pln pts md_clear flush_l1d`\r\n",
      "\r\n",
      "Best Regards\n",
      "issue labels - \n",
      "TF 2.4\n",
      "stat:awaiting response\n",
      "subtype: ubuntu/linux\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  How to read .bin file using Tensorflow C++ api?\n",
      "issue body -  Hey all,\r\n",
      "\r\n",
      "I need to read lidar point cloud from .bin folder. But I could not able to find an API in c++ to achieve my task. could anyone help me with this? \r\n",
      "\r\n",
      "Tensorflow version 1.4.0\r\n",
      "\r\n",
      "Thanks in advance.\r\n",
      "\n",
      "issue labels - \n",
      "TF 1.14\n",
      "comp:runtime\n",
      "stat:awaiting response\n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  Fix some message typos\n",
      "issue body -  absense -> absence\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:lite\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Tensorflow 2.4  CUDA 11   CUDA_ERROR_LAUNCH_FAILED\n",
      "issue body -  ------------------------\r\n",
      "\r\n",
      "### System information\r\n",
      "\r\n",
      "-   **Have I written custom code (as opposed to using a stock example script\r\n",
      "    provided in TensorFlow)**: No\r\n",
      "-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Win10  -2004\r\n",
      "-   **TensorFlow installed from (source or binary)**: binary\r\n",
      "-   **TensorFlow version (use command below)**:  2.4\r\n",
      "-   **Python version**: 3.6.1\r\n",
      "-   **CUDA/cuDNN version**: CUDA 11.0 -  cudNN 8.0.4\r\n",
      "-   **GPU model and memory**: GTX 1660 Super 6GB (+ GT 730 2GB) with 456.55 driver.\r\n",
      "-   **Exact command to reproduce**: py -3.6 generate.py\r\n",
      "\r\n",
      "### Describe the problem\r\n",
      "Hello, I have had a problem with tensorflow and cuda for days ... the program works with the CPU but I would like to use the latter with my GPU. However, the program refuses to end. the program starts but as soon as it arrives at 5/54 or 6/54 it crashes with the message bellow.\r\n",
      "I tried to change the version of cuda + cudnn, change the driver version of my gpu but nothing works, I still find this error.\r\n",
      "The program is [here](https://github.com/Migateak/tensorflowtextgen) with data, it's not mine but it's for a generation of auto text.\r\n",
      "\r\n",
      "### Source code / logs\r\n",
      "\r\n",
      "> Epoch 1/50\r\n",
      ">  5/54 [=>............................] - ETA: 18s - loss: 4.20602020-12-27 12:11:08.578867: E tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_FAILED: \r\n",
      "> unspecified launch failure\r\n",
      "> 2020-12-27 12:11:08.578954: E tensorflow/stream_executor/dnn.cc:616] CUDNN_STATUS_EXECUTION_FAILED\r\n",
      "> in tensorflow/stream_executor/cuda/cuda_dnn.cc(1972): 'cudnnRNNBackwardData( cudnn.handle(), rnn_desc.handle(), model_dims.max_seq_length, output_desc.handles(), output_data.opaque(), output_desc.handles(), output_backprop_data.opaque(), output_h_desc.handle(), output_h_backprop_data.opaque(), output_c_desc.handle(), output_c_backprop_data.opaque(), rnn_desc.params_handle(), params.opaque(), input_h_desc.handle(), input_h_data.opaque(), input_c_desc.handle(), input_c_data.opaque(), input_desc.handles(), input_backprop_data->opaque(), input_h_desc.handle(), input_h_backprop_data->opaque(), input_c_desc.handle(), input_c_backprop_data->opaque(), workspace.opaque(), workspace.size(), reserve_space_data->opaque(), reserve_space_data->size())'\r\n",
      "> 2020-12-27 12:11:08.579262: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:220] Unexpected Event status: 1\r\n",
      "> 2020-12-27 12:11:08.579796: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at cudnn_rnn_ops.cc:1926 : Internal: Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 1024, 1024, 1, 200, 64, 1024]\r\n",
      "> \n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:gpu\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  TFLite Benchmark Binary Erroring with FastSpeech Model\n",
      "issue body -  <em>Please make sure that this is a bug. As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:bug_template</em>\r\n",
      "\r\n",
      "I used [nightly pre-built binary](https://storage.googleapis.com/tensorflow-nightly-public/prod/tensorflow/release/lite/tools/nightly/latest/android_aarch64_benchmark_model_plus_flex) from [here](https://www.tensorflow.org/lite/performance/measurement#native_benchmark_binary)\r\n",
      "\r\n",
      "FastSpeech Model used for benchmark can be downloaded from [here](https://github.com/tulasiram58827/TTS_TFLite/blob/main/models/fastspeech_quant.tflite)\r\n",
      "\r\n",
      "Error : \r\n",
      "\r\n",
      "```\r\n",
      "The input model file size (MB): 31.1646\r\n",
      " Initialized session in 13.661ms.\r\n",
      "Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\r\n",
      "ERROR: tensorflow/lite/kernels/expand_dims.cc:40 axis <= input_dims.size was not true.\r\n",
      "ERROR: Node number 553 (EXPAND_DIMS) failed to prepare.\r\n",
      "\r\n",
      "ERROR: tensorflow/lite/kernels/expand_dims.cc:40 axis <= input_dims.size was not true.\r\n",
      "ERROR: Node number 553 (EXPAND_DIMS) failed to prepare.\r\n",
      "\r\n",
      "ERROR: tensorflow/lite/kernels/expand_dims.cc:40 axis <= input_dims.size was not true.\r\n",
      "ERROR: Node number 553 (EXPAND_DIMS) failed to prepare.\r\n",
      "\r\n",
      "ERROR: tensorflow/lite/kernels/expand_dims.cc:40 axis <= input_dims.size was not true.\r\n",
      "ERROR: Node number 553 (EXPAND_DIMS) failed to prepare.\r\n",
      "\r\n",
      "ERROR: tensorflow/lite/kernels/expand_dims.cc:40 axis <= input_dims.size was not true.\r\n",
      "```\r\n",
      "\r\n",
      "After this error I converted the model with fixed size and I got this error.\r\n",
      "\r\n",
      "```\r\n",
      "The input model file size (MB): 31.1646\r\n",
      "Initialized session in 13.661ms.\r\n",
      "Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\r\n",
      "ERROR: tensorflow/lite/kernels/range.cc:45 (start >= limit && delta < 0) || (start <= limit && delta > 0) was not true.\r\n",
      "ERROR: Node number 477 (RANGE) failed to invoke.\r\n",
      "\r\n",
      "ERROR: tensorflow/lite/kernels/range.cc:45 (start >= limit && delta < 0) || (start <= limit && delta > 0) was not true.\r\n",
      "ERROR: Node number 477 (RANGE) failed to invoke.\r\n",
      "\r\n",
      "ERROR: tensorflow/lite/kernels/range.cc:45 (start >= limit && delta < 0) || (start <= limit && delta > 0) was not true.\r\n",
      "ERROR: Node number 477 (RANGE) failed to invoke.\r\n",
      "\r\n",
      "ERROR: tensorflow/lite/kernels/range.cc:45 (start >= limit && delta < 0) || (start <= limit && delta > 0) was not true.\r\n",
      "ERROR: Node number 477 (RANGE) failed to invoke.\r\n",
      "```\r\n",
      "\n",
      "issue labels - \n",
      "comp:lite\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Install Error \"github.com/tensorflow/examples\"\n",
      "issue body -  Would you tell me?\r\n",
      "I want install \"tensorflow example\" from Anaconda for windows10.\r\n",
      "\r\n",
      "However, no matter how many times I repeat it, I get an error.\r\n",
      "\r\n",
      "Installed anaconda3\r\n",
      "\r\n",
      "**Procedure so far\r\n",
      "conda create -n insta1\r\n",
      "conda activate insta1\r\n",
      "conda install python=3.7.5\r\n",
      "git clone https://github.com/tensorflow/examples\r\n",
      "(insta1) C:\\installation\\insta2020-01\\examples\\tensorflow_examples\\lite\\model_maker\\pip_package>pip install -e .\r\n",
      "\r\n",
      "**Error contents\r\n",
      "---------------------------------\r\n",
      "(insta1) C:\\installation\\insta2020-01\\examples\\tensorflow_examples\\lite\\model_maker\\pip_package>pip install -e .\r\n",
      "Obtaining file:///C:/installation/insta2020-01/examples/tensorflow_examples/lite/model_maker/pip_package\r\n",
      "    ERROR: Command errored out with exit status 1:\r\n",
      "     command: 'C:\\anaconda3\\python.exe' -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\installation\\\\insta2020-01\\\\examples\\\\tensorflow_examples\\\\lite\\\\model_maker\\\\pip_package\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\installation\\\\insta2020-01\\\\examples\\\\tensorflow_examples\\\\lite\\\\model_maker\\\\pip_package\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base 'C:\\Users\\toshi\\AppData\\Local\\Temp\\pip-pip-egg-info-su_59rf8'\r\n",
      "         cwd: C:\\installation\\insta2020-01\\examples\\tensorflow_examples\\lite\\model_maker\\pip_package\\\r\n",
      "    Complete output (11 lines):\r\n",
      "    Traceback (most recent call last):\r\n",
      "      File \"<string>\", line 1, in <module>\r\n",
      "      File \"C:\\installation\\insta2020-01\\examples\\tensorflow_examples\\lite\\model_maker\\pip_package\\setup.py\", line 232, in <module>\r\n",
      "        setup_extra = prepare_package_src()\r\n",
      "      File \"C:\\installation\\insta2020-01\\examples\\tensorflow_examples\\lite\\model_maker\\pip_package\\setup.py\", line 188, in prepare_package_src\r\n",
      "        namespace_packages = find_namespace_packages(where=BUILD_ROOT)\r\n",
      "      File \"C:\\anaconda3\\lib\\site-packages\\setuptools\\__init__.py\", line 64, in find\r\n",
      "        convert_path(where),\r\n",
      "      File \"C:\\anaconda3\\lib\\distutils\\util.py\", line 121, in convert_path\r\n",
      "        if pathname[0] == '/':\r\n",
      "    TypeError: 'WindowsPath' object is not subscriptable\r\n",
      "    ----------------------------------------\r\n",
      "ERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\n",
      "issue labels - \n",
      "stalled\n",
      "stat:awaiting response\n",
      "subtype:windows\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  Parallel execution of ops on tensors in unstacked list \n",
      "issue body -  <em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n",
      "\r\n",
      "\r\n",
      "**System information**\r\n",
      "- TensorFlow version (you are using): 2.3.1\r\n",
      "- Are you willing to contribute it (Yes/No): No\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**Describe the feature and the current behavior/state.**\r\n",
      "Currently, mapping/vector operations execute only batches in parallel, implying that if an op has to be performed on multiple tensors in parallel, all of them should be stacked, and therefore have the same shape.\r\n",
      "This feature will allow execution of all tensors in a python iterable in parallel, without requiring them to be stacked. It will be analogous to the `map` function in python.\r\n",
      "\r\n",
      "**Will this change the current api? How?**\r\n",
      "This will add a new function to the API, accepting two arguments: A callable and a python iterable of tensors, not necessarily of the same shape.\r\n",
      "The callable takes a single argument, which can be a single or nested tensor.\r\n",
      "\r\n",
      "**Who will benefit with this feature?**\r\n",
      "This feature has a wide range of applications, and can benefit the entire community.\r\n",
      "\r\n",
      "**Any Other info.**\r\n",
      "\n",
      "issue labels - \n",
      "comp:ops\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:feature\n",
      "\n",
      "\n",
      "issue title -  Simplify GPU usage model\n",
      "issue body -  <em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n",
      "\r\n",
      "\r\n",
      "**System information**\r\n",
      "- TensorFlow version (you are using): 2.1.0 - Python\r\n",
      "- Are you willing to contribute it (Yes/No): Yes\r\n",
      "\r\n",
      "\r\n",
      "**Describe the feature and the current behavior/state.**\r\n",
      "IMO many new developers are using TF as it supports CUDA, and they have a single GPU they want to use. So one common use case is a single GPU. To utilize a GPU device script writers are using the following code \r\n",
      "\r\n",
      "```python\r\n",
      "physical_devices = tf.config.list_physical_devices('GPU')\r\n",
      "tf.config.experimental.set_memory_growth(physical_devices[0], True)\r\n",
      "```\r\n",
      "\r\n",
      "Can we have a simple API that enables the GPU first device ?\r\n",
      "\r\n",
      "proposed API\r\n",
      "\r\n",
      "experimental:\r\n",
      "```python\r\n",
      "configure_default_device(string physical_device_name, int device_number)\r\n",
      "get_default_device()\r\n",
      "```\r\n",
      "\r\n",
      "example usage:\r\n",
      "```python\r\n",
      "tf.config.experimental.configure_default_device('GPU', 0)\r\n",
      "\r\n",
      "get_default_device()\r\n",
      "     example settings above would return\r\n",
      "                  \"GPU\" device 0\r\n",
      "\r\n",
      "```\r\n",
      "**Will this change the current api? How?**\r\n",
      "Yes\r\n",
      "Simplify a common use case, and abstract away these settings so they can be changed in future without developers having to re-write their scripts ie when moving to later TF version\r\n",
      "\r\n",
      "**Who will benefit with this feature?**\r\n",
      "Many new developers with a single GPU\r\n",
      "\r\n",
      "**Any Other info.**\r\n",
      "It would be good to extend the API to handle specific GPU devices, or to set the engine to use all of them. Another API would show the current settings.\n",
      "issue labels - \n",
      "comp:gpu\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Running custom tflite model, segfault only on CPU, tflite 2.4.0 built from sources\n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android 8.1\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Meizu 16th\r\n",
      "- TensorFlow installed from (source or binary): source\r\n",
      "- TensorFlow version (use command below): 2.4.0, also 5a8dc94c30a\r\n",
      "- Python version: no \r\n",
      "- Bazel version (if compiling from source): 3.7.2\r\n",
      "- GCC/Compiler version (if compiling from source): 7.5.0\r\n",
      "- CUDA/cuDNN version: no\r\n",
      "- GPU model and memory: no\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "trying to run custom [converted](https://github.com/pb-julian/liteface) tflite [model](https://www.dropbox.com/s/akxeqp99jvsd6z7/model-MobileFaceNet-arcface-ms1m-refine-v1.zip?dl=0) from insigthface on android. It works with standard tflite.aar from jcenter. Also works with built from sources libtensorflowlite_gpu_delegate.so. But when i tried to run on CPU with built from source libtensorflowlite.so, i got segfault.\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "Expected to work with manually built libtensorflowlite.so\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "- *.so files was built with docker from repo and [guide](https://www.tensorflow.org/lite/guide/build_android) in site.\r\n",
      "- libtensorflowlite.so was built with commands: `bazel build tensorflow/lite:libtensorflowlite.so --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --config=android --cpu=arm64-v8a` and  `bazel build tensorflow/lite:libtensorflowlite.so --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --config=android --cpu=armeabi-v7a`\r\n",
      "- code for running was adapted from [sample of usage](https://www.tensorflow.org/lite/guide/inference#load_and_run_a_model_in_c)\r\n",
      "\r\n",
      "**Other info / logs** Include any logs or source code that would be helpful to\r\n",
      "traceback:\r\n",
      "<img width=\"1250\" alt=\"Снимок экрана 2020-12-27 в 05 13 36\" src=\"https://user-images.githubusercontent.com/32731602/103161348-56e64c80-4802-11eb-87de-e40365ea69e0.png\">\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:lite\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  TF2.3; tensorflow.python.framework.errors_impl.UnknownError: Failed to rename; : Access is denied. ; Input/output error\n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n",
      "- TensorFlow installed from (source or binary): Conda; TF 2.3; \r\n",
      "- TensorFlow version (use command below): 2.3\r\n",
      "- Python version: 3.7.9\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version:\r\n",
      "- GPU model and memory:\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "Stackoverflow: https://stackoverflow.com/questions/65461750/tensorflow-python-framework-errors-impl-unknownerror-failed-to-rename-access\r\n",
      "I am not able to download and load tensorflow dataset on my Windows 10 machine. It works okay on Google colab. \r\n",
      "\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "I should be able to download the datasets. \r\n",
      "\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "\r\n",
      "```\r\n",
      "import tensorflow_datasets as tfds\r\n",
      "datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True\r\n",
      "```\r\n",
      "\r\n",
      "**Error:**\r\n",
      "\r\n",
      "\r\n",
      "```\r\n",
      "\r\n",
      "Writing...:   0%|          | 0/2500 [00:00<?, ? examples/s]\r\n",
      "Shuffling...:  90%|█████████ | 18/20 [00:01<00:00, 14.15 shard/s]\r\n",
      "Reading...: 0 examples [00:00, ? examples/s]\r\n",
      "                                            \r\n",
      "Writing...:   0%|          | 0/2500 [00:00<?, ? examples/s]\r\n",
      "                                                           \r\n",
      "Reading...: 0 examples [00:00, ? examples/s]\r\n",
      "                                            \r\n",
      "Writing...:   0%|          | 0/2500 [00:00<?, ? examples/s]\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"C:\\Anaconda3\\envs\\ml_tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3418, in run_code\r\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\r\n",
      "  File \"<ipython-input-2-3b586bfe81d7>\", line 3, in <module>\r\n",
      "    datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)\r\n",
      "  File \"C:\\Anaconda3\\envs\\ml_tf\\lib\\site-packages\\tensorflow_datasets\\core\\api_utils.py\", line 52, in disallow_positional_args_dec\r\n",
      "    return fn(*args, **kwargs)\r\n",
      "  File \"C:\\Anaconda3\\envs\\ml_tf\\lib\\site-packages\\tensorflow_datasets\\core\\registered.py\", line 300, in load\r\n",
      "    dbuilder.download_and_prepare(**download_and_prepare_kwargs)\r\n",
      "  File \"C:\\Anaconda3\\envs\\ml_tf\\lib\\site-packages\\tensorflow_datasets\\core\\api_utils.py\", line 52, in disallow_positional_args_dec\r\n",
      "    return fn(*args, **kwargs)\r\n",
      "  File \"C:\\Anaconda3\\envs\\ml_tf\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py\", line 307, in download_and_prepare\r\n",
      "    self.info.write_to_directory(self._data_dir)\r\n",
      "  File \"C:\\Anaconda3\\envs\\ml_tf\\lib\\contextlib.py\", line 119, in __exit__\r\n",
      "    next(self.gen)\r\n",
      "  File \"C:\\Anaconda3\\envs\\ml_tf\\lib\\site-packages\\tensorflow_datasets\\core\\file_format_adapter.py\", line 200, in incomplete_dir\r\n",
      "    tf.io.gfile.rename(tmp_dir, dirname)\r\n",
      "  File \"C:\\Anaconda3\\envs\\ml_tf\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\", line 546, in rename_v2\r\n",
      "    compat.as_bytes(src), compat.as_bytes(dst), overwrite)\r\n",
      "tensorflow.python.framework.errors_impl.UnknownError: Failed to rename: C:\\Users\\User\\tensorflow_datasets\\imdb_reviews\\plain_text\\0.1.0.incomplete5JQVCL to: C:\\Users\\User\\tensorflow_datasets\\imdb_reviews\\plain_text\\0.1.0 : Access is denied.\r\n",
      "; Input/output error\r\n",
      "```\r\n",
      "\r\n",
      "**Conda list**\r\n",
      "\r\n",
      "```\r\n",
      "tensorboard               2.3.0              pyh4dce500_0\r\n",
      "tensorboard-plugin-wit    1.6.0                      py_0\r\n",
      "tensorflow                2.3.0           mkl_py37h3bad0a6_0\r\n",
      "tensorflow-base           2.3.0           eigen_py37h17acbac_0\r\n",
      "tensorflow-datasets       1.2.0                    py37_0\r\n",
      "tensorflow-estimator      2.3.0              pyheb71bc4_0\r\n",
      "tensorflow-metadata       0.14.0             pyhe6710b0_1\r\n",
      "tensorflow-mkl            2.3.0                h93d2e19_0\r\n",
      "```\n",
      "issue labels - \n",
      "TF 2.3\n",
      "comp:apis\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  tf.keras.preprocessing.image.save_img() flips dimensions of images created from tf.keras.preprocessing.image_dataset_from_directory()\n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Big Sur 11.1\r\n",
      "- TensorFlow installed from (source or binary): pip\r\n",
      "- TensorFlow version (use command below): v2.4.0-rc4-71-g582c8d236cb 2.4.0\r\n",
      "- Python version: 3.8.6\r\n",
      "- CUDA/cuDNN version: n/a\r\n",
      "- GPU model and memory: Intel Iris 1536 MB\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "\r\n",
      "-> A directory hierarchy with the following format is created\r\n",
      "\r\n",
      "/Project\r\n",
      "|-- data\r\n",
      "|    -- class-1\r\n",
      "|        -- image1.jpg\r\n",
      "|        -- image2.jpg\r\n",
      "|        -- ...\r\n",
      "|    -- class-2\r\n",
      "|        -- image1.jpg\r\n",
      "|        -- image2.jpg\r\n",
      "|-- venv (tensorflow, pillow)\r\n",
      "|    -- ...\r\n",
      "|-- bug.py\r\n",
      "\r\n",
      "in which the images have uneven dimensions (ie. 640x360 as opposed to 256x256)\r\n",
      "\r\n",
      "-> A dataset is imported from a directory using tf.keras.preprocessing.image_dataset_from_directory()\r\n",
      "-> A single image is taken from the dataset using next(dataset)\r\n",
      "-> The image is saved using tf.keras.preprocessing.image.save_img()\r\n",
      "\r\n",
      "**Unexpected Behavior:** the resulting image has dimensions 360x640 instead of 640x360 and is heavily warped\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "\r\n",
      "The image should have the same dimensions as when it was imported ie. 640x360\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "Provide a reproducible test case that is the bare minimum necessary to generate\r\n",
      "the problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n",
      "\r\n",
      "Github repo (couldn't figure out how to get it into a Jupyter notebook)\r\n",
      "https://github.com/atw1020/tensorflowBug\r\n",
      "\r\n",
      "**Other info / logs**\r\n",
      "\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:keras\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  MaskRCNN TensorFlow Lite Inference Issue. No output from TFLite Model.\n",
      "issue body -  **System information**\r\n",
      "- OS Platform and Distribution ( Ubuntu 18.04.5 LTS (GNU/Linux 5.4.0-1034-azure x86_64)):\r\n",
      "- TensorFlow installed from (source- Pip Install):\r\n",
      "- TensorFlow version (2.3.0):\r\n",
      "\r\n",
      "\r\n",
      "**Command used to run the converter**\r\n",
      "```\r\n",
      "converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\r\n",
      "converter.allow_custom_ops = True\r\n",
      "converter.experimental_new_converter = True\r\n",
      "converter.target_spec.supported_ops = [\r\n",
      "    tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\r\n",
      "    tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\r\n",
      "]\r\n",
      "\r\n",
      "converter.optimizations = [ tf.lite.Optimize.DEFAULT ]\r\n",
      "\r\n",
      "tflite_model = converter.convert()\r\n",
      "```\r\n",
      "\r\n",
      "**link to Jupyter notebook and tflite model**\r\n",
      "\r\n",
      "https://drive.google.com/drive/folders/1pTB33fTSo5ENzevobTvuG7hN4YmiCPF_?usp=sharing\r\n",
      "\r\n",
      "\r\n",
      "**Commands used for inference**\r\n",
      "```\r\n",
      "### Load the TFLite model and allocate tensors.\r\n",
      "interpreter = tf.lite.Interpreter(model_path=\"model_2.3.tflite\")\r\n",
      "interpreter.allocate_tensors()\r\n",
      "\r\n",
      "### Get input and output tensors.\r\n",
      "input_details = interpreter.get_input_details()\r\n",
      "output_details = interpreter.get_output_details()\r\n",
      "\r\n",
      "### Test the model on random input data.\r\n",
      "input_data_1 = np.array(np.random.random_sample(input_details[0]['shape']), dtype=np.float32)\r\n",
      "input_data_2 = np.array(np.random.random_sample(input_details[1]['shape']), dtype=np.float32)\r\n",
      "input_data_3 = np.array(np.random.random_sample(input_details[2]['shape']), dtype=np.float32)\r\n",
      "\r\n",
      "interpreter.set_tensor(input_details[0]['index'], input_data_1)\r\n",
      "interpreter.set_tensor(input_details[1]['index'], input_data_2)\r\n",
      "interpreter.set_tensor(input_details[2]['index'], input_data_3)\r\n",
      "\r\n",
      "interpreter.invoke() ---> Kernel is getting stuck here. No output. I am executing the code from jupyter.\r\n",
      "\r\n",
      "```\r\n",
      "\r\n",
      "**The output from the converter invocation**\r\n",
      "\r\n",
      "No output in Jupyter. \r\n",
      "\r\n",
      "Segmentation fault (core dumped) -- When executed in command line.\r\n",
      "\r\n",
      "**Failure details**\r\n",
      "\r\n",
      "Conversion is successful. But there is no output from model.\r\n",
      "\r\n",
      "Could you guys please provide some ideas? I am stuck here and don't know how to proceed!\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.3\n",
      "TFLiteConverter\n",
      "comp:keras\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -   No OpKernel was registered to support Op 'VarHandleOp' used by {{node dense/kernel}}with these attrs: [shape=[1,1], shared_name=\"dense/kernel\", _class=[\"loc:@dense/kernel\"], dtype=DT_FLOAT, container=\"\"]\n",
      "issue body -  I have created a tensorflow 1.13.1 model and try to used it in an android app. When I \"init\" the variables running   sess.runner().addTarget(\"init\").run(); I got this error: No OpKernel was registered to support Op 'VarHandleOp' used by {{node dense/kernel}}with these attrs: [shape=[1,1], shared_name=\"dense/kernel\", _class=[\"loc:@dense/kernel\"], dtype=DT_FLOAT, container=\"\"]\r\n",
      "\r\n",
      "This is the code I used to create the graph.pb file:\r\n",
      "\r\n",
      "model = tf.keras.models.Sequential([tf.keras.layers.Dense(1, input_shape=(1, )),\r\n",
      "tf.keras.layers.Dense(25, activation=tf.keras.activations.relu),  \r\n",
      "tf.keras.layers.Dense(1, activation=tf.keras.activations.relu)])\r\n",
      "\r\n",
      "model.compile(optimizer=tf.keras.optimizers.Adam(),loss=tf.keras.losses.mean_squared_error)\r\n",
      "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\r\n",
      "\r\n",
      "init = tf.global_variables_initializer()\r\n",
      "saver_def = tf.train.Saver().as_saver_def()\r\n",
      "with open('graph.pb', 'wb') as f:\r\n",
      "  f.write(tf.get_default_graph().as_graph_def().SerializeToString())\r\n",
      "\r\n",
      "print('Operation to initialize variables:       ', init.name)\r\n",
      "print('Tensor to be fed for checkpoint filename:', saver_def.filename_tensor_name)\r\n",
      "print('Operation to save a checkpoint:          ', saver_def.save_tensor_name)\r\n",
      "print('Operation to restore a checkpoint:       ', saver_def.restore_op_name)\r\n",
      "print('Trainable variables: ', tf.trainable_variables())\r\n",
      "\r\n",
      "Operation to initialize variables:        init\r\n",
      "Tensor to be fed for checkpoint filename: save/Const:0\r\n",
      "Operation to save a checkpoint:           save/control_dependency:0\r\n",
      "Operation to restore a checkpoint:        save/restore_all\r\n",
      "Trainable variables:  [<tf.Variable 'dense/kernel:0' shape=(1, 1) dtype=float32>, <tf.Variable 'dense/bias:0' shape=(1,) dtype=float32>, <tf.Variable 'dense_1/kernel:0' shape=(1, 25) dtype=float32>, <tf.Variable 'dense_1/bias:0' shape=(25,) dtype=float32>, <tf.Variable 'dense_2/kernel:0' shape=(25, 1) dtype=float32>, <tf.Variable 'dense_2/bias:0' shape=(1,) dtype=float32>]\r\n",
      "\r\n",
      "The tensorflow android version is:  'org.tensorflow:tensorflow-android:1.13.1'\r\n",
      "\r\n",
      "It worked with other linnear regression model but I dont know what is wrong,\r\n",
      "\r\n",
      "Regards,\r\n",
      "Alejandro.\n",
      "issue labels - \n",
      "TF 1.13\n",
      "comp:keras\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  tf.debugging.assert_type raising error for wrong reason\n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n",
      "- TensorFlow installed from (source or binary): binary (pip install)\r\n",
      "- TensorFlow version (use command below): 2.3.1\r\n",
      "- Python version: 3.7\r\n",
      "- Bazel version (if compiling from source): n/a\r\n",
      "- GCC/Compiler version (if compiling from source): n/a\r\n",
      "- CUDA/cuDNN version: n/a\r\n",
      "- GPU model and memory: n/a\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "```\r\n",
      ">>> tf.debugging.assert_type(tf.constant(0.0), tf_type=(tf.float32,))\r\n",
      "TypeError:  tensor must be of type (tf.float32,)\r\n",
      "```\r\n",
      "is incorrect.\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "Error should be raised because of incorrect argument type for `tf_type` (which needs to be a tf float type, not an iterable of them), not because tensor is not of type `(tf.float32,)`\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:apis\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  grpc 1.32.0 doesn't support python 3.9 in windows 10, dependency should be updated to 1.34.0\n",
      "issue body -  <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n",
      "- TensorFlow installed from (source or binary): source\r\n",
      "- TensorFlow version: 2.4.0\r\n",
      "- Python version: 3.9.1\r\n",
      "- Installed using virtualenv? pip? conda?: N/A\r\n",
      "- Bazel version (if compiling from source): 3.7.2\r\n",
      "- GCC/Compiler version (if compiling from source):  Visual Studio 2019\r\n",
      "- CUDA/cuDNN version: 11.2 / 8.0.5\r\n",
      "- GPU model and memory:\r\n",
      "RTX 3070 GDDR6 8GB\r\n",
      "\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "cannot build grpcio 1.32.0\r\n",
      "**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n",
      "bazel build and install\r\n",
      "\r\n",
      "**Any other info / logs**\r\n",
      "Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n",
      "\r\n",
      "grpcio 1.32.0 does not support python 3.9.1 in Windows 10\r\n",
      "\r\n",
      "tensorflow dependency should be updated as with grpcio 1.34.0\n",
      "issue labels - \n",
      "TF 2.4\n",
      "subtype:windows\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  Android Benchmark tool Build Failed\n",
      "issue body -  <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "- TensorFlow installed from (source or binary): Source\r\n",
      "- TensorFlow version: master branch\r\n",
      "- Python version: 3.8\r\n",
      "- Installed using virtualenv? pip? conda?:\r\n",
      "- Bazel version (if compiling from source): 3.1.0\r\n",
      "- GCC/Compiler version (if compiling from source): 7.5.0\r\n",
      "- CUDA/cuDNN version:\r\n",
      "- GPU model and memory: GTX 1660 and 6GB RAM\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "\r\n",
      "Build failed when building android benchmark tool for tflite models.\r\n",
      "\r\n",
      "**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n",
      "I just ran this command \r\n",
      "\r\n",
      "> bazel build -c opt --config=android_arm64 tensorflow/lite/tools/benchmark/android:benchmark_model\r\n",
      "\r\n",
      "**Any other info / logs**\r\n",
      "> INFO: Options provided by the client:\r\n",
      "  Inherited 'common' options: --isatty=1 --terminal_columns=204\r\n",
      "INFO: Reading rc options for 'build' from /home/ram/tensorflow/.bazelrc:\r\n",
      "  Inherited 'common' options: --experimental_repo_remote_exec\r\n",
      "INFO: Reading rc options for 'build' from /home/ram/tensorflow/.bazelrc:\r\n",
      "  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2\r\n",
      "INFO: Found applicable config definition build:short_logs in file /home/ram/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\n",
      "INFO: Found applicable config definition build:v2 in file /home/ram/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\n",
      "INFO: Found applicable config definition build:android_arm64 in file /home/ram/tensorflow/.bazelrc: --config=android --cpu=arm64-v8a --fat_apk_cpu=arm64-v8a\r\n",
      "INFO: Found applicable config definition build:android in file /home/ram/tensorflow/.bazelrc: --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --noenable_platform_specific_config --copt=-w --cxxopt=-std=c++14 --host_cxxopt=-std=c++14\r\n",
      "ERROR: /home/ram/.cache/bazel/_bazel_ram/a1828cafbf783c13dba0538b4a59550c/external/local_config_cc/BUILD:47:1: in cc_toolchain_suite rule @local_config_cc//:toolchain: cc_toolchain_suite '@local_config_cc//:toolchain' does not contain a toolchain for cpu 'arm64-v8a'\r\n",
      "ERROR: Analysis of target '//tensorflow/lite/tools/benchmark/android:benchmark_model' failed; build aborted: Analysis of target '@local_config_cc//:toolchain' failed\r\n",
      "INFO: Elapsed time: 0.180s\r\n",
      "INFO: 0 processes.\r\n",
      "FAILED: Build did NOT complete successfully (2 packages loaded, 1 target configured)\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "comp:lite\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  Attempting to use uninitialized value \n",
      "issue body -  Hello, \r\n",
      "I have a trained tensorflow model which is trained by tensorflow v1.14 and the model is saved as meta graph model. What I'm trying to do is to convert this model to Keras since I cannot work with the model with SHAP package. The Shap package doesn't support the meta graph tensorflow model. So I want to convert it to Keras. I have done this with another TF model with VGG architecture and this works perfectly fine.  For this purpose I create the architecture in Keras and then I set the weight and bias values to the new model. \r\n",
      "Now I have a trained model with Resnet-18 architecture. But the problem is when I want to get the weights I get this error: \r\n",
      "\r\n",
      "Attempting to use uninitialized value scale2/block1/A/weights\r\n",
      "\t [[{{node _retval_scale2/block1/A/weights_0_0}}]]\r\n",
      "\r\n",
      "This works fine when I want to get the non-nested layer's values. \r\n",
      "\r\n",
      "\r\n",
      "```\r\n",
      "with tf.compat.v1.Session() as sess, tf.device('/device:gpu:0'):\r\n",
      "    saver = tf.compat.v1.train.import_meta_graph('./snapshot-110.meta')\r\n",
      "    saver.restore(sess,tf.train.latest_checkpoint('./'))\r\n",
      "    graph = tf.compat.v1.get_default_graph()\r\n",
      "\r\n",
      "    vars_global = tf.compat.v1.global_variables()\r\n",
      "    sess.as_default()\r\n",
      "    model_vars = {}\r\n",
      "    for var in vars_global:\r\n",
      "        model_vars[var.name] = var.eval()\r\n",
      "```\n",
      "issue labels - \n",
      "TF 1.14\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  Apollo3 project upload issue \n",
      "issue body -  @tensorflow/micro\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Windows 10\r\n",
      "- Arduino IDE\r\n",
      "- TensorFlow Lite 2.1\r\n",
      "- Sparkfun Apollo3 blue\r\n",
      "\r\n",
      "I make all the steps correct from the sparkfun site to i can run the apollo3 from arduino ide.\r\n",
      "\r\n",
      "When i upload the micro speech project into apollo3 from Arduino IDE i get this message #include \"tensorflow/lite/c/c_api_internal.h\"\r\n",
      "How i can resolve this ?\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.1\n",
      "comp:lite\n",
      "comp:micro\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Package build failure\n",
      "issue body -  The package build using \r\n",
      "bazel build //tensorflow/tools/pip_package:build_pip_package\r\n",
      "\r\n",
      "fails with the message: \r\n",
      "\r\n",
      "ERROR: \r\n",
      "//tensorflow/tools/pip_package: licenses depends on @aws//:LICENSE in repository @aws which failed to fetch. no such package '@aws//': java.io.IOException: Error downloading [https://mirror.bazel.build/github.com/aws/aws-sdk-cpp/archive/1.7.336.tar.gz, https://github.com/aws/aws-sdk-cpp/archive/1.7.336.tar.gz] to /private/var/tmp/_bazel_kevinlano/58ba076afb76e123cd689325196ab4ac/external/aws/temp6281506492855719619/1.7.336.tar.gz: Tried to reconnect at offset 15,240,407 but server didn't support it\r\n",
      "\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "stalled\n",
      "stat:awaiting response\n",
      "subtype:bazel\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  tflite android gpu delegate init error, what does it mean?  Init: MUL: 1x1088x1x1  cannot be reduced to linear.\n",
      "issue body -  @tensorflow/micro\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "error log:\r\n",
      "java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: \r\n",
      "TfLiteGpuDelegate Init: MUL: 1x1088x1x1  cannot be reduced to linear.\r\n",
      "TfLiteGpuDelegate Prepare: delegate is not initialized\r\n",
      " Node number 23608 (TfLiteGpuDelegateV2) failed to prepare.\r\n",
      "\r\n",
      "when i run my own model convert from pytorch to TFLite.\r\n",
      "\r\n",
      "device: HUAWEI Mate20\r\n",
      "dependence:\r\n",
      "implementation('org.tensorflow:tensorflow-lite:0.0.0-nightly')\r\n",
      "implementation('org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly')\r\n",
      "implementation('org.tensorflow:tensorflow-lite-support:0.0.0-nightly')\r\n",
      "\r\n",
      "**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n",
      "When I run the model on the android GPU\r\n",
      "\n",
      "issue labels - \n",
      "comp:lite\n",
      "comp:micro\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  sparse_categorical_crossentropy and temporal sample weight\n",
      "issue body -  I am training a (BERT-like) language model with Keras, using _sparse_categorical_crossentropy_ loss. As with any training of language models, I have input sequences of different lengths, meaning that I need to add padding at the ends of most input sequences in order for them to fit a uniform batch size. When it comes to the outputs, however, I do not want to force the model to learn unnecessary padding tokens; rather, I'd just like it to ignore the outputs corresponding to positions where the input has a padding token.\r\n",
      "\r\n",
      "The most sensible solution I was able to think of for doing that was using a _sample_weight_ with zeros at positions corresponding to padding in the input, and ones at all other (actual) positions. This requires setting _sample_weight_mode = 'temporal'_.\r\n",
      "\r\n",
      "The problem is that _sparse_categorical_crossentropy_ doesn't seem to support that, and I get the following error:\r\n",
      "_**ValueError: Found a sample_weight array for an input with shape (8108, 512). Timestep-wise sample weighting (use of sample_weight_mode=\"temporal\") is restricted to outputs that are at least 3D, i.e. that have a time dimension.**_\r\n",
      "\r\n",
      "Is there a way of using temporal sample weight with _sparse_categorical_crossentropy_  in Keras? \r\n",
      "Alternatively, is there another approach I should consider to achieve what I want, or any workaround I could resort to (for making Keras ignore positions with padding in the input during training)?\r\n",
      "\r\n",
      "Thank you!\n",
      "issue labels - \n",
      "TF 2.0\n",
      "comp:keras\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Download dependent 404（Build did NOT complete successfully）\n",
      "issue body -  INFO: Found applicable config definition build:linux in file /home/hortor/Desktop/data1/work/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\n",
      "INFO: Found applicable config definition build:dynamic_kernels in file /home/hortor/Desktop/data1/work/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\n",
      "DEBUG: Rule 'io_bazel_rules_go' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1557349968 -0400\"\r\n",
      "DEBUG: Repository io_bazel_rules_go instantiated at:\r\n",
      "  no stack (--record_rule_instantiation_callstack not enabled)\r\n",
      "Repository rule git_repository defined at:\r\n",
      "  /home/hortor/.cache/bazel/_bazel_hortor/c4d98c77c9b8b7dcb6bae627296ef8a4/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18: in <toplevel>\r\n",
      "DEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\"\r\n",
      "DEBUG: Repository io_bazel_rules_docker instantiated at:\r\n",
      "  no stack (--record_rule_instantiation_callstack not enabled)\r\n",
      "Repository rule git_repository defined at:\r\n",
      "  /home/hortor/.cache/bazel/_bazel_hortor/c4d98c77c9b8b7dcb6bae627296ef8a4/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18: in <toplevel>\r\n",
      "WARNING: Download from https://mirror.bazel.build/github.com/aws/aws-sdk-cpp/archive/1.7.336.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\n",
      "WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/f402e682d0ef5598eeffc9a21a691b03e602ff58.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\n",
      "INFO: Analyzed target //tensorflow/tools/lib_package:libtensorflow (190 packages loaded, 20055 targets configured).\r\n",
      "INFO: Found 1 target...\r\n",
      "ERROR: /home/hortor/.cache/bazel/_bazel_hortor/c4d98c77c9b8b7dcb6bae627296ef8a4/external/aws/BUILD.bazel:12:1: C++ compilation of rule '@aws//:aws' failed (Exit 1): gcc failed: error executing command \r\n",
      "  (cd /home/hortor/.cache/bazel/_bazel_hortor/c4d98c77c9b8b7dcb6bae627296ef8a4/execroot/org_tensorflow && \\\r\n",
      "  exec env - \\\r\n",
      "    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \\\r\n",
      "    PWD=/proc/self/cwd \\\r\n",
      "  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/host/bin/external/aws/_objs/aws/S3Client.pic.d '-frandom-seed=bazel-out/host/bin/external/aws/_objs/aws/S3Client.pic.o' -fPIC -DCURL_STATICLIB -DPLATFORM_LINUX -DENABLE_CURL_CLIENT -DOPENSSL_IS_BORINGSSL -iquote external/aws -iquote bazel-out/host/bin/external/aws -iquote external/aws-c-common -iquote bazel-out/host/bin/external/aws-c-common -iquote external/aws-c-event-stream -iquote bazel-out/host/bin/external/aws-c-event-stream -iquote external/aws-checksums -iquote bazel-out/host/bin/external/aws-checksums -iquote external/boringssl -iquote bazel-out/host/bin/external/boringssl -iquote external/curl -iquote bazel-out/host/bin/external/curl -iquote external/zlib -iquote bazel-out/host/bin/external/zlib -isystem external/aws/aws-cpp-sdk-core/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-core/include -isystem external/aws/aws-cpp-sdk-s3/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-s3/include -isystem external/aws/aws-cpp-sdk-transfer/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-transfer/include -isystem external/aws-c-common/include -isystem bazel-out/host/bin/external/aws-c-common/include -isystem external/aws-c-event-stream/include -isystem bazel-out/host/bin/external/aws-c-event-stream/include -isystem external/aws-checksums/include -isystem bazel-out/host/bin/external/aws-checksums/include -isystem external/boringssl/src/include -isystem bazel-out/host/bin/external/boringssl/src/include -isystem external/curl/include -isystem bazel-out/host/bin/external/curl/include -isystem external/zlib -isystem bazel-out/host/bin/external/zlib -g0 -w '-march=native' -g0 '-std=c++14' -DENABLE_OPENSSL_ENCRYPTION '-DAWS_SDK_VERSION_MAJOR=1' '-DAWS_SDK_VERSION_MINOR=7' '-DAWS_SDK_VERSION_PATCH=266' -DOPENSSL_IS_BORINGSSL -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c external/aws/aws-cpp-sdk-s3/source/S3Client.cpp -o bazel-out/host/bin/external/aws/_objs/aws/S3Client.pic.o)\r\n",
      "Execution platform: @local_execution_config_platform//:platform\r\n",
      "gcc: fatal error: Killed signal terminated program cc1plus\r\n",
      "compilation terminated.\r\n",
      "Target //tensorflow/tools/lib_package:libtensorflow failed to build\r\n",
      "INFO: Elapsed time: 2784.337s, Critical Path: 123.30s\r\n",
      "INFO: 3070 processes: 3070 local.\r\n",
      "FAILED: Build did NOT complete successfully\r\n",
      "\n",
      "issue labels - \n",
      "stalled\n",
      "stat:awaiting response\n",
      "subtype: ubuntu/linux\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  NotFoundError: '_MklMatMul'\n",
      "issue body -  <em>Please make sure that this is a bug. As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:bug_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): \r\n",
      " No\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): \r\n",
      "c0-deeplearning-tf2-2-4-tpu-v20201215-debian-10\r\n",
      "Description\r\n",
      "Google, Deep Learning Image: TensorFlow 2.4, m60 TPU, A debian-10 Linux based image with TensorFlow 2.4 pre-installed.\r\n",
      "Location\r\n",
      "asia (Asia Pacific), eu (European Union), us (United States)\r\n",
      "Labels\r\n",
      "release : m60\r\n",
      "Creation time\r\n",
      "Dec 16, 2020, 5:10:12 PM UTC-08:00\r\n",
      "Family\r\n",
      "tf2-2-4-tpu-debian-10\r\n",
      "Encryption type\r\n",
      "Google managed\r\n",
      "\r\n",
      "You can collect some of this information using our environment capture\r\n",
      "[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n",
      "You can also obtain the TensorFlow version with:\r\n",
      "1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n",
      "2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n",
      "\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "After initializing the tpu - I'm using version 2.4 on the tpu as well, tf.matmul doesn't work anymore.\r\n",
      "\r\n",
      "I'm initializing the tpu with:\r\n",
      "\r\n",
      "```\r\n",
      "def get_strategy():\r\n",
      "\r\n",
      "    try:\r\n",
      "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver('tpu-3')\r\n",
      "        print('Running on TPU ', tpu.master())\r\n",
      "    except:\r\n",
      "        tpu = None\r\n",
      "\r\n",
      "    if tpu:\r\n",
      "        tf.config.experimental_connect_to_cluster(tpu)\r\n",
      "        tf.tpu.experimental.initialize_tpu_system(tpu)\r\n",
      "        strategy = tf.distribute.TPUStrategy(tpu)\r\n",
      "    \r\n",
      "    else:\r\n",
      "        strategy = tf.distribute.get_strategy()\r\n",
      "        for d in tf.config.list_physical_devices():\r\n",
      "            print(d)\r\n",
      "            \r\n",
      "    return strategy\r\n",
      "\r\n",
      "strategy = get_strategy()\r\n",
      "```\r\n",
      "\r\n",
      "Here is the full error:\r\n",
      "\r\n",
      "```\r\n",
      "NotFoundError: '_MklMatMul' is neither a type of a primitive operation nor a name of a function registered in binary running on n-1f5a3a66-w-0. Make sure the operation or function is registered in the binary running in this process.\r\n",
      "```\r\n",
      "\r\n",
      "It works fine on the cpu before the tpu is initialized.\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:tpus\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Programm ends Drasicly\n",
      "issue body -  Works fine with other Code\r\n",
      "\r\n",
      "I try to run a convolution script for my network, whan i was running it on my CPU it was fine, then i installed CUDA and CUDNN and now it stopped working. I tried searching for the issue got a few got a few fixes, but it still dosn't work. Now it only says:\r\n",
      "2020-12-24 14:31:49.491561: I\r\n",
      "and not: W or E ; so i guess thats a good thing?\r\n",
      "\r\n",
      "**System information**\r\n",
      "- OS: Windows 10\r\n",
      "- TensorFlow: 2.4.0\r\n",
      "- Python version: 3.8\r\n",
      "- Installed using: pip install tensorflow-gpu\r\n",
      "- CUDA/cuDNN version: 11.0 / 8.0.5.39\r\n",
      "- GPU model and memory: 1050 Mobile 4GB\r\n",
      "\r\n",
      "Im using pycharm IDE:\r\n",
      "\r\n",
      "Output:\r\n",
      "2020-12-24 14:31:42.522978: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n",
      "3-conv-64-nodes-0-dense-1608816708\r\n",
      "2020-12-24 14:31:48.503990: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n",
      "2020-12-24 14:31:48.505733: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n",
      "2020-12-24 14:31:49.421165: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\n",
      "pciBusID: 0000:01:00.0 name: GeForce GTX 1050 computeCapability: 6.1\r\n",
      "coreClock: 1.493GHz coreCount: 5 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.43GiB/s\r\n",
      "2020-12-24 14:31:49.421594: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n",
      "2020-12-24 14:31:49.455737: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n",
      "2020-12-24 14:31:49.456022: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n",
      "2020-12-24 14:31:49.462110: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n",
      "2020-12-24 14:31:49.467640: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n",
      "2020-12-24 14:31:49.480366: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n",
      "2020-12-24 14:31:49.484829: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n",
      "2020-12-24 14:31:49.486190: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n",
      "2020-12-24 14:31:49.486542: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n",
      "2020-12-24 14:31:49.487316: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\r\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n",
      "2020-12-24 14:31:49.488676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\n",
      "pciBusID: 0000:01:00.0 name: GeForce GTX 1050 computeCapability: 6.1\r\n",
      "coreClock: 1.493GHz coreCount: 5 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.43GiB/s\r\n",
      "2020-12-24 14:31:49.489674: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n",
      "2020-12-24 14:31:49.490059: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n",
      "2020-12-24 14:31:49.490445: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n",
      "2020-12-24 14:31:49.490811: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n",
      "2020-12-24 14:31:49.491164: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n",
      "2020-12-24 14:31:49.491561: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n",
      "2020-12-24 14:31:49.491986: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n",
      "2020-12-24 14:31:49.492414: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n",
      "2020-12-24 14:31:49.492930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n",
      "2020-12-24 14:31:50.647138: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n",
      "2020-12-24 14:31:50.647372: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \r\n",
      "2020-12-24 14:31:50.647516: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \r\n",
      "2020-12-24 14:31:50.647835: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2989 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n",
      "2020-12-24 14:31:50.649096: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n",
      "2020-12-24 14:31:51.001350: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.\r\n",
      "2020-12-24 14:31:51.001582: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.\r\n",
      "2020-12-24 14:31:51.001816: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1365] Profiler found 1 GPUs\r\n",
      "2020-12-24 14:31:51.004339: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cupti64_110.dll\r\n",
      "2020-12-24 14:31:51.158879: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.\r\n",
      "2020-12-24 14:31:51.159215: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1487] CUPTI activity buffer flushed\r\n",
      "2020-12-24 14:31:52.099653: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n",
      "Epoch 1/10\r\n",
      "2020-12-24 14:31:53.954017: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n",
      "2020-12-24 14:31:54.522501: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n",
      "2020-12-24 14:31:54.544413: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n",
      "\r\n",
      "Process finished with exit code -1073740791 (0xC0000409)\r\n",
      "\r\n",
      "\r\n",
      "(If you'd need any additional information ask)\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "subtype:windows\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  A long period of GPU/CPU idle time in timeline\n",
      "issue body -  <em>Please make sure that this is an issue related to performance of TensorFlow.\r\n",
      "As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:performance_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux version 3.10.0\r\n",
      "- TensorFlow installed from (source or binary): pip\r\n",
      "- TensorFlow version (use command below): tensorflow-gpu==1.15\r\n",
      "- Python version: 3.6.8\r\n",
      "- CUDA/cuDNN version: 10.2\r\n",
      "- GPU model and memory: Tesla-V100, 16G\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "There is a long period of GPU and CPU idle time in timeline. The model is trained in one GPU and there is no problem of network transmission.\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "There should not be such a long period of idle time.\r\n",
      "\r\n",
      "**Other info / logs** Include any logs or source code that would be helpful to\r\n",
      "diagnose the problem. If including tracebacks, please include the full\r\n",
      "traceback. Large logs and files should be attached.\r\n",
      "![屏幕快照 2020-12-24 下午8 17 36](https://user-images.githubusercontent.com/5723913/103089708-c2c18d00-4629-11eb-9059-43aeb8b20136.png)\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 1.15\n",
      "comp:gpu\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:performance\n",
      "\n",
      "\n",
      "issue title -  Nontrainable Custom Convolution at the start of model\n",
      "issue body -  <em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n",
      "\r\n",
      "\r\n",
      "**System information**\r\n",
      "- TensorFlow version (you are using):\r\n",
      "- Are you willing to contribute it (Yes/No):\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**Describe the feature and the current behavior/state.**\r\n",
      "\r\n",
      "**Will this change the current api? How?**\r\n",
      "\r\n",
      "**Who will benefit with this feature?**\r\n",
      "\r\n",
      "**Any Other info.**\r\n",
      "I am creating a CNN model ,but at the start I want my image to be passed through high pass filter(based on a matrix convolution) how to do that,or How to add a custom non trainable Convolution layer with a constant matrix at the start of model\r\n",
      ".Please refer GNCNN model , how to do the HIgh pass filter part in keras , using flow from directory method\n",
      "issue labels - \n",
      "comp:keras\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Is it possible to build tf2.4 with cuda 10.0?\n",
      "issue body -  <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): centos 7\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: \r\n",
      "- TensorFlow installed from (source or binary): source\r\n",
      "- TensorFlow version: 2.4\r\n",
      "- Python version: 3.6\r\n",
      "- Installed using virtualenv? pip? conda?:\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version: cuda 10.0.130, cudnn 7.5\r\n",
      "- GPU model and memory: K80\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "\r\n",
      "I would like to know if is it possible to build newest tensorflow against cuda 10.0? Is there any workaround with cuda 10.0? It looks like libcublasLt.so.10.0 is missing in cuda10.0 ...\r\n",
      "\r\n",
      "**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n",
      "\r\n",
      "During the build I get following error:\r\n",
      "\r\n",
      "    $: bazel build //tensorflow/tools/pip_package:build_pip_package\r\n",
      "    ....\r\n",
      "    Repository command failed\r\n",
      "    No library found under: /usr/local/cuda-10.0/targets/x86_64-linux/lib/libcublasLt.so.10.0\r\n",
      "    INFO: Elapsed time: 1.836s\r\n",
      "    INFO: 0 processes.\r\n",
      "    FAILED: Build did NOT complete successfully (0 packages loaded)\r\n",
      "        currently loading: tensorflow/tools/pip_package\r\n",
      "\r\n",
      "\r\n",
      "**Any other info / logs**\r\n",
      "Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "stalled\n",
      "stat:awaiting response\n",
      "subtype: ubuntu/linux\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  Using Tensorflow 2.3 in r (conda environment) to reproduce the examples of Platypus R-package\n",
      "issue body -  the example included in Platypus package: https://github.com/maju116/platypus/blob/yolo3_fix/examples/Blood%20Cell%20Detection/Blood-Cell-Detection.md\r\n",
      "\r\n",
      "my session trying to reproduce the result:\r\n",
      "> history <- blood_yolo %>%\r\n",
      "+   yolo3_fit_generator(\r\n",
      "+     generator = train_blood_yolo_generator,\r\n",
      "+     epochs = 3,\r\n",
      "+     steps_per_epoch = 3,\r\n",
      "+     validation_generator = valid_blood_yolo_generator,\r\n",
      "+     validation_steps_per_epoch = 9,\r\n",
      "+     model_filepath = \"development/BCCD/blood_w.hdf5\",\r\n",
      "+     save_best_only = TRUE,\r\n",
      "+     monitor = \"val_loss\"\r\n",
      "+   )\r\n",
      "WARNING:tensorflow:AutoGraph could not transform <function make_python_function.<locals>.python_function at 0x0000024526D8AB80> and will run it as-is.\r\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\n",
      "Cause: module 'gast' has no attribute 'Index'\r\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\n",
      "WARNING:tensorflow:AutoGraph could not transform <function make_python_function.<locals>.python_function at 0x00000245244990D0> and will run it as-is.\r\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\n",
      "Cause: module 'gast' has no attribute 'Index'\r\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\n",
      "WARNING:tensorflow:AutoGraph could not transform <function make_python_function.<locals>.python_function at 0x00000245244E0EE0> and will run it as-is.\r\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\n",
      "Cause: module 'gast' has no attribute 'Index'\r\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\n",
      "WARNING:tensorflow:AutoGraph could not transform <function make_python_function.<locals>.python_function at 0x0000024561F4C280> and will run it as-is.\r\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\n",
      "Cause: module 'gast' has no attribute 'Index'\r\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\n",
      "WARNING:tensorflow:AutoGraph could not transform <function make_python_function.<locals>.python_function at 0x0000024561F97D30> and will run it as-is.\r\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\n",
      "Cause: module 'gast' has no attribute 'Index'\r\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\n",
      "WARNING:tensorflow:AutoGraph could not transform <function make_python_function.<locals>.python_function at 0x0000024562187C10> and will run it as-is.\r\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\n",
      "Cause: module 'gast' has no attribute 'Index'\r\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "issue labels - \n",
      "comp:autograph\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Tf.contrib.layers.Bias_add fails to operate due to uninitialized value\n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n",
      "- TensorFlow installed from source:\r\n",
      "- TensorFlow version (use command below): 1.14/1.15\r\n",
      "- Python version: 3.7\r\n",
      "- CUDA/cuDNN version: 10\r\n",
      "\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "I want to add bias layers to slicing parts of a reshape layer. I conduct it with `tf.contrib.layers.bias_add` and then concatenate them back. However, I can't do that and it keeps raising the bug of uninitialized for the `Bias_add op` even I have given it\r\n",
      "\r\n",
      "The code I use is below:\r\n",
      "\r\n",
      "   ```\r\n",
      " input = keras.Input(shape=[shape, shape, 256])\r\n",
      "\r\n",
      "    kernel_init = tf.keras.initializers.RandomNormal(0.0, 0.01)\r\n",
      "\r\n",
      "    # seg_layer = _seg_layer(output_filters,num, f_l,kernel_init,bias_init)\r\n",
      "    head = keras.layers.Conv2D(256, 3, padding=\"same\", kernel_initializer=kernel_init)(input)\r\n",
      "    head = keras.layers.ReLU()(head)\r\n",
      "    for _ in range(3):\r\n",
      "        head = keras.layers.Conv2D(256, 3, padding=\"same\", kernel_initializer=kernel_init)(head)\r\n",
      "        head = keras.layers.ReLU()(head)\r\n",
      "    head = keras.layers.Conv2D(\r\n",
      "        output_filters,\r\n",
      "        3,\r\n",
      "        1,\r\n",
      "        padding=\"same\",\r\n",
      "        kernel_initializer=kernel_init,\r\n",
      "        use_bias=False,\r\n",
      "    )(head)\r\n",
      "    # head = tf.identity(head)\r\n",
      "    head = keras.layers.Reshape([-1, int(output_filters/9) ])(head)\r\n",
      "    slicing = int(f_l/num)\r\n",
      "    segment = []\r\n",
      "    for i in range(num):\r\n",
      "        tempt = head[:,slicing*i:slicing*(i+1),:]\r\n",
      "        tempt = tf.contrib.layers.bias_add(\r\n",
      "            tempt,\r\n",
      "            activation_fn=None,\r\n",
      "            initializer=tf.zeros_initializer(),\r\n",
      "            regularizer=None,\r\n",
      "            reuse=None,\r\n",
      "            variables_collections=None,\r\n",
      "            outputs_collections=None,\r\n",
      "            trainable=True,\r\n",
      "            data_format='NHWC',\r\n",
      "            scope=None\r\n",
      "        )\r\n",
      "        segment.append(tempt)\r\n",
      "    output = keras.layers.Concatenate(axis=1)(segment)\r\n",
      "    model = keras.models.Model(input,output)\r\n",
      "```\r\n",
      "\r\n",
      "Error:\r\n",
      "```\r\n",
      "tensorflow.python.framework.errors_impl.FailedPreconditionError: 2 root error(s) found.\r\n",
      "  (0) Failed precondition: Attempting to use uninitialized value BiasAdd/biases\r\n",
      "         [[{{node BiasAdd/biases/read}}]]\r\n",
      "         [[BiasAdd/biases/read/_687]]\r\n",
      "  (1) Failed precondition: Attempting to use uninitialized value BiasAdd/biases\r\n",
      "         [[{{node BiasAdd/biases/read}}]]\r\n",
      "0 successful operations.\r\n",
      "0 derived errors ignored.\r\n",
      "```\n",
      "issue labels - \n",
      "TF 1.15\n",
      "contrib\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  AttributeError: module 'tensorflow._api.v1.config' has no attribute 'list_physical_devices'\n",
      "issue body -  c:\\users\\gwinivac\\.conda\\envs\\chatboty\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: greenlet.greenlet size changed, may indicate binary incompatibility. Expected 144 from C header, got 152 from PyObject\r\n",
      "  return f(*args, **kwds)\r\n",
      "c:\\users\\gwinivac\\.conda\\envs\\chatboty\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: greenlet.greenlet size changed, may indicate binary incompatibility. Expected 144 from C header, got 152 from PyObject\r\n",
      "  return f(*args, **kwds)\r\n",
      "c:\\users\\gwinivac\\.conda\\envs\\chatboty\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: greenlet.greenlet size changed, may indicate binary incompatibility. Expected 144 from C header, got 152 from PyObject\r\n",
      "  return f(*args, **kwds)\r\n",
      "c:\\users\\gwinivac\\.conda\\envs\\chatboty\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: greenlet.greenlet size changed, may indicate binary incompatibility. Expected 144 from C header, got 152 from PyObject\r\n",
      "  return f(*args, **kwds)\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"c:\\users\\gwinivac\\.conda\\envs\\chatboty\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n",
      "    \"__main__\", mod_spec)\r\n",
      "  File \"c:\\users\\gwinivac\\.conda\\envs\\chatboty\\lib\\runpy.py\", line 85, in _run_code\r\n",
      "    exec(code, run_globals)\r\n",
      "  File \"C:\\Users\\Gwinivac\\.conda\\envs\\chatboty\\Scripts\\rasa.exe\\__main__.py\", line 7, in <module>\r\n",
      "  File \"c:\\users\\gwinivac\\.conda\\envs\\chatboty\\lib\\site-packages\\rasa\\__main__.py\", line 115, in main\r\n",
      "    rasa.telemetry.initialize_error_reporting()\r\n",
      "  File \"c:\\users\\gwinivac\\.conda\\envs\\chatboty\\lib\\site-packages\\rasa\\telemetry.py\", line 230, in decorated\r\n",
      "    return f(*args, **kwargs)\r\n",
      "  File \"c:\\users\\gwinivac\\.conda\\envs\\chatboty\\lib\\site-packages\\rasa\\telemetry.py\", line 651, in initialize_error_reporting\r\n",
      "    default_context = _default_context_fields()\r\n",
      "  File \"c:\\users\\gwinivac\\.conda\\envs\\chatboty\\lib\\site-packages\\rasa\\telemetry.py\", line 480, in _default_context_fields\r\n",
      "    \"gpu\": len(tf.config.list_physical_devices(\"GPU\")),\r\n",
      "  File \"c:\\users\\gwinivac\\.conda\\envs\\chatboty\\lib\\site-packages\\tensorflow_core\\python\\util\\module_wrapper.py\", line 193, in __getattr__\r\n",
      "    attr = getattr(self._tfmw_wrapped_module, name)\r\n",
      "AttributeError: module 'tensorflow._api.v1.config' has no attribute 'list_physical_devices'\r\n",
      "\r\n",
      "\r\n",
      "running on:\r\n",
      "tensorflow version: 1.15.0\r\n",
      "installed using conda\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 1.15\n",
      "comp:apis\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Tensorflow Lite speed -> mobile vs desktop\n",
      "issue body -  As I am currently building a mobile application with pretty heavy duty object detection, I switched to tensorflow lite as a way to speed up computing. Everything is lightning fast as expected, but I am seeing 99% of the computation time being overpowered by the interpretation.invoke() function which is obviously critical for inference as described in https://www.tensorflow.org/api_docs/python/tf/lite/Interpreter. \r\n",
      "\r\n",
      "With over 1400 images to loop my model through, I am seeing the invoke() function take roughly 1 sec for each image extending the total inference time to over 20 min. One extremely important detail to note is that this is run through xcode on my desktop computer and not a mobile device. After extensive research, I have discovered to expect better performance on a mobile device with tensorflow lite.\r\n",
      "\r\n",
      "My question is - are the performance gains from tensorflow lite on a mobile device exponentially faster than desktop? If my run time on desktop is over 20 min, is it even possible for this to decrease to a few minutes on mobile? Tons of forums out there are concluding tflite is obviously faster on mobile than desktop, but I am looking for logistics here. Generally HOW MUCH FASTER? Please help! Thanks!!\n",
      "issue labels - \n",
      "comp:lite\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:performance\n",
      "\n",
      "\n",
      "issue title -  tf.read_file have different speed to read the same images\n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04\r\n",
      "- TensorFlow installed from (source or binary):docker image: `docker pull tensorflow/tensorflow:1.13.1-gpu-py3`\r\n",
      "- TensorFlow version (use command below):1.13.1\r\n",
      "- Python version:3.5.2\r\n",
      "- CUDA/cuDNN version:10.0\r\n",
      "- GPU model and memory:1080TI/11G\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "Hi,\r\n",
      "I had pull tf1.13.1 docker image from dockerhub: \r\n",
      "\r\n",
      "```sh\r\n",
      "$ docker pull tensorflow/tensorflow:1.13.1-gpu-py3\r\n",
      "```\r\n",
      "I use `tf.data.TextLineDataset` to load data from HHD, here is my code:\r\n",
      "\r\n",
      "```python\r\n",
      "def _parse_function(data):\r\n",
      "    data_str_split = tf.string_split([data]).values\r\n",
      "    filename = data_str_split[0]\r\n",
      "    image_string = tf.read_file(filename)\r\n",
      "    return image_string\r\n",
      "\r\n",
      "def input_fn(is_training, filename, params):\r\n",
      "    batch_size = params['batch_size']\r\n",
      "    num_parallel_calls = params['num_parallel_calls']\r\n",
      "    if num_parallel_calls is None:\r\n",
      "        num_parallel_calls = tf.data.experimental.AUTOTUNE\r\n",
      "    parse_fn = lambda data: _parse_function(data)    # anchor\r\n",
      "    dataset = tf.data.TextLineDataset(filename)\r\n",
      "    if is_training:\r\n",
      "        dataset = dataset.apply(tf.data.experimental.map_and_batch(\r\n",
      "            map_func=parse_fn, batch_size=batch_size, num_parallel_calls=num_parallel_calls))\r\n",
      "        dataset = dataset.shuffle(300)\r\n",
      "    else:\r\n",
      "        dataset = dataset.apply(tf.data.experimental.map_and_batch(\r\n",
      "            map_func=parse_fn, batch_size=batch_size, num_parallel_calls=num_parallel_calls))\r\n",
      "\r\n",
      "    # create reinitializable iterator from dataset\r\n",
      "    iterator = dataset.make_initializable_iterator()\r\n",
      "    images = iterator.get_next()\r\n",
      "    iterator_init_op = iterator.initializer\r\n",
      "    inputs = {'images': images, 'iterator_init_op': iterator_init_op}\r\n",
      "    return inputs\r\n",
      "```\r\n",
      "I tested the reading speed of the first file: `total.txt`(about 4,000,000 lines), the reading speed is about 2000～3000it/s(if i test it a second time, it will up to 8000~9000it/s):\r\n",
      "```python\r\n",
      "if __name__ == \"__main__\":\r\n",
      "    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\r\n",
      "    params = {\r\n",
      "        'batch_size': 32,\r\n",
      "        'num_parallel_calls': 64\r\n",
      "    }\r\n",
      "\r\n",
      "    test_txt = '${ROOT}/total.txt'\r\n",
      "    # test_txt = '${ROOT}/train.txt'\r\n",
      "    # test_txt = '${ROOT}/valid.txt'\r\n",
      "    test_inputs = input_fn(True, test_txt, params)\r\n",
      "\r\n",
      "    import time\r\n",
      "    ts = time.time()\r\n",
      "    last_processed = 0\r\n",
      "    num_processed = 0\r\n",
      "    with tf.Session() as sess:\r\n",
      "        sess.run(test_inputs['iterator_init_op'])\r\n",
      "        while True:\r\n",
      "            try:\r\n",
      "                images = sess.run(test_inputs['images'])\r\n",
      "            except tf.errors.OutOfRangeError:\r\n",
      "                sess.run(test_inputs['iterator_init_op'])\r\n",
      "                continue\r\n",
      "            num_processed += images.shape[0]\r\n",
      "            tn = time.time()\r\n",
      "            if tn-ts > 1.0:\r\n",
      "                print(\"\\rnum_parallel_calls[{}]:{:5.2f}it/s\".format(params['num_parallel_calls'],\r\n",
      "                                                                    (num_processed-last_processed)/(tn-ts)), end='')\r\n",
      "                last_processed = num_processed\r\n",
      "                ts = tn\r\n",
      "```\r\n",
      "Then, I shuffled the file `total.txt` and splited to `train.txt/valid.txt`, and test the reading speed,\r\n",
      "but the speed drops to **100it/s**!\r\n",
      "\r\n",
      "The 2 txt file pointed to the same images, why is there such a big difference?\r\n",
      "\r\n",
      "Can you tell me how to solve this problem, I have tried to split the `train.txt` to several small `.txt` file, but it's useless.\r\n",
      "\r\n",
      "Thanks a lot!\n",
      "issue labels - \n",
      "TF 1.13\n",
      "comp:ops\n",
      "type:performance\n",
      "\n",
      "\n",
      "issue title -  fix the hyperlink in the doc\n",
      "issue body -  the text and link are reversed.\n",
      "issue labels - \n",
      "cla: yes\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Tensorflow does not use the GPU during training with eager execution, despite manual activation\n",
      "issue body -  **NOTE** This issue is being the same issue [here](https://github.com/tensorflow/tensorflow/issues/45546) and it's being re-opened because this [person](https://github.com/sanjoy) decided to close it without resolution / discussing his decision, so there we go again ...\r\n",
      "\r\n",
      "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\n",
      "As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:performance_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n",
      "- OS Platform and Distribution (e.g., Linux (Google colab)):\r\n",
      "- TensorFlow installed using `pip install tensorflow-gpu`:\r\n",
      "- TensorFlow version (2.3.1):\r\n",
      "- Python version: 3.6.9\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version: 10.1.243\r\n",
      "- GPU model and memory: Tesla T4 computeCapability: 7.5 coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.73GiB deviceMemoryBandwidth: 298.08GiB/s\r\n",
      "\r\n",
      "You can collect some of this information using our environment capture\r\n",
      "[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n",
      "You can also obtain the TensorFlow version with:\r\n",
      "1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n",
      "2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "I'm experiencing a very slow training time when running a third party [script](https://github.com/marload/DeepRL-TensorFlow2/blob/master/DQN/DQN_Discrete.py) which is the same(0% difference between GPU and CPU). I use the following command for activating the GPU on google colab after installing `tensorflow-gpu` using `pip`:\r\n",
      "\r\n",
      "```\r\n",
      "physical_devices = tf.config.experimental.list_physical_devices('GPU')\r\n",
      "if len(physical_devices) > 0:\r\n",
      "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\r\n",
      "```\r\n",
      "I add the lines above in `main()` in the script I referred to earlier and I use [wandb](https://github.com/wandb/client) for monitoring the training. Here are the [graphs](https://drive.google.com/file/d/1Fgn7tlm6HBUyZIcPelVjxNz_RWvY7QU_/view?usp=sharing) within a few minutes of training showing 0% GPU utilization.\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "\r\n",
      "A fast performance which results in a remarkable difference in speeds (CPU vs GPU) and GPU utilization above 0% if the metrics are accurate and if they are not, I'm still experiencing the same speed when run on CPU or GPU.\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "Provide a reproducible test case that is the bare minimum necessary to generate\r\n",
      "the problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n",
      "\r\n",
      "```\r\n",
      "import wandb\r\n",
      "import tensorflow as tf\r\n",
      "from tensorflow.keras.layers import Input, Dense\r\n",
      "from tensorflow.keras.optimizers import Adam\r\n",
      "\r\n",
      "import gym\r\n",
      "import argparse\r\n",
      "import numpy as np\r\n",
      "from collections import deque\r\n",
      "import random\r\n",
      "\r\n",
      "tf.keras.backend.set_floatx('float64')\r\n",
      "wandb.init(name='DQN', project=\"deep-rl-tf2\")\r\n",
      "\r\n",
      "parser = argparse.ArgumentParser()\r\n",
      "parser.add_argument('--gamma', type=float, default=0.95)\r\n",
      "parser.add_argument('--lr', type=float, default=0.005)\r\n",
      "parser.add_argument('--batch_size', type=int, default=32)\r\n",
      "parser.add_argument('--eps', type=float, default=1.0)\r\n",
      "parser.add_argument('--eps_decay', type=float, default=0.995)\r\n",
      "parser.add_argument('--eps_min', type=float, default=0.01)\r\n",
      "\r\n",
      "args = parser.parse_args()\r\n",
      "\r\n",
      "class ReplayBuffer:\r\n",
      "    def __init__(self, capacity=10000):\r\n",
      "        self.buffer = deque(maxlen=capacity)\r\n",
      "    \r\n",
      "    def put(self, state, action, reward, next_state, done):\r\n",
      "        self.buffer.append([state, action, reward, next_state, done])\r\n",
      "    \r\n",
      "    def sample(self):\r\n",
      "        sample = random.sample(self.buffer, args.batch_size)\r\n",
      "        states, actions, rewards, next_states, done = map(np.asarray, zip(*sample))\r\n",
      "        states = np.array(states).reshape(args.batch_size, -1)\r\n",
      "        next_states = np.array(next_states).reshape(args.batch_size, -1)\r\n",
      "        return states, actions, rewards, next_states, done\r\n",
      "    \r\n",
      "    def size(self):\r\n",
      "        return len(self.buffer)\r\n",
      "\r\n",
      "class ActionStateModel:\r\n",
      "    def __init__(self, state_dim, aciton_dim):\r\n",
      "        self.state_dim  = state_dim\r\n",
      "        self.action_dim = aciton_dim\r\n",
      "        self.epsilon = args.eps\r\n",
      "        \r\n",
      "        self.model = self.create_model()\r\n",
      "    \r\n",
      "    def create_model(self):\r\n",
      "        model = tf.keras.Sequential([\r\n",
      "            Input((self.state_dim,)),\r\n",
      "            Dense(32, activation='relu'),\r\n",
      "            Dense(16, activation='relu'),\r\n",
      "            Dense(self.action_dim)\r\n",
      "        ])\r\n",
      "        model.compile(loss='mse', optimizer=Adam(args.lr))\r\n",
      "        return model\r\n",
      "    \r\n",
      "    def predict(self, state):\r\n",
      "        return self.model.predict(state)\r\n",
      "    \r\n",
      "    def get_action(self, state):\r\n",
      "        state = np.reshape(state, [1, self.state_dim])\r\n",
      "        self.epsilon *= args.eps_decay\r\n",
      "        self.epsilon = max(self.epsilon, args.eps_min)\r\n",
      "        q_value = self.predict(state)[0]\r\n",
      "        if np.random.random() < self.epsilon:\r\n",
      "            return random.randint(0, self.action_dim-1)\r\n",
      "        return np.argmax(q_value)\r\n",
      "\r\n",
      "    def train(self, states, targets):\r\n",
      "        self.model.fit(states, targets, epochs=1, verbose=0)\r\n",
      "    \r\n",
      "\r\n",
      "class Agent:\r\n",
      "    def __init__(self, env):\r\n",
      "        self.env = env\r\n",
      "        self.state_dim = self.env.observation_space.shape[0]\r\n",
      "        self.action_dim = self.env.action_space.n\r\n",
      "\r\n",
      "        self.model = ActionStateModel(self.state_dim, self.action_dim)\r\n",
      "        self.target_model = ActionStateModel(self.state_dim, self.action_dim)\r\n",
      "        self.target_update()\r\n",
      "\r\n",
      "        self.buffer = ReplayBuffer()\r\n",
      "\r\n",
      "    def target_update(self):\r\n",
      "        weights = self.model.model.get_weights()\r\n",
      "        self.target_model.model.set_weights(weights)\r\n",
      "    \r\n",
      "    def replay(self):\r\n",
      "        for _ in range(10):\r\n",
      "            states, actions, rewards, next_states, done = self.buffer.sample()\r\n",
      "            targets = self.target_model.predict(states)\r\n",
      "            next_q_values = self.target_model.predict(next_states).max(axis=1)\r\n",
      "            targets[range(args.batch_size), actions] = rewards + (1-done) * next_q_values * args.gamma\r\n",
      "            self.model.train(states, targets)\r\n",
      "    \r\n",
      "    def train(self, max_episodes=1000):\r\n",
      "        for ep in range(max_episodes):\r\n",
      "            done, total_reward = False, 0\r\n",
      "            state = self.env.reset()\r\n",
      "            while not done:\r\n",
      "                action = self.model.get_action(state)\r\n",
      "                next_state, reward, done, _ = self.env.step(action)\r\n",
      "                self.buffer.put(state, action, reward*0.01, next_state, done)\r\n",
      "                total_reward += reward\r\n",
      "                state = next_state\r\n",
      "            if self.buffer.size() >= args.batch_size:\r\n",
      "                self.replay()\r\n",
      "            self.target_update()\r\n",
      "            print('EP{} EpisodeReward={}'.format(ep, total_reward))\r\n",
      "            wandb.log({'Reward': total_reward})\r\n",
      "\r\n",
      "\r\n",
      "def main():\r\n",
      "    physical_devices = tf.config.experimental.list_physical_devices('GPU')\r\n",
      "    if len(physical_devices) > 0:\r\n",
      "        tf.config.experimental.set_memory_growth(physical_devices[0], True)\r\n",
      "    env = gym.make('CartPole-v1')\r\n",
      "    agent = Agent(env)\r\n",
      "    agent.train(max_episodes=1000)\r\n",
      "\r\n",
      "if __name__ == \"__main__\":\r\n",
      "    main()\r\n",
      "```\r\n",
      "\r\n",
      "**Other info / logs** Include any logs or source code that would be helpful to\r\n",
      "diagnose the problem. If including tracebacks, please include the full\r\n",
      "traceback. Large logs and files should be attached.\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.3\n",
      "comp:gpu\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  https://youtu.be/IGpHWo0ySBU\n",
      "issue body -  <em>Please make sure that this is a bug. As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:bug_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "- TensorFlow installed from (source or binary):\r\n",
      "- TensorFlow version (use command below):\r\n",
      "- Python version:\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version:\r\n",
      "- GPU model and memory:\r\n",
      "\r\n",
      "You can collect some of this information using our environment capture\r\n",
      "[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n",
      "You can also obtain the TensorFlow version with:\r\n",
      "1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n",
      "2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n",
      "\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "Provide a reproducible test case that is the bare minimum necessary to generate\r\n",
      "the problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n",
      "\r\n",
      "**Other info / logs** Include any logs or source code that would be helpful to\r\n",
      "diagnose the problem. If including tracebacks, please include the full\r\n",
      "traceback. Large logs and files should be attached.\r\n",
      "\n",
      "issue labels - \n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  https://youtu.be/IGpHWo0ySBU\n",
      "issue body -  <em>Please make sure that this is a bug. As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:bug_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "- TensorFlow installed from (source or binary):\r\n",
      "- TensorFlow version (use command below):\r\n",
      "- Python version:\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version:\r\n",
      "- GPU model and memory:\r\n",
      "\r\n",
      "You can collect some of this information using our environment capture\r\n",
      "[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n",
      "You can also obtain the TensorFlow version with:\r\n",
      "1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n",
      "2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n",
      "\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "Provide a reproducible test case that is the bare minimum necessary to generate\r\n",
      "the problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n",
      "\r\n",
      "**Other info / logs** Include any logs or source code that would be helpful to\r\n",
      "diagnose the problem. If including tracebacks, please include the full\r\n",
      "traceback. Large logs and files should be attached.\r\n",
      "\n",
      "issue labels - \n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  SavedModel: KeyError on concrete_functions \n",
      "issue body -  \r\n",
      "**System information**\r\n",
      "-Google Colab\r\n",
      "-Tensorflow version 2.4.0\r\n",
      "\r\n",
      "I wrote encoder_network, decoder_network using Keras Functional API and saved in SavedModel Format.\r\n",
      "But when I use 'saved_model_cli show --dir model  --all', it shows those errors\r\n",
      "\r\n",
      "2020-12-23 17:23:49.847954: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n",
      "\r\n",
      "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\r\n",
      "\r\n",
      "signature_def['__saved_model_init_op']:\r\n",
      "  The given SavedModel SignatureDef contains the following input(s):\r\n",
      "  The given SavedModel SignatureDef contains the following output(s):\r\n",
      "    outputs['__saved_model_init_op'] tensor_info:\r\n",
      "        dtype: DT_INVALID\r\n",
      "        shape: unknown_rank\r\n",
      "        name: NoOp\r\n",
      "  Method name is: \r\n",
      "\r\n",
      "signature_def['serving_default']:\r\n",
      "  The given SavedModel SignatureDef contains the following input(s):\r\n",
      "    inputs['input_2'] tensor_info:\r\n",
      "        dtype: DT_FLOAT\r\n",
      "        shape: (-1, 110, 470, 3)\r\n",
      "        name: serving_default_input_2:0\r\n",
      "    inputs['input_3'] tensor_info:\r\n",
      "        dtype: DT_INT32\r\n",
      "        shape: (-1, -1)\r\n",
      "        name: serving_default_input_3:0\r\n",
      "  The given SavedModel SignatureDef contains the following output(s):\r\n",
      "    outputs['decoder_network'] tensor_info:\r\n",
      "        dtype: DT_FLOAT\r\n",
      "        shape: (-1, -1, 37)\r\n",
      "        name: StatefulPartitionedCall:0\r\n",
      "  Method name is: tensorflow/serving/predict\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/usr/local/bin/saved_model_cli\", line 8, in <module>\r\n",
      "    sys.exit(main())\r\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/tools/saved_model_cli.py\", line 1185, in main\r\n",
      "    args.func(args)\r\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/tools/saved_model_cli.py\", line 715, in show\r\n",
      "    _show_all(args.dir)\r\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/tools/saved_model_cli.py\", line 307, in _show_all\r\n",
      "    _show_defined_functions(saved_model_dir)\r\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/tools/saved_model_cli.py\", line 187, in _show_defined_functions\r\n",
      "    trackable_object = load.load(saved_model_dir)\r\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\", line 603, in load\r\n",
      "    return load_internal(export_dir, tags, options)\r\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\", line 633, in load_internal\r\n",
      "    ckpt_options)\r\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\", line 130, in __init__\r\n",
      "    self._load_all()\r\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\", line 141, in _load_all\r\n",
      "    self._load_nodes()\r\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\", line 283, in _load_nodes\r\n",
      "    node, setter = self._recreate(proto, node_id)\r\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\", line 393, in _recreate\r\n",
      "    return factory[kind]()\r\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\", line 382, in <lambda>\r\n",
      "    \"function\": lambda: self._recreate_function(proto.function),\r\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\", line 421, in _recreate_function\r\n",
      "    proto, self._concrete_functions), setattr\r\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/function_deserialization.py\", line 261, in recreate_function\r\n",
      "    concrete_function_objects.append(concrete_functions[concrete_function_name])\r\n",
      "KeyError: '__inference_functional_3_layer_call_fn_718866'\r\n",
      "\r\n",
      "\r\n",
      "Do you have any hint how to fix this ? What am I doing wrong here ? is it because I am using Keras Functional API?\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:keras\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Tensorflow 1.14 is not recognizing custom operator: Posenet_Decoder_Op\n",
      "issue body -  <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Amazon Linux 2\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: --\r\n",
      "- TensorFlow installed from (source or binary): source\r\n",
      "- TensorFlow version: 1.14\r\n",
      "- Python version: 3.7\r\n",
      "- Installed using virtualenv? pip? conda?: using virtualenv and pip\r\n",
      "- Bazel version (if compiling from source): 0.24.1\r\n",
      "- GCC/Compiler version (if compiling from source):  gcc (GCC) 7.3.1 20180712 (Red Hat 7.3.1-11)\r\n",
      "- CUDA/cuDNN version: --\r\n",
      "- GPU model and memory: --\r\n",
      "\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "I have been trying to run this model: [posenet](https://github.com/google-coral/project-posenet/blob/master/models/mobilenet/posenet_mobilenet_v1_075_481_641_quant_decoder.tflite) but it retrieves an error in the decoder part indicating you need a custom operator called PosenetDecoderOp. I investigated what to do and I approached this problem installing Tensorflow from source adding the custom operators. After compilation and instalation of Tensorflow I tried to run my code again:\r\n",
      "\r\n",
      "              _``import numpy as np\r\n",
      "              import tensorflow as tf\r\n",
      "              \r\n",
      "              #Load the TFLite model and allocate tensors.\r\n",
      "              interpreter = tf.lite.Interpreter(model_path=\"posenet_mobilenet_v1_075_481_641_quant_decoder.tflite\")\r\n",
      "              interpreter.allocate_tensors()\r\n",
      "              \r\n",
      "              #Get input and output tensors.\r\n",
      "              input_details = interpreter.get_input_details()\r\n",
      "              output_details = interpreter.get_output_details()\r\n",
      "              \r\n",
      "              #Test the model on random input data.\r\n",
      "              input_shape = input_details[0]['shape']\r\n",
      "              input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\r\n",
      "              interpreter.set_tensor(input_details[0]['index'], input_data)\r\n",
      "              \r\n",
      "              interpreter.invoke()\r\n",
      "              output_data = interpreter.get_tensor(output_details[0]['index'])\r\n",
      "              print(output_data)``_\r\n",
      "\r\n",
      "And I got the following error:\r\n",
      "`  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n",
      "/home/ec2-user/decoder/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"tf_run_model.py\", line 5, in <module>\r\n",
      "    interpreter = tf.lite.Interpreter(model_path=\"posenet_mobilenet_v1_075_481_641_quant_decoder.tflite\")\r\n",
      "  File \"/home/ec2-user/decoder/lib/python3.7/site-packages/tensorflow/lite/python/interpreter.py\", line 76, in __init__\r\n",
      "    _interpreter_wrapper.InterpreterWrapper_CreateWrapperCPPFromFile(\r\n",
      "  File \"/home/ec2-user/decoder/lib/python3.7/site-packages/tensorflow/python/util/lazy_loader.py\", line 62, in __getattr__\r\n",
      "    module = self._load()\r\n",
      "  File \"/home/ec2-user/decoder/lib/python3.7/site-packages/tensorflow/python/util/lazy_loader.py\", line 45, in _load\r\n",
      "    module = importlib.import_module(self.__name__)\r\n",
      "  File \"/usr/local/lib/python3.7/importlib/__init__.py\", line 127, in import_module\r\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\r\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n",
      "  File \"/home/ec2-user/decoder/lib/python3.7/site-packages/tensorflow/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py\", line 28, in <module>\r\n",
      "    _tensorflow_wrap_interpreter_wrapper = swig_import_helper()\r\n",
      "  File \"/home/ec2-user/decoder/lib/python3.7/site-packages/tensorflow/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py\", line 24, in swig_import_helper\r\n",
      "    _mod = imp.load_module('_tensorflow_wrap_interpreter_wrapper', fp, pathname, description)\r\n",
      "  File \"/usr/local/lib/python3.7/imp.py\", line 242, in load_module\r\n",
      "    return load_dynamic(name, filename, file)\r\n",
      "  File \"/usr/local/lib/python3.7/imp.py\", line 342, in load_dynamic\r\n",
      "    return _load(spec)\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 696, in _load\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 670, in _load_unlocked\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 583, in module_from_spec\r\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1043, in create_module\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n",
      "ImportError: /home/ec2-user/decoder/lib/python3.7/site-packages/tensorflow/lite/python/interpreter_wrapper/_tensorflow_wrap_interpreter_wrapper.so: undefined symbol: _ZN6tflite3ops6custom27Register_POSENET_DECODER_OPEv`\r\n",
      "\r\n",
      "**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n",
      "\r\n",
      "\r\n",
      "**Any other info / logs**\r\n",
      "Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n",
      "\n",
      "issue labels - \n",
      "TF 1.14\n",
      "comp:lite\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Problem about distributed training with XLA compiling.\n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n",
      "  - custom layer and custom training step\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n",
      "  - I have tested on Windows 10, Ubuntu 16.04, and Ubuntu 18.04.\r\n",
      "- TensorFlow installed from (source or binary):\r\n",
      "  - both\r\n",
      "- TensorFlow version (use command below):\r\n",
      "  - I have tried TF 2.4, 2.5 distributed version and source installed 2.4\r\n",
      "- Python version:\r\n",
      "  - 3.7\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "  - 3.5.0\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "  - 7.5\r\n",
      "- CUDA/cuDNN version:\r\n",
      "  - 10.1 and 11.0\r\n",
      "- GPU model and memory:\r\n",
      "  - 1080ti x4\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "When I train my model on multi-gpu with XLA compiling below error is occurred.\r\n",
      "```\r\n",
      "Training starts\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"FFP_/train_w_pruning.py\", line 76, in <module>\r\n",
      "    train_step(*data)\r\n",
      "  File \"/home/cvip/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 787, in __call__\r\n",
      "    result = self._call(*args, **kwds)\r\n",
      "  File \"/home/cvip/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 854, in _call\r\n",
      "    filtered_flat_args, self._concrete_stateful_fn.captured_inputs)  # pylint: disable=protected-access\r\n",
      "  File \"/home/cvip/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1920, in _call_flat\r\n",
      "    ctx, args, cancellation_manager=cancellation_manager))\r\n",
      "  File \"/home/cvip/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 561, in call\r\n",
      "    ctx=ctx)\r\n",
      "  File \"/home/cvip/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\r\n",
      "    inputs, attrs, num_outputs)\r\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: Trying to access resource ResNet/conv/kernel/replica_1_879 located in device /job:localhost/replica:0/task:0/device:GPU:0 [Op:__inference_train_step_dist_88943]\r\n",
      "```\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "I want to compile my multi-gpu code but it seems unavailable.\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "https://github.com/sseung0703/TF2-multi-gpu-training\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:dist-strat\n",
      "comp:xla\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Support OpenCL 1.1 properly by forcing buffer usage in kernels that don't check .SupportsImageBuffer()\n",
      "issue body -  On an OpenCL device (AMD G-series SOC with Mesa drivers) with 1.1 support we can meaningfully run the opencl gpu delegate. However given the lack of Image support some kernels need to force buffer usage instead which isn't always done.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:lite\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  TensorFlow 1.5 API : Error : 'DatasetV1Adapter' object does not support indexing\n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code: Yes\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.4\r\n",
      "- TensorFlow installed from : Binary (from docker image)\r\n",
      "- TensorFlow version : 1.15\r\n",
      "- Python version: 2.7.17\r\n",
      "- CUDA/cuDNN version:\r\n",
      "- GPU model and memory:\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "Error emitted by TensorFlow API:\r\n",
      "TypeError: 'DatasetV1Adapter' object does not support indexing\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "No error\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "images, labels = read_list ( data_dir, data_list ) <= here data_dir is full path of dir. containing image & label files. data_list is a text file, 2 column containing names of image file and label file.\r\n",
      "The output is an array consisting of full path of images & corresponding labels.\r\n",
      "\r\n",
      "queue = tf.data.Dataset.from_tensor_slices([images, labels])\r\n",
      "img_contents = tf.io.read_file(queue[0]) <= Error location\r\n",
      "label_contents = tf.io.read_file(queue[1])\r\n",
      "\r\n",
      "TypeError: 'DatasetV1Adapter' object does not support indexing\r\n",
      "\r\n",
      "\r\n",
      "**Other info / logs** Include any logs or source code that would be helpful to\r\n",
      "diagnose the problem. If including tracebacks, please include the full\r\n",
      "traceback. Large logs and files should be attached.\r\n",
      "\r\n",
      "Here is the traceback of the real code - \r\n",
      "\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"wasr_train_noimu.py\", line 462, in <module>\r\n",
      "    main()\r\n",
      "  File \"wasr_train_noimu.py\", line 315, in main\r\n",
      "    coord)\r\n",
      "  File \"/wasr/wasr_models/image_reader.py\", line 249, in __init__\r\n",
      "    self.image, self.label, self.imu = read_images_from_disk(self.queue, self.input_size, random_scale, random_mirror, ignore_label, img_mean)\r\n",
      "  File \"/wasr/wasr_models/image_reader.py\", line 180, in read_images_from_disk\r\n",
      "    img_contents = tf.io.read_file(input_queue[0])\r\n",
      "TypeError: 'DatasetV1Adapter' object does not support indexing\r\n",
      "\r\n",
      "The code in 'Standalone code' section is a gist of the actual code. \r\n",
      "\r\n",
      "I have looked at various posts including [this](https://github.com/tensorflow/tensorflow/issues/28995) one where it is mentioned that the issue is fixed in version 1.5 but I am still encountering it. I have also gone through the TensorFlow 1.5 documentation but could not understand why I am seeing this issue or what is the correct way to use the API if my code is not correct.\r\n",
      "Any help will be highly appreciated.\r\n",
      "Thanks,\r\n",
      "-Shailesh\r\n",
      "\n",
      "issue labels - \n",
      "TF 1.15\n",
      "comp:data\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Name argument ignored in operations\n",
      "issue body -  <em>Please make sure that this is a bug. As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:bug_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): N/A\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n",
      "- TensorFlow installed from (source or binary): Google Colab\r\n",
      "- TensorFlow version (use command below): v2.4.0-0-g582c8d236cb, 2.4.0\r\n",
      "- Python version: 3.6.9\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version:\r\n",
      "- GPU model and memory:\r\n",
      "\r\n",
      "You can collect some of this information using our environment capture\r\n",
      "[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n",
      "You can also obtain the TensorFlow version with:\r\n",
      "1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n",
      "2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n",
      "\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "When I try to create a named operation is just ignored and the default value is generated. \r\n",
      "`\r\n",
      "x = tf.keras.Input(shape=4, name=\"X\", dtype=\"float32\")\r\n",
      "y = tf.keras.Input(shape=4, name=\"Y\", dtype=\"float32\")\r\n",
      "ex = tf.math.exp(x, name=\"exponential_X\")\r\n",
      "lx = tf.math.log(x, name=\"log_X\")\r\n",
      "xy = tf.multiply(x, y, name=\"XY\")\r\n",
      "nx = tf.norm(x, name=\"norm_X\")\r\n",
      "rmx =  tf.reduce_mean(x, name=\"reduce_mean_X\")\r\n",
      "print(x.name)\r\n",
      "print(ex.name)\r\n",
      "print(lx.name)\r\n",
      "print(xy.name)\r\n",
      "print(nx.name)\r\n",
      "print(rmx.name)\r\n",
      "\r\n",
      "z = tf.Variable(1.0, name=\"Z\")\r\n",
      "ez = tf.math.exp(x, name=\"exponential_Z\")\r\n",
      "print(ez.name)\r\n",
      "`\r\n",
      "X\r\n",
      "tf.math.exp_11/Exp:0\r\n",
      "tf.math.log_6/Log:0\r\n",
      "tf.math.multiply_5/Mul:0\r\n",
      "tf.compat.v1.norm_3/norm/Squeeze:0\r\n",
      "tf.math.reduce_mean_2/Mean:0\r\n",
      "tf.math.exp_12/Exp:0\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "Output should be the names defined on the operation.\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "[Above example in Google Colab.](https://colab.research.google.com/drive/1Yc8pQoXK1MiGIPEGH-Ls9TKHpUMsrVqz)\r\n",
      "\r\n",
      "**Other info / logs** Include any logs or source code that would be helpful to\r\n",
      "diagnose the problem. If including tracebacks, please include the full\r\n",
      "traceback. Large logs and files should be attached.\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:ops\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  [Intel MKL] CNMS performance optimization by using priority queue\n",
      "issue body -  Replace vector with priority queue to improve the performance in CNMS kernel\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:mkl\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  Fill only currently supports int32, int64, float32, bool, string for input 1, got 9.Node number 5 (FILL) failed to invoke.\n",
      "issue body -  <em>Please make sure that this is a bug. As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:bug_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "- TensorFlow installed from (source or binary): Binary\r\n",
      "- TensorFlow version (use command below): 2.5.0-dev20201222\r\n",
      "- Python version:\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version:\r\n",
      "- GPU model and memory:\r\n",
      "\r\n",
      "You can collect some of this information using our environment capture\r\n",
      "[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n",
      "You can also obtain the TensorFlow version with:\r\n",
      "1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n",
      "2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n",
      "\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "\r\n",
      "I have been able to successfully convert the [Boundless model](https://arxiv.org/pdf/1908.07007.pdf) with full integer quantization but it fails during inference. I have made sure to scale the inputs w.r.t. quantization i.e. the following - \r\n",
      "\r\n",
      "```python\r\n",
      "if quantization==\"integer\":\r\n",
      "    input_img = preprocess_image(\"test_image.jpg\")\r\n",
      "    input_scale, input_zero_point = input_details[0][\"quantization\"]\r\n",
      "    input_img = input_img / input_scale + input_zero_point\r\n",
      "    input_img = input_img.astype(input_details[0][\"dtype\"])\r\n",
      "```\r\n",
      "\r\n",
      "I am running into - \r\n",
      "\r\n",
      "```python\r\n",
      "---------------------------------------------------------------------------\r\n",
      "RuntimeError                              Traceback (most recent call last)\r\n",
      "<ipython-input-35-1e81bd6daa76> in <module>()\r\n",
      "      2 interpreter.set_tensor(input_details[0]['index'], input_img)\r\n",
      "      3 start = time.time()\r\n",
      "----> 4 interpreter.invoke()\r\n",
      "      5 print(f\"Inference took: {time.time()-start} seconds\")\r\n",
      "\r\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/interpreter.py in invoke(self)\r\n",
      "    758     \"\"\"\r\n",
      "    759     self._ensure_safe()\r\n",
      "--> 760     self._interpreter.Invoke()\r\n",
      "    761 \r\n",
      "    762   def reset_all_variables(self):\r\n",
      "\r\n",
      "RuntimeError: Fill only currently supports int32, int64, float32, bool, string for input 1, got 9.Node number 5 (FILL) failed to invoke.\r\n",
      "```\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "\r\n",
      "Currently, it's not clear to me why it is failing. Essentially, I would expect it to work. \r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "Colab Notebook: https://colab.research.google.com/github/sayakpaul/Adventures-in-TensorFlow-Lite/blob/master/Boundless_TFLite.ipynb.\r\n",
      "\r\n",
      "**Other info / logs** Include any logs or source code that would be helpful to\r\n",
      "diagnose the problem. If including tracebacks, please include the full\r\n",
      "traceback. Large logs and files should be attached.\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.5\n",
      "comp:lite\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Float model size is lesser than dynamic-range model size\n",
      "issue body -  <em>Please make sure that this is a bug. As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:bug_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "- TensorFlow installed from (source or binary): Binary\r\n",
      "- TensorFlow version (use command below): 2.5.0-dev20201222\r\n",
      "- Python version:\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version:\r\n",
      "- GPU model and memory:\r\n",
      "\r\n",
      "You can collect some of this information using our environment capture\r\n",
      "[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n",
      "You can also obtain the TensorFlow version with:\r\n",
      "1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n",
      "2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n",
      "\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "\r\n",
      "I am converting the [Boundless model](https://arxiv.org/pdf/1908.07007.pdf) to TensorFlow Lite. While the model conversion is successful and the inference works as expected there's an inconsistency in the model sizes. Currently, the dynamic-range quantized model size is ~14 MB whereas the float16 quantized model size is ~7MB. \r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "\r\n",
      "Float16 model size should be higher, technically twice the size of the dynamic-range quantized model. \r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "Colab Notebook - https://colab.research.google.com/github/sayakpaul/Adventures-in-TensorFlow-Lite/blob/master/Boundless_TFLite.ipynb. \r\n",
      "\r\n",
      "**Other info / logs** Include any logs or source code that would be helpful to\r\n",
      "diagnose the problem. If including tracebacks, please include the full\r\n",
      "traceback. Large logs and files should be attached.\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.5\n",
      "comp:lite\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Could not load dynamic library 'libcudart.so.11.0'\n",
      "issue body -  <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n",
      "- TensorFlow installed from (source or binary): binary\r\n",
      "- TensorFlow version: 2.4.0\r\n",
      "- Python version: 3.7.9\r\n",
      "- Installed using virtualenv? pip? conda?: pip\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version: libcudnn8_8.0.5.39-1+cuda11.0_amd64.deb (I also installed the dev version)\r\n",
      "- NVIDIA Driver Version: 450.80.02\r\n",
      "- GPU model and memory: GeForce RTX 2080 Super with Max-Q Design\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "\r\n",
      "**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n",
      "I tried following the instructions as described in: https://www.tensorflow.org/install/gpu\r\n",
      "- I installed Ubuntu 20.04 (which installed the NVIDIA driver)\r\n",
      "- Installed python using pyenv\r\n",
      "- sudo apt install nvidia-cuda-toolkit\r\n",
      "- pip install tensorflow\r\n",
      "- Installed: libcudnn8_8.0.5.39-1+cuda11.0_amd64.deb\r\n",
      "- (saw that it failed to find 'libcudart.so.11.0')\r\n",
      "- Installed: libcudnn8-dev_8.0.5.39-1+cuda11.0_amd64.deb\r\n",
      "- (still failed to find 'libcudart.so.11.0')\r\n",
      "\r\n",
      "Is there a way for me to check which part of the installation broke?\r\n",
      "Any ideas on what I can do to fix this?\r\n",
      "Thanks!\r\n",
      "\r\n",
      "**Any other info / logs**\r\n",
      "Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n",
      "```\r\n",
      "PyDev console: starting.\r\n",
      "Python 3.7.9 (default, Dec 22 2020, 21:13:51) \r\n",
      "[GCC 9.3.0] on linux\r\n",
      ">>> import tensorflow as tf\r\n",
      ">>> print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\r\n",
      "2020-12-22 22:56:35.044676: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\r\n",
      "2020-12-22 22:56:35.044691: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n",
      "/home/ido/TeraResearch/venv/lib/python3.7/site-packages/pandas/compat/__init__.py:120: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\r\n",
      "  warnings.warn(msg)\r\n",
      "2020-12-22 22:56:35.938373: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n",
      "2020-12-22 22:56:35.938801: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n",
      "2020-12-22 22:56:37.494184: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\r\n",
      "2020-12-22 22:56:37.494273: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: ido-ml\r\n",
      "2020-12-22 22:56:37.494291: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: ido-ml\r\n",
      "2020-12-22 22:56:37.494480: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 450.80.2\r\n",
      "2020-12-22 22:56:37.494544: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 450.80.2\r\n",
      "2020-12-22 22:56:37.494561: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 450.80.2\r\n",
      "Num GPUs Available:  0\r\n",
      "```\n",
      "issue labels - \n",
      "TF 2.4\n",
      "stalled\n",
      "stat:awaiting response\n",
      "subtype: ubuntu/linux\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  [ROCm] Update to use ROCm 4.0 (when building TF with --config=rocm)\n",
      "issue body -  /cc @cheshire @chsigg @nvining-work\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:gpu\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  No algorithm Worked\n",
      "issue body -  Getting this error when using the code in collab\r\n",
      "\r\n",
      "Found 14400 images belonging to 2 classes.\r\n",
      "Found 400 images belonging to 2 classes.\r\n",
      "Epoch 1/50\r\n",
      "---------------------------------------------------------------------------\r\n",
      "NotFoundError                             Traceback (most recent call last)\r\n",
      "<ipython-input-28-86598d638a85> in <module>()\r\n",
      "     15         epochs=50,\r\n",
      "     16         validation_data=test_generator,\r\n",
      "---> 17         validation_steps=800)\r\n",
      "\r\n",
      "6 frames\r\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n",
      "   1098                 _r=1):\r\n",
      "   1099               callbacks.on_train_batch_begin(step)\r\n",
      "-> 1100               tmp_logs = self.train_function(iterator)\r\n",
      "   1101               if data_handler.should_sync:\r\n",
      "   1102                 context.async_wait()\r\n",
      "\r\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n",
      "    826     tracing_count = self.experimental_get_tracing_count()\r\n",
      "    827     with trace.Trace(self._name) as tm:\r\n",
      "--> 828       result = self._call(*args, **kwds)\r\n",
      "    829       compiler = \"xla\" if self._experimental_compile else \"nonXla\"\r\n",
      "    830       new_tracing_count = self.experimental_get_tracing_count()\r\n",
      "\r\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)\r\n",
      "    853       # In this case we have created variables on the first call, so we run the\r\n",
      "    854       # defunned version which is guaranteed to never create variables.\r\n",
      "--> 855       return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\r\n",
      "    856     elif self._stateful_fn is not None:\r\n",
      "    857       # Release the lock early so that multiple threads can perform the call\r\n",
      "\r\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)\r\n",
      "   2941        filtered_flat_args) = self._maybe_define_function(args, kwargs)\r\n",
      "   2942     return graph_function._call_flat(\r\n",
      "-> 2943         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\r\n",
      "   2944 \r\n",
      "   2945   @property\r\n",
      "\r\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n",
      "   1917       # No tape is watching; skip to running the function.\r\n",
      "   1918       return self._build_call_outputs(self._inference_function.call(\r\n",
      "-> 1919           ctx, args, cancellation_manager=cancellation_manager))\r\n",
      "   1920     forward_backward = self._select_forward_and_backward_functions(\r\n",
      "   1921         args,\r\n",
      "\r\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in call(self, ctx, args, cancellation_manager)\r\n",
      "    558               inputs=args,\r\n",
      "    559               attrs=attrs,\r\n",
      "--> 560               ctx=ctx)\r\n",
      "    561         else:\r\n",
      "    562           outputs = execute.execute_with_cancellation(\r\n",
      "\r\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n",
      "     58     ctx.ensure_initialized()\r\n",
      "     59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n",
      "---> 60                                         inputs, attrs, num_outputs)\r\n",
      "     61   except core._NotOkStatusException as e:\r\n",
      "     62     if name is not None:\r\n",
      "\r\n",
      "NotFoundError:  No algorithm worked!\r\n",
      "\t [[node sequential_2/conv2d_6/Conv2D (defined at <ipython-input-26-b3f11172c6da>:17) ]] [Op:__inference_train_function_3219]\r\n",
      "\r\n",
      "Function call stack:\r\n",
      "train_function\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "My code on colab i was just testing a random CNN architecture on my dataset\r\n",
      "https://colab.research.google.com/drive/1mb7c3Dg6CDwcjYitp85uNpa_A6il1J6f?usp=sharing\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:keras\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  label_image build fails for stm32f1\n",
      "issue body -  @tensorflow/micro\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Host OS Platform and Distribution : ubuntu 18.04\r\n",
      "- TensorFlow installed from : Source\r\n",
      "- Tensorflow commit: bba12d0401800fbf873ea35f34517a8c47a54272 (Branch:Master)\r\n",
      "- Target platform : Arm\r\n",
      "\r\n",
      "I am trying to build target 'stm32f1' at location tensorflow/lite/tools/make/targets in file stm32f1_makefile.inc \r\n",
      "for this I am using toolchain provided at https://developer.arm.com/tools-and-software/open-source-software/developer-tools/gnu-toolchain/gnu-rm for x86_64 which is Pre-built GNU bare-metal toolchain for 32-bit Arm processors.\r\n",
      "while building label_image I am getting error saying that it was not able to find #include <dlfcn.h>\r\n",
      "\r\n",
      "Following are the steps to produce this issue: (in tensorflow dir)\r\n",
      "$./tensorflow/lite/tools/make/download_dependencies.sh\r\n",
      "export the tools from toolchain downloaded from above link.\r\n",
      "And modified the build_rpi_build.sh where I changed TARGET_ARCH to TARGET and passes TARGET=stm32f1.\n",
      "issue labels - \n",
      "TF 2.5\n",
      "comp:lite\n",
      "comp:micro\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  Feature clolumns  cannot be used in the keras model, it may be a bug\n",
      "issue body -  ## URL(s) with the issue:\r\n",
      "https://tensorflow.google.cn/api_docs/python/tf/keras/layers/DenseFeatures\r\n",
      "\r\n",
      "## Description of issue (what needs changing):\r\n",
      "\r\n",
      "### Clear description\r\n",
      "\r\n",
      "I have made a keras function API model, which uses' fit_generator 'for multiple input. If you don't use tensorflow feature columns, the model can work normally, but if you use feature columns to input to densefeatures layer, the model will not work. It looks like a bug, because the same code has been tried before, and the model works\r\n",
      "For example, why should someone use this method? How is it useful?\r\n",
      "\r\n",
      "### Correct links\r\n",
      "### Usage example\r\n",
      "\r\n",
      "my code:\r\n",
      "set use_feature_columns=False can work but,set use_feature_columns=True,it can't work.\r\n",
      "\r\n",
      "\r\n",
      "https://colab.research.google.com/drive/18e-T5UYi__uzsMmavXTj86pM9GPhqe7I?usp=sharing\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "comp:keras\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Loss reported by fit vs custom_loop\n",
      "issue body -  ## URL(s) with the issue:\r\n",
      "\r\n",
      "https://www.tensorflow.org/tutorials/distribute/custom_training\r\n",
      "\r\n",
      "## Description of issue (what needs changing):\r\n",
      "\r\n",
      "I wanted to compare the `fit` performance vs a \"custom loop\" with a `MultiWorker` strategy according to the tutorials at https://www.tensorflow.org/tutorials/distribute/keras and https://www.tensorflow.org/tutorials/distribute/custom_training \r\n",
      "\r\n",
      "However I'm seeing differences in the loss reported. Examples for an MNIST dataset with a Lenet:\r\n",
      "- fit: Eval loss: 32.09, Eval Accuracy: 0.9307\r\n",
      "- custom loop: Eval loss: 1.742, Eval Accuracy: 0.965\r\n",
      "\r\n",
      "Also the loss reported in the `logs` member of the callbacks seems to be higher by some factor compared to the custom_loop.\r\n",
      "\r\n",
      "### Clear description\r\n",
      "\r\n",
      "At https://www.tensorflow.org/tutorials/distribute/custom_training#define_the_loss_function I found the following:\r\n",
      "\r\n",
      "> - Using tf.reduce_mean is not recommended. Doing so divides the loss by actual per replica batch size which may vary step to step.\r\n",
      "> - This reduction and scaling is done automatically in keras `model.compile` and `model.fit`\r\n",
      "> - ... SUM_OVER_BATCH_SIZE is disallowed because currently it would only divide by per replica batch size, and leave the dividing by number of replicas to the user, which might be easy to miss. So instead we ask the user do the reduction themselves explicitly.\r\n",
      "\r\n",
      "So question here which isn't clear to me from reading the docs/tutorials: How is the loss calculated by `model.compile` and `model.fit` and is that expected to be the same as the one shown in the tutorial for custom_loops? \r\n",
      "\r\n",
      "What I recon is that it should be the average loss per sample over the whole batch (on all replicas). So the sum of all `per_example_loss` divided by the number of examples.\r\n",
      "The custom_loop tutorial divides by the global batch size, i.e. the local batch size times the number of replicas. This would be wrong for the mentioned case where the \"per replica batch size [...] may vary step to step.\" So this sounds like `tf.reduce_mean` would be useful with an additional step of averaging the `per_replica_losses` instead of summing them after the `strategy.run` part. Why is that not recommended?\n",
      "issue labels - \n",
      "comp:dist-strat\n",
      "stat:awaiting response\n",
      "type:docs-bug\n",
      "\n",
      "\n",
      "issue title -  AutoGraph could not transform custom Train_Step function\n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code\r\n",
      "- OS Platform: Windows 10 64-bit, Desktop\r\n",
      "- TensorFlow installed from binary:\r\n",
      "- TensorFlow version: 2.3.1\r\n",
      "- Python version: 3.7.9\r\n",
      "- CUDA/cuDNN version: Cuda compilation tools, release 10.1, V10.1.243\r\n",
      "- GPU model and memory: two system linked Nvidia GeForce 2070 RX.  8gb ram each (16gb total).\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "I get the following warning message when using tf.function to convert my function:\r\n",
      "\"\r\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Train_Step at 0x000002935E41C048> and will run it as-is.\r\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\r\n",
      "\"\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "I expected that when I used a @tf.function decorator the function would compile properly not throw out a warning.\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "\"\r\n",
      "import tensorflow as tf\r\n",
      "import numpy as np\r\n",
      "\r\n",
      "batch_size = 100\r\n",
      "feature_length = 45\r\n",
      "length_target = 1\r\n",
      "array_sample_data = np.asarray(np.random.random((batch_size, feature_length)), dtype = np.float32)\r\n",
      "array_target_data = np.asarray(np.random.random((batch_size, length_target)), dtype = np.float32)\r\n",
      "array_std_data = np.std(array_sample_data, axis = 0)\r\n",
      "\r\n",
      "model = tf.keras.Sequential()\r\n",
      "model.add(tf.keras.layers.Dense(50, activation = 'relu', input_shape = [feature_length]))\r\n",
      "model.add(tf.keras.layers.Dense(length_target, activation = 'sigmoid'))\r\n",
      "\r\n",
      "optimizer = tf.keras.optimizers.Adam()\r\n",
      "loss = tf.keras.losses.MSE\r\n",
      "model.compile(optimizer = optimizer, loss = loss)\r\n",
      "\r\n",
      "tensor_sample_data = tf.constant(array_sample_data, dtype = 'float32')\r\n",
      "tensor_target_data = tf.constant(array_target_data, dtype = 'float32')\r\n",
      "tensor_weight = None\r\n",
      "\r\n",
      "clip_norm = 1\r\n",
      "@tf.function\t\r\n",
      "def Train_Step(tensor_sample_data, tensor_target_data, tensor_weight):\r\n",
      "\twith tf.GradientTape() as tape:\r\n",
      "\t\ty_pred = model.call(tensor_sample_data, training = True)\r\n",
      "\t\tloss = model.compiled_loss(y_true = tensor_target_data, y_pred = y_pred, \\\r\n",
      "sample_weight = tensor_weight, regularization_losses = model.losses) \r\n",
      "\r\n",
      "\tper_replica_gradients = tape.gradient(loss, model.trainable_variables)\r\n",
      "\tgradients = tf.distribute.get_replica_context().all_reduce('sum', per_replica_gradients) #summed all grads across devices\r\n",
      "\tloss = tf.distribute.get_replica_context().all_reduce('sum', loss)\r\n",
      "\ty_pred = tf.distribute.get_replica_context().all_reduce('sum', y_pred)\r\n",
      "\t(gradients, global_norm_value) = tf.clip_by_global_norm(t_list = gradients, clip_norm = clip_norm)\r\n",
      "\tmodel.optimizer.apply_gradients(zip(gradients, model.trainable_variables), experimental_aggregate_gradients = False)\r\n",
      "\r\n",
      "\treturn (y_pred, loss)\r\n",
      "\r\n",
      "epochs = 3\r\n",
      "for index_epoch in range(epochs):\r\n",
      "\t(y_pred, loss) = Train_Step(tensor_sample_data = tensor_sample_data, tensor_target_data = tensor_target_data, tensor_weight = tensor_weight)\r\n",
      "\tprint('Training loss is {} on epoch {}.'.format(loss, index_epoch + 1))\r\n",
      "\r\n",
      "\"\r\n",
      "Thanks for your time and consideration.\n",
      "issue labels - \n",
      "TF 2.3\n",
      "comp:autograph\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Error using custom activation function while mixed precision enable\n",
      "issue body -  Config\r\n",
      "```\r\n",
      "TensorFlow 2.3\r\n",
      "```\r\n",
      "\r\n",
      "I already posted in [SO](https://stackoverflow.com/questions/65403976/typeerror-using-custom-activation-function-while-mixed-precision-enabled), a few hours ago without any response until now. But I need a quick pointer. \r\n",
      "\r\n",
      "----\r\n",
      "\r\n",
      "### Problem\r\n",
      "\r\n",
      "I was trying to use a **custom activation** in **mixed-precision** enabled training pipelines but faced the following error:\r\n",
      "\r\n",
      "```\r\n",
      "TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type float16 of argument 'x'.\r\n",
      "```\r\n",
      "\r\n",
      "### Reproduce \r\n",
      "\r\n",
      "Enabling Mixed precision...\r\n",
      "\r\n",
      "```\r\n",
      "import tensorflow as tf \r\n",
      "\r\n",
      "policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\r\n",
      "tf.keras.mixed_precision.experimental.set_policy(policy)\r\n",
      "print('Mixed precision enabled')\r\n",
      "```\r\n",
      "\r\n",
      "Custom activation... \r\n",
      "\r\n",
      "```\r\n",
      "def ARelu(x, alpha=0.90, beta=2.0):\r\n",
      "    alpha = tf.clip_by_value(alpha, clip_value_min=0.01, clip_value_max=0.99)\r\n",
      "    beta  = 1 + tf.math.sigmoid(beta)\r\n",
      "    return tf.nn.relu(x) * beta - tf.nn.relu(-x) * alpha\r\n",
      "```\r\n",
      "\r\n",
      "Training...\r\n",
      "\r\n",
      "```\r\n",
      "import tensorflow as tf\r\n",
      "\r\n",
      "(xtrain, ytrain), (xtest, ytest) = tf.keras.datasets.mnist.load_data()\r\n",
      "\r\n",
      "def pre_process(inputs, targets):\r\n",
      "    inputs  = tf.expand_dims(inputs, -1)\r\n",
      "    targets = tf.one_hot(targets, depth=10)\r\n",
      "    return tf.divide(inputs, 255), targets\r\n",
      "\r\n",
      "train_data = tf.data.Dataset.from_tensor_slices((xtrain, ytrain)).\\\r\n",
      "    take(10_000).shuffle(10_000).batch(8).map(pre_process)\r\n",
      "test_data = tf.data.Dataset.from_tensor_slices((xtest, ytest)).\\\r\n",
      "    take(1_000).shuffle(1_000).batch(8).map(pre_process)\r\n",
      "\r\n",
      "model = tf.keras.Sequential([\r\n",
      "                             \r\n",
      "            tf.keras.layers.Conv2D(filters=16, kernel_size=(3, 3), strides=(1, 1),\r\n",
      "                                   input_shape=(28, 28, 1), activation=ARelu),\r\n",
      "            tf.keras.layers.MaxPool2D(pool_size=(2, 2)),\r\n",
      "\r\n",
      "            tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), \r\n",
      "                                   activation=ARelu),\r\n",
      "            tf.keras.layers.MaxPool2D(pool_size=(2, 2)),\r\n",
      "\r\n",
      "            tf.keras.layers.Flatten(),\r\n",
      "            tf.keras.layers.Dense(64, activation=ARelu), \r\n",
      "            tf.keras.layers.Dense(10, activation='softmax', dtype=tf.float32)]) \r\n",
      "\r\n",
      "opt = tf.keras.optimizers.Adam()\r\n",
      "\r\n",
      "model.compile(loss='categorical_crossentropy', optimizer=opt)\r\n",
      "history = model.fit(train_data, validation_data=test_data, epochs=10)\r\n",
      "\r\n",
      "# ------------------\r\n",
      "\r\n",
      "TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type float16 of argument 'x'.\r\n",
      "```\r\n",
      "\r\n",
      "However, without mixed-precision, it works. I understand the problem simply types miss match but where I should look into it? \r\n",
      "\r\n",
      "Additionally, while trying to solve it, I've found that using `tf.keras.mixed_precision.LossScaleOptimizer` is safe to avoid numeric underflow. Is it something that we should use for mixed-precision training? \n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:keras\n",
      "stat:awaiting tensorflower\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  SavedModelBundle.java do not has serialize，so in spark do not broadcast the load model, only run local or one executor \n",
      "issue body -  https://github.com/tensorflow/tensorflow/blob/master/tensorflow/java/src/main/java/org/tensorflow/SavedModelBundle.java\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "comp:apis\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Survival/Drop Connect Rate (aka Stochastic Depth) is not implemented in tf.keras.applications.efficientnet\n",
      "issue body -  In the original implementation, [the `drop_connect` function](https://github.com/tensorflow/tpu/blob/341c3b628409a03b74d2d66d33fd09532690e04e/models/official/efficientnet/utils.py#L276-L291) is implemented in the following way:\r\n",
      "\r\n",
      "```python\r\n",
      "def drop_connect(inputs, is_training, survival_prob):\r\n",
      "  \"\"\"Drop the entire conv with given survival probability.\"\"\"\r\n",
      "  # \"Deep Networks with Stochastic Depth\", https://arxiv.org/pdf/1603.09382.pdf\r\n",
      "  if not is_training:\r\n",
      "    return inputs\r\n",
      "\r\n",
      "  # Compute tensor.\r\n",
      "  batch_size = tf.shape(inputs)[0]\r\n",
      "  random_tensor = survival_prob\r\n",
      "  random_tensor += tf.random_uniform([batch_size, 1, 1, 1], dtype=inputs.dtype)\r\n",
      "  binary_tensor = tf.floor(random_tensor)\r\n",
      "  # Unlike conventional way that multiply survival_prob at test time, here we\r\n",
      "  # divide survival_prob at training time, such that no addition compute is\r\n",
      "  # needed at test time.\r\n",
      "  output = tf.div(inputs, survival_prob) * binary_tensor\r\n",
      "  return output\r\n",
      "```\r\n",
      "\r\n",
      "This function is used to decide whether to keep a block or not with a given probability `p_l`; this described in depth in [the original paper](https://arxiv.org/abs/1603.09382). \r\n",
      "\r\n",
      "Looking at the `tf.keras.applications.efficientnet` implementation, we can notice that the value is used as a dropout rate instead:\r\n",
      "\r\n",
      "https://github.com/tensorflow/tensorflow/blob/5855c1c8ae2d2b36bf9d6f906b717046bb4afb36/tensorflow/python/keras/applications/efficientnet.py#L349-L353\r\n",
      "\r\n",
      "https://github.com/tensorflow/tensorflow/blob/5855c1c8ae2d2b36bf9d6f906b717046bb4afb36/tensorflow/python/keras/applications/efficientnet.py#L413-L423\r\n",
      "\r\n",
      "https://github.com/tensorflow/tensorflow/blob/5855c1c8ae2d2b36bf9d6f906b717046bb4afb36/tensorflow/python/keras/applications/efficientnet.py#L514-L517\r\n",
      "\r\n",
      "\r\n",
      "The difference in implementation would be that the connection dropped is at a module level in the original implementation and at a unit level in the `tf.keras.applications` implementation.\n",
      "issue labels - \n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  tf.train.Saver and saver.restore do not work when data_augmentation_options rgb_to_gray and random_vertical_flip are configured in the pipeline config file\n",
      "issue body -  <em>Please make sure that this is a bug. As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:bug_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No (but added a workaround to overcome first issue which will be explained)\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux 7\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "- TensorFlow installed from (source or binary): source\r\n",
      "- TensorFlow version (use command below): 1.13.2 (also with 1.14.0)\r\n",
      "- Python version: 3.6.4\r\n",
      "- Bazel version (if compiling from source): 0.20.0\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version:\r\n",
      "- GPU model and memory:\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "I am using object detection api and training my model using faster_rcnn_resnet152. Training and evaluation worked fine until I added below data augmentation options in the pipeline config file. The evaluation script (I am using old eval.py on purpose) fails with below error. The error seems to be due to rgb_to_gray option. Pipeline config file has also subtract_channel_mean  and random_horizontal_flip options, however they work fine until the other two configurations are added. To overcome the error I added \"reshape=True\" argument to saver = tf.train.Saver(variables_to_restore) line in the legcy/evaluator.py line 270 and it started to work if only saver.restore(sess, latest_checkpoint) is commented out in the line 274. When the line is enabled this time it throws the second exception given below. Could you propose a solution/workaround without having to upgrade tensorflow/python etc versions?\r\n",
      "\r\n",
      "  data_augmentation_options {\r\n",
      "    rgb_to_gray {\r\n",
      "    }\r\n",
      "  }\r\n",
      "  data_augmentation_options {\r\n",
      "    random_vertical_flip {\r\n",
      "    }\r\n",
      "\r\n",
      "ERROR:1\r\n",
      "------------------------------\r\n",
      "Assign requires shapes of both tensors to match. lhs shape= [7,7,3,64] rhs shape= [7,7,1,64]\r\n",
      "         [[node save/Assign_780 (defined at /truba/home/iuzun/Tensorflow/models/research/object_detection/legacy/evaluator.py:270) ]]\r\n",
      "\r\n",
      "Caused by op 'save/Assign_780', defined at:\r\n",
      "  File \"legacy/eval.py\", line 149, in <module>\r\n",
      "    tf.app.run()\r\n",
      "  File \"/truba/home/iuzun/anaconda3/envs/tensorflow_cpu1/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n",
      "    _sys.exit(main(argv))\r\n",
      "  File \"/truba/home/iuzun/anaconda3/envs/tensorflow_cpu1/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 324, in new_func\r\n",
      "    return func(*args, **kwargs)\r\n",
      "  File \"legacy/eval.py\", line 145, in main\r\n",
      "    graph_hook_fn=graph_rewriter_fn)\r\n",
      "  File \"/truba/home/iuzun/Tensorflow/models/research/object_detection/legacy/evaluator.py\", line 270, in evaluate\r\n",
      "    saver = tf.train.Saver(variables_to_restore)\r\n",
      "  File \"/truba/home/iuzun/anaconda3/envs/tensorflow_cpu1/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 832, in __init__\r\n",
      "    self.build()\r\n",
      "  File \"/truba/home/iuzun/anaconda3/envs/tensorflow_cpu1/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 844, in build\r\n",
      "    self._build(self._filename, build_save=True, build_restore=True)\r\n",
      "  File \"/truba/home/iuzun/anaconda3/envs/tensorflow_cpu1/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 881, in _build\r\n",
      "    build_save=build_save, build_restore=build_restore)\r\n",
      "  File \"/truba/home/iuzun/anaconda3/envs/tensorflow_cpu1/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 513, in _build_internal\r\n",
      "    restore_sequentially, reshape)\r\n",
      "  File \"/truba/home/iuzun/anaconda3/envs/tensorflow_cpu1/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 354, in _AddRestoreOps\r\n",
      "    assign_ops.append(saveable.restore(saveable_tensors, shapes))\r\n",
      "  File \"/truba/home/iuzun/anaconda3/envs/tensorflow_cpu1/lib/python3.6/site-packages/tensorflow/python/training/saving/saveable_object_util.py\", line 73, in restore\r\n",
      "    self.op.get_shape().is_fully_defined())\r\n",
      "  File \"/truba/home/iuzun/anaconda3/envs/tensorflow_cpu1/lib/python3.6/site-packages/tensorflow/python/ops/state_ops.py\", line 223, in assign\r\n",
      "    validate_shape=validate_shape)\r\n",
      "  File \"/truba/home/iuzun/anaconda3/envs/tensorflow_cpu1/lib/python3.6/site-packages/tensorflow/python/ops/gen_state_ops.py\", line 64, in assign\r\n",
      "    use_locking=use_locking, name=name)\r\n",
      "  File \"/truba/home/iuzun/anaconda3/envs/tensorflow_cpu1/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\r\n",
      "    op_def=op_def)\r\n",
      "  File \"/truba/home/iuzun/anaconda3/envs/tensorflow_cpu1/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n",
      "    return func(*args, **kwargs)\r\n",
      "  File \"/truba/home/iuzun/anaconda3/envs/tensorflow_cpu1/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\r\n",
      "    op_def=op_def)\r\n",
      "  File \"/truba/home/iuzun/anaconda3/envs/tensorflow_cpu1/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\r\n",
      "    self._traceback = tf_stack.extract_stack()\r\n",
      "\r\n",
      "InvalidArgumentError (see above for traceback): Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\r\n",
      "\r\n",
      "Assign requires shapes of both tensors to match. lhs shape= [7,7,3,64] rhs shape= [7,7,1,64]\r\n",
      "         [[node save/Assign_780 (defined at /truba/home/iuzun/Tensorflow/models/research/object_detection/legacy/evaluator.py:270) ]]\r\n",
      "------------------------------------------\r\n",
      "\r\n",
      "ERROR:2\r\n",
      "----------------------------------------------------------------------------------------------------------\r\n",
      "\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"legacy/eval.py\", line 154, in <module>\r\n",
      "    tf.app.run()\r\n",
      "  File \"/truba/home/iuzun/anaconda3/envs/tensorflow_cpu1/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n",
      "    _sys.exit(main(argv))\r\n",
      "  File \"/truba/home/iuzun/anaconda3/envs/tensorflow_cpu1/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 324, in new_func\r\n",
      "    return func(*args, **kwargs)\r\n",
      "  File \"legacy/eval.py\", line 150, in main\r\n",
      "    graph_hook_fn=graph_rewriter_fn)\r\n",
      "  File \"/truba/home/iuzun/Tensorflow/models/research/object_detection/legacy/evaluator.py\", line 302, in evaluate\r\n",
      "    eval_export_path=eval_config.export_path)\r\n",
      "  File \"/truba/home/iuzun/Tensorflow/models/research/object_detection/eval_util.py\", line 519, in repeated_checkpoint_run\r\n",
      "    process_metrics_fn=process_metrics_fn)\r\n",
      "  File \"/truba/home/iuzun/Tensorflow/models/research/object_detection/eval_util.py\", line 314, in _run_checkpoint_once\r\n",
      "    restore_fn(sess)\r\n",
      "  File \"/truba/home/iuzun/Tensorflow/models/research/object_detection/legacy/evaluator.py\", line 280, in _restore_latest_checkpoint\r\n",
      "    saver.restore(sess, latest_checkpoint)\r\n",
      "  File \"/truba/home/iuzun/anaconda3/envs/tensorflow_cpu1/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1312, in restore\r\n",
      "    err, \"a mismatch between the current graph and the graph\")\r\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\r\n",
      "\r\n",
      "Input to reshape is a tensor with 3136 values, but the requested shape has 9408\r\n",
      "         [[node save/Reshape_780 (defined at /truba/home/iuzun/Tensorflow/models/research/object_detection/legacy/evaluator.py:270) ]]\r\n",
      "\r\n",
      "Caused by op 'save/Reshape_780', defined at:\r\n",
      "  File \"legacy/eval.py\", line 154, in <module>\r\n",
      "    tf.app.run()\r\n",
      "  File \"/truba/home/iuzun/anaconda3/envs/tensorflow_cpu1/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n",
      "    _sys.exit(main(argv))\r\n",
      "  File \"/truba/home/iuzun/anaconda3/envs/tensorflow_cpu1/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 324, in new_func\r\n",
      "    return func(*args, **kwargs)\r\n",
      "  File \"legacy/eval.py\", line 150, in main\r\n",
      "    graph_hook_fn=graph_rewriter_fn)\r\n",
      "  File \"/truba/home/iuzun/Tensorflow/models/research/object_detection/legacy/evaluator.py\", line 270, in evaluate\r\n",
      "    saver = tf.train.Saver(var_list=variables_to_restore,reshape=True)\r\n",
      "  File \"/truba/home/iuzun/anaconda3/envs/tensorflow_cpu1/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 832, in __init__\r\n",
      "    self.build()\r\n",
      "  File \"/truba/home/iuzun/anaconda3/envs/tensorflow_cpu1/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 844, in build\r\n",
      "    self._build(self._filename, build_save=True, build_restore=True)\r\n",
      "  File \"/truba/home/iuzun/anaconda3/envs/tensorflow_cpu1/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 881, in _build\r\n",
      "    build_save=build_save, build_restore=build_restore)\r\n",
      "  File \"/truba/home/iuzun/anaconda3/envs/tensorflow_cpu1/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 513, in _build_internal\r\n",
      "    restore_sequentially, reshape)\r\n",
      "  File \"/truba/home/iuzun/anaconda3/envs/tensorflow_cpu1/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 354, in _AddRestoreOps\r\n",
      "    assign_ops.append(saveable.restore(saveable_tensors, shapes))\r\n",
      "  File \"/truba/home/iuzun/anaconda3/envs/tensorflow_cpu1/lib/python3.6/site-packages/tensorflow/python/training/saving/saveable_object_util.py\", line 68, in restore\r\n",
      "    restored_tensor = array_ops.reshape(restored_tensor, restored_shapes[0])\r\n",
      "  File \"/truba/home/iuzun/anaconda3/envs/tensorflow_cpu1/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 7179, in reshape\r\n",
      "    \"Reshape\", tensor=tensor, shape=shape, name=name)\r\n",
      "  File \"/truba/home/iuzun/anaconda3/envs/tensorflow_cpu1/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\r\n",
      "    op_def=op_def)\r\n",
      "  File \"/truba/home/iuzun/anaconda3/envs/tensorflow_cpu1/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n",
      "    return func(*args, **kwargs)\r\n",
      "  File \"/truba/home/iuzun/anaconda3/envs/tensorflow_cpu1/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\r\n",
      "    op_def=op_def)\r\n",
      "  File \"/truba/home/iuzun/anaconda3/envs/tensorflow_cpu1/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\r\n",
      "    self._traceback = tf_stack.extract_stack()\r\n",
      "\r\n",
      "InvalidArgumentError (see above for traceback): Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\r\n",
      "\r\n",
      "Input to reshape is a tensor with 3136 values, but the requested shape has 9408\r\n",
      "         [[node save/Reshape_780 (defined at /truba/home/iuzun/Tensorflow/models/research/object_detection/legacy/evaluator.py:270) ]]\r\n",
      "----------------------------------------------------------------------------------------------------------\r\n",
      "\r\n",
      "Executed command:\r\n",
      "python legacy/eval.py --logtostderr --pipeline_config_path=training/faster_rcnn_resnet152_coco.config --checkpoint_dir=training/ --eval_dir=eval/\r\n",
      "\r\n",
      "Thanks in advance\n",
      "issue labels - \n",
      "TF 1.13\n",
      "comp:apis\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  TF 2.4 shows shape mismatch where 2.3.1 did not\n",
      "issue body -  <em>Please make sure that this is a bug. As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:bug_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:No\r\n",
      "- TensorFlow installed from (source or binary):binary\r\n",
      "- TensorFlow version (use command below):2.4\r\n",
      "- Python version:3.7\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version:\r\n",
      "- GPU model and memory:\r\n",
      "\r\n",
      "You can collect some of this information using our environment capture\r\n",
      "[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n",
      "You can also obtain the TensorFlow version with:\r\n",
      "1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n",
      "2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n",
      "\r\n",
      "i_model = Model([obs1.input, obs2.input], x, name='icm_inverse_model') returns a shape mismatch under the current version of tensorflow.\r\n",
      "\r\n",
      "\r\n",
      "No mismatch should be given as was the case in version 2.3.1 unless I am missing somthing...\r\n",
      "\r\n",
      "```from keras.models import Model, Sequential\r\n",
      "from keras.layers import Input, Concatenate, GRU, Dense, Reshape\r\n",
      "from keras.optimizers import Adam\r\n",
      "from keras.backend import clear_session\r\n",
      "from pathlib import Path\r\n",
      "from subprocess import Popen, PIPE, STDOUT, TimeoutExpired\r\n",
      "import numpy as np\r\n",
      "import tensorflow as tf\r\n",
      "from rl.agents import SARSAAgent\r\n",
      "from rl.policy import BoltzmannQPolicy\r\n",
      "import os\r\n",
      "\r\n",
      "cmd = 'echo Hello World!'\r\n",
      "env_reward = 0\r\n",
      "length_penalty = .25\r\n",
      "learning_reward = 10\r\n",
      "\r\n",
      "hidden_layers = 4\r\n",
      "layer_neurons = 128\r\n",
      "learning_rate = 0.001\r\n",
      "nb_actions = 96\r\n",
      "\r\n",
      "tf.get_logger().setLevel('ERROR')\r\n",
      "\r\n",
      "done = False\r\n",
      "cmd_in = True\r\n",
      "obs_last = None\r\n",
      "initialize = True\r\n",
      "\r\n",
      "while True:\r\n",
      "    if cmd_in:\r\n",
      "        proc = Popen(cmd, shell=True, stdin=PIPE, stdout=PIPE, stderr=STDOUT, close_fds=True)\r\n",
      "        try:\r\n",
      "            stdout = proc.communicate(timeout=1)[0].decode()\r\n",
      "            exitcode = proc.returncode\r\n",
      "        except TimeoutExpired:\r\n",
      "            proc.kill()\r\n",
      "            stdout = proc.communicate()[0].decode()\r\n",
      "            exitcode = proc.returncode\r\n",
      "        nnin = ''.join(char for char in stdout if char.isprintable())\r\n",
      "        filename = Path('mem.txt')\r\n",
      "        filename.touch(exist_ok=True)\r\n",
      "        if not nnin:\r\n",
      "            nnin = 'Done!'\r\n",
      "            stdout = nnin\r\n",
      "        if exitcode == 0:\r\n",
      "            done = True\r\n",
      "            with open('mem.txt', 'r+') as mem:\r\n",
      "                for line in stdout.splitlines():\r\n",
      "                    if line + '\\n' not in mem:\r\n",
      "                        mem.write(line + '\\n')\r\n",
      "                        env_reward += learning_reward\r\n",
      "        cmd = ''\r\n",
      "        print('\\n')\r\n",
      "        print(stdout)\r\n",
      "        print('# ', end='', flush=True)\r\n",
      "    else:\r\n",
      "        nnin = cmd\r\n",
      "        print(nnin[-1], end='', flush=True)\r\n",
      "        env_reward -= length_penalty\r\n",
      "    idxs = (np.frombuffer(nnin.encode(), dtype=np.uint8) - 32) / 100 \r\n",
      "    env = tf.reshape(idxs, idxs.shape + (1,))\r\n",
      "    shape = env.shape\r\n",
      "\r\n",
      "\r\n",
      "    def build_actor_model(shape, nb_actions):\r\n",
      "        model = Sequential()\r\n",
      "        model.add(Reshape(shape[1::], input_shape=shape))\r\n",
      "        for layer in range(hidden_layers):\r\n",
      "            model.add(GRU(layer_neurons, name='GRU' + str(layer), return_sequences=True))\r\n",
      "        model.add(GRU(layer_neurons, name='GRU' + str(hidden_layers)))\r\n",
      "        model.add(Dense(nb_actions, name='output', activation='softmax'))\r\n",
      "        return model\r\n",
      "\r\n",
      "\r\n",
      "    def build_main(shape, name_prefix='main.'):\r\n",
      "        inputs = Input(shape=shape)\r\n",
      "        x = inputs\r\n",
      "        for layer in range(hidden_layers):\r\n",
      "            x = GRU(layer_neurons, name=name_prefix + ('GRU' + str(layer)), return_sequences=True)(x)\r\n",
      "        x = GRU(layer_neurons, name=name_prefix + ('GRU' + str(hidden_layers)))(x)\r\n",
      "        model = Model(inputs, x, name=name_prefix + 'main')\r\n",
      "        return model\r\n",
      "\r\n",
      "\r\n",
      "    def build_inverse_model(obs1, obs2, nb_actions):\r\n",
      "        x = Concatenate()([obs1.output, obs2.output])\r\n",
      "        x = Dense(nb_actions, name='icm_i.output', activation='sigmoid')(x)\r\n",
      "        i_model = Model([obs1.input, obs2.input], x, name='icm_inverse_model')\r\n",
      "        return i_model\r\n",
      "\r\n",
      "\r\n",
      "    def build_forward_model(obs1, nb_actions):\r\n",
      "        act1 = Input(shape=nb_actions)\r\n",
      "        x = Concatenate()([obs1.output, act1])\r\n",
      "        output_shape = obs1.output_shape[1]\r\n",
      "        x = Dense(output_shape, name='icm_f.output', activation='linear')(x)\r\n",
      "        f_model = Model([obs1.input, act1], x, name='icm_forward_model')\r\n",
      "        return f_model\r\n",
      "\r\n",
      "\r\n",
      "    inv_weights_fname = '{}_inv_weights.h5f'.format(\"SMB\")\r\n",
      "    fwd_weights_fname = '{}_fwd_weights.h5f'.format(\"SMB\")\r\n",
      "    agent_weights_fname = '{}_agent_weights.h5f'.format(\"SMB\")\r\n",
      "\r\n",
      "    main = build_main(shape)\r\n",
      "    main2 = build_main(shape, name_prefix='main2.')\r\n",
      "    inverse_model = build_inverse_model(main, main2, nb_actions)\r\n",
      "    inverse_model.compile(Adam(learning_rate), loss='mse', metrics=['mse'])\r\n",
      "    forward_model = build_forward_model(main, nb_actions)\r\n",
      "    forward_model.compile(Adam(learning_rate), loss='mse', metrics=['mse'])\r\n",
      "    model = build_actor_model((1,) + shape, nb_actions)\r\n",
      "    policy = BoltzmannQPolicy()\r\n",
      "    agent = SARSAAgent(model=model, nb_actions=nb_actions, policy=policy)\r\n",
      "    agent.compile(Adam(learning_rate), metrics=['mae'])\r\n",
      "    agent.reset_states()\r\n",
      "\r\n",
      "    if initialize:\r\n",
      "        if os.path.isfile(inv_weights_fname):\r\n",
      "            inverse_model.load_weights(inv_weights_fname)\r\n",
      "        if os.path.isfile(fwd_weights_fname):\r\n",
      "            forward_model.load_weights(fwd_weights_fname)\r\n",
      "        if os.path.isfile(agent_weights_fname):\r\n",
      "            agent.load_weights(agent_weights_fname)\r\n",
      "        initialize = False\r\n",
      "    agent.training = True\r\n",
      "\r\n",
      "    obs_now = env\r\n",
      "    if obs_last is None:\r\n",
      "        obs_last = obs_now\r\n",
      "    action = agent.forward(obs_now)\r\n",
      "    icm_action = np.zeros(nb_actions)\r\n",
      "    icm_action[action] = 1\r\n",
      "    inv_loss = inverse_model.train_on_batch([np.expand_dims(obs_last, 0), np.expand_dims(obs_now, 0)],\r\n",
      "                                            [np.expand_dims(icm_action, 0)])\r\n",
      "    features_now = main.predict(np.expand_dims(obs_now, 0))\r\n",
      "    fwd_loss = forward_model.train_on_batch([np.expand_dims(obs_last, 0), np.expand_dims(icm_action, 0)],\r\n",
      "                                            [features_now])\r\n",
      "    obs_last = obs_now\r\n",
      "    r_intr = (fwd_loss[0] ** 0.5) / 100\r\n",
      "    reward = r_intr + env_reward\r\n",
      "    agent.backward(reward, done)\r\n",
      "    clear_session()\r\n",
      "    done = False\r\n",
      "\r\n",
      "    enc_ascii = action + 32\r\n",
      "    if enc_ascii != 127:\r\n",
      "        cmd += chr(enc_ascii)\r\n",
      "        cmd_in = False\r\n",
      "        continue\r\n",
      "    cmd_in = True\r\n",
      "    inverse_model.save_weights(inv_weights_fname, overwrite=True)\r\n",
      "    forward_model.save_weights(fwd_weights_fname, overwrite=True)\r\n",
      "    agent.save_weights(agent_weights_fname, overwrite=True)```\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:keras\n",
      "regression issue\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  ERROR: Could not install packages due to an EnvironmentError: [WinError 5] Access is denied: 'c:\\\\programdata\\\\anaconda3\\\\lib\\\\site-packages\\\\cloudpickle-1.6.0.dist-info\\\\direct_url.json' Consider using the `--user` option or check the permissions\n",
      "issue body -  i am facing this issue while intalling rasa\n",
      "issue labels - \n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  Important performance difference between training and evaluation mode\n",
      "issue body -  <em>Please make sure that this is an issue related to performance of TensorFlow.\r\n",
      "As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:performance_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "- TensorFlow installed from (source or binary): source\r\n",
      "- TensorFlow version (use command below): 2.4.0\r\n",
      "- Python version: 3.7\r\n",
      "\r\n",
      "Training a model using .fit method, there is an important difference of performance between training and validation even if the train split is also used for validation. The difference is the training and testing mode for the batchnorm but this should not give a difference in performance.\r\n",
      "\r\n",
      "```python\r\n",
      "import tensorflow as tf\r\n",
      "import tensorflow_datasets as tfds\r\n",
      "\r\n",
      "\r\n",
      "class Residual3x3Unit(tf.keras.layers.Layer):\r\n",
      "    def __init__(self, channels_in, channels_out, stride, droprate=0., activate_before_residual=False):\r\n",
      "        super(Residual3x3Unit, self).__init__()\r\n",
      "        self.bn_0 = tf.keras.layers.BatchNormalization(momentum=0.999)\r\n",
      "        self.relu_0 = tf.keras.layers.LeakyReLU(alpha=0.1)\r\n",
      "        self.conv_0 = tf.keras.layers.Conv2D(channels_out, kernel_size=3, strides=stride, padding='same', use_bias=False)\r\n",
      "        self.bn_1 = tf.keras.layers.BatchNormalization(momentum=0.999)\r\n",
      "        self.relu_1 = tf.keras.layers.LeakyReLU(alpha=0.1)\r\n",
      "        self.conv_1 = tf.keras.layers.Conv2D(channels_out, kernel_size=3, strides=1, padding='same', use_bias=False)\r\n",
      "        self.downsample = channels_in != channels_out\r\n",
      "        self.shortcut = tf.keras.layers.Conv2D(channels_out, kernel_size=1, strides=stride, use_bias=False)\r\n",
      "        self.activate_before_residual = activate_before_residual\r\n",
      "        self.dropout = tf.keras.layers.Dropout(rate=droprate)\r\n",
      "        self.droprate = droprate\r\n",
      "\r\n",
      "    def call(self, x, training=True):\r\n",
      "        if self.downsample and self.activate_before_residual:\r\n",
      "            x = self.relu_0(self.bn_0(x, training=training))\r\n",
      "        elif not self.downsample:\r\n",
      "            out = self.relu_0(self.bn_0(x, training=training))\r\n",
      "        out = self.relu_1(self.bn_1(self.conv_0(x if self.downsample else out), training=training))\r\n",
      "        if self.droprate > 0.:\r\n",
      "            out = self.dropout(out)\r\n",
      "        out = self.conv_1(out)\r\n",
      "        return out + (self.shortcut(x) if self.downsample else x)\r\n",
      "\r\n",
      "\r\n",
      "class ResidualBlock(tf.keras.layers.Layer):\r\n",
      "    def __init__(self, n_units, channels_in, channels_out, unit, stride, droprate=0., activate_before_residual=False):\r\n",
      "        super(ResidualBlock, self).__init__()\r\n",
      "        self.units = self._build_unit(n_units, unit, channels_in, channels_out, stride, droprate, activate_before_residual)\r\n",
      "\r\n",
      "    def _build_unit(self, n_units, unit, channels_in, channels_out, stride, droprate, activate_before_residual):\r\n",
      "        units = []\r\n",
      "        for i in range(n_units):\r\n",
      "            units.append(unit(channels_in if i == 0 else channels_out, channels_out, stride if i == 0 else 1, droprate, activate_before_residual))\r\n",
      "        return units\r\n",
      "\r\n",
      "    def call(self, x, training=True):\r\n",
      "        for unit in self.units:\r\n",
      "            x = unit(x, training=training)\r\n",
      "        return x\r\n",
      "\r\n",
      "\r\n",
      "class WideResNet(tf.keras.Model):\r\n",
      "    def __init__(self, num_classes, depth=28, width=2, droprate=0., input_shape=(None, 32, 32, 3), **kwargs):\r\n",
      "        super(WideResNet, self).__init__(input_shape, **kwargs)\r\n",
      "        assert (depth - 4) % 6 == 0\r\n",
      "        N = int((depth - 4) / 6)\r\n",
      "        channels = [16, 16 * width, 32 * width, 64 * width]\r\n",
      "\r\n",
      "        self.conv_0 = tf.keras.layers.Conv2D(channels[0], kernel_size=3, strides=1, padding='same', use_bias=False)\r\n",
      "        self.block_0 = ResidualBlock(N, channels[0], channels[1], Residual3x3Unit, 1, droprate, True)\r\n",
      "        self.block_1 = ResidualBlock(N, channels[1], channels[2], Residual3x3Unit, 2, droprate)\r\n",
      "        self.block_2 = ResidualBlock(N, channels[2], channels[3], Residual3x3Unit, 2, droprate)\r\n",
      "        self.bn_0 = tf.keras.layers.BatchNormalization(momentum=0.999)\r\n",
      "        self.relu_0 = tf.keras.layers.LeakyReLU(alpha=0.1)\r\n",
      "        self.avg_pool = tf.keras.layers.AveragePooling2D((8, 8), (1, 1))\r\n",
      "        self.flatten = tf.keras.layers.Flatten()\r\n",
      "        self.dense = tf.keras.layers.Dense(num_classes, activation='softmax')\r\n",
      "\r\n",
      "    def call(self, inputs, training=True):\r\n",
      "        x = inputs\r\n",
      "        x = self.conv_0(x)\r\n",
      "        x = self.block_0(x, training=training)\r\n",
      "        x = self.block_1(x, training=training)\r\n",
      "        x = self.block_2(x, training=training)\r\n",
      "        x = self.relu_0(self.bn_0(x, training=training))\r\n",
      "        x = self.avg_pool(x)\r\n",
      "        x = self.flatten(x)\r\n",
      "        x = self.dense(x)\r\n",
      "        return x\r\n",
      "\r\n",
      "train_ds = tfds.load('cifar10', split=\"train\")\r\n",
      "\r\n",
      "test_ds = tfds.load('cifar10', split=\"train\")\r\n",
      "\r\n",
      "def preprocess(ex):\r\n",
      "\r\n",
      "    image = tf.cast(ex['image'], tf.float32) / 255.\r\n",
      "    return image, ex['label']\r\n",
      "\r\n",
      "train_ds = train_ds.map(preprocess).shuffle(1024).repeat().batch(64)\r\n",
      "test_ds = test_ds.map(preprocess).repeat().batch(64)\r\n",
      "\r\n",
      "model = WideResNet(10)\r\n",
      "\r\n",
      "optimizer = tf.keras.optimizers.SGD(0.03, momentum=0.9)\r\n",
      "model.compile(optimizer=optimizer, loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])\r\n",
      "\r\n",
      "model.fit(train_ds, epochs=10, steps_per_epoch=100, validation_data=test_ds, validation_steps=100)\r\n",
      "```\r\n",
      "\r\n",
      "The output is the following:\r\n",
      "\r\n",
      "```\r\n",
      "Epoch 1/10\r\n",
      "100/100 [==============================] - 15s 58ms/step - loss: 1.9948 - accuracy: 0.2491 - val_loss: 3.6047 - val_accuracy: 0.1061\r\n",
      "Epoch 2/10\r\n",
      "100/100 [==============================] - 5s 51ms/step - loss: 1.6504 - accuracy: 0.3899 - val_loss: 2.1616 - val_accuracy: 0.2086\r\n",
      "Epoch 3/10\r\n",
      "100/100 [==============================] - 5s 51ms/step - loss: 1.4418 - accuracy: 0.4759 - val_loss: 5.3945 - val_accuracy: 0.1839\r\n",
      "Epoch 4/10\r\n",
      "100/100 [==============================] - 5s 51ms/step - loss: 1.3975 - accuracy: 0.4871 - val_loss: 2.5811 - val_accuracy: 0.2809\r\n",
      "Epoch 5/10\r\n",
      "100/100 [==============================] - 5s 52ms/step - loss: 1.2669 - accuracy: 0.5371 - val_loss: 2.1341 - val_accuracy: 0.3431\r\n",
      "Epoch 6/10\r\n",
      "100/100 [==============================] - 5s 51ms/step - loss: 1.1513 - accuracy: 0.5834 - val_loss: 3.0617 - val_accuracy: 0.3303\r\n",
      "Epoch 7/10\r\n",
      "100/100 [==============================] - 5s 51ms/step - loss: 1.1181 - accuracy: 0.5982 - val_loss: 2.0735 - val_accuracy: 0.4008\r\n",
      "Epoch 8/10\r\n",
      "100/100 [==============================] - 5s 51ms/step - loss: 1.0284 - accuracy: 0.6285 - val_loss: 3.5450 - val_accuracy: 0.3250\r\n",
      "Epoch 9/10\r\n",
      "100/100 [==============================] - 5s 49ms/step - loss: 1.0219 - accuracy: 0.6352 - val_loss: 1.9222 - val_accuracy: 0.4300\r\n",
      "Epoch 10/10\r\n",
      "100/100 [==============================] - 5s 49ms/step - loss: 0.9897 - accuracy: 0.6519 - val_loss: 2.6546 - val_accuracy: 0.4075\r\n",
      "```\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:keras\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  TF-TRT conversion failing (maybe because of 2GB limit on GraphDef)\n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu Linux 16.04\r\n",
      "- TensorFlow installed from (source or binary): No\r\n",
      "- TensorFlow version (use command below): 2.2.0\r\n",
      "- Python version: 3.6\r\n",
      "- CUDA/cuDNN version: 10.1\r\n",
      "- GPU model and memory: RTX 2080\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "While trying to convert a RoBERTa model to TRT, it throws an error: `KeyError: \"The name 'input_word_ids:0' refers to a Tensor which does not exist. The operation, 'input_word_ids', does not exist in the graph.\"`\r\n",
      "\r\n",
      "But after checking with `saved_model_cli`, it is indeed part of the graph. Investigating a bit further, this might happen because of another (previous) error:\r\n",
      "\r\n",
      "```\r\n",
      "[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/message_lite.cc:406] tensorflow.GraphDef exceeded maximum protobuf size of 2GB: 2222908841\r\n",
      "```\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "\r\n",
      "It should convert it to TRT, as it successfully happens when using a smaller model (e.g. `DistilRoBERTa`).\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "```\r\n",
      "from tensorflow.python.compiler.tensorrt import trt_convert as trt\r\n",
      "\r\n",
      "converter = trt.TrtGraphConverterV2(\r\n",
      "    input_saved_model_dir=<saved model directory>, \r\n",
      "    conversion_params=trt.TrtConversionParams()\r\n",
      ")\r\n",
      "\r\n",
      "converter.convert()\r\n",
      "converter.save(<output dir>)\r\n",
      "```\r\n",
      "\r\n",
      "Additional info:\r\n",
      "Related to https://github.com/tensorflow/tensorrt/issues/229\n",
      "issue labels - \n",
      "TF 2.2\n",
      "comp:gpu:tensorrt\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  asdasdas\n",
      "issue body -  **System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n",
      "- TensorFlow installed from (source or binary):\r\n",
      "- TensorFlow version (or github SHA if from source):\r\n",
      "\r\n",
      "\r\n",
      "**Command used to run the converter or code if you’re using the Python API**\r\n",
      "If possible, please share a link to Colab/Jupyter/any notebook.\r\n",
      "\r\n",
      "```\r\n",
      "# Copy and paste here the exact command\r\n",
      "```\r\n",
      "\r\n",
      "**The output from the converter invocation**\r\n",
      "\r\n",
      "```\r\n",
      "# Copy and paste the output here.\r\n",
      "```\r\n",
      "\r\n",
      "**Also, please include a link to the saved model or GraphDef**\r\n",
      "\r\n",
      "```\r\n",
      "# Put link here or attach to the issue.\r\n",
      "```\r\n",
      "\r\n",
      "**Failure details**\r\n",
      "If the conversion is successful, but the generated model is wrong,\r\n",
      "state what is wrong:\r\n",
      "- Producing wrong results and/or decrease in accuracy\r\n",
      "- Producing correct results, but the model is slower than expected (model generated from old converter)\r\n",
      "\r\n",
      "\r\n",
      "**RNN conversion support**\r\n",
      "If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.\r\n",
      "\r\n",
      "**Any other info / logs**\r\n",
      "\r\n",
      "Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n",
      "\n",
      "issue labels - \n",
      "TFLiteConverter\n",
      "invalid\n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  Update SQLite to the lastest sqlite-amalgamation-3340000\n",
      "issue body -  This PR updates SQLite to the latest sqlite-amalgamation-3340000\r\n",
      "\r\n",
      "Signed-off-by: Yong Tang <yong.tang.github@outlook.com>\n",
      "issue labels - \n",
      "cla: yes\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Value of num_replicas parameter now propagates.\n",
      "issue body -  The value of `num_replicas` parameter of the `_report` function in `resnet50_test.py` was not passed to the `report` function in `resnet50_test_util.py`, as a relevant parameter (with the same name) was assigned a value in the call. \r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:eager\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  TF 2.4.0 Python dependency issues\n",
      "issue body -  <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- OS Platform and Distribution : OSX, Centos\r\n",
      "- TensorFlow installed from (source or binary): package installed with pipenv from pypi\r\n",
      "- TensorFlow version: 2.4.0\r\n",
      "- Python version: 3.8.6\r\n",
      "- Installed using virtualenv? pip? conda?: pipenv\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "\r\n",
      "**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n",
      "\r\n",
      "```pipenv install``` from the following Pipfile, which now picks up the latest TF 2.4.0:\r\n",
      "```\r\n",
      "[[source]]\r\n",
      "name = \"pypi\"\r\n",
      "url = \"https://pypi.org/simple\"\r\n",
      "verify_ssl = true\r\n",
      "\r\n",
      "[dev-packages]\r\n",
      "pylint = \"*\"\r\n",
      "pytest-cov = \"*\"\r\n",
      "grpcio-tools = \"*\"\r\n",
      "\r\n",
      "[packages]\r\n",
      "scikit-learn = \"*\"\r\n",
      "pytest = \"*\"\r\n",
      "pytest-docker = \"*\"\r\n",
      "numpy = \"*\"\r\n",
      "tensorflow-serving-api = \"*\"\r\n",
      "tensorflow.python.framework = \"*\"\r\n",
      "grpcio = \"*\"\r\n",
      "grpcio-reflection = \"*\"\r\n",
      "spacy = \"*\"\r\n",
      "py-grpc-prometheus = \"*\"\r\n",
      "protobuf = \"*\"\r\n",
      "dynaconf = \"*\"\r\n",
      "google-cloud-storage = \"*\"\r\n",
      "\r\n",
      "[requires]\r\n",
      "python_version = \"3.8\"\r\n",
      "```\r\n",
      "\r\n",
      "**Any other info / logs**\r\n",
      "Here is the log from ```pipenv check``` after installation:\r\n",
      "```\r\n",
      "Checking PEP 508 requirements...\r\n",
      "Passed!\r\n",
      "Checking installed package safety...\r\n",
      "38932: cryptography <=3.2 resolved (3.1.1 installed)!\r\n",
      "Cryptography 3.2 was released with the warning that its maintainers became aware of a Bleichenbacher vulnerability that they were only partly able to mitigate. See: CVE-2020-25659.\r\n",
      "```\r\n",
      "However, this was from an existing ```Pipfile.lock```. Deleting ```Pipfile.lock``` and re-installing in a new venv resulted in the following dependency mismatches:\r\n",
      "```\r\n",
      "[pipenv.exceptions.ResolutionFailure]: Warning: Your dependencies could not be resolved. You likely have a mismatch in your sub-dependencies.\r\n",
      "  First try clearing your dependency cache with $ pipenv lock --clear, then try the original command again.\r\n",
      " Alternatively, you can use $ pipenv install --skip-lock to bypass this mechanism, then run $ pipenv graph to inspect the situation.\r\n",
      "  Hint: try $ pipenv lock --pre if it is a pre-release dependency.\r\n",
      "ERROR: Could not find a version that matches grpcio>=1.0<2,>=1.10.0,>=1.34.0,~=1.32.0 (from -r /var/folders/ky/kytzhfhs1cdfkwgs_4hyfpbmbtbvby/T/pipenvjgjmye8frequirements/pipenv-fz4mci2f-constraints.txt (line 5))\r\n",
      "Tried: 0.13.0, 0.13.1, 0.14.0, 0.15.0, 1.0.0, 1.0.1, 1.0.2, 1.0.3, 1.0.4, 1.1.0, 1.1.3, 1.2.0, 1.2.1, 1.3.0, 1.3.5, 1.4.0, 1.6.0, 1.6.3, 1.7.0, 1.7.3, 1.8.1, 1.8.2, 1.8.3, 1.8.4, 1.8.6, 1.9.0, 1.9.1, 1.10.0, 1.10.1, 1.11.0, 1.11.1, 1.12.0, 1.12.1, 1.13.0, 1.14.0, 1.14.1, 1.14.2, 1.15.0, 1.16.0, 1.16.1, 1.17.0, 1.17.1, 1.18.0, 1.19.0, 1.20.0, 1.20.1, 1.21.1, 1.22.0, 1.22.1, 1.23.0, 1.23.1, 1.24.0, 1.24.1, 1.24.3, 1.25.0, 1.26.0, 1.26.0, 1.27.1, 1.27.1, 1.27.2, 1.27.2, 1.28.1, 1.28.1, 1.29.0, 1.29.0, 1.30.0, 1.30.0, 1.31.0, 1.31.0, 1.32.0, 1.32.0, 1.33.1, 1.33.1, 1.33.2, 1.33.2, 1.34.0, 1.34.0\r\n",
      "Skipped pre-versions: 0.4.0a0, 0.4.0a1, 0.4.0a2, 0.4.0a3, 0.4.0a4, 0.4.0a5, 0.4.0a6, 0.4.0a7, 0.4.0a8, 0.4.0a13, 0.4.0a14, 0.5.0a0, 0.5.0a1, 0.5.0a2, 0.9.0a0, 0.9.0a1, 0.10.0a0, 0.11.0b0, 0.11.0b1, 0.12.0b0, 0.13.1rc1, 0.14.0rc1, 1.0.0rc1, 1.0.0rc2, 1.0.1rc1, 1.0.2rc0, 1.1.0rc1, 1.2.0rc1, 1.2.0rc2, 1.4.0rc1, 1.6.0rc1, 1.7.0rc1, 1.8.0rc1, 1.8.0rc2, 1.8.0rc3, 1.9.0rc1, 1.9.0rc2, 1.9.0rc3, 1.10.0rc1, 1.10.0rc2, 1.10.1rc1, 1.10.1rc2, 1.11.0rc1, 1.11.0rc2, 1.11.1rc1, 1.12.0rc1, 1.13.0rc1, 1.13.0rc2, 1.13.0rc3, 1.14.0rc1, 1.14.0rc2, 1.14.2rc1, 1.15.0rc1, 1.16.0rc1, 1.16.1rc1, 1.17.0rc1, 1.17.1rc1, 1.18.0rc1, 1.19.0rc1, 1.20.0rc1, 1.20.0rc2, 1.20.0rc3, 1.21.0rc1, 1.21.1rc1, 1.22.0rc1, 1.23.0rc1, 1.24.0rc1, 1.25.0rc1, 1.26.0rc1, 1.26.0rc1, 1.27.0rc1, 1.27.0rc1, 1.27.0rc2, 1.27.0rc2, 1.28.0.dev0, 1.28.0.dev0, 1.28.0rc1, 1.28.0rc1, 1.28.0rc2, 1.28.0rc2, 1.28.0rc3, 1.28.0rc3, 1.30.0rc1, 1.30.0rc1, 1.31.0rc1, 1.31.0rc1, 1.31.0rc2, 1.31.0rc2, 1.32.0rc1, 1.32.0rc1, 1.33.0rc1, 1.33.0rc1, 1.33.0rc2, 1.33.0rc2, 1.34.0rc1, 1.34.0rc1\r\n",
      "There are incompatible versions in the resolved dependencies:\r\n",
      "  grpcio (from -r /var/folders/ky/kytzhfhs1cdfkwgs_4hyfpbmbtbvby/T/pipenvjgjmye8frequirements/pipenv-fz4mci2f-constraints.txt (line 5))\r\n",
      "  grpcio>=1.0<2 (from tensorflow-serving-api==2.4.0->-r /var/folders/ky/kytzhfhs1cdfkwgs_4hyfpbmbtbvby/T/pipenvjgjmye8frequirements/pipenv-fz4mci2f-constraints.txt (line 13))\r\n",
      "  grpcio>=1.10.0 (from py-grpc-prometheus==0.2.0->-r /var/folders/ky/kytzhfhs1cdfkwgs_4hyfpbmbtbvby/T/pipenvjgjmye8frequirements/pipenv-fz4mci2f-constraints.txt (line 12))\r\n",
      "  grpcio>=1.34.0 (from grpcio-reflection==1.34.0->-r /var/folders/ky/kytzhfhs1cdfkwgs_4hyfpbmbtbvby/T/pipenvjgjmye8frequirements/pipenv-fz4mci2f-constraints.txt (line 10))\r\n",
      "  grpcio~=1.32.0 (from tensorflow==2.4.0->-r /var/folders/ky/kytzhfhs1cdfkwgs_4hyfpbmbtbvby/T/pipenvjgjmye8frequirements/pipenv-fz4mci2f-constraints.txt (line 3))\r\n",
      "```\r\n",
      "The problem here is TF 2.4.0 depends on ```grpcio``` 1.32.0, which does not exist.\r\n",
      "\r\n",
      "Fixing version to TF 2.3.1 and tensorflow-serving-api to 2.3.0 resolves the pipenv install and the check failure. Pipfile as follows:\r\n",
      "```\r\n",
      "[[source]]\r\n",
      "name = \"pypi\"\r\n",
      "url = \"https://pypi.org/simple\"\r\n",
      "verify_ssl = true\r\n",
      "\r\n",
      "[dev-packages]\r\n",
      "pylint = \"*\"\r\n",
      "pytest-cov = \"*\"\r\n",
      "grpcio-tools = \"*\"\r\n",
      "\r\n",
      "[packages]\r\n",
      "scikit-learn = \"*\"\r\n",
      "pytest = \"*\"\r\n",
      "pytest-docker = \"*\"\r\n",
      "numpy = \"*\"\r\n",
      "tensorflow = \"==2.3.1\"\r\n",
      "tensorflow-serving-api = \"==2.3.0\"\r\n",
      "grpcio = \"*\"\r\n",
      "grpcio-reflection = \"*\"\r\n",
      "spacy = \"*\"\r\n",
      "py-grpc-prometheus = \"*\"\r\n",
      "protobuf = \"*\"\r\n",
      "dynaconf = \"*\"\r\n",
      "google-cloud-storage = \"*\"\r\n",
      "\r\n",
      "[requires]\r\n",
      "python_version = \"3.8\"\r\n",
      "```\n",
      "issue labels - \n",
      "TF 2.4\n",
      "stat:awaiting response\n",
      "subtype: ubuntu/linux\n",
      "subtype:macOS\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  Entmax-alpha in addons\n",
      "issue body -  I would like to implement the entmax-alpha feature as is described in https://arxiv.org/abs/1905.05702 in the addons. For alpha=1, it is softmax, and for alpha=2, it is sparsemax. \r\n",
      "\r\n",
      "This feature is a key to realize sparse seq-to-seq model. It will not change the current API. \r\n",
      "\r\n",
      "I have almost finished implementing the feature. Therefore, I would like to know if this has any chance of being merged before continuing. \r\n",
      "\n",
      "issue labels - \n",
      "stat:awaiting response\n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  regression: dropped Support of CUDA® architecture 5.0?\n",
      "issue body -  I'm using trying to use tensorflow 2.4 with nvidia 960m (compute compatibility 5.0)\r\n",
      "\r\n",
      "According to the following change: https://github.com/tensorflow/docs/commit/cb886cfdd16d66ff7f8d1430676ff395b02910e6\r\n",
      "\r\n",
      "Support of my card and many others have been dropped (My card at least definitely worked fine with tensorflow 2.2)\r\n",
      "\r\n",
      "This is a major regression for many who are trying to upgrade, and currently, it is not even clear to me if it's intended and what is the rational behind it. If it is intended, please document it prominently in the 2.4 release. (and please consider not dropping the support of this architecture)\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:gpu\n",
      "stat:awaiting tensorflower\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  How to catch internal TensorFlow error: 'GPU sync failed' in python?\n",
      "issue body -  TF version 1.x and 2.x.\r\n",
      "\r\n",
      "Is there a way to catch this error with try-expect in python?\r\n",
      "\r\n",
      "`tensorflow.python.framework.errors_impl.InternalError: GPU sync failed`\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:gpu\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  How to catch/expect the error: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:273] Unexpected Event status: 1\n",
      "issue body -  TF version 1.x and 2.x.\r\n",
      "\r\n",
      "Is there a way to catch this error with try-expect in python?\r\n",
      "\r\n",
      "`F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:273] Unexpected Event status: 1`\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:gpu\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Training a keras model on TPU pods?\n",
      "issue body -  Hi!\r\n",
      "\r\n",
      "Are there any examples on how to run a Keras model on TPU pods? I have a model which runs fine on a V3-8. However, when trying to run the same code on V3-32, it fails with the following error:\r\n",
      "\r\n",
      "`Failed copying input tensor from /job:worker/replica:0/task:0/device:CPU:0 to /job:worker/replica:0/task:1/device:CPU:0 in order to run DatasetFromGraph: FetchOutputs node : not found [Op:DatasetFromGraph]`\r\n",
      "\r\n",
      "The model is created and compiled within the TPU scope as recommended:\r\n",
      "\r\n",
      "```\r\n",
      "with strategy.scope():\r\n",
      "    keras_model = create_model()\r\n",
      "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08)\r\n",
      "    keras_model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\r\n",
      "\r\n",
      "```\r\n",
      "I am running Tensorflow 2.3.1 whith the same version on the TPU.\r\n",
      "\r\n",
      "I have already asked on StackOverflow without getting any good pointers / answers, see link here:\r\n",
      "\r\n",
      "https://stackoverflow.com/questions/65331321/training-a-keras-model-using-tpu-pods?noredirect=1#comment115577267_65331321\r\n",
      "\r\n",
      "Thanks!\n",
      "issue labels - \n",
      "TF 2.3\n",
      "comp:tpus\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Merge pull request #43269 from rhdong:Mutablehashtable lookup support…\n",
      "issue body -  This PR has been merged to master: https://github.com/tensorflow/tensorflow/pull/43269\r\n",
      "This PR is one part of [RFC#313](https://github.com/tensorflow/community/pull/313), [RFC#237](https://github.com/tensorflow/community/pull/237)\n",
      "issue labels - \n",
      "cla: yes\n",
      "size:M\n",
      "waiting for patch release\n",
      "\n",
      "\n",
      "issue title -  tf.data.experimental.assert_cardinality incompatible with INFINITE_CARDINALITY\n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code: yes\r\n",
      "- OS Platform and Distribution: Ubuntu 18.04\r\n",
      "- TensorFlow installed from: binary\r\n",
      "- TensorFlow version: v1.12.1-47912-gec43aacb56f 2.5.0-dev20201219\r\n",
      "- Python version: 3.7.7\r\n",
      "\r\n",
      "**Describe the current behavior**: `tf.data.experimental.assert_cardinality`  can be used to fix `Dataset.cardinality` in instances where it cannot be inferred. In situations where the cardinality is infinite, this raises an error where it shouldn't.\r\n",
      "\r\n",
      "**Describe the expected behavior** if `ds = ds_base.apply(tf.data.experimental.assert_cardinality(tf.data.INFINITE_CARDINALITY)`, then an error should be raised if `ds_base` stops producing elements, as opposed to when the first element is produced.\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "[Notebook](https://colab.research.google.com/drive/13s9geZzR4xUtYJ6ImINl0kuisL-n8H_6?usp=sharing)\r\n",
      "\r\n",
      "Code (same as notebook):\r\n",
      "```python\r\n",
      "import tensorflow as tf\r\n",
      "\r\n",
      "ds = tf.data.Dataset.range(5).repeat().flat_map(lambda i: tf.data.Dataset.range(i))\r\n",
      "print(ds.cardinality() == tf.data.INFINITE_CARDINALITY)  # False\r\n",
      "ds = ds.apply(tf.data.experimental.assert_cardinality(tf.data.INFINITE_CARDINALITY))\r\n",
      "print(ds.cardinality() == tf.data.INFINITE_CARDINALITY)  # True\r\n",
      "\r\n",
      "for example in ds.take(1):\r\n",
      "    pass\r\n",
      "# tensorflow.python.framework.errors_impl.FailedPreconditionError:\r\n",
      "# Input dataset was expected to contain -1 elements but contained at least 1 element.\r\n",
      "```\n",
      "issue labels - \n",
      "TF 2.5\n",
      "comp:data\n",
      "stat:awaiting tensorflower\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Disable failing test\n",
      "issue body -  \n",
      "issue labels - \n",
      "cla: yes\n",
      "\n",
      "\n",
      "issue title -  Can't use contrib.slim in tf 1.14\n",
      "issue body -  I am running on google colab. \r\n",
      "`import tensorflow\r\n",
      "print(tensorflow.__version__)`\r\n",
      "after running this cell the output is 1.14.0 so I assume the above command `!pip install tensorflow==1.14` worked\r\n",
      "but i have the following error: \r\n",
      "`import tensorflow.contrib.slim as slim`\r\n",
      "---> ImportError: No module named contrib.slim\r\n",
      "I've tried changing the import to `import tf_slim as slim` but i get a similar error: ImportError: No module named tf_slim\n",
      "issue labels - \n",
      "TF 1.14\n",
      "contrib\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Disable failing test\n",
      "issue body -  \n",
      "issue labels - \n",
      "cla: yes\n",
      "\n",
      "\n",
      "issue title -  Need to install tensorflow 1.12.0 for gpt-2, not working\n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Mint 20 Ulyana\r\n",
      "- TensorFlow installed from (source or binary): source\r\n",
      "- TensorFlow version (use command below): 12\r\n",
      "- Python version: 3.8.3\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "\r\n",
      "I am trying `pip3 install tensorflow==1.12.0`\r\n",
      "\r\n",
      "Which returns:\r\n",
      "`ERROR: Could not find a version that satisfies the requirement tensorflow==1.12.0 (from versions: 2.2.0rc1, 2.2.0rc2, 2.2.0rc3, 2.2.0rc4, 2.2.0, 2.2.1, 2.3.0rc0, 2.3.0rc1, 2.3.0rc2, 2.3.0, 2.3.1, 2.4.0rc0, 2.4.0rc1, 2.4.0rc2, 2.4.0rc3, 2.4.0rc4)\r\n",
      "ERROR: No matching distribution found for tensorflow==1.12.0`\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "\r\n",
      "1.12.0 installing\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "Provide a reproducible test case that is the bare minimum necessary to generate\r\n",
      "the problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n",
      "\r\n",
      "**Other info / logs** Include any logs or source code that would be helpful to\r\n",
      "diagnose the problem. If including tracebacks, please include the full\r\n",
      "traceback. Large logs and files should be attached.\r\n",
      "\n",
      "issue labels - \n",
      "TF 1.12\n",
      "subtype: ubuntu/linux\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  Fixed build issue on windows: realpath command not found\n",
      "issue body -  When I build on windows with CUDA enabled I receive the following error immediatly when starting building:\r\n",
      "```\r\n",
      "3>CUSTOMBUILD : error : no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n",
      "3>\tFile \"C:/workspace/fast/build_release/external/tensorflow/src/tensorflow_download/third_party/gpus/cuda_configure.bzl\", line 1369\r\n",
      "3>\t\t_create_local_cuda_repository(<1 more arguments>)\r\n",
      "3>\tFile \"C:/workspace/fast/build_release/external/tensorflow/src/tensorflow_download/third_party/gpus/cuda_configure.bzl\", line 1213, in _create_local_cuda_repository\r\n",
      "3>\t\tto_list_of_strings(<1 more arguments>)\r\n",
      "3>\tFile \"C:/workspace/fast/build_release/external/tensorflow/src/tensorflow_download/third_party/gpus/cuda_configure.bzl\", line 1214, in to_list_of_strings\r\n",
      "3>\t\t_cuda_include_path(<2 more arguments>)\r\n",
      "3>\tFile \"C:/workspace/fast/build_release/external/tensorflow/src/tensorflow_download/third_party/gpus/cuda_configure.bzl\", line 363, in _cuda_include_path\r\n",
      "3>\t\tinc_entries.append(<1 more arguments>)\r\n",
      "3>\tFile \"C:/workspace/fast/build_release/external/tensorflow/src/tensorflow_download/third_party/gpus/cuda_configure.bzl\", line 363, in inc_entries.append\r\n",
      "3>\t\trealpath(repository_ctx, <1 more arguments>)\r\n",
      "3>\tFile \"C:/workspace/fast/build_release/external/tensorflow/src/tensorflow_download/third_party/remote_config/common.bzl\", line 268, in realpath\r\n",
      "3>\t\texecute(repository_ctx, <1 more arguments>)\r\n",
      "3>\tFile \"C:/workspace/fast/build_release/external/tensorflow/src/tensorflow_download/third_party/remote_config/common.bzl\", line 208, in execute\r\n",
      "3>\t\tfail(<1 more arguments>)\r\n",
      "3>Repository command failed\r\n",
      "3>/usr/bin/bash: realpath: command not found\r\n",
      "```\r\n",
      "\r\n",
      "I found a post on msys2's gitter chat commenting the same error, but fixed it with just adding the -l option to bash when running the command. I testet this with msys from the terminal:\r\n",
      "```\r\n",
      "PS C:\\dev_tools\\msys64\\usr\\bin> .\\bash.exe -c realpath\r\n",
      "/usr/bin/bash: realpath: command not found\r\n",
      "PS C:\\dev_tools\\msys64\\usr\\bin> .\\bash.exe -cl realpath\r\n",
      "realpath: missing operand\r\n",
      "Try 'realpath --help' for more information.\r\n",
      "```\r\n",
      "Thus adding the -l option helps, why it helps I don't know. From the manual (https://www.gnu.org/software/bash/manual/html_node/Invoking-Bash.html) it says:\r\n",
      "\r\n",
      "> -l Make this shell act as if it had been directly invoked by login. When the shell is interactive, this is equivalent to starting a login shell with ‘exec -l bash’. When the shell is not interactive, the login shell startup files will be executed. ‘exec bash -l’ or ‘exec bash --login’ will replace the current shell with a Bash login shell. See Bash Startup Files, for a description of the special behavior of a login shell.\r\n",
      "\r\n",
      "\r\n",
      "This pull request just adds this -l option to the realpath command in third_party/remote_config/common.bzl\r\n",
      "\r\n",
      "Some system info:\r\n",
      "* Windows 10\r\n",
      "* Python 3.8 64 bit\r\n",
      "* MSVC 16.8.4\r\n",
      "* Bazel 3.1.0\r\n",
      "* CUDA 10.1\r\n",
      "* msys2 64 bit 20201109\n",
      "issue labels - \n",
      "cla: yes\n",
      "ready to pull\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Problem getting python include path\n",
      "issue body -  **System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 & Cygwin\r\n",
      "- TensorFlow installed from (source or binary): source\r\n",
      "- TensorFlow version: v2.4.0-rc2\r\n",
      "- Python version: 3.6.10\r\n",
      "- Bazel version (if compiling from source): 3.7.2\r\n",
      "- GCC/Compiler version (if compiling from source): 10.2.0\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "\r\n",
      "```\r\n",
      "ERROR: An error occurred during the fetch of repository 'local_config_python':\r\n",
      "   Traceback (most recent call last):\r\n",
      "        File \"C:/cygwin64/home/username/tensorflow/third_party/py/python_configure.bzl\", line 267, column 40, in _python_autoconf_impl\r\n",
      "                _create_local_python_repository(repository_ctx)\r\n",
      "        File \"C:/cygwin64/home/username/tensorflow/third_party/py/python_configure.bzl\", line 212, column 41, in _create_local_python_repository\r\n",
      "                python_include = _get_python_include(repository_ctx, python_bin)\r\n",
      "        File \"C:/cygwin64/home/username/tensorflow/third_party/py/python_configure.bzl\", line 152, column 21, in _get_python_include\r\n",
      "                result = execute(\r\n",
      "        File \"C:/cygwin64/home/username/tensorflow/third_party/remote_config/common.bzl\", line 217, column 13, in execute\r\n",
      "                fail(\r\n",
      "Error in fail: Problem getting python include path.\r\n",
      "java.io.IOException: ERROR: src/main/native/windows/process.cc(202): CreateProcessW(\"C:\\cygwin64\\home\\username\\_bazel_username\\5ud3wb25\\external\\local_config_python\\usr\\bin\\python3\" -c \"from __future__ import print_function;from distutils import sysconfig;print(sysconfig.get_python_inc())\"): The system cannot find the file specified.\r\n",
      " (error: 2)\r\n",
      "Is the Python binary path set up right? (See ./configure or PYTHON_BIN_PATH.) Is distutils installed?\r\n",
      "INFO: Repository llvm-project instantiated at:\r\n",
      "  C:/cygwin64/home/username/tensorflow/WORKSPACE:19:16: in <toplevel>\r\n",
      "  C:/cygwin64/home/username/tensorflow/tensorflow/workspace.bzl:690:20: in tf_repositories\r\n",
      "Repository rule tf_http_archive defined at:\r\n",
      "  C:/cygwin64/home/username/tensorflow/third_party/repo.bzl:131:34: in <toplevel>\r\n",
      "ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Problem getting python include path.\r\n",
      "java.io.IOException: ERROR: src/main/native/windows/process.cc(202): CreateProcessW(\"C:\\cygwin64\\home\\username\\_bazel_username\\5ud3wb25\\external\\local_config_python\\usr\\bin\\python3\" -c \"from __future__ import print_function;from distutils import sysconfig;print(sysconfig.get_python_inc())\"): The system cannot find the file specified.\r\n",
      " (error: 2)\r\n",
      "Is the Python binary path set up right? (See ./configure or PYTHON_BIN_PATH.) Is distutils installed?\r\n",
      "INFO: Elapsed time: 2.198s\r\n",
      "INFO: 0 processes.\r\n",
      "FAILED: Build did NOT complete successfully (1 packages loaded, 0 targets configured)\r\n",
      "    Fetching @local_execution_config_python; fetching\r\n",
      "```\r\n",
      "\r\n",
      "`C:\\cygwin64\\home\\username\\_bazel_username\\5ud3wb25\\external\\local_config_python\\` is an empty directory...\r\n",
      "\r\n",
      "\r\n",
      "**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n",
      "\r\n",
      "any of the following\r\n",
      "```\r\n",
      "$ bazel build '//tensorflow/tools/pip_package:build_pip_package'\r\n",
      "$ env PYTHON_BIN_PATH='/usr/bin' bazel build '//tensorflow/tools/pip_package:build_pip_package'\r\n",
      "$ env PYTHON_BIN_PATH='C:/cygwin64/usr/bin' bazel build '//tensorflow/tools/pip_package:build_pip_package'\r\n",
      "```\r\n",
      "\r\n",
      "**Any other info / logs**\r\n",
      "\r\n",
      "I would of assumed it has something to do with bazel getting confused by cygwin paths (e.g. it can not handle any paths that have a space character in their own build script), but that would not explain why it is using such a bizarre path in the first place.\r\n",
      "\r\n",
      "bazel can not be compiled from source, so that is not an option.\r\n",
      "tensorflow can not be installed via pip, so that is not an option.\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "stalled\n",
      "stat:awaiting response\n",
      "subtype:windows\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  Colab session getting restarted during conversion of the Boundless model\n",
      "issue body -  **System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n",
      "- TensorFlow installed from (source or binary): Binary\r\n",
      "- TensorFlow version (or github SHA if from source): 2.4.0\r\n",
      "\r\n",
      "\r\n",
      "## Colab Notebook for reproducing the issue\r\n",
      "https://colab.research.google.com/gist/sayakpaul/baf963933cbc3e627e66f6330d66c519/boundless_tflite.ipynb\r\n",
      "\r\n",
      "## Issue\r\n",
      "\r\n",
      "When trying to convert the [Boundless model](https://tfhub.dev/google/boundless/quarter/1) Colab Notebook session is getting restarted and the converter is unable to generate the TensorFlow Lite model. \r\n",
      "\r\n",
      "## Code\r\n",
      "\r\n",
      "```python\r\n",
      "model_handle = https://tfhub.dev/google/boundless/quarter/1\r\n",
      "model = hub.load(model_handle)\r\n",
      "concrete_function = model.signatures['default']\r\n",
      "\r\n",
      "converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_function])\r\n",
      "converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n",
      "tflite_model = converter.convert()\r\n",
      "```\r\n",
      "\r\n",
      "**Note** that I did try to run the conversion without `converter.optimizations = [tf.lite.Optimize.DEFAULT]` and it does not help. \r\n",
      "\r\n",
      "## Logs\r\n",
      "\r\n",
      "```\r\n",
      "WARNING:tensorflow:Issue encountered when serializing model_variables.\r\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\r\n",
      "to_proto not supported in EAGER mode.\r\n",
      "WARNING:tensorflow:Issue encountered when serializing model_variables.\r\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\r\n",
      "to_proto not supported in EAGER mode.\r\n",
      "WARNING:tensorflow:Issue encountered when serializing variables.\r\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\r\n",
      "to_proto not supported in EAGER mode.\r\n",
      "WARNING:tensorflow:Issue encountered when serializing variables.\r\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\r\n",
      "to_proto not supported in EAGER mode.\r\n",
      "WARNING:tensorflow:Issue encountered when serializing trainable_variables.\r\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\r\n",
      "to_proto not supported in EAGER mode.\r\n",
      "WARNING:tensorflow:Issue encountered when serializing trainable_variables.\r\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\r\n",
      "to_proto not supported in EAGER mode.\r\n",
      "```\r\n",
      "\r\n",
      "## Useful references\r\n",
      "\r\n",
      "The pre-trained Boundless model on TensorFlow Hub comes with [this tutorial](https://www.tensorflow.org/hub/tutorials/boundless). \r\n",
      "\r\n",
      "Anything I am missing out on during the conversion process? \n",
      "issue labels - \n",
      "TFLiteConverter\n",
      "\n",
      "\n",
      "issue title -  Disable failing test\n",
      "issue body -  \n",
      "issue labels - \n",
      "cla: yes\n",
      "\n",
      "\n",
      "issue title -  Ubuntu18.04 running on WSL2: \"libcuda.so.1\" does not exist. How to create that?\n",
      "issue body -  ------------------------\r\n",
      "\r\n",
      "### System information\r\n",
      "\r\n",
      "-   **Have I written custom code (as opposed to using a stock example script\r\n",
      "    provided in TensorFlow)**:\r\n",
      "\r\n",
      "```\r\n",
      "def test_sum():\r\n",
      "    assert sum([1, 2, 3]) == 6, \"Should be 6\"\r\n",
      "\r\n",
      "if __name__ == \"__main__\":\r\n",
      "    test_sum()\r\n",
      "    print(\"Everything passed\")\r\n",
      "\r\n",
      "    # import tensorflow as tf\r\n",
      "    import tensorflow.compat.v1 as tf       # To get TF 1.x like behaviour in TF 2.0 one can run\r\n",
      "    tf.disable_v2_behavior()\r\n",
      "\r\n",
      "    print(tf.__version__)\r\n",
      "    print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\r\n",
      "```\r\n",
      "\r\n",
      "**System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 running on WSL2\r\n",
      "- TensorFlow installed from (source or binary):\r\n",
      "- TensorFlow version: 2.4.0\r\n",
      "- Python version: 3.8\r\n",
      "- Installed using virtualenv? pip? conda?:: installed using conda\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version: CUDA11.0/ cuDNN8\r\n",
      "- GPU model and memory: \r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 running on WSL2\r\n",
      "- TensorFlow installed from (source or binary):\r\n",
      "- TensorFlow version: 2.4.0\r\n",
      "- Python version:\r\n",
      "- Installed using virtualenv? pip? conda?:: installed using conda\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version: CUDA11.0/ cuDNN8\r\n",
      "- GPU model and memory: Quadro RTX 4000, 8GB\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "-   **Exact command to reproduce**:\r\n",
      "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\r\n",
      "\r\n",
      "\r\n",
      "### Describe the problem\r\n",
      "The number of GPU available is being shown to be zero. \r\n",
      "\r\n",
      "I am getting following output:\r\n",
      " \r\n",
      "```\r\n",
      "(base) dushyant@DESKTOP-U96RKFC:/mnt/c/Windows/System32$ python3 /home/$USER/test.py\r\n",
      "Everything passed\r\n",
      "2020-12-19 12:47:55.757762: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "WARNING:tensorflow:From /home/dushyant/miniconda3/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "non-resource variables are not supported in the long term\r\n",
      "2.4.0\r\n",
      "2020-12-19 12:47:56.642478: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n",
      "2020-12-19 12:47:56.645707: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.0/lib64:\r\n",
      "2020-12-19 12:47:56.645745: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\r\n",
      "2020-12-19 12:47:56.645767: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DESKTOP-U96RKFC): /proc/driver/nvidia/version does not exist\r\n",
      "Num GPUs Available:  0\r\n",
      "```\r\n",
      "Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.0/lib64:\r\n",
      "\r\n",
      "I searched for libcuda.so.1 in my directory: \"LD_LIBRARY_PATH: /usr/local/cuda-11.0/lib64:\" and elsewhere. It does not seem to exist.\r\n",
      "\r\n",
      "I also search for 'libcuda.so*' and found the following:\r\n",
      "```\r\n",
      "(base) dushyant@DESKTOP-U96RKFC:/mnt/c/Windows/System32$ find /usr/ -name 'libcuda.so*'\r\n",
      "/usr/local/cuda-11.0/doc/man/man7/libcuda.so.7\r\n",
      "/usr/local/cuda-11.0/targets/x86_64-linux/lib/stubs/libcuda.so\r\n",
      "```\r\n",
      "\r\n",
      "I am under impression that I can create \"libcuda.so.1\". However, I have no idea how to create that. Any help would be appreciated.\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:gpu\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  When tried to Make Project or Build the app got an error \"Task :lib_task_api:downloadModelFile FAILED\"\n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Emulator Pixel 3 API 29\r\n",
      "- TensorFlow version (use command below): 2.2.0\r\n",
      "- Python version: 3.7.3 \r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "When try to Debug or Run, Android Studio gives the following error (Sometimes it happen when I do Make Project too):\r\n",
      "```\r\n",
      "> Task :lib_interpreter:downloadModelFile\r\n",
      "Download https://storage.googleapis.com/download.tensorflow.org/models/tflite/text_classification/text_classification_v2.tflite\r\n",
      "\r\n",
      "> Task :lib_interpreter:downloadModelFile FAILED\r\n",
      "\r\n",
      "FAILURE: Build failed with an exception.\r\n",
      "\r\n",
      "* What went wrong:\r\n",
      "Execution failed for task ':lib_interpreter:downloadModelFile'.\r\n",
      "> javax.net.ssl.SSLException: Connection reset\r\n",
      "\r\n",
      "```\r\n",
      "There was no issue with the internet connection at that time. However, this work without errors previously, suddenly started getting this error.\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "It should build without errors.\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "Download text_classification example from https://github.com/tensorflow/examples/tree/master/lite/examples/text_classification/android\r\n",
      "After downloading, try to Make Project or Run/Debug. \r\n",
      "\r\n",
      "**Other info / logs** Include any logs or source code that would be helpful to\r\n",
      "diagnose the problem. If including tracebacks, please include the full\r\n",
      "traceback. Large logs and files should be attached.\r\n",
      "\r\n",
      "```\r\n",
      "WARNING: The specified Android SDK Build Tools version (29.0.0) is ignored, as it is below the minimum supported version (29.0.2) for Android Gradle Plugin 4.0.0.\r\n",
      "Android SDK Build Tools 29.0.2 will be used.\r\n",
      "To suppress this warning, remove \"buildToolsVersion '29.0.0'\" from your build.gradle file, as each version of the Android Gradle Plugin now has a default version of the build tools.\r\n",
      "WARNING: The specified Android SDK Build Tools version (29.0.0) is ignored, as it is below the minimum supported version (29.0.2) for Android Gradle Plugin 4.0.0.\r\n",
      "Android SDK Build Tools 29.0.2 will be used.\r\n",
      "To suppress this warning, remove \"buildToolsVersion '29.0.0'\" from your build.gradle file, as each version of the Android Gradle Plugin now has a default version of the build tools.\r\n",
      "> Task :app:preBuild UP-TO-DATE\r\n",
      "> Task :app:preInterpreterDebugBuild UP-TO-DATE\r\n",
      "\r\n",
      "> Task :lib_interpreter:downloadModelFile\r\n",
      "Download https://storage.googleapis.com/download.tensorflow.org/models/tflite/text_classification/text_classification_v2.tflite\r\n",
      "\r\n",
      "> Task :lib_interpreter:downloadModelFile FAILED\r\n",
      "\r\n",
      "FAILURE: Build failed with an exception.\r\n",
      "\r\n",
      "* What went wrong:\r\n",
      "Execution failed for task ':lib_interpreter:downloadModelFile'.\r\n",
      "> javax.net.ssl.SSLException: Connection reset\r\n",
      "\r\n",
      "* Try:\r\n",
      "Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. Run with --scan to get full insights.\r\n",
      "\r\n",
      "* Get more help at https://help.gradle.org\r\n",
      "\r\n",
      "BUILD FAILED in 22s\r\n",
      "1 actionable task: 1 executed\r\n",
      "```\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.2\n",
      "comp:lite\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  When tried to Make Project or Build the app got an error \"\n",
      "issue body -  <em>Please make sure that this is a bug. As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:bug_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n",
      "- TensorFlow installed from (source or binary):\r\n",
      "- TensorFlow version (use command below):\r\n",
      "- Python version:\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version:\r\n",
      "- GPU model and memory:\r\n",
      "\r\n",
      "You can collect some of this information using our environment capture\r\n",
      "[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n",
      "You can also obtain the TensorFlow version with:\r\n",
      "1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n",
      "2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n",
      "\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "Provide a reproducible test case that is the bare minimum necessary to generate\r\n",
      "the problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n",
      "\r\n",
      "**Other info / logs** Include any logs or source code that would be helpful to\r\n",
      "diagnose the problem. If including tracebacks, please include the full\r\n",
      "traceback. Large logs and files should be attached.\r\n",
      "\n",
      "issue labels - \n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  tf.compat.v1.get_variable does not reuse variables within a scope and same variable name\n",
      "issue body -  1-  Tensorflow version = **2.4.0**\r\n",
      "\r\n",
      "2- **Google Colab Notebook** \r\n",
      "\r\n",
      "The following code from the documentation of [tf.compat.v1.variable_scope](https://www.tensorflow.org/api_docs/python/tf/compat/v1/variable_scope) throws **assertionError**\r\n",
      "\r\n",
      "```\r\n",
      "def foo():\r\n",
      "  with tf.compat.v1.variable_scope(\"foo\", reuse=tf.compat.v1.AUTO_REUSE):\r\n",
      "    v = tf.compat.v1.get_variable(\"v\", [1])\r\n",
      "  return v\r\n",
      "\r\n",
      "v1 = foo()  # Creates v.\r\n",
      "v2 = foo()  # Gets the same, existing v.\r\n",
      "assert v1 == v2\r\n",
      "```\r\n",
      "\r\n",
      "Here is a screenshot of the error:\r\n",
      "![Screenshot from 2020-12-19 19-52-16](https://user-images.githubusercontent.com/35839837/102691633-e6549400-4233-11eb-86a7-58a860acb263.png)\r\n",
      "\n",
      "issue labels - \n",
      "comp:apis\n",
      "stat:awaiting response\n",
      "type:docs-bug\n",
      "\n",
      "\n",
      "issue title -  How to create Flex-free model in TFLite\n",
      "issue body -  **System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n",
      "- TensorFlow installed from (source or binary): Installed via PIP\r\n",
      "- TensorFlow version (or github SHA if from source): Tensorflow 2.3.1\r\n",
      "\r\n",
      "**Command used to run the converter or code if you’re using the Python API**\r\n",
      "```python\r\n",
      "import tensorflow as tf\r\n",
      "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\r\n",
      "converter.target_spec.supported_ops = [\r\n",
      "  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\r\n",
      "  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\r\n",
      "]\r\n",
      "tflite_model = converter.convert()\r\n",
      "open(\"converted_model.tflite\", \"wb\").write(tflite_model)\r\n",
      "\r\n",
      "```\r\n",
      "NOTE: Model is successfully converted.\r\n",
      "\r\n",
      "**Failure details**\r\n",
      "If the conversion is successful, but the generated model is wrong,\r\n",
      "state what is wrong:\r\n",
      "- TFlite model is working fine at CPU Linux 16.04 and RaspberryPi. But it gives BUS ERROR at edge device. [Details](https://github.com/tensorflow/tensorflow/issues/45504).\r\n",
      "\r\n",
      "The original model is in PyTorch. I have used PyTorch-ONNX-Tensorflow-TFLite approach to convert the model in TFLite.\r\n",
      "I am sharing graphs of ONNX and TFLite model. [Link](https://drive.google.com/drive/folders/1jzLZKus-9Dey2vgcMWjspBZfCRJbLCWW?usp=sharing)\r\n",
      "\r\n",
      "Geeks says \r\n",
      "> Using Flex op requires to build TF op kernels which is difficult to support various targets.\r\n",
      "If the required Flex op is only FlexMul, I think you might be able to create Flex-free model with some refactoring.\r\n",
      "\r\n",
      "How can do this \"create Flex-free \"? \r\n",
      "\n",
      "issue labels - \n",
      "TF 2.3\n",
      "TFLiteConverter\n",
      "comp:lite\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Nightly\n",
      "issue body -  To clear the annotations warning in workflow, I would like to suggest this change.\r\n",
      "(Initial Warning obtained in summary part of workflow actions : Ubuntu-latest workflows will use Ubuntu-20.04 soon)\r\n",
      "\r\n",
      "Changed runs-on in update-nightly.yml from ubuntu-latest to Ubuntu-20.04\n",
      "issue labels - \n",
      "cla: no\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Illegal instruction  CPUs under version 2.4.0\n",
      "issue body -  System information\r\n",
      "\r\n",
      "OS: Ubuntu 18.04\r\n",
      "TensorFlow binary installed using pip\r\n",
      "TensorFlow version 2.4.0rc0\r\n",
      "Python version 3.8.6\r\n",
      "CUDA/cuDNN version: 11.0/8.0.4\r\n",
      "GPU model: GTX 1080 ti\r\n",
      "Describe the current behavior\r\n",
      "\r\n",
      "Attempting to import tensorflow produces an \"Illegal instruction\" error.\r\n",
      "\r\n",
      "Describe the expected behavior\r\n",
      "\r\n",
      "Import tensorflow without error. (Illegal instruction)\r\n",
      "\r\n",
      "python -c \"import tensorflow as tf\"\r\n",
      "\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "stalled\n",
      "stat:awaiting response\n",
      "subtype: ubuntu/linux\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  Documentation for tf.keras.backend.max is missing\n",
      "issue body -  ## URL(s) with the issue:\r\n",
      "\r\n",
      "Please provide a link to the documentation entry:\r\n",
      "https://www.tensorflow.org/api_docs/python/tf/keras/backend/max?hl=FA\r\n",
      "\r\n",
      "## Description of issue (what needs changing): \r\n",
      "Go to https://www.tensorflow.org/s/results?q=keras.backend.max and click on the first link (in fact, any link related to that API).\r\n",
      "\r\n",
      "The link to the API, **`tf.keras.backend.max`** is not available. Is it intentional, because, in the [Source Code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/backend.py#L2272), it is set **not to generate the Docs**.\r\n",
      "\r\n",
      "If it is intentional, can you please explain why? Is there a better alternative to this API, or you have plans to Deprecate it, etc..\r\n",
      "\r\n",
      "### Correct links\r\n",
      "\r\n",
      "Is the link to the source code correct? : No (Link for the API itself is missing)\r\n",
      "\r\n",
      "### Parameters defined\r\n",
      "\r\n",
      "Are all parameters defined and formatted correctly? : No\r\n",
      "\r\n",
      "### Returns defined\r\n",
      "\r\n",
      "Are return values defined? : No\r\n",
      "\r\n",
      "### Raises listed and defined : No\r\n",
      "\r\n",
      "### Usage example\r\n",
      "\r\n",
      "Is there a usage example? : No\r\n",
      "\r\n",
      "### Submit a pull request? : \r\n",
      "I'm ready to submit a PR to remove this [line of code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/backend.py#L2272) if it is fine with you.\n",
      "issue labels - \n",
      "comp:apis\n",
      "type:docs-feature\n",
      "\n",
      "\n",
      "issue title -   Extract reference for operator FLOOR_DIV to standalone header\n",
      "issue body -  Move the reference implementation to its own header so that micro\r\n",
      "can use it without the unrelated depedencies of reference_ops.h.\r\n",
      "\r\n",
      "This PR is part of the work to port operator FLOOR_DIV from lite to micro,\r\n",
      "as tracked in Issue #45657.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:lite\n",
      "comp:micro\n",
      "prtype:bugfix\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -   Extract a function for parsing operator FLOOR_DIV\n",
      "issue body -  Extract the parsing out of a switch statement case to create a\r\n",
      "standalone function which can be called by the micro op resolver.\r\n",
      "\r\n",
      "This PR is part of the work to port operator FLOOR_DIV from lite to micro,\r\n",
      "as tracked in Issue #45657.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:lite\n",
      "comp:micro\n",
      "prtype:bugfix\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -   Extract reference for operator FLOOR_MOD to standalone header\n",
      "issue body -  Move the reference implementation to its own header so that micro\r\n",
      "can use it without the unrelated depedencies of reference_ops.h.\r\n",
      "\r\n",
      "This PR is part of the work to port operator FLOOR_MOD from lite to micro,\r\n",
      "as tracked in Issue #45749\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:lite\n",
      "comp:micro\n",
      "prtype:bugfix\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  Extract a function for parsing operator FLOOR_MOD\n",
      "issue body -  Extract the parsing out of a switch statement case to create a\r\n",
      "standalone function which can be called by the micro op resolver.\r\n",
      "\r\n",
      "This PR is part of the work to port operator FLOOR_MOD from lite to micro,\r\n",
      "as tracked in Issue #45749.\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:lite\n",
      "comp:micro\n",
      "prtype:bugfix\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  Illegal instruction on older CPUs under version 2.4.0 \n",
      "issue body -  \r\n",
      "**System information**\r\n",
      "- Ubuntu 18.04 and 20.04, Scientific Linux 7\r\n",
      "- binary installed via pip\r\n",
      "- version 2.4.0\r\n",
      "- Python 3.8\r\n",
      "- installed via pip (either inside or not inside a Conda environment)\r\n",
      "- various CPU-only and GPU-hosting machines\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "\r\n",
      "`import tensorflow` produces \"Illegal instruction (core dumped)\" on older machines (seemingly those that do not support AVX2 instructions). There is no problem on new machines (seemingly those that support AVX2 instructions). \r\n",
      "\r\n",
      "**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n",
      "\r\n",
      "```\r\n",
      "pip install tensorflow\r\n",
      "python -c \"import tensorflow\"\r\n",
      "```\r\n",
      "\r\n",
      "**Any other info / logs**\r\n",
      "\r\n",
      "The core dump occurs on various machines with various types of CPUs. The common thread seems to be that it occurs on machines that don't support AVX2 instructions. Most of the machines on which this occurs do support AVX instructions. \r\n",
      "\r\n",
      "The issue does not occur with Tensorflow 2.3.1 nor with Tensorflow 2.5.0 installed via tf-nightly. \r\n",
      "\r\n",
      "Any chance Tensorflow 2.4.0 was built in a way (perhaps unintentionally) that requires AVX2 instructions or some other requirement that causes it to fail on somewhat older (but not really old) machines? Based on what I am seeing, it seems that using 2.4.0 on many machines will fail.\r\n",
      "\r\n",
      "The same issue occurs when running in the official Tensorflow Docker container.\r\n",
      "\r\n",
      "This seems related to issue #44668.\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "regression issue\n",
      "stat:awaiting tensorflower\n",
      "subtype: ubuntu/linux\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -   micro: copy operator DIV kernel from lite \n",
      "issue body -  This is a copy without modification of the kernel and test for\r\n",
      "operator DIV from tensorflow/lite/kernels at 635e8a0.\r\n",
      "Adaptations to micro and addition to the micro build to follow.\r\n",
      "\r\n",
      "PR step 3 for issue #45431\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:lite\n",
      "comp:micro\n",
      "prtype:bugfix\n",
      "ready to pull\n",
      "size:L\n",
      "\n",
      "\n",
      "issue title -  Converting speech_embedding hub module to tflite results in `Encountered unresolved custom op: TensorArrayV3.Node`\n",
      "issue body -  **System information**\r\n",
      "- Colab default settings\r\n",
      "- TF version: 2.4.0\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "Converting speech_embedding module ''https://tfhub.dev/google/speech_embedding/1'' to tflite results in:\r\n",
      "\r\n",
      "`RuntimeError: Encountered unresolved custom op: TensorArrayV3.Node number 2 (TensorArrayV3) failed to prepare.`\r\n",
      "\r\n",
      "during inference.\r\n",
      "\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "\r\n",
      "```\r\n",
      "HUB_URL = 'https://tfhub.dev/google/speech_embedding/1'\r\n",
      "TEST_PATH = '.'\r\n",
      "\r\n",
      "embedding_layer = hub.KerasLayer(HUB_URL, input_shape=(16000,), trainable=False)\r\n",
      "\r\n",
      "model = tf.keras.Sequential([\r\n",
      "    embedding_layer\r\n",
      "])\r\n",
      "\r\n",
      "model.save(TEST_PATH)\r\n",
      "converter = tf.lite.TFLiteConverter.from_saved_model(TEST_PATH)\r\n",
      "\r\n",
      "\r\n",
      "converter.allow_custom_ops = True\r\n",
      "tflite_model = converter.convert()\r\n",
      "tflite_model_file = 'converted_model.tflite'\r\n",
      "\r\n",
      "with open(tflite_model_file, \"wb\") as f:\r\n",
      "  f.write(tflite_model)\r\n",
      "\r\n",
      "interpreter = tf.lite.Interpreter(model_path=tflite_model_file)\r\n",
      "interpreter.allocate_tensors()\r\n",
      "```\r\n",
      "\r\n",
      "**Output**\r\n",
      "\r\n",
      "```---------------------------------------------------------------------------\r\n",
      "RuntimeError                              Traceback (most recent call last)\r\n",
      "<ipython-input-16-5f54d02787e4> in <module>()\r\n",
      "      1 interpreter = tf.lite.Interpreter(model_path=tflite_model_file)\r\n",
      "----> 2 interpreter.allocate_tensors()\r\n",
      "      3 \r\n",
      "      4 input_index = interpreter.get_input_details()[0][\"index\"]\r\n",
      "      5 output_index = interpreter.get_output_details()[0][\"index\"]\r\n",
      "\r\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/interpreter.py in allocate_tensors(self)\r\n",
      "    257   def allocate_tensors(self):\r\n",
      "    258     self._ensure_safe()\r\n",
      "--> 259     return self._interpreter.AllocateTensors()\r\n",
      "    260 \r\n",
      "    261   def _safe_to_run(self):\r\n",
      "\r\n",
      "RuntimeError: Encountered unresolved custom op: TensorArrayV3.Node number 2 (TensorArrayV3) failed to prepare.```\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "I am aware that STFT may not be supported by tflite yet. If this is the issue, is there a quick workaround?\r\n",
      "\r\n",
      "Thanks,\r\n",
      "Bryan\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:lite\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  [TFTRT - Dynamic Shape Phase 3] Add Dynamic Shape Testing for ConvertArgMinMax\n",
      "issue body -  @bixia1 @tfeher for review\r\n",
      "\r\n",
      "Feature Tracker: #45481\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:gpu:tensorrt\n",
      "ready to pull\n",
      "size:L\n",
      "\n",
      "\n",
      "issue title -  Tensorflow's hwloc build force-enables use of sys/sysctl.h, which breaks on recent Linux/glibc\n",
      "issue body -  **System information**\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux, openSUSE Tumbleweed, tested on various snapshots up to 20201216 .\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not tested.\r\n",
      "- TensorFlow installed from (source or binary): Source.\r\n",
      "- TensorFlow version: 1.15.2\r\n",
      "- Python version: 3.6.9\r\n",
      "- Installed using virtualenv? pip? conda?: pip\r\n",
      "- Bazel version (if compiling from source): 0.26.1\r\n",
      "- GCC/Compiler version (if compiling from source): 9.3\r\n",
      "- CUDA/cuDNN version: N/A\r\n",
      "- GPU model and memory: N/A\r\n",
      "\r\n",
      "**Describe the problem**\r\n",
      "\r\n",
      "Tensorflow vendors the hwloc library and heavily customizes the way in which this library is built. I am not familiar enough with Bazel to fully understand the details of what you are doing here and the reasons why you are doing it, but unfortunately, what I do know is that on my machine, the net result is a broken hwloc build...\r\n",
      "\r\n",
      "The immediate symptom is that some hwloc source files do not compile because they are configured to include the `<sys/sysctl.h>` header, which [has been removed from glibc >=2.32](https://sourceware.org/pipermail/libc-announce/2020/000029.html) because the underlying system call has been removed from the Linux kernel since release 5.5.\r\n",
      "\r\n",
      "This is not a hwloc bug/incompatibility however, as it would intuitively seem, because the hwloc build system is perfectly able to figure out that this header does not exist and the hwloc source code knows how to avoid using it when that happens.\r\n",
      "\r\n",
      "The actual problem is this line of the tensorflow build system: https://github.com/tensorflow/tensorflow/blob/1cd185160a061a1213e8e8d05eb078e880ac9e46/third_party/hwloc/BUILD.bazel#L113\r\n",
      "\r\n",
      "For some reason that I do not know, it is pretty clear that you force-set the `HAVE_SYS_SYSCTL_H` define, which would normally be unset by the hwloc build system after it correctly detects that there is no `sysctl.h` header...\r\n",
      "\r\n",
      "Removing this line of the `BUILD.bazel` file fixes the build on my machine, but I can only assume that you added it for some reason (most likely to make the build work on an operating system that does use the `sysctl.h` header, but on which the hwloc build system does not correctly detect said header ?), which means that the actual tensorflow patch will need to be more nuanced and only perform this patch on the OS configurations where it is necessary.\r\n",
      "\r\n",
      "I am not able to easily share the build instructions that I followed because they are inside a mildly complicated build system within a closed-source project. But from my understanding of the problem, detailed reproducer instructions should not be necessary here, you should be able to easily replicate this issue just by trying to build tensorflow from source, through any method of your choosing, on any Linux distribution that uses glibc >=2.32. Although I personally observed this problem on openSUSE Tumbleweed, I would also expect it to reproduce identically on Gentoo, Arch, or Fedora 34...\n",
      "issue labels - \n",
      "TF 1.15\n",
      "subtype: ubuntu/linux\n",
      "type:build/install\n",
      "\n",
      "\n",
      "issue title -  TF2.4 doc missing keras.backend methods\n",
      "issue body -  Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\n",
      "policy, we only address code/doc bugs, performance issues, feature requests, and\r\n",
      "build/installation issues on GitHub.\r\n",
      "\r\n",
      "The TensorFlow docs are open source! To get involved, read the documentation\r\n",
      "contributor guide: https://www.tensorflow.org/community/contribute/docs\r\n",
      "\r\n",
      "## URL(s) with the issue:\r\n",
      "\r\n",
      "https://www.tensorflow.org/api_docs/python/tf/keras/backend\r\n",
      "\r\n",
      "## Description of issue (what needs changing):\r\n",
      "\r\n",
      "Missing a lot of `keras.backend` method documentation\r\n",
      "\r\n",
      "### Clear description\r\n",
      "\r\n",
      "For example, why should someone use this method? How is it useful?\r\n",
      "\r\n",
      "In TF2.4 doc, a lot of methods in keras.backend are missing. E.g., Fig1 is the screenshot of v2.4, while Fig2 is the screenshot of v2.3.\r\n",
      "\r\n",
      "![image](https://user-images.githubusercontent.com/5104719/102656451-bec4d500-4128-11eb-913e-cc4a3e27e58e.png)\r\n",
      "![image](https://user-images.githubusercontent.com/5104719/102656515-d8661c80-4128-11eb-95eb-d7ba74c5d1fc.png)\r\n",
      "\r\n",
      "\r\n",
      "### Correct links\r\n",
      "\r\n",
      "Is the link to the source code correct? N/A\r\n",
      "\r\n",
      "### Parameters defined\r\n",
      "\r\n",
      "Are all parameters defined and formatted correctly? N/A\r\n",
      "\r\n",
      "### Returns defined\r\n",
      "\r\n",
      "Are return values defined? N/A\r\n",
      "\r\n",
      "### Raises listed and defined\r\n",
      "\r\n",
      "Are the errors defined? For example,\r\n",
      "https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n",
      "\r\n",
      "N/A\r\n",
      "\r\n",
      "### Usage example\r\n",
      "\r\n",
      "Is there a usage example? N/A\r\n",
      "\r\n",
      "See the API guide: https://www.tensorflow.org/community/contribute/docs_ref\r\n",
      "on how to write testable usage examples.\r\n",
      "\r\n",
      "### Request visuals, if applicable\r\n",
      "\r\n",
      "Are there currently visuals? If not, will it clarify the content?\r\n",
      "\r\n",
      "### Submit a pull request?\r\n",
      "\r\n",
      "Are you planning to also submit a pull request to fix the issue? See the docs\r\n",
      "contributor guide: https://www.tensorflow.org/community/contribute/docs,\r\n",
      "docs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\n",
      "docs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n",
      "\n",
      "issue labels - \n",
      "comp:keras\n",
      "stat:awaiting tensorflower\n",
      "type:docs-bug\n",
      "\n",
      "\n",
      "issue title -  Fixes added to remove CorrectTensorEndianness in AllocateTensors\n",
      "issue body -  This PR request is raised with respect to bug #45858 .\r\n",
      "A previous discussion was done on a PR #45790 where it was suggested to raise a separate PR with only this fix.\r\n",
      "\r\n",
      "Changes done:\r\n",
      "Calling of CorrectTensorEndianness function is removed as the FlatBufferVectorToTfLiteTypeArray already converts flatbuffer tensor data from little endian to big endian during StartModelAllocation function call within AllocateTensors function.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "prtype:bugfix\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  Need to remove CorrectTensorEndianness in AllocateTensors\n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 18.04\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:N/A\r\n",
      "- TensorFlow installed from (source or binary):source\r\n",
      "- TensorFlow version (use command below):2.3.1\r\n",
      "- Python version:3.6.9\r\n",
      "- Bazel version (if compiling from source):3.4.1\r\n",
      "- GCC/Compiler version (if compiling from source):Ubuntu 7.5.0-3ubuntu1~18.04\r\n",
      "- CUDA/cuDNN version:N/A\r\n",
      "- GPU model and memory:N/A\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "On testing  ```//tensorflow/lite/micro:memory_arena_threshold_test``` on s390x, the TC was failing with a segmentation fault. \r\n",
      "On debugging, it was found the `CorrectTensorDataEndianness` function was byte swapping a tensor which made it cross its data type limit. It looks like the `FlatBufferVectorToTfLiteTypeArray` already converts flatbuffer tensor data from little endian to big endian during `StartModelAllocation` function call within `AllocateTensors` function and ```CorrectTensorDataEndianness``` is not required anymore.\r\n",
      "To make the test case pass, Allocation size values were also changed in `memory_arena_threshold_test.cc` file. Although as suggested in my last PR request #45790, I am raising this bug with a separate PR with removal of  `CorrectTensorDataEndianness` function call. \r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "`CorrectTensorDataEndianness` is not required as the tensor data is already converted to BE format.\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "Code to reproduce the issue:\r\n",
      "```bazel --host_jvm_args=\"-Xms1024m\" --host_jvm_args=\"-Xmx2048m\" test --host_javabase=\"@local_jdk//:jdk\" --test_tag_filters=-gpu,-benchmark-test,-v1only,-no_oss,-oss_serial  -k --test_timeout 300,450,1200,3600 --build_tests_only --test_output=errors -- //tensorflow/lite/micro:memory_arena_threshold_test```\r\n",
      "\r\n",
      "**Other info / logs**\r\n",
      "I am creating a new PR request with only this change as suggested by @advaitjain. \n",
      "issue labels - \n",
      "TF 2.3\n",
      "comp:lite\n",
      "comp:micro\n",
      "stat:awaiting tensorflower\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Fixing conv_ops_benchmark_test\n",
      "issue body -  This PR fixes the test //tensorflow/core/kernels:conv_ops_benchmark_test which was no longer runnable with below command due to recent changes to the test framework.\r\n",
      "bazel -c opt //tensorflow/core/kernels:conv_ops_benchmark_test -- --benchmarks=all\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:core\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  MultiHeadAttention masking mechanism\n",
      "issue body -  Hello!\r\n",
      "\r\n",
      "I wonder how we should apply masks (both padding and look-ahead) to the MultiHeadAttention layer, described in:\r\n",
      "\r\n",
      "[https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/multi_head_attention.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/multi_head_attention.py)\r\n",
      "\r\n",
      "I have been trying to adapt the tutorial [https://www.tensorflow.org/tutorials/text/transformer](https://www.tensorflow.org/tutorials/text/transformer) with this layer but it seems like I'm having some masking problems (loss strangely ~100 times lower than usual and worse empirical results). The tutorial implementation works fine, but replacing the MultiHeadAttention with the one in tf.keras.layers just breaks it.\r\n",
      "\r\n",
      "I'm pretty sure I'm missing something, but I can't figure it out by reading the implementation. What about including an example for both padding and look-ahead masks? I think it would be easy and useful.\r\n",
      "\r\n",
      "Another option would be updating the tutorial (as it happened with LayerNormalization) though I'm afraid that would have more impact.\r\n",
      "\r\n",
      "cc @tanzhenyu Sorry for the spam but I believe you are the one on charge about this.\r\n",
      "\r\n",
      "EDIT: I don't know if on charge, but I see you every time I come around issues/commits pages.\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:keras\n",
      "type:docs-bug\n",
      "\n",
      "\n",
      "issue title -  Unexpected behavior when a function which involves tf.reshape is run using strategy.run on a TPU\n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n",
      "- TensorFlow version (use command below): 2.4.0\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "When computing a function which involves `tf.reshape` using `strategy.run` and a TPU, there is an unexpected behavior and errors are thrown. The function works fine if called directly.\r\n",
      "This is the same error I get when I use that function as a loss function in a model.\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "I would expect the same behavior when the function is called directly and using strategy.run. Is there something I am missing out?\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "https://colab.research.google.com/drive/18yHVyEwbbfYXJ_r3XfP_NaStiVHqwkzN?usp=sharing\r\n",
      "\r\n",
      "I am not completely sure this is a bug of Tensorflow and not a bug in my code, if you could help me I would really appreciate it.\r\n",
      "Thank you very much for your help\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:tpus\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  [INTEL MKL] Graph pattern matcher for grappler.\n",
      "issue body -  This PR is to help finding large pattern (e.g. tf.nn.gelu) in a the data flow graph for grappler graph optimization. This has been motivated from pattern matching that exists in the graph_transforms tools (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md)\r\n",
      "\r\n",
      "Intended use is for remapper optimizer. User can get an initial match for a pattern consisting of op types in the nodes with a simple grammar like \r\n",
      "\r\n",
      "```\r\n",
      "leaf_pattern ::= `{` op_type `}`\r\n",
      "pattern ::= leaf_pattern |\r\n",
      "                  `{` op_type `,` `{` pattern `,` ... `,` pattern `}` `}`\r\n",
      "```\r\n",
      "\r\n",
      "Here a pattern syntax has a root and children (typically input ops feeding the root). Each child is a sub-pattern in a recursive manner. After an initial match has been found, additional constraint like device, shapes, constant values etc. will be used for determining a true match for a fusion.\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:grappler\n",
      "ready to pull\n",
      "size:L\n",
      "\n",
      "\n",
      "issue title -  Could not load dynamic library 'libcusolver.so.10' with official TF 2.4, even though it's installed\n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04 LTS\r\n",
      "- TensorFlow installed from (source or binary): official 2.4 binary, installed via Python `pip`\r\n",
      "- TensorFlow version (use command below): 2.4.0 (git version v2.4.0-rc4-71-g582c8d236cb)\r\n",
      "- Python version: 3.8.5\r\n",
      "- CUDA/cuDNN version: 11.2.0-1 / 8.0.5.39-1 (both installed from Nvidia repo)\r\n",
      "- GPU model and memory: Tesla P100 (16 GB)\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "After installing latest CUDA and cuDNN via the Nvidia repo I installed latest TF 2.4 via Python `pip`. Then I run the following:\r\n",
      "\r\n",
      "```python\r\n",
      "In [1]: import tensorflow as tf\r\n",
      "2020-12-18 14:38:28.563109: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "\r\n",
      "In [2]: tf.config.list_physical_devices('GPU')\r\n",
      "2020-12-18 14:38:30.711190: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n",
      "2020-12-18 14:38:30.711736: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n",
      "2020-12-18 14:38:30.738163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n",
      "2020-12-18 14:38:30.738690: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:\r\n",
      "pciBusID: 0000:01:00.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\r\n",
      "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\r\n",
      "2020-12-18 14:38:30.738710: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "2020-12-18 14:38:30.741099: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n",
      "2020-12-18 14:38:30.741136: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n",
      "2020-12-18 14:38:30.741931: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n",
      "2020-12-18 14:38:30.742126: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n",
      "2020-12-18 14:38:30.742245: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory\r\n",
      "2020-12-18 14:38:30.742805: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n",
      "2020-12-18 14:38:30.742917: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n",
      "2020-12-18 14:38:30.742930: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\n",
      "Skipping registering GPU devices...\r\n",
      "```\r\n",
      "\r\n",
      "So apparently the library `libcusolver.so.10` cannot be found.\r\n",
      "However, this library file is installed on my machine:\r\n",
      "\r\n",
      "```shell\r\n",
      "$ locate libcusolver.so.10\r\n",
      "/usr/lib/x86_64-linux-gnu/libcusolver.so.10\r\n",
      "/usr/lib/x86_64-linux-gnu/libcusolver.so.10.2.0.243\r\n",
      "```\r\n",
      "\r\n",
      "Any ideas what I am missing here?\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:gpu\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  TensorFlow Lite NNAPI with Quantisation: Invalid Zero Point\n",
      "issue body -  **System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 LTS\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung Galaxy S10 5G\r\n",
      "- TensorFlow installed from (source or binary): binary\r\n",
      "- TensorFlow version (use command below): 2.5.0-dev20201210\r\n",
      "- Python version: 3.7.5\r\n",
      "- CUDA/cuDNN version: 11.1\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "TensorFlow Lite raises an error when doing inference with NNAPI on a mobile device, with a custom model quantised to int8. Issue happens both when I use int8 quantisation with float fallback and strict int8 quantisation. If I take PReLU out of the model the error goes away.\r\n",
      "\r\n",
      "Terminal output below:\r\n",
      "```\r\n",
      "\r\n",
      "2020-12-18 12:01:37.452 6713-7171/org.tensorflow.benchmarking I/tflite: Created TensorFlow Lite delegate for NNAPI.\r\n",
      "2020-12-18 12:01:37.454 6713-7171/org.tensorflow.benchmarking I/tflite: Initialized TensorFlow Lite runtime.\r\n",
      "2020-12-18 12:01:37.454 6713-7171/org.tensorflow.benchmarking I/Manager: DeviceManager::DeviceManager\r\n",
      "2020-12-18 12:01:37.454 6713-7171/org.tensorflow.benchmarking I/Manager: findAvailableDevices\r\n",
      "2020-12-18 12:01:37.455 6713-7171/org.tensorflow.benchmarking I/Manager: Found interface armnn\r\n",
      "2020-12-18 12:01:37.458 6713-7171/org.tensorflow.benchmarking I/Manager: Capab {.relaxedFloat32toFloat16PerformanceScalar = {.execTime = 0.900000, .powerUsage = 0.000000}, .relaxedFloat32toFloat16PerformanceTensor = {.execTime = 0.000000, .powerUsage = 0.900000}, .operandPerformance = [16]{{.type = FLOAT32, .info = {.execTime = 0.400000, .powerUsage = 0.400000}}, {.type = INT32, .info = {.execTime = 0.600000, .powerUsage = 0.600000}}, {.type = UINT32, .info = {.execTime = 340282346638528859811704183484516925440.000000, .powerUsage = 340282346638528859811704183484516925440.000000}}, {.type = TENSOR_FLOAT32, .info = {.execTime = 0.400000, .powerUsage = 0.400000}}, {.type = TENSOR_INT32, .info = {.execTime = 0.600000, .powerUsage = 0.600000}}, {.type = TENSOR_QUANT8_ASYMM, .info = {.execTime = 0.600000, .powerUsage = 0.600000}}, {.type = BOOL, .info = {.execTime = 340282346638528859811704183484516925440.000000, .powerUsage = 340282346638528859811704183484516925440.000000}}, {.type = TENSOR_QUANT16_SYMM, .info = {.execTime = 0.600000, .powerUsage = 0.600000}}, {.type = TENSOR_FLOAT16\r\n",
      "2020-12-18 12:01:37.458 6713-7171/org.tensorflow.benchmarking I/Manager: Found interface liteadaptor\r\n",
      "2020-12-18 12:01:37.459 6713-6713/org.tensorflow.benchmarking D/RtgSchedIpcFile: setCommandByIoctl failed ret:-1, cmdid:32, errno:13\r\n",
      "2020-12-18 12:01:37.453 6713-6713/org.tensorflow.benchmarking W/ow.benchmarking: type=1400 audit(0.0:22623): avc: denied { ioctl } for pid=6713 path=\"/proc/6713/rtg\" dev=\"proc\" ino=478805 ioctlcmd=0xab20 scontext=u:r:untrusted_app:s0:c155,c256,c512,c768 tcontext=u:r:untrusted_app:s0:c155,c256,c512,c768 tclass=file permissive=0\r\n",
      "2020-12-18 12:01:37.460 6713-7171/org.tensorflow.benchmarking I/Manager: Capab {.relaxedFloat32toFloat16PerformanceScalar = {.execTime = 0.100000, .powerUsage = 0.100000}, .relaxedFloat32toFloat16PerformanceTensor = {.execTime = 0.100000, .powerUsage = 0.100000}, .operandPerformance = [14]{{.type = FLOAT32, .info = {.execTime = 1.000000, .powerUsage = 1.000000}}, {.type = INT32, .info = {.execTime = 0.500000, .powerUsage = 0.500000}}, {.type = UINT32, .info = {.execTime = 0.500000, .powerUsage = 0.500000}}, {.type = TENSOR_FLOAT32, .info = {.execTime = 1.000000, .powerUsage = 1.000000}}, {.type = TENSOR_INT32, .info = {.execTime = 0.500000, .powerUsage = 0.500000}}, {.type = TENSOR_QUANT8_ASYMM, .info = {.execTime = 0.200000, .powerUsage = 0.200000}}, {.type = BOOL, .info = {.execTime = 0.500000, .powerUsage = 0.500000}}, {.type = TENSOR_QUANT16_SYMM, .info = {.execTime = 1.000000, .powerUsage = 1.000000}}, {.type = TENSOR_FLOAT16, .info = {.execTime = 0.100000, .powerUsage = 0.100000}}, {.type = TENSOR_BOOL8, .info = {.execTime = 0.500000, .powerUsage = 0.500000}}, {.type = FLOA\r\n",
      "2020-12-18 12:01:37.460 6713-7171/org.tensorflow.benchmarking I/TypeManager: Failed to read /vendor/etc/nnapi_extensions_app_allowlist ; No app allowlisted for vendor extensions use.\r\n",
      "2020-12-18 12:01:37.461 6713-7171/org.tensorflow.benchmarking E/ExecutionBuilder: NN_RET_CHECK failed (frameworks/ml/nn/common/Utils.cpp:396): type.zeroPoint == 0 (type.zeroPoint = -128, 0 = 0) ANeuralNetworksModel_addOperand invalid zeroPoint: -128\r\n",
      "2020-12-18 12:01:37.461 6713-7171/org.tensorflow.benchmarking E/ExecutionBuilder: NN_RET_CHECK failed (frameworks/ml/nn/common/Utils.cpp:463): validateQuant8SymmParams(type, tag) \r\n",
      "2020-12-18 12:01:37.462 6713-7171/org.tensorflow.benchmarking E/org.tensorflow.benchmarking.MainActivity: Error initialising tflite interpreter\r\n",
      "    java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: NN API returned error ANEURALNETWORKS_BAD_DATA at line 1380 while adding operand for tensor 'model_tf_nlhd_nld/p_re_lu/add;model_tf_nlhd_nld/p_re_lu/Relu;model_tf_nlhd_nld/p_re_lu/Neg_1;model_tf_nlhd_nld/p_re_lu/Relu_1;model_tf_nlhd_nld/p_re_lu/mul'.\r\n",
      "    \r\n",
      "```\r\n",
      "\r\n",
      "I've also added the model file below.\r\n",
      "\r\n",
      "[model.zip](https://github.com/tensorflow/tensorflow/files/5715611/model.zip)\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.5\n",
      "comp:lite\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -   tf.keras.experimental.WideDeepModel saved model fails with 2 optimizers\n",
      "issue body -  tensorflow 2.4\r\n",
      "\r\n",
      "    \r\n",
      "```\r\n",
      "    wide_deep_model = tf.keras.experimental.WideDeepModel(linear_model, dnn_model, activation='sigmoid')\r\n",
      "    wide_deep_model.compile(optimizer= [linear_optimizer,dnn_optimizer]\r\n",
      "                            loss=tf.keras.losses.BinaryCrossentropy(),\r\n",
      "                            metrics=tf.keras.metrics.BinaryAccuracy())\r\n",
      "\r\n",
      "    ...\r\n",
      "    model.fit(dataset.batch(100).shuffle(100), epochs=2000, callbacks=[tensorboard_callback])\r\n",
      "    tf.saved_model.save(model, 'model/{}'.format(int(time.time())))\r\n",
      "```\r\n",
      "\r\n",
      "saved_mode will fail if given 2 optimizer to  tf.keras.experimental.WideDeepModel, the error is straight forward(self.optimizer is a list, so has no get_config method):\r\n",
      "\r\n",
      "Lib\\site-packages\\tensorflow\\python\\keras\\saving\\saving_utils.py\r\n",
      "\r\n",
      "```\r\n",
      "      else:\r\n",
      "        optimizer_config = {\r\n",
      "            'class_name':\r\n",
      "                generic_utils.get_registered_name(model.optimizer.__class__),\r\n",
      "            'config':\r\n",
      "                model.optimizer.get_config()\r\n",
      "        }\r\n",
      "      metadata['training_config']['optimizer_config'] = optimizer_config\r\n",
      "  return metadata\r\n",
      "```\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:keras\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  Risc-V Linux port for TensorFlow Lite\n",
      "issue body -  This PR has addition for riscv_makefile.inc\r\n",
      "Pre-requistes:\r\n",
      "- Install RISC-V GNU Toolchain for Linux. Refer https://github.com/riscv/riscv-gnu-toolchain\r\n",
      "- Here we are using riscv64-unknown-linux-gnu- tools created for \"linux\" from above link.\r\n",
      "The changes builds executables and static libs  in \"linux_riscv64\", in \"gen\" directory.\r\n",
      "Tested on RISC-V HiFive Unleashed.\r\n",
      "Test Results on RISC-V HiFive Unleashed:\r\n",
      "$./label_image --tflite_model ./models/mobilenet_v1_1.0_224.tflite --labels labels.txt -i ./images/grace_hopper.bmp \r\n",
      "INFO: Loaded model ./models/mobilenet_v1_1.0_224.tflite\r\n",
      "INFO: resolved reporter\r\n",
      "INFO: invoked\r\n",
      "INFO: average time: 1553.19 ms\r\n",
      "INFO: 0.860174: 653 653:military uniform\r\n",
      "INFO: 0.0481021: 907 907:Windsor tie\r\n",
      "INFO: 0.00786705: 466 466:bulletproof vest\r\n",
      "INFO: 0.00644936: 514 514:cornet, horn, trumpet, trump\r\n",
      "INFO: 0.00608029: 543 543:drumstick\r\n",
      "\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:lite\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  Initial port of Transpose from lite to micro\n",
      "issue body -  An initial port of the Transpose kernel from lite to micro, without any changes to the code. Changes to make the code run in micro will be delivered in a later PR. See #45695 \n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  \"InternalError: cudaGetDevice() failed. Status: initialization error\"  while using Sequential() and other\n",
      "issue body -  **System information**\r\n",
      "- I wrote a very simple code.\r\n",
      "- OS Platform and Distribution: ManjaroLinux 20.2\r\n",
      "- CPU: ryzen7 3750H \r\n",
      "- TensorFlow installed from (source or binary): pip install tensorflow-gpu\r\n",
      "- TensorFlow version (use command below): v2.4.0-rc4-71-g582c8d236cb 2.4.0\r\n",
      "- Python version: 3.8.6\r\n",
      "- Bazel version (if compiling from source):\r\n",
      "- GCC/Compiler version (if compiling from source):\r\n",
      "- CUDA/cuDNN version: CUDA : V11.1.105 / cuDNN : cudnn-8.0.5.39-1   \r\n",
      "- GPU model and memory: GTX 1660ti Max-Q 6 GB\r\n",
      "\r\n",
      "```\r\n",
      "\r\n",
      "== check python ===================================================\r\n",
      "python version: 3.8.6\r\n",
      "python branch: \r\n",
      "python build version: ('default', 'Sep 30 2020 04:00:38')\r\n",
      "python compiler version: GCC 10.2.0\r\n",
      "python implementation: CPython\r\n",
      "\r\n",
      "\r\n",
      "== check os platform ===============================================\r\n",
      "\r\n",
      "== are we in docker =============================================\r\n",
      "No\r\n",
      "\r\n",
      "== compiler =====================================================\r\n",
      "c++ (GCC) 10.2.0\r\n",
      "Copyright (C) 2020 Free Software Foundation, Inc.\r\n",
      "This is free software; see the source for copying conditions.  There is NO\r\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n",
      "\r\n",
      "\r\n",
      "== check pips ===================================================\r\n",
      "numpy                        1.19.2\r\n",
      "protobuf                     3.14.0\r\n",
      "tensorflow-estimator         2.4.0\r\n",
      "tensorflow-gpu               2.4.0\r\n",
      "\r\n",
      "== check for virtualenv =========================================\r\n",
      "False\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "== env ==========================================================\r\n",
      "LD_LIBRARY_PATH :/opt/cuda/lib64\r\n",
      "DYLD_LIBRARY_PATH is unset\r\n",
      "\r\n",
      "== nvidia-smi ===================================================\r\n",
      "Fri Dec 18 17:15:55 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 440.100      Driver Version: 440.100      CUDA Version: 10.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce GTX 166...  Off  | 00000000:01:00.0 Off |                  N/A |\r\n",
      "| N/A   56C    P8    10W /  N/A |     27MiB /  5944MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0       936      G   /usr/lib/Xorg                                 14MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "\r\n",
      "== cuda libs  ===================================================\r\n",
      "\r\n",
      "== tensorflow installed from info ==================\r\n",
      "\r\n",
      "== python version  ==============================================\r\n",
      "(major, minor, micro, releaselevel, serial)\r\n",
      "(3, 8, 6, 'final', 0)\r\n",
      "\r\n",
      "== bazel version  ===============================================\r\n",
      "```\r\n",
      "\" tensorflow import\" part is too long so I uploaded here [https://justpaste.it/9olm4](https://justpaste.it/9olm4)\r\n",
      "\r\n",
      "\r\n",
      "This is my code\r\n",
      "```\r\n",
      "import tensorflow as tf\r\n",
      "from keras.models import Sequential\r\n",
      "from keras.layers import Dense\r\n",
      "from keras.layers import LSTM\r\n",
      "from keras.layers import Dropout\r\n",
      "gpus = tf.config.experimental.list_physical_devices('GPU')\r\n",
      "print(gpus)\r\n",
      "if gpus:\r\n",
      "    try:\r\n",
      "        for gpu in gpus:\r\n",
      "            tf.config.experimental.set_memory_growth(gpu, True)\r\n",
      "    except RuntimeError as e:\r\n",
      "        print(e)\r\n",
      "model = Sequential()\r\n",
      "```\r\n",
      "Output\r\n",
      "```\r\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\r\n",
      "\r\n",
      "---------------------------------------------------------------------------\r\n",
      "InternalError                             Traceback (most recent call last)\r\n",
      "<ipython-input-3-65dd765202ab> in <module>\r\n",
      "     12     except RuntimeError as e:\r\n",
      "     13         print(e)\r\n",
      "---> 14 model = Sequential()\r\n",
      "\r\n",
      "~/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\r\n",
      "    515     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n",
      "    516     try:\r\n",
      "--> 517       result = method(self, *args, **kwargs)\r\n",
      "    518     finally:\r\n",
      "    519       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n",
      "\r\n",
      "~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py in __init__(self, layers, name)\r\n",
      "    115     \"\"\"\r\n",
      "    116     # Skip the init in FunctionalModel since model doesn't have input/output yet\r\n",
      "--> 117     super(functional.Functional, self).__init__(  # pylint: disable=bad-super-call\r\n",
      "    118         name=name, autocast=False)\r\n",
      "    119     base_layer.keras_api_gauge.get_cell('Sequential').set(True)\r\n",
      "\r\n",
      "~/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\r\n",
      "    515     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n",
      "    516     try:\r\n",
      "--> 517       result = method(self, *args, **kwargs)\r\n",
      "    518     finally:\r\n",
      "    519       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n",
      "\r\n",
      "~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py in __init__(self, *args, **kwargs)\r\n",
      "    291     self._steps_per_execution = None\r\n",
      "    292 \r\n",
      "--> 293     self._init_batch_counters()\r\n",
      "    294     self._base_model_initialized = True\r\n",
      "    295 \r\n",
      "\r\n",
      "~/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\r\n",
      "    515     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n",
      "    516     try:\r\n",
      "--> 517       result = method(self, *args, **kwargs)\r\n",
      "    518     finally:\r\n",
      "    519       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n",
      "\r\n",
      "~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py in _init_batch_counters(self)\r\n",
      "    299     # `evaluate`, and `predict`.\r\n",
      "    300     agg = variables.VariableAggregationV2.ONLY_FIRST_REPLICA\r\n",
      "--> 301     self._train_counter = variables.Variable(0, dtype='int64', aggregation=agg)\r\n",
      "    302     self._test_counter = variables.Variable(0, dtype='int64', aggregation=agg)\r\n",
      "    303     self._predict_counter = variables.Variable(\r\n",
      "\r\n",
      "~/.local/lib/python3.8/site-packages/tensorflow/python/ops/variables.py in __call__(cls, *args, **kwargs)\r\n",
      "    260       return cls._variable_v1_call(*args, **kwargs)\r\n",
      "    261     elif cls is Variable:\r\n",
      "--> 262       return cls._variable_v2_call(*args, **kwargs)\r\n",
      "    263     else:\r\n",
      "    264       return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n",
      "\r\n",
      "~/.local/lib/python3.8/site-packages/tensorflow/python/ops/variables.py in _variable_v2_call(cls, initial_value, trainable, validate_shape, caching_device, name, variable_def, dtype, import_scope, constraint, synchronization, aggregation, shape)\r\n",
      "    242     if aggregation is None:\r\n",
      "    243       aggregation = VariableAggregation.NONE\r\n",
      "--> 244     return previous_getter(\r\n",
      "    245         initial_value=initial_value,\r\n",
      "    246         trainable=trainable,\r\n",
      "\r\n",
      "~/.local/lib/python3.8/site-packages/tensorflow/python/ops/variables.py in <lambda>(**kws)\r\n",
      "    235                         shape=None):\r\n",
      "    236     \"\"\"Call on Variable class. Useful to force the signature.\"\"\"\r\n",
      "--> 237     previous_getter = lambda **kws: default_variable_creator_v2(None, **kws)\r\n",
      "    238     for _, getter in ops.get_default_graph()._variable_creator_stack:  # pylint: disable=protected-access\r\n",
      "    239       previous_getter = _make_getter(getter, previous_getter)\r\n",
      "\r\n",
      "~/.local/lib/python3.8/site-packages/tensorflow/python/ops/variable_scope.py in default_variable_creator_v2(next_creator, **kwargs)\r\n",
      "   2652   shape = kwargs.get(\"shape\", None)\r\n",
      "   2653 \r\n",
      "-> 2654   return resource_variable_ops.ResourceVariable(\r\n",
      "   2655       initial_value=initial_value,\r\n",
      "   2656       trainable=trainable,\r\n",
      "\r\n",
      "~/.local/lib/python3.8/site-packages/tensorflow/python/ops/variables.py in __call__(cls, *args, **kwargs)\r\n",
      "    262       return cls._variable_v2_call(*args, **kwargs)\r\n",
      "    263     else:\r\n",
      "--> 264       return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n",
      "    265 \r\n",
      "    266 \r\n",
      "\r\n",
      "~/.local/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py in __init__(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape)\r\n",
      "   1572       self._init_from_proto(variable_def, import_scope=import_scope)\r\n",
      "   1573     else:\r\n",
      "-> 1574       self._init_from_args(\r\n",
      "   1575           initial_value=initial_value,\r\n",
      "   1576           trainable=trainable,\r\n",
      "\r\n",
      "~/.local/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py in _init_from_args(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape)\r\n",
      "   1715               self._update_uid = initial_value.checkpoint_position.restore_uid\r\n",
      "   1716               initial_value = initial_value.wrapped_value\r\n",
      "-> 1717             initial_value = ops.convert_to_tensor(initial_value,\r\n",
      "   1718                                                   name=\"initial_value\",\r\n",
      "   1719                                                   dtype=dtype)\r\n",
      "\r\n",
      "~/.local/lib/python3.8/site-packages/tensorflow/python/profiler/trace.py in wrapped(*args, **kwargs)\r\n",
      "    161         with Trace(trace_name, **trace_kwargs):\r\n",
      "    162           return func(*args, **kwargs)\r\n",
      "--> 163       return func(*args, **kwargs)\r\n",
      "    164 \r\n",
      "    165     return wrapped\r\n",
      "\r\n",
      "~/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\r\n",
      "   1538 \r\n",
      "   1539     if ret is None:\r\n",
      "-> 1540       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n",
      "   1541 \r\n",
      "   1542     if ret is NotImplemented:\r\n",
      "\r\n",
      "~/.local/lib/python3.8/site-packages/tensorflow/python/framework/tensor_conversion_registry.py in _default_conversion_function(***failed resolving arguments***)\r\n",
      "     50 def _default_conversion_function(value, dtype, name, as_ref):\r\n",
      "     51   del as_ref  # Unused.\r\n",
      "---> 52   return constant_op.constant(value, dtype, name=name)\r\n",
      "     53 \r\n",
      "     54 \r\n",
      "\r\n",
      "~/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name)\r\n",
      "    262     ValueError: if called on a symbolic tensor.\r\n",
      "    263   \"\"\"\r\n",
      "--> 264   return _constant_impl(value, dtype, shape, name, verify_shape=False,\r\n",
      "    265                         allow_broadcast=True)\r\n",
      "    266 \r\n",
      "\r\n",
      "~/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)\r\n",
      "    274       with trace.Trace(\"tf.constant\"):\r\n",
      "    275         return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\r\n",
      "--> 276     return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\r\n",
      "    277 \r\n",
      "    278   g = ops.get_default_graph()\r\n",
      "\r\n",
      "~/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py in _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\r\n",
      "    299 def _constant_eager_impl(ctx, value, dtype, shape, verify_shape):\r\n",
      "    300   \"\"\"Implementation of eager constant.\"\"\"\r\n",
      "--> 301   t = convert_to_eager_tensor(value, ctx, dtype)\r\n",
      "    302   if shape is None:\r\n",
      "    303     return t\r\n",
      "\r\n",
      "~/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py in convert_to_eager_tensor(value, ctx, dtype)\r\n",
      "     95     except AttributeError:\r\n",
      "     96       dtype = dtypes.as_dtype(dtype).as_datatype_enum\r\n",
      "---> 97   ctx.ensure_initialized()\r\n",
      "     98   return ops.EagerTensor(value, ctx.device_name, dtype)\r\n",
      "     99 \r\n",
      "\r\n",
      "~/.local/lib/python3.8/site-packages/tensorflow/python/eager/context.py in ensure_initialized(self)\r\n",
      "    524         if self._use_tfrt is not None:\r\n",
      "    525           pywrap_tfe.TFE_ContextOptionsSetTfrt(opts, self._use_tfrt)\r\n",
      "--> 526         context_handle = pywrap_tfe.TFE_NewContext(opts)\r\n",
      "    527       finally:\r\n",
      "    528         pywrap_tfe.TFE_DeleteContextOptions(opts)\r\n",
      "\r\n",
      "InternalError: cudaGetDevice() failed. Status: initialization error\r\n",
      "```\r\n",
      "\r\n",
      "Jupyter Terminal Output\r\n",
      "```\r\n",
      "2020-12-18 17:52:13.476509: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n",
      "2020-12-18 17:52:13.476810: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n",
      "2020-12-18 17:52:13.478279: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\n",
      "pciBusID: 0000:01:00.0 name: GeForce GTX 1660 Ti with Max-Q Design computeCapability: 7.5\r\n",
      "coreClock: 1.335GHz coreCount: 24 deviceMemorySize: 5.80GiB deviceMemoryBandwidth: 268.26GiB/s\r\n",
      "2020-12-18 17:52:13.478341: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "2020-12-18 17:52:13.478429: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n",
      "2020-12-18 17:52:13.478465: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n",
      "2020-12-18 17:52:13.478496: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n",
      "2020-12-18 17:52:13.478525: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n",
      "2020-12-18 17:52:13.478553: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n",
      "2020-12-18 17:52:13.478582: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n",
      "2020-12-18 17:52:13.478610: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n",
      "2020-12-18 17:52:13.478836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n",
      "2020-12-18 17:52:13.480513: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n",
      "2020-12-18 17:52:13.481882: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n",
      "\r\n",
      "```\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:gpu\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  fix softnms bug when set iou threshold\n",
      "issue body -  cc @mingxingtan \n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:core\n",
      "ready to pull\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  Fix: tf.contrib.distribute.CollectiveAllReduceStrategy can't load model from checkpoint #45839\n",
      "issue body -  About [issue 45839](https://github.com/tensorflow/tensorflow/issues/45839)\r\n",
      "\r\n",
      "I analyzed and debug the source code and found the following problems:\r\n",
      "\r\n",
      "1.  The value of `worker_context.should_checkpoint` will change the basic path of checkpoint. But the restore action is related to\r\n",
      "\r\n",
      " the base path of the checkpoint.\r\n",
      "```\r\n",
      "venv/lib/python3.6/site-packages/tensorflow_core/python/training/monitored_session.py line: 341\r\n",
      "def _create_monitored_session_with_worker_context(...)\r\n",
      "  ...\r\n",
      "  if (((save_checkpoint_secs and save_checkpoint_secs > 0) or\r\n",
      "       (save_checkpoint_steps and save_checkpoint_steps > 0)) and\r\n",
      "      checkpoint_dir):\r\n",
      "    if worker_context.should_checkpoint:\r\n",
      "      all_hooks.append(\r\n",
      "          basic_session_run_hooks.CheckpointSaverHook(\r\n",
      "              checkpoint_dir,\r\n",
      "              save_steps=save_checkpoint_steps,\r\n",
      "              save_secs=save_checkpoint_secs,\r\n",
      "              scaffold=scaffold))\r\n",
      "    elif tmpdir:\r\n",
      "      all_hooks.append(\r\n",
      "          basic_session_run_hooks.CheckpointSaverHook(\r\n",
      "              os.path.join(checkpoint_dir, tmpdir),\r\n",
      "              save_steps=save_checkpoint_steps,\r\n",
      "              save_secs=save_checkpoint_secs,\r\n",
      "              scaffold=scaffold))\r\n",
      "  ...\r\n",
      "```\r\n",
      "\r\n",
      "2. The value of `worker_context.should_checkpoint` depends on the _is_chief attribute of Class `Collective AllReduce Extended`\r\n",
      "```\r\n",
      "venv/lib/python3.6/sitepackages/tensorflow_core/python/distribute/collective_all_reduce_strategy.py line 231\r\n",
      "\r\n",
      "def _initialize_multi_worker(self, cluster_resolver):\r\n",
      "    \"\"\"Initializes the object for multi-worker training.\"\"\"\r\n",
      "    ....\r\n",
      "    self._is_chief = multi_worker_util.is_chief(\r\n",
      "        cluster_spec, task_type,task_id)\r\n",
      "```\r\n",
      "3. `multi_worker_util.py is_chief()` is the key to the problem \r\n",
      "```\r\n",
      "/venv/lib/python3.6/site-packages/tensorflow_core/python/distribute/multi_worker_util.py\r\n",
      "line 93: is_chief\r\n",
      "def is_chief(cluster_spec=None, task_type=None, task_id=None):\r\n",
      "  ...\r\n",
      "  if task_type == \"chief\" or task_type == \"evaluator\":\r\n",
      "    return True\r\n",
      "\r\n",
      "  if (\"chief\" not in cluster_spec and task_type == \"worker\" and task_id == 0):\r\n",
      "    return True\r\n",
      "  return False\r\n",
      "  ...\r\n",
      "```\r\n",
      "\r\n",
      "My sulotion as follow:\r\n",
      "```\r\n",
      "def is_chief_for_collective_all_reduce_strategy(cluster_spec=None, task_type=None, task_id=None):\r\n",
      "  # Fix the bug that the model fails to load from checkpoint when using collective_all_reduce_strategy\r\n",
      "  # if task_type == \"chief\" or task_type == \"worker\" or task_type == \"evaluator\":\r\n",
      "  if task_type == \"chief\" or task_type == \"evaluator\":\r\n",
      "    return True\r\n",
      "  return False\r\n",
      "```\r\n",
      "\r\n",
      "I will use this solution in kubeflow in production environment.  Is there any risk in this?\r\n",
      "\r\n",
      "\r\n",
      "Hope the PR can pass, if any questions, please give me some advice.\r\n",
      "\r\n",
      "Thanks a lot!!!\n",
      "issue labels - \n",
      "cla: yes\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  tf.contrib.distribute.CollectiveAllReduceStrategy can't load model from checkpoint\n",
      "issue body -  <em>Please make sure that this is a bug. As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:bug_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n",
      "- TensorFlow installed from (source or binary): binary\r\n",
      "- TensorFlow version (use command below): v1.15.4\r\n",
      "- Python version: 3.6.9\r\n",
      "- Bazel version (if compiling from source): N/A\r\n",
      "- GCC/Compiler version (if compiling from source): N/A\r\n",
      "- CUDA/cuDNN version: N/A\r\n",
      "- GPU model and memory: N/A\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "I am working on distributed learning in tensorflow through estimators API using below simple code template:\r\n",
      "```\r\n",
      "def main(argv):\r\n",
      "\r\n",
      "    # Init, set model dir| export dir | log dir.\r\n",
      "    model_dir, export_dir = init()\r\n",
      "    # Loading dataset\r\n",
      "    download_dataset()\r\n",
      "\r\n",
      "    # Select distribute strategy, such as sync, async, allReduce, etc.\r\n",
      "    # CollectiveAllReduceStrategy for allReduce\r\n",
      "    dist_strategy = tf.contrib.distribute.CollectiveAllReduceStrategy(num_gpus_per_worker=FLAGS.num_gpus_per_worker)\r\n",
      "    # Set run config, including checkpoint saving strategy, maximum number of checkpoints saved, etc.\r\n",
      "    run_config = tf.estimator.RunConfig(train_distribute=dist_strategy,\r\n",
      "                                        eval_distribute=dist_strategy,\r\n",
      "                                        # save_checkpoints_secs=10,\r\n",
      "                                        save_checkpoints_steps=FLAGS.save_checkpoints_steps,\r\n",
      "                                        keep_checkpoint_max=FLAGS.keep_checkpoint_max)\r\n",
      "\r\n",
      "    # Feature columns describe how to use the input.\r\n",
      "    my_feature_columns = get_feature_columns()\r\n",
      "\r\n",
      "    # Model\r\n",
      "    # Build 2 hidden layer DNN with 10, 10 units respectively.\r\n",
      "    classifier = Net(model_dir, my_feature_columns, run_config).net\r\n",
      "\r\n",
      "    # TrainSpec for training\r\n",
      "    train_spec = tf.estimator.TrainSpec(\r\n",
      "        input_fn=lambda: csv_input_fn(TRAIN_PATH, FLAGS.batch_size, True),\r\n",
      "        max_steps=FLAGS.train_steps,\r\n",
      "        hooks=[])\r\n",
      "    # EvalSpec for test\r\n",
      "    eval_spec = tf.estimator.EvalSpec(\r\n",
      "        input_fn=lambda: csv_input_fn(TEST_PATH, FLAGS.batch_size, True))\r\n",
      "    print(\"---training and testing---\")\r\n",
      "    tf.estimator.train_and_evaluate(classifier, train_spec, eval_spec)\r\n",
      "    print(\"---training finished---\")\r\n",
      "\r\n",
      "    # All role are workers, pick the task_index with 0 to save model\r\n",
      "    if FLAGS.task_index == 0:\r\n",
      "        classifier.export_saved_model(export_dir, serving_input_receiver_fn)\r\n",
      "        # classifier.export_savedmodel(export_dir, serving_input_receiver_fn, strip_default_attrs=True)\r\n",
      "    print(\"finish...\")\r\n",
      "```\r\n",
      "Firstly, I train for 1000 rounds(train_stpes=1000) and save the checkpoint, it works normally.\r\n",
      "\r\n",
      "Then I set the train_steps to 2000, only the is_chief role can restore the model from the checkpoint without any error.\r\n",
      "\r\n",
      "Chief output as follow:\r\n",
      "```\r\n",
      "INFO:tensorflow:Graph was finalized.\r\n",
      "I1218 15:24:28.721170 139824828032832 monitored_session.py:240] Graph was finalized.\r\n",
      "INFO:tensorflow:Restoring parameters from /tmp/iris/iris-chief-0/checkpoint/model.ckpt-1000\r\n",
      "I1218 15:24:28.722573 139824828032832 saver.py:1284] Restoring parameters from /tmp/iris/iris-chief-0/checkpoint/model.ckpt-1000\r\n",
      "WARNING:tensorflow:From /root/PycharmProjects/multi_gpu_demo/venv/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py:1069: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use standard file utilities to get mtimes.\r\n",
      "W1218 15:24:28.753480 139824828032832 deprecation.py:323] From /root/PycharmProjects/multi_gpu_demo/venv/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py:1069: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use standard file utilities to get mtimes.\r\n",
      "INFO:tensorflow:Running local_init_op.\r\n",
      "I1218 15:24:28.782264 139824828032832 session_manager.py:500] Running local_init_op.\r\n",
      "INFO:tensorflow:Done running local_init_op.\r\n",
      "I1218 15:24:28.938534 139824828032832 session_manager.py:502] Done running local_init_op.\r\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into /tmp/iris/iris-chief-0/checkpoint/model.ckpt.\r\n",
      "I1218 15:24:29.215120 139824828032832 basic_session_run_hooks.py:606] Saving checkpoints for 1000 into /tmp/iris/iris-chief-0/checkpoint/model.ckpt.\r\n",
      "```\r\n",
      "\r\n",
      "Worker output as follow:\r\n",
      "```\r\n",
      "INFO:tensorflow:Graph was finalized.\r\n",
      "I1218 15:24:59.388847 140629246773056 monitored_session.py:240] Graph was finalized.\r\n",
      "```\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "\r\n",
      "I expect that when I increase train_steps（from 1000 to 2000）, no matter which roles should be able to restore the model from \r\n",
      "\r\n",
      "checkpoint and continue training. Because I think each role only saves a part of the parameters of the model on allReduce mode, \r\n",
      "\r\n",
      "so each role should participate in restoring the model from checkpoint.\r\n",
      "\r\n",
      "But, I analyzed and debug the source code and found only roles with chief or worker index=0 can restore model parameters \r\n",
      "\r\n",
      "from checkpoint\r\n",
      "```\r\n",
      "/venv/lib/python3.6/site-packages/tensorflow_core/python/distribute/multi_worker_util.py\r\n",
      "line 93: is_chief\r\n",
      "def is_chief(cluster_spec=None, task_type=None, task_id=None):\r\n",
      "  ...\r\n",
      "  if task_type == \"chief\" or task_type == \"evaluator\":\r\n",
      "    return True\r\n",
      "\r\n",
      "  if (\"chief\" not in cluster_spec and task_type == \"worker\" and task_id == 0):\r\n",
      "    return True\r\n",
      "  return False\r\n",
      "  ...\r\n",
      "```\r\n",
      "\n",
      "issue labels - \n",
      "TF 1.15\n",
      "contrib\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Op type not registered 'StatefulPartitionedCall' in binary running\n",
      "issue body -  使用spark-2.3，然后通过maven下载依赖包，libtensorflow-1.9.0.jar,libtensorflow_jni-1.9.0.jar；\r\n",
      "在spark-submit提交，读取模型，报如下错：\r\n",
      "\r\n",
      "org.tensorflow.TensorFlowException: Op type not registered 'StatefulPartitionedCall' in binary running on [机器名]. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.\r\n",
      "\n",
      "issue labels - \n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:others\n",
      "\n",
      "\n",
      "issue title -  Fix: NoneType' object has no attribute 'UnimplementedError\n",
      "issue body -  At shutdown, ` tensorflow.python.framework.errors` may have been garbage collected.\n",
      "issue labels - \n",
      "cla: yes\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  v1.15.4 NoneType' object has no attribute 'UnimplementedError\n",
      "issue body -  <em>Please make sure that this is a bug. As per our\r\n",
      "[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n",
      "we only address code/doc bugs, performance issues, feature requests and\r\n",
      "build/installation issues on GitHub. tag:bug_template</em>\r\n",
      "\r\n",
      "**System information**\r\n",
      "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n",
      "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n",
      "- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n",
      "- TensorFlow installed from (source or binary): binary\r\n",
      "- TensorFlow version (use command below): 1.15.4\r\n",
      "- Python version: 3.6.9\r\n",
      "- Bazel version (if compiling from source): N/A\r\n",
      "- GCC/Compiler version (if compiling from source): N/A\r\n",
      "- CUDA/cuDNN version:  N/A\r\n",
      "- GPU model and memory: N/A\r\n",
      "\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "I am working on distributed learning in tensorflow through estimators API using below simple code template:\r\n",
      "\r\n",
      "```\r\n",
      "# TrainSpec for training\r\n",
      "    train_spec = tf.estimator.TrainSpec(\r\n",
      "        input_fn=lambda: csv_input_fn(TRAIN_PATH, FLAGS.batch_size, True),\r\n",
      "        max_steps=FLAGS.train_steps,\r\n",
      "        hooks=[])\r\n",
      "    # EvalSpec for test\r\n",
      "    eval_spec = tf.estimator.EvalSpec(\r\n",
      "        input_fn=lambda: csv_input_fn(TEST_PATH, FLAGS.batch_size, True))\r\n",
      "    print(\"---training and testing---\")\r\n",
      "    tf.estimator.train_and_evaluate(classifier, train_spec, eval_spec)\r\n",
      "    print(\"---training finished---\")\r\n",
      "\r\n",
      "    # All role are workers, pick the task_index with 0 to save model\r\n",
      "    if FLAGS.task_index == 0:\r\n",
      "        classifier.export_saved_model(export_dir, serving_input_receiver_fn)\r\n",
      "        # classifier.export_savedmodel(export_dir, serving_input_receiver_fn, strip_default_attrs=True)\r\n",
      "    print(\"finish...\")\r\n",
      "```\r\n",
      "\r\n",
      "I encountered the following error during training.\r\n",
      "\r\n",
      "```\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/server_lib.py\", line 158, in __del__\r\n",
      "AttributeError: 'NoneType' object has no attribute 'UnimplementedError'\r\n",
      "```\r\n",
      "\r\n",
      "I analyzed the source code and found that the errors object may be garbage collected.\r\n",
      "\r\n",
      "I will submit a PR later, please pass it. Because I will use v1.15.4(v1.15.4 version on our production environment) \r\n",
      "\r\n",
      "for distributed training on kubeflow, \r\n",
      "\r\n",
      "I don’t want to modify the code there every time.\r\n",
      "\r\n",
      "Thanks!!!\r\n",
      "\n",
      "issue labels - \n",
      "TF 1.15\n",
      "comp:dist-strat\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:bug\n",
      "\n",
      "\n",
      "issue title -  Tensorflow 2.4 Throwing Errors \n",
      "issue body -  **System information**\r\n",
      "- OS Platform and Distribution: Windows 10 v20H2 \r\n",
      "- TensorFlow installed from (source or binary): Through pip command\r\n",
      "- TensorFlow version (use command below): v2.4.0\r\n",
      "- Python version: v3.8.5\r\n",
      "- CUDA/cuDNN version: v11.0 (update 1)/v8.0.4\r\n",
      "- GPU model and memory: Nvidia GTX 1650 4GB GDDR5\r\n",
      "\r\n",
      "-tf.version.GIT_VERSION: library cudart64_110.dll\r\n",
      "-tf.version.VERSION: v2.4.0-rc4-71-g582c8d236cb 2.4.0\r\n",
      "\r\n",
      "**Describe the current behavior**\r\n",
      "UnknownError:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n",
      "\t [[node sequential/conv2d/Conv2D (defined at <ipython-input-4-f922f88cbe61>:19) ]] [Op:__inference_train_function_753]\r\n",
      "Function call stack:\r\n",
      "train_function\r\n",
      "\r\n",
      "**Describe the expected behavior**\r\n",
      "It should be training the network with no issues. The same code worked on tensorflow v2.3.0.\r\n",
      "\r\n",
      "**Standalone code to reproduce the issue**\r\n",
      "Notebook at:\r\n",
      "https://colab.research.google.com/drive/14vdfsSkqGn375E54vd11--yXy0WKemWm?usp=sharing\r\n",
      "It was working fine on v2.3.0 at my end but after upgrading to 2.4.0 its throwing error.  Also I tried one more CNN network with different architecture which is working fine with no errors. I have also renamed a dll file in order to make tensorflow open CUPTI library.\r\n",
      "\r\n",
      "**Other info / logs** Logs are included.\r\n",
      "[Expected_log.txt](https://github.com/tensorflow/tensorflow/files/5713830/Expected_log.txt)\r\n",
      "[Current_log.txt](https://github.com/tensorflow/tensorflow/files/5713831/Current_log.txt)\r\n",
      "\r\n",
      "\r\n",
      "\n",
      "issue labels - \n",
      "TF 2.4\n",
      "comp:gpu\n",
      "stalled\n",
      "stat:awaiting response\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  I think I only have cudart64_110.dll\n",
      "issue body -  I think I only have cudart64_110.dll\r\n",
      "(The one that I highlighted)\r\n",
      "![image](https://user-images.githubusercontent.com/68514251/93034064-d0b54a00-f606-11ea-991e-429bb7713d58.png)\r\n",
      "\r\n",
      "_Originally posted by @CalendulaED in https://github.com/tensorflow/tensorflow/issues/43193#issuecomment-691760016_\r\n",
      "\r\n",
      "please pass me this file cudart64_110.dll its misssing here, i dont know what else i can do\r\n",
      "\n",
      "issue labels - \n",
      "comp:gpu\n",
      "type:support\n",
      "\n",
      "\n",
      "issue title -  created pizza.txt\n",
      "issue body -  \n",
      "issue labels - \n",
      "cla: no\n",
      "invalid\n",
      "size:XS\n",
      "\n",
      "\n",
      "issue title -  Add GPU kernels for SparseApply[Proximal]Adagrad\n",
      "issue body -  Also applies to Resource and V2 versions of the ops.\r\n",
      "\r\n",
      "This is a follow-up to https://github.com/tensorflow/tensorflow/pull/44919\r\n",
      "\r\n",
      "cc @nluehr \n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:core\n",
      "ready to pull\n",
      "size:L\n",
      "\n",
      "\n",
      "issue title -  [INTEL MKL] DNN 0.x code cleanup - Fused Matmul op\n",
      "issue body -  This PR replaces the old one https://github.com/tensorflow/tensorflow/pull/45564 (which is not approved/merged), \r\n",
      "\r\n",
      "\r\n",
      "DNN 0.x cleanup of Fused Matmul op:\r\n",
      "\r\n",
      "(1) Remove all DNN 0.x related code;\r\n",
      "\r\n",
      "(2) Replace all DNN 1.x macro usages\r\n",
      "\r\n",
      "And minor change in quantized op (macro replacement in a couple of places)\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:mkl\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  Copy TFL kernel EXP and exp_test into TFLM kernel without changes\n",
      "issue body -  Issue #45415 PR3: copy the reference kernel from lite to micro without making any changes. At this point the kernel is in micro but it is not part of the build\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "prtype:bugfix\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  Copy TFL kernel CAST and cast_test into TFLM kernel without changes\n",
      "issue body -  Issue #45608 PR3: Create a TFLite Micro copy of the TFLite CAST operator and its test code\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:lite\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:L\n",
      "\n",
      "\n",
      "issue title -  Refactor Lite reference op CAST from reference_ops.h into cast.h\n",
      "issue body -  Issue #45608 PR2: Refactor reference implementation from lite/kernels/internal/reference/reference_ops.h into its own header cast.h without making any changes\n",
      "issue labels - \n",
      "awaiting review\n",
      "cla: yes\n",
      "comp:lite\n",
      "comp:micro\n",
      "size:S\n",
      "\n",
      "\n",
      "issue title -  Add renode downloads to the Makefile.\n",
      "issue body -  Since the first integration with the portable renode, we have converged\r\n",
      "on adding synchronous calls to download scripts as part of the Makefile\r\n",
      "and this change integrates the renode download with that flow.\r\n",
      "\r\n",
      "Addresses http://b/172939049\n",
      "issue labels - \n",
      "cla: yes\n",
      "comp:micro\n",
      "ready to pull\n",
      "size:M\n",
      "\n",
      "\n",
      "issue title -  Fix bad cherrypick\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(issues_list)):\n",
    "    issue=issues_list[i]\n",
    "    issue_title=issue[\"title\"]\n",
    "    issue_body=issue[\"body\"]\n",
    "    print(\"\\n\\nissue title - \",issue_title)\n",
    "    print(\"issue body - \",issue_body)\n",
    "    print(\"issue labels - \")\n",
    "    issue_labels_list = issue[\"labels\"]\n",
    "    for j in range(len(issue_labels_list)):\n",
    "        label=issue_labels_list[j]\n",
    "        label_name=label[\"name\"]\n",
    "        print(label_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "labels_list=[]\n",
    "for i in range(len(issues_list)):\n",
    "    single_data=[]\n",
    "    issue=issues_list[i]\n",
    "    issue_title=issue[\"title\"]\n",
    "    issue_body=issue[\"body\"]\n",
    "    \n",
    "    single_data=single_data+[issue_title,issue_body];\n",
    "    \n",
    "    data=data+[single_data];\n",
    "    \n",
    "    issue_labels_list = issue[\"labels\"]\n",
    "    single_label_list=[]\n",
    "    for j in range(len(issue_labels_list)):\n",
    "        label=issue_labels_list[j]\n",
    "        label_name=label[\"name\"]\n",
    "        single_label_list=single_label_list+[label_name]\n",
    "    labels_list=labels_list+[single_label_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4029 4029\n"
     ]
    }
   ],
   "source": [
    "print(len(data),len(labels_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['type:bug']\n",
      "['type:bug']\n",
      "['cla: yes', 'size:M']\n",
      "['cla: yes', 'comp:micro', 'size:M']\n",
      "['type:performance']\n",
      "['type:others']\n",
      "['TF 2.4', 'comp:micro', 'stat:awaiting response', 'type:support']\n",
      "['TF 1.15', 'comp:dist-strat', 'stat:awaiting response', 'type:support']\n",
      "['TF 2.4', 'comp:keras', 'type:docs-bug']\n",
      "['comp:ops', 'type:bug']\n",
      "['comp:lite', 'stat:awaiting response', 'type:docs-bug']\n",
      "['stat:awaiting response', 'type:others']\n",
      "['comp:tpus', 'stat:awaiting response', 'type:support']\n",
      "['TF 2.4', 'comp:data', 'stat:awaiting response', 'type:support']\n",
      "['comp:data', 'stat:awaiting response', 'type:bug']\n",
      "['TF 1.15', 'comp:gpu:tensorrt', 'stat:awaiting response', 'type:support']\n",
      "['TF 2.4', 'stat:awaiting response', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['TF 2.4', 'comp:keras', 'type:bug']\n",
      "['TF 2.4', 'comp:gpu', 'stat:awaiting response', 'type:support']\n",
      "['TF 2.4', 'comp:lite', 'type:support']\n",
      "['TF 2.4', 'stat:awaiting response', 'type:bug']\n",
      "['TF 2.2', 'comp:keras', 'stat:awaiting response', 'type:bug']\n",
      "['cla: yes', 'comp:keras', 'size:XS']\n",
      "['comp:lite', 'type:support']\n",
      "[]\n",
      "['cla: yes', 'comp:micro', 'comp:micro:arm', 'prtype:bugfix', 'size:M']\n",
      "['TF 2.4', 'comp:ops', 'regression issue', 'type:bug']\n",
      "['cla: yes', 'comp:core', 'size:XS']\n",
      "['cla: yes', 'comp:micro', 'size:L']\n",
      "['TF 2.4', 'comp:apis', 'stat:awaiting response', 'type:support']\n",
      "['TF 2.2', 'comp:gpu', 'stat:awaiting response', 'type:support']\n",
      "['cla: yes', 'size:L']\n",
      "['cla: yes', 'comp:keras', 'size:S']\n",
      "['cla: yes', 'ready to pull', 'size:S']\n",
      "['cla: yes', 'comp:lite', 'size:S']\n",
      "['TF 2.2', 'comp:dist-strat', 'stat:awaiting response', 'type:performance']\n",
      "['cla: yes', 'comp:keras', 'size:XS']\n",
      "['cla: yes', 'comp:micro', 'ready to pull', 'size:S']\n",
      "['TF 2.4', 'comp:keras', 'type:support']\n",
      "['awaiting review', 'cla: yes', 'size:XS']\n",
      "['TF 2.3', 'comp:apis', 'type:support']\n",
      "['TF 2.4', 'comp:apis', 'type:support']\n",
      "['cla: yes', 'comp:keras', 'size:XS']\n",
      "['TF 2.4', 'comp:keras', 'type:bug']\n",
      "['TFLiteConverter', 'comp:lite', 'type:feature']\n",
      "['TFLiteConverter', 'type:feature']\n",
      "['comp:micro', 'type:feature']\n",
      "['cla: yes', 'comp:core', 'size:XS', 'stat:awaiting tensorflower']\n",
      "['stat:awaiting response', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['cla: yes', 'comp:core', 'ready to pull', 'size:S']\n",
      "['comp:micro']\n",
      "['comp:mkl', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:micro']\n",
      "['cla: yes', 'comp:keras', 'size:S']\n",
      "['TF 2.4', 'comp:gpu', 'type:support']\n",
      "['cla: yes', 'comp:keras', 'size:M']\n",
      "['TF 2.4', 'comp:ops', 'type:bug']\n",
      "['type:build/install']\n",
      "['TF 2.4', 'comp:keras', 'type:bug']\n",
      "['type:bug']\n",
      "['comp:lite', 'type:build/install']\n",
      "['comp:lite', 'comp:micro', 'type:build/install']\n",
      "['cla: yes', 'comp:xla', 'size:L']\n",
      "['cla: yes', 'comp:keras', 'size:XS']\n",
      "['stat:awaiting response', 'type:bug']\n",
      "['TF 2.4', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "[]\n",
      "['comp:apis', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.4', 'comp:keras', 'type:bug']\n",
      "['cla: yes', 'size:XS']\n",
      "[]\n",
      "['awaiting review', 'cla: yes', 'comp:micro', 'size:L']\n",
      "['comp:lite', 'comp:runtime', 'type:performance']\n",
      "['TF 2.5', 'type:bug']\n",
      "['cla: yes', 'comp:ops', 'ready to pull', 'size:XS']\n",
      "['TF 2.4', 'stat:awaiting response', 'type:bug']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.4', 'comp:gpu', 'comp:keras', 'type:bug']\n",
      "['cla: yes', 'comp:micro', 'ready to pull', 'size:XS']\n",
      "['TF 2.4', 'comp:keras', 'type:support']\n",
      "['TF 2.4', 'stat:awaiting response', 'subtype:macOS', 'type:build/install']\n",
      "['stat:awaiting response', 'type:others']\n",
      "['comp:gpu', 'comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.5', 'TFLiteConverter', 'comp:lite', 'type:feature']\n",
      "['comp:gpu', 'type:performance']\n",
      "['TF 2.4', 'comp:lite', 'type:support']\n",
      "['cla: yes', 'size:M']\n",
      "['stat:awaiting response', 'type:build/install']\n",
      "['cla: yes', 'comp:xla', 'size:S']\n",
      "['comp:keras', 'type:support']\n",
      "['comp:dist-strat', 'stat:awaiting tensorflower', 'type:support']\n",
      "['cla: yes', 'comp:keras', 'size:XS']\n",
      "['cla: yes', 'comp:core', 'size:L']\n",
      "['TF 2.4', 'comp:keras', 'type:bug']\n",
      "['TF 2.4', 'comp:ops', 'type:bug']\n",
      "['TF 2.4', 'comp:gpu', 'stat:awaiting response', 'type:performance']\n",
      "['TF 2.3', 'comp:lite', 'comp:micro', 'type:support']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.3', 'comp:dist-strat', 'comp:keras', 'type:performance']\n",
      "['TF 2.4', 'comp:apis', 'type:bug']\n",
      "['TF 2.3', 'TFLiteConverter', 'comp:lite', 'type:bug']\n",
      "['comp:apis', 'stat:awaiting response', 'type:support']\n",
      "['stat:awaiting response', 'type:bug']\n",
      "['cla: yes', 'comp:data', 'ready to pull', 'size:L']\n",
      "['cla: yes', 'comp:lite', 'size:M', 'type:build/install']\n",
      "['cla: yes', 'size:M']\n",
      "['cla: yes', 'comp:data', 'size:L']\n",
      "['TF 2.5', 'stat:awaiting response', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['comp:keras', 'stat:awaiting response', 'type:support']\n",
      "['TF 2.4', 'stat:awaiting response', 'type:bug']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.3', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['TF 2.4', 'comp:ops', 'type:support']\n",
      "['cla: yes', 'comp:micro', 'ready to pull', 'size:XS']\n",
      "['stat:awaiting tensorflower', 'type:build/install', 'type:feature']\n",
      "['TF 2.4', 'comp:keras', 'type:bug']\n",
      "['cla: yes', 'size:XS']\n",
      "['TF 2.5', 'stalled', 'stat:awaiting response', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['TF 2.4', 'type:bug']\n",
      "['comp:micro', 'type:bug']\n",
      "['comp:micro', 'type:feature']\n",
      "['TF 2.1', 'TF 2.4', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['type:docs-bug']\n",
      "['comp:keras', 'type:docs-feature']\n",
      "['TF 2.4', 'comp:mkl', 'stat:awaiting tensorflower', 'type:build/install']\n",
      "['TF 2.4', 'comp:model', 'type:bug']\n",
      "['TF 2.4', 'comp:keras', 'stat:awaiting response', 'type:bug']\n",
      "['cla: yes', 'comp:mkl', 'ready to pull', 'size:M']\n",
      "['comp:dist-strat', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:dist-strat', 'type:support']\n",
      "['TF 2.4', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.1', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['cla: yes', 'size:S']\n",
      "['comp:lite', 'type:feature']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.4', 'TFLiteConverter', 'comp:lite', 'type:support']\n",
      "['TF 2.4', 'comp:apis', 'type:bug']\n",
      "['TF 2.4', 'comp:gpu', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['Fixed in Nightly', 'TF 2.4', 'comp:keras', 'type:bug']\n",
      "['TF 2.4', 'comp:keras', 'stat:awaiting response', 'type:bug']\n",
      "['comp:model', 'stalled', 'stat:awaiting response', 'type:bug']\n",
      "['stalled', 'stat:awaiting response', 'type:bug']\n",
      "['TF 2.4', 'stat:awaiting response', 'type:support']\n",
      "['TF 2.3', 'stat:awaiting response', 'subtype:macOS', 'type:build/install']\n",
      "['stalled', 'stat:awaiting response', 'type:build/install']\n",
      "['TF 2.3', 'comp:gpu']\n",
      "['cla: yes', 'ready to pull', 'size:XS']\n",
      "['TF 2.4', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['TF 2.4', 'comp:tpus', 'stat:awaiting response', 'type:support']\n",
      "['cla: yes', 'comp:lite', 'size:M']\n",
      "['TF 2.4', 'comp:keras', 'type:bug']\n",
      "['comp:lite', 'comp:micro', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['cla: yes', 'size:XL']\n",
      "['TF 2.3', 'comp:dist-strat', 'type:feature']\n",
      "['comp:micro']\n",
      "['stalled', 'stat:awaiting response', 'type:support']\n",
      "['ModelOptimizationToolkit', 'TF 2.3', 'comp:lite', 'stat:awaiting response', 'type:support']\n",
      "['cla: yes', 'comp:core', 'ready to pull', 'size:L']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.4', 'comp:tf.function', 'stat:awaiting response', 'type:support']\n",
      "['TF 2.4', 'comp:lite', 'type:build/install']\n",
      "['cla: yes', 'size:M']\n",
      "['TF 2.4', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.4', 'comp:lite', 'type:performance']\n",
      "['TF 2.4', 'comp:keras', 'type:bug']\n",
      "['cla: yes', 'comp:micro', 'size:XS']\n",
      "['stalled', 'stat:awaiting response', 'type:support']\n",
      "['comp:lite', 'type:feature']\n",
      "['TF 2.4', 'comp:keras', 'type:bug']\n",
      "['TF 2.4', 'comp:ops', 'stalled', 'stat:awaiting response', 'type:support']\n",
      "['ModelOptimizationToolkit']\n",
      "['TF 2.3', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['TF 2.4', 'comp:xla', 'type:bug']\n",
      "['TF 2.4', 'TFLiteConverter', 'comp:lite', 'type:bug']\n",
      "['TF 2.5', 'subtype:macOS', 'type:build/install']\n",
      "['TF 2.4', 'comp:dist-strat', 'type:support']\n",
      "['cla: yes', 'size:S']\n",
      "['comp:dist-strat', 'type:performance']\n",
      "['TF 2.4', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.4', 'comp:keras', 'comp:tf.function', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.4', 'stalled', 'stat:awaiting response', 'subtype:windows', 'type:build/install']\n",
      "['TF 2.4', 'comp:keras', 'regression issue', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:apis', 'comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.5', 'comp:lite', 'type:bug']\n",
      "['TF 2.4', 'comp:xla', 'type:bug']\n",
      "['TF 2.4', 'type:bug']\n",
      "['TF 2.4', 'comp:ops', 'type:bug']\n",
      "['TF 2.4', 'stalled', 'stat:awaiting response', 'type:others']\n",
      "['stalled', 'stat:awaiting response']\n",
      "['comp:lite', 'stalled', 'stat:awaiting response', 'type:support']\n",
      "['TF 2.3', 'TF 2.4', 'comp:gpu', 'comp:keras', 'type:bug']\n",
      "['TF 2.4', 'comp:apis', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TF 2.0', 'comp:ops', 'stalled', 'stat:awaiting response', 'type:others']\n",
      "['TF 2.4', 'comp:data', 'stat:awaiting tensorflower', 'type:performance']\n",
      "['TF 2.4', 'comp:xla', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['cla: yes', 'comp:core', 'size:M', 'stat:awaiting response']\n",
      "['TFLiteGpuDelegate', 'comp:lite', 'type:bug']\n",
      "['TF 2.4', 'comp:lite', 'subtype:windows', 'type:build/install']\n",
      "['cla: yes', 'comp:keras', 'size:M']\n",
      "['TF 2.2', 'comp:apis', 'stat:awaiting response', 'type:support']\n",
      "['TF 2.4', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.4', 'comp:ops', 'type:bug']\n",
      "['comp:micro', 'type:bug']\n",
      "['TF 2.5', 'comp:keras', 'type:support']\n",
      "['TF 2.2', 'stalled', 'stat:awaiting response', 'subtype:windows', 'type:build/install']\n",
      "['TF 1.14', 'comp:data', 'type:bug']\n",
      "['TF 2.4', 'stat:awaiting response', 'subtype:windows', 'type:build/install']\n",
      "['TF 2.3', 'stat:awaiting tensorflower', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['comp:lite', 'stat:awaiting response', 'type:feature']\n",
      "['stat:awaiting tensorflower', 'subtype:macOS', 'type:build/install']\n",
      "['TF 1.12', 'stat:awaiting response', 'type:others']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:others']\n",
      "['TF 2.4', 'comp:keras', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TF 2.0', 'comp:keras', 'stalled', 'stat:awaiting response', 'type:build/install']\n",
      "['cla: yes', 'comp:keras', 'prtype:bugfix', 'size:S']\n",
      "['comp:lite', 'type:build/install']\n",
      "['TF 2.5', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['TF 2.4', 'comp:keras', 'regression issue', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.4', 'comp:keras', 'stat:awaiting response', 'type:support']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:docs-feature']\n",
      "['TF 2.3', 'comp:lite', 'stalled', 'stat:awaiting response', 'type:performance']\n",
      "['TF 2.3', 'comp:lite', 'type:others']\n",
      "['cla: yes', 'comp:lite', 'comp:micro', 'prtype:bugfix', 'size:L']\n",
      "['awaiting review', 'cla: yes', 'size:XS']\n",
      "['TF 2.4', 'comp:lite', 'comp:ops']\n",
      "['TF 2.4', 'comp:keras', 'regression issue', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['awaiting review', 'cla: yes', 'comp:keras', 'prtype:bugfix', 'size:XS']\n",
      "['cla: yes', 'comp:keras', 'size:M']\n",
      "['TF 2.4', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.4', 'comp:dist-strat', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.4', 'comp:gpu', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['awaiting review', 'cla: yes', 'size:S']\n",
      "['TF 1.15', 'comp:apis', 'type:support']\n",
      "['cla: yes', 'size:L']\n",
      "['TF 2.4', 'comp:gpu', 'stat:awaiting tensorflower', 'type:build/install']\n",
      "['cla: yes', 'comp:core', 'size:S']\n",
      "['comp:lite', 'comp:micro', 'comp:micro:arm', 'type:bug']\n",
      "['comp:micro', 'comp:micro:arm', 'type:bug']\n",
      "['ModelOptimizationToolkit', 'TF 2.5', 'comp:lite', 'type:performance']\n",
      "['TF 2.4', 'comp:data', 'type:performance']\n",
      "['comp:gpu', 'stat:awaiting tensorflower', 'type:build/install', 'type:support']\n",
      "['TF 1.15', 'type:build/install']\n",
      "['cla: yes', 'comp:micro', 'comp:micro:espressif', 'size:S']\n",
      "['TF 2.5', 'comp:gpu', 'comp:tf.function', 'type:bug']\n",
      "['TF 2.4', 'comp:gpu', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['awaiting review', 'cla: yes', 'comp:core', 'size:M']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.4', 'comp:apis', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['type:build/install']\n",
      "['TF 2.4', 'comp:apis', 'type:support']\n",
      "['TF 2.4', 'stat:awaiting tensorflower', 'subtype:macOS', 'type:build/install']\n",
      "['cla: yes', 'size:S', 'stat:awaiting response']\n",
      "['TF 2.4', 'comp:apis', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.2', 'comp:apis', 'stalled', 'stat:awaiting response', 'type:bug']\n",
      "['TF 2.4', 'comp:keras', 'type:bug']\n",
      "['TF 2.4', 'comp:keras', 'type:bug']\n",
      "['TF 2.4', 'comp:data', 'stalled', 'stat:awaiting response', 'type:support']\n",
      "['TF 2.3', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:keras', 'type:bug']\n",
      "['TF 2.4', 'comp:gpu', 'comp:ops', 'type:bug']\n",
      "['TF 2.4', 'comp:dist-strat', 'stat:awaiting response', 'type:support']\n",
      "['awaiting review', 'cla: yes', 'comp:keras', 'prtype:bugfix', 'size:M']\n",
      "['comp:keras', 'type:feature']\n",
      "['TF 2.3', 'comp:keras', 'stalled', 'stat:awaiting response', 'type:bug']\n",
      "['comp:lite', 'comp:lite-xnnpack', 'subtype:windows', 'type:build/install']\n",
      "['subtype: ubuntu/linux', 'type:build/install']\n",
      "['TF 2.1', 'comp:apis', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.1', 'comp:apis', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['cla: yes', 'comp:lite', 'prtype:bugfix', 'ready to pull', 'size:M']\n",
      "['comp:lite', 'stalled', 'stat:awaiting response', 'type:bug']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.4', 'comp:micro', 'comp:micro:arm', 'type:support']\n",
      "['comp:lite', 'comp:lite-examples', 'type:bug']\n",
      "['comp:apis', 'stat:awaiting tensorflower', 'type:docs-feature']\n",
      "['comp:apis', 'type:support']\n",
      "['TF 2.5', 'stat:awaiting tensorflower', 'type:build/install']\n",
      "['TF 2.4', 'comp:gpu', 'type:bug']\n",
      "['awaiting review', 'cla: yes', 'size:S']\n",
      "['TF 2.3', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['comp:model', 'stalled', 'stat:awaiting response', 'type:support']\n",
      "['cla: no', 'size:S']\n",
      "['cla: yes', 'comp:core', 'size:XL']\n",
      "['comp:lite', 'type:build/install', 'type:others']\n",
      "['TF 2.4', 'comp:data', 'stat:awaiting tensorflower', 'type:performance']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.4', 'comp:gpu', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.4', 'comp:data', 'comp:keras', 'type:bug']\n",
      "['comp:lite', 'comp:lite-examples', 'type:docs-feature']\n",
      "['TF 2.4', 'comp:keras', 'type:bug']\n",
      "['TF 2.1', 'TF 2.4', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.1', 'comp:apis', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.4', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.4', 'comp:apis', 'type:bug']\n",
      "['TF 2.4', 'comp:tf.function', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['ModelOptimizationToolkit', 'TF 2.3', 'comp:lite', 'type:bug']\n",
      "['TF 2.4', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.1', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.4', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.1', 'TF 2.4', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.1', 'TF 2.4', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.4', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['ModelOptimizationToolkit', 'TF 2.1', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.1', 'TF 2.4', 'comp:ops', 'comp:tensorboard', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.4', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:ops', 'type:bug']\n",
      "['TF 2.5', 'comp:lite', 'type:support']\n",
      "['TF 2.4', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.4', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.1', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.4', 'comp:keras', 'type:bug']\n",
      "['awaiting review', 'cla: yes', 'comp:xla', 'size:M']\n",
      "['TF 2.4', 'comp:ops', 'type:bug']\n",
      "['TF 2.1', 'TF 2.4', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.1', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.4', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.1', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 1.15', 'comp:lite', 'type:bug']\n",
      "['comp:lite', 'comp:micro', 'stat:awaiting tensorflower', 'type:support']\n",
      "['cla: yes', 'comp:lite', 'comp:micro', 'prtype:bugfix', 'size:L']\n",
      "['cla: yes', 'comp:lite', 'size:M']\n",
      "['TF 2.4', 'comp:tpus', 'stat:awaiting response', 'type:bug']\n",
      "['TF 2.4', 'TFLiteGpuDelegate', 'comp:lite', 'type:bug']\n",
      "['cla: yes', 'ready to pull', 'size:L']\n",
      "['cla: yes', 'ready to pull', 'size:L']\n",
      "['TF 2.4', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['awaiting review', 'cla: yes', 'comp:micro', 'ready to pull', 'size:XS']\n",
      "['TF 2.4', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.4', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:keras', 'type:others']\n",
      "['TF 2.4', 'comp:core', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.4', 'comp:keras', 'type:support']\n",
      "['comp:lite', 'type:feature']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:lite', 'comp:micro', 'type:support']\n",
      "['stat:awaiting tensorflower', 'subtype:bazel', 'type:build/install']\n",
      "['cla: yes', 'comp:core', 'size:L']\n",
      "['TF 2.4', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['awaiting review', 'cla: yes', 'size:L']\n",
      "['TF 2.4', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:micro', 'comp:micro:arm', 'type:bug']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['comp:lite', 'subtype:windows', 'type:build/install']\n",
      "['TF 2.4', 'TFLiteConverter', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.4', 'TFLiteConverter', 'comp:lite', 'type:bug']\n",
      "['TF 2.4', 'comp:ops', 'comp:tf.function', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['ModelOptimizationToolkit']\n",
      "['cla: yes', 'ready to pull', 'size:M']\n",
      "['comp:apis', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['awaiting review', 'cla: yes', 'comp:gpu', 'size:S']\n",
      "['TF 2.4', 'comp:dist-strat', 'stat:awaiting tensorflower', 'type:performance']\n",
      "['ModelOptimizationToolkit', 'TF 2.3', 'TFLiteConverter', 'comp:lite', 'type:support']\n",
      "['TF 2.3', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['awaiting review', 'cla: yes', 'comp:mkl', 'size:M']\n",
      "['TF 2.4', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.2', 'comp:keras', 'type:bug']\n",
      "['cla: yes', 'comp:micro', 'size:M', 'stat:awaiting response']\n",
      "['comp:apis', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:apis', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:apis', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.4', 'stat:awaiting tensorflower', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['awaiting review', 'cla: yes', 'comp:keras', 'size:S']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['TF 2.5', 'comp:keras', 'type:bug']\n",
      "['ModelOptimizationToolkit', 'TF 2.5', 'comp:lite', 'type:bug']\n",
      "['comp:lite', 'type:support']\n",
      "['cla: yes', 'comp:core', 'prtype:bugfix', 'ready to pull', 'size:XS']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.4', 'comp:apis', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.4', 'comp:dist-strat', 'stat:awaiting response', 'type:support']\n",
      "['TF 2.4', 'comp:dist-strat', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['cla: yes', 'comp:core', 'size:M']\n",
      "['awaiting review', 'cla: yes', 'prtype:bugfix', 'size:S']\n",
      "['ModelOptimizationToolkit', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.3', 'subtype:windows', 'type:build/install']\n",
      "['comp:lite', 'type:others']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.4', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['awaiting review', 'cla: yes', 'size:XS']\n",
      "['awaiting review', 'cla: yes', 'comp:core', 'size:L']\n",
      "['comp:micro', 'type:bug']\n",
      "['TF 2.2', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:lite', 'comp:lite-examples', 'type:docs-bug']\n",
      "['cla: yes', 'comp:core', 'prtype:bugfix', 'ready to pull', 'size:S']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.4', 'comp:keras', 'type:bug']\n",
      "['comp:apis', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.4', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.4', 'comp:ops', 'stat:awaiting tensorflower', 'type:performance']\n",
      "['TF 2.3', 'comp:data', 'stat:awaiting tensorflower', 'type:performance']\n",
      "['TF 2.1', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.1', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.1', 'comp:ops', 'type:bug']\n",
      "['awaiting review', 'cla: yes', 'size:XS']\n",
      "['TF 2.1', 'comp:keras', 'type:bug']\n",
      "['comp:micro', 'stat:awaiting tensorflower', 'type:support']\n",
      "['comp:micro', 'comp:micro:espressif', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.4', 'subtype:windows', 'type:build/install']\n",
      "['ModelOptimizationToolkit', 'type:bug']\n",
      "['comp:core', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.1', 'comp:apis', 'stalled', 'stat:awaiting response', 'type:bug']\n",
      "['TF 2.4', 'comp:keras', 'regression issue', 'stat:awaiting tensorflower', 'type:performance']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:micro', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TF 2.4', 'comp:gpu', 'type:build/install']\n",
      "['TF 2.4', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.4', 'comp:tpus', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TF 2.3', 'comp:keras', 'java', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.4', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['stat:awaiting tensorflower', 'subtype:bazel', 'type:build/install']\n",
      "['TF 2.4', 'comp:gpu', 'comp:keras', 'type:bug']\n",
      "['TF 2.4', 'subtype:macOS', 'type:build/install']\n",
      "['TF 2.4', 'comp:keras', 'stat:awaiting response', 'type:bug']\n",
      "['TF 2.3', 'comp:apis', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.4', 'comp:gpu', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TFLiteConverter']\n",
      "['type:build/install', 'type:feature']\n",
      "['TF 2.4', 'comp:autograph', 'stat:awaiting tensorflower', 'type:support']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['TF 2.4', 'comp:gpu', 'type:bug']\n",
      "['TF 2.3', 'stat:awaiting tensorflower', 'subtype:windows', 'type:build/install']\n",
      "['TF 2.4', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:keras', 'type:others']\n",
      "['TF 2.4', 'comp:keras', 'type:bug']\n",
      "['comp:tensorboard', 'type:bug']\n",
      "['TF 2.4', 'comp:lite', 'type:bug']\n",
      "['TF 2.4', 'subtype:macOS', 'type:build/install']\n",
      "['TF 2.2', 'comp:lite', 'type:bug']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['cla: yes', 'comp:mkl', 'ready to pull', 'size:M']\n",
      "['TF 2.3', 'comp:micro', 'type:bug']\n",
      "['TF 2.4', 'comp:apis', 'type:support']\n",
      "['cla: yes', 'comp:core', 'size:S']\n",
      "['cla: yes', 'ready to pull', 'size:L']\n",
      "['awaiting review', 'cla: yes', 'size:S']\n",
      "['TF 2.1', 'comp:apis', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:apis', 'type:bug']\n",
      "['TF 2.4', 'comp:autograph', 'type:support']\n",
      "['TF 2.4', 'comp:mkl', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.4', 'comp:keras', 'type:bug']\n",
      "['TF 2.4', 'comp:tpus', 'type:support']\n",
      "['TF 2.4', 'comp:ops', 'type:performance']\n",
      "['awaiting review', 'cla: yes', 'size:L']\n",
      "['TF 2.4', 'regression issue', 'type:performance']\n",
      "['comp:lite', 'subtype:macOS', 'type:build/install']\n",
      "['TF 2.4', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['API review', 'cla: yes', 'size:M', 'stat:awaiting response']\n",
      "['comp:lite', 'comp:micro', 'type:bug']\n",
      "['comp:gpu', 'type:build/install']\n",
      "['Fixed in Nightly', 'TF 2.3', 'comp:lite', 'type:bug']\n",
      "['TFLiteGpuDelegate', 'comp:lite']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.3', 'TFLiteConverter', 'type:bug']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.4', 'comp:gpu', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:lite', 'type:performance']\n",
      "['comp:autograph', 'type:others']\n",
      "['type:others']\n",
      "['TF 2.4', 'comp:gpu', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.4', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.5', 'comp:gpu', 'comp:lite', 'type:bug']\n",
      "['TF 2.4', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.2', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['cla: yes', 'comp:mkl', 'size:XL']\n",
      "['TF 2.4', 'comp:apis', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.4', 'comp:lite', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['TF 2.5', 'comp:gpu:tensorrt', 'regression issue', 'type:bug']\n",
      "['cla: yes', 'comp:core', 'ready to pull', 'size:M']\n",
      "['TF 2.4', 'comp:ops', 'comp:xla', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['cla: yes', 'comp:core', 'size:S']\n",
      "['TF 2.5', 'comp:lite', 'type:bug']\n",
      "['awaiting review', 'cla: yes', 'comp:keras', 'size:XS']\n",
      "['TF 2.4', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.4', 'subtype:windows', 'type:build/install']\n",
      "['TF 2.5', 'comp:keras', 'type:support']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.4', 'comp:gpu', 'stat:awaiting tensorflower', 'type:build/install']\n",
      "['comp:keras', 'type:docs-bug']\n",
      "['TF 2.4', 'comp:keras', 'type:bug']\n",
      "['awaiting review', 'cla: yes', 'comp:core', 'size:M']\n",
      "['TF 2.5', 'comp:ops', 'regression issue', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.4', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.4', 'comp:keras', 'type:support']\n",
      "['TF 2.4', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:lite', 'comp:micro', 'stat:awaiting response', 'type:support']\n",
      "['TF 2.4', 'comp:lite', 'type:bug']\n",
      "['TF 2.4', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.4', 'TFLiteConverter', 'comp:lite', 'type:feature']\n",
      "['awaiting review', 'cla: yes', 'comp:core', 'size:M']\n",
      "['TF 2.4', 'comp:dist-strat', 'type:support']\n",
      "['TF 2.4', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:lite', 'comp:micro', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.4', 'comp:keras', 'type:bug']\n",
      "['comp:lite', 'type:support']\n",
      "['subtype:macOS', 'type:build/install']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.4', 'subtype:windows', 'type:build/install']\n",
      "['TF 2.4', 'comp:apis', 'type:bug']\n",
      "['TF 2.4', 'comp:autograph', 'type:bug']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['awaiting review', 'cla: yes', 'size:M']\n",
      "['TF 2.4', 'TFLiteConverter', 'type:support']\n",
      "['TF 2.4', 'TFLiteConverter', 'comp:lite', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['awaiting review', 'cla: yes', 'comp:core', 'size:M']\n",
      "['comp:lite', 'type:feature']\n",
      "['TF 2.5', 'subtype:windows', 'type:build/install']\n",
      "['TF 2.4', 'comp:eager', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['cla: yes', 'comp:lite', 'size:L']\n",
      "['TF 2.4', 'comp:ops', 'type:bug']\n",
      "['TF 2.5', 'comp:gpu', 'type:support']\n",
      "['comp:micro', 'comp:micro:espressif', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TF 2.4', 'comp:apis', 'type:support']\n",
      "['TF 2.4', 'comp:lite', 'stalled', 'stat:awaiting response', 'type:others']\n",
      "['comp:micro', 'type:others']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:micro', 'type:others']\n",
      "['TF 2.4', 'comp:keras', 'type:bug']\n",
      "['TF 2.4', 'comp:gpu:tensorrt', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.4', 'comp:tensorboard', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.4', 'comp:gpu', 'type:bug']\n",
      "['TF 2.4', 'comp:keras', 'type:support']\n",
      "['comp:micro', 'stat:awaiting tensorflower', 'type:support']\n",
      "['comp:data', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.2', 'comp:keras', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TF 2.5', 'comp:apis', 'type:support']\n",
      "['cla: yes', 'comp:micro', 'comp:micro:ceva', 'size:XL', 'stalled']\n",
      "['TF 2.3', 'comp:keras', 'type:support']\n",
      "['awaiting review', 'cla: yes', 'comp:lite', 'comp:micro', 'prtype:bugfix', 'size:L']\n",
      "['cla: yes', 'comp:lite', 'comp:micro', 'prtype:bugfix', 'size:L']\n",
      "['cla: yes', 'comp:lite', 'comp:micro', 'prtype:bugfix', 'size:M']\n",
      "['TF 2.4', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.4', 'comp:keras', 'stat:awaiting tensorflower', 'type:support']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['awaiting review', 'cla: yes', 'comp:keras', 'prtype:bugfix', 'size:M']\n",
      "['TF 2.3', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['cla: yes', 'comp:ops', 'prtype:bugfix', 'size:M']\n",
      "['TF 2.4', 'comp:autograph', 'type:support']\n",
      "['TF 2.4', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['cla: yes', 'comp:core', 'size:L']\n",
      "['comp:apis', 'comp:gpu', 'type:performance']\n",
      "['awaiting review', 'cla: yes', 'comp:core', 'size:L']\n",
      "['cla: yes', 'size:M']\n",
      "['TF 2.4', 'comp:gpu', 'stat:awaiting tensorflower', 'type:performance']\n",
      "['TF 2.3', 'comp:lite', 'type:bug']\n",
      "['comp:lite', 'comp:micro', 'type:bug']\n",
      "['comp:lite', 'comp:micro', 'type:bug']\n",
      "['TF 2.4', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.4', 'comp:lite', 'type:bug']\n",
      "['cla: yes', 'ready to pull', 'size:XS']\n",
      "['TF 2.4', 'comp:lite', 'type:bug']\n",
      "['TF 2.4', 'comp:lite', 'type:performance']\n",
      "['comp:data', 'type:feature']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.3', 'stat:awaiting tensorflower', 'subtype:windows', 'type:build/install']\n",
      "['awaiting review', 'cla: yes', 'size:M']\n",
      "['comp:runtime', 'type:feature']\n",
      "['TF 2.4', 'comp:keras', 'type:performance']\n",
      "['TF 2.5', 'comp:lite', 'type:bug']\n",
      "['comp:lite', 'type:feature']\n",
      "['TF 2.4', 'TFLiteConverter', 'comp:lite', 'type:support']\n",
      "['awaiting review', 'cla: yes', 'comp:keras', 'size:M']\n",
      "['comp:gpu', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.4', 'comp:keras', 'type:bug']\n",
      "['TF 2.3', 'comp:data', 'type:performance']\n",
      "['comp:lite', 'comp:micro', 'comp:micro:espressif', 'type:bug']\n",
      "['TFLiteConverter', 'comp:lite', 'type:bug']\n",
      "['TF 2.2', 'TF 2.3', 'comp:lite', 'type:build/install']\n",
      "['TF 2.5', 'comp:gpu', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:eager', 'type:support']\n",
      "['TF 2.5', 'comp:lite', 'type:support']\n",
      "['comp:keras', 'type:support']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:gpu', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:gpu', 'type:build/install', 'type:feature']\n",
      "['TF 2.4', 'TFLiteConverter', 'comp:lite', 'type:bug']\n",
      "['ModelOptimizationToolkit', 'TF 2.3', 'TFLiteConverter', 'comp:keras', 'comp:lite', 'type:support']\n",
      "['comp:core', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['cla: yes', 'ready to pull', 'size:S']\n",
      "['TF 2.4', 'stat:contributions welcome', 'subtype:macOS', 'type:build/install']\n",
      "['TF 2.4', 'comp:gpu', 'type:performance']\n",
      "['TF 2.4', 'comp:ops', 'type:bug']\n",
      "['comp:lite', 'type:support']\n",
      "['comp:micro', 'type:others']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.4', 'comp:lite', 'subtype:macOS', 'type:build/install']\n",
      "['comp:cloud', 'comp:dist-strat', 'comp:tpus', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:lite', 'type:performance']\n",
      "['TF 2.4', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'subtype:windows', 'type:build/install']\n",
      "['TF 2.4', 'comp:keras', 'stat:awaiting tensorflower', 'type:support']\n",
      "['comp:lite', 'type:support']\n",
      "['comp:lite', 'comp:micro', 'type:support']\n",
      "['cla: yes', 'comp:lite', 'comp:micro', 'prtype:bugfix', 'size:L']\n",
      "['TF 2.4', 'subtype:windows', 'type:build/install']\n",
      "['TF 2.4', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.4', 'stat:awaiting tensorflower', 'subtype:windows', 'type:build/install']\n",
      "['TF 2.4', 'comp:keras', 'stat:awaiting tensorflower', 'type:performance']\n",
      "['TF 2.4', 'comp:keras', 'regression issue', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.4', 'comp:lite', 'type:bug']\n",
      "['stat:awaiting tensorflower', 'type:docs-feature']\n",
      "['comp:data', 'type:performance']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.4', 'comp:lite', 'type:support']\n",
      "['TF 2.4', 'comp:lite', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.4', 'comp:gpu', 'comp:tensorboard', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.4', 'comp:apis', 'comp:model', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:ops', 'type:feature']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['type:bug']\n",
      "['TF 2.4', 'comp:apis', 'type:support']\n",
      "['TF 2.4', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.2', 'comp:micro', 'type:bug']\n",
      "['TF 2.3', 'comp:lite', 'comp:micro', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.4', 'comp:ops', 'regression issue', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.4', 'comp:dist-strat', 'regression issue', 'type:support']\n",
      "['awaiting review', 'cla: yes', 'comp:mkl', 'size:M']\n",
      "['TF 2.4', 'comp:keras', 'regression issue', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.4', 'WIP', 'comp:lite', 'type:support']\n",
      "['cla: yes', 'size:S', 'stalled', 'stat:awaiting response']\n",
      "['TF 2.4', 'stat:awaiting tensorflower', 'type:build/install', 'type:docs-feature']\n",
      "['TF 2.5', 'comp:keras', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TF 2.4', 'comp:keras', 'comp:model', 'regression issue', 'stalled', 'stat:awaiting response', 'type:performance']\n",
      "['TF 2.5', 'comp:lite', 'type:bug']\n",
      "['comp:keras', 'stat:awaiting response', 'type:docs-bug']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['awaiting review', 'cla: yes', 'comp:micro', 'size:S']\n",
      "['TF 2.5', 'comp:keras', 'type:support']\n",
      "['TF 2.3', 'comp:cloud', 'comp:tpus', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['awaiting review', 'cla: yes', 'comp:mkl', 'size:M']\n",
      "['TF 2.3', 'comp:lite', 'comp:micro:arm', 'type:bug']\n",
      "['awaiting review', 'cla: yes', 'comp:mkl', 'size:L']\n",
      "['comp:mkl', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.4', 'comp:tpus', 'type:bug']\n",
      "['TF 2.4', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.4', 'comp:lite', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.5', 'comp:keras', 'stat:awaiting tensorflower', 'type:support']\n",
      "['cla: yes', 'comp:grappler', 'size:M', 'stat:awaiting tensorflower']\n",
      "['comp:micro', 'type:bug']\n",
      "['TF 2.4', 'comp:ops', 'regression issue', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['awaiting review', 'cla: yes', 'comp:mkl', 'size:L']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:feature', 'type:others']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.4', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.4', 'comp:keras', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TF 2.4', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.4', 'comp:runtime', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:lite', 'type:support']\n",
      "['TF 2.4', 'comp:gpu', 'regression issue', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['cla: yes', 'size:XL']\n",
      "['TF 2.4', 'comp:keras', 'type:bug']\n",
      "['comp:apis', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['TF 2.1', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.1', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:lite', 'comp:micro', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['cla: yes', 'comp:core', 'size:M']\n",
      "['TF 2.3', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:lite', 'comp:micro', 'type:bug']\n",
      "['TF 2.4', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.4', 'comp:gpu', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:keras', 'stat:awaiting response', 'type:bug', 'type:docs-feature']\n",
      "['stat:awaiting tensorflower', 'type:build/install', 'type:docs-feature']\n",
      "['comp:micro', 'comp:micro:arm', 'type:bug']\n",
      "['comp:lite', 'comp:micro', 'type:bug']\n",
      "['TF 2.4', 'comp:lite', 'comp:micro', 'stat:awaiting tensorflower', 'type:build/install']\n",
      "['awaiting review', 'cla: yes', 'comp:keras', 'size:XS']\n",
      "['TF 2.3', 'comp:eager', 'comp:gpu', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:lite', 'comp:micro', 'type:support']\n",
      "['TF 2.4', 'comp:gpu', 'regression issue', 'type:performance']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.4', 'comp:gpu', 'type:bug']\n",
      "['TF 2.4', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.4', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.4', 'comp:ops', 'type:bug']\n",
      "['TF 2.4', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:lite', 'comp:micro', 'type:bug']\n",
      "['comp:lite', 'type:support']\n",
      "['TF 2.3', 'comp:gpu', 'regression issue', 'type:bug']\n",
      "['cla: yes', 'comp:micro', 'size:L']\n",
      "['awaiting review', 'cla: yes', 'comp:keras', 'size:S']\n",
      "['cla: yes', 'comp:micro', 'comp:micro:ceva', 'size:XL', 'stat:awaiting response']\n",
      "['awaiting review', 'cla: yes', 'comp:keras', 'size:XS']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:docs-feature']\n",
      "['TF 2.3', 'subtype:windows', 'type:build/install']\n",
      "['comp:lite', 'type:others']\n",
      "['comp:lite', 'type:build/install']\n",
      "['stat:awaiting response', 'type:docs-bug']\n",
      "['comp:lite', 'comp:micro', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['cla: yes', 'comp:gpu:tensorrt', 'size:S', 'stalled', 'stat:awaiting response']\n",
      "['TF 2.3', 'TF 2.4', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:performance']\n",
      "['TF 2.4', 'comp:gpu', 'type:bug']\n",
      "['TF 2.4', 'comp:keras', 'type:bug']\n",
      "['TF 2.3', 'comp:apis', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['TF 2.3', 'comp:keras', 'type:bug']\n",
      "['TF 2.4', 'comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.3', 'comp:keras', 'regression issue', 'type:bug']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:lite', 'type:bug']\n",
      "['TF 2.3', 'comp:gpu', 'type:bug']\n",
      "['comp:ops', 'type:bug']\n",
      "['TF 2.4', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.0', 'comp:gpu', 'type:support']\n",
      "['cla: yes', 'comp:mkl', 'size:M', 'stat:awaiting tensorflower']\n",
      "['cla: yes', 'comp:mkl', 'size:M', 'stat:awaiting tensorflower']\n",
      "['TF 2.3', 'comp:lite', 'type:bug']\n",
      "['comp:gpu:tensorrt', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['cla: yes', 'comp:mkl', 'size:M', 'stat:awaiting tensorflower']\n",
      "['cla: yes', 'comp:mkl', 'size:M', 'stat:awaiting tensorflower']\n",
      "['comp:apis', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.3', 'comp:lite', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.4', 'comp:lite', 'type:bug']\n",
      "['comp:apis', 'comp:tfdbg', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.3', 'TFLiteConverter', 'comp:lite', 'type:bug']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:lite', 'comp:micro', 'type:bug']\n",
      "['Fixed in Nightly', 'TF 2.3', 'comp:apis', 'type:bug']\n",
      "['TF 2.3', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['TF 2.3', 'comp:lite', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.5', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:lite', 'comp:micro', 'type:bug']\n",
      "['TF 2.2', 'comp:apis', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:xla', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['awaiting review', 'cla: yes', 'comp:core', 'size:L']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.3', 'stat:awaiting tensorflower', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.3', 'TFLiteConverter', 'type:bug']\n",
      "['TF 2.3', 'comp:keras', 'type:bug']\n",
      "['TF 2.3', 'comp:ops', 'type:bug']\n",
      "['TF 2.3', 'comp:lite', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:keras', 'type:bug']\n",
      "['TFLiteConverter', 'comp:lite', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:lite', 'comp:micro', 'type:support']\n",
      "['cla: yes', 'comp:lite', 'size:L', 'stat:awaiting response']\n",
      "['cla: yes', 'ready to pull', 'size:M']\n",
      "['awaiting review', 'cla: yes', 'comp:keras', 'size:XS']\n",
      "['TF 2.3', 'TFLiteConverter', 'comp:lite', 'type:bug']\n",
      "['TF 2.5', 'TFLiteConverter', 'comp:lite', 'type:bug']\n",
      "['TF 2.3', 'comp:data', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:keras', 'type:support']\n",
      "['TF 2.5', 'comp:ops', 'regression issue', 'stat:awaiting tensorflower', 'type:performance']\n",
      "['TF 2.3', 'comp:lite', 'type:bug']\n",
      "['TF 2.3', 'comp:lite', 'type:bug']\n",
      "['comp:lite', 'comp:micro', 'type:bug']\n",
      "['TF 2.4', 'comp:lite', 'type:support']\n",
      "['TF 2.3', 'TFLiteConverter', 'comp:lite', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['awaiting review', 'cla: yes', 'size:S']\n",
      "['TF 2.4', 'comp:lite', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.5', 'comp:gpu', 'type:bug']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:keras', 'type:support']\n",
      "['TF 2.4', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['comp:lite', 'comp:micro', 'stat:awaiting tensorflower', 'type:support']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:gpu', 'stat:awaiting tensorflower', 'type:build/install']\n",
      "['TF 2.4', 'comp:gpu', 'type:bug']\n",
      "['TF 2.3', 'comp:micro', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.4', 'stat:awaiting tensorflower', 'subtype:bazel', 'type:build/install']\n",
      "['TF 2.4', 'comp:data', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.5', 'comp:gpu', 'type:feature']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:performance']\n",
      "['TF 2.3', 'stat:awaiting tensorflower', 'subtype: ubuntu/linux', 'subtype:bazel', 'type:build/install']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.4', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:lite', 'comp:micro', 'type:support']\n",
      "['cla: yes', 'comp:micro', 'size:S', 'stat:awaiting response']\n",
      "['comp:lite', 'type:feature']\n",
      "['cla: yes', 'comp:grappler', 'ready to pull', 'size:S']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:micro', 'comp:micro:espressif', 'type:bug']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:support']\n",
      "['cla: yes', 'size:XS', 'stalled', 'stat:awaiting response']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['subtype:windows', 'type:build/install']\n",
      "['TF 2.1', 'comp:tpus', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TF 2.3', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.4', 'comp:gpu', 'regression issue', 'type:bug']\n",
      "['comp:lite', 'comp:micro', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:lite', 'type:build/install', 'type:support']\n",
      "['type:build/install', 'type:feature']\n",
      "['cla: yes', 'comp:lite', 'comp:micro', 'comp:micro:arm', 'ready to pull', 'size:L']\n",
      "['TF 2.4', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:lite', 'stat:awaiting response', 'type:docs-feature']\n",
      "['TF 2.3', 'subtype:macOS', 'type:build/install']\n",
      "['awaiting review', 'cla: yes', 'comp:mkl', 'size:L']\n",
      "['TF 2.5', 'comp:data', 'comp:keras', 'stat:awaiting tensorflower', 'type:performance']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['stat:awaiting tensorflower', 'type:docs-feature']\n",
      "['cla: yes', 'comp:mkl', 'size:L', 'stat:awaiting tensorflower']\n",
      "['awaiting review', 'cla: yes', 'comp:mkl', 'size:L']\n",
      "['comp:lite', 'type:performance']\n",
      "['TF 2.3', 'comp:autograph', 'comp:core', 'type:bug']\n",
      "['comp:lite', 'type:feature']\n",
      "['cla: yes', 'comp:lite', 'comp:micro', 'size:S', 'stat:awaiting tensorflower']\n",
      "['comp:micro', 'type:bug']\n",
      "['awaiting review', 'cla: yes', 'comp:core', 'size:L']\n",
      "['TF 2.3', 'comp:gpu', 'comp:keras', 'stat:awaiting tensorflower', 'type:performance']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:lite', 'comp:micro:espressif', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:lite', 'stat:awaiting tensorflower', 'type:support']\n",
      "['cla: yes', 'comp:data', 'size:L', 'stat:awaiting response']\n",
      "['TF 2.2', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:gpu:tensorrt', 'stat:awaiting tensorflower', 'subtype:bazel', 'subtype:windows', 'type:build/install']\n",
      "['TF 2.4', 'comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.4', 'comp:gpu', 'type:support']\n",
      "['TF 2.3', 'comp:micro', 'type:build/install']\n",
      "['stat:awaiting tensorflower', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['TF 2.3', 'comp:apis', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.4', 'comp:dist-strat', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:xla', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'subtype:windows', 'type:build/install']\n",
      "['comp:lite', 'type:bug']\n",
      "['TF 2.4', 'stat:awaiting tensorflower', 'subtype:bazel', 'type:build/install']\n",
      "['TF 2.3', 'comp:gpu', 'stat:awaiting tensorflower', 'type:build/install']\n",
      "['TF 2.2', 'comp:lite', 'type:bug']\n",
      "['comp:data', 'type:feature']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.3', 'comp:runtime', 'type:bug']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:performance']\n",
      "['cla: yes', 'size:L', 'stalled', 'stat:awaiting response']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.4', 'comp:autograph', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'stat:awaiting tensorflower', 'subtype:windows', 'type:build/install']\n",
      "['TF 2.2', 'comp:lite', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['awaiting review', 'cla: yes', 'comp:mkl', 'size:M']\n",
      "['comp:lite', 'type:build/install']\n",
      "['TF 2.3', 'TFLiteConverter', 'comp:lite', 'type:support']\n",
      "['TF 2.5', 'comp:gpu', 'stat:awaiting tensorflower', 'type:performance']\n",
      "['TF 2.3', 'comp:data', 'type:feature']\n",
      "['awaiting review', 'cla: yes', 'comp:mkl', 'size:M']\n",
      "['awaiting review', 'cla: yes', 'comp:mkl', 'size:XL']\n",
      "['TF 2.2', 'TFLiteConverter', 'comp:lite', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TF 2.4', 'comp:keras', 'type:performance']\n",
      "['TF 2.5', 'subtype:windows', 'type:build/install']\n",
      "['comp:lite', 'comp:micro', 'stat:awaiting tensorflower', 'type:others']\n",
      "['TF 2.3', 'comp:micro', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.4', 'comp:xla', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:lite', 'type:performance']\n",
      "['comp:lite', 'type:support']\n",
      "['TF 2.3', 'comp:apis', 'comp:tfdbg', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.3', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:xla', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.3', 'comp:data', 'stat:awaiting tensorflower', 'type:performance']\n",
      "['TFLiteConverter', 'comp:lite', 'type:support']\n",
      "['TF 2.2', 'comp:ops', 'type:bug']\n",
      "['TF 2.2', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:lite', 'comp:micro', 'comp:micro:arm', 'type:feature']\n",
      "['comp:lite', 'type:others']\n",
      "['comp:lite', 'type:support']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:docs-feature']\n",
      "['TFLiteConverter', 'comp:lite', 'type:support']\n",
      "['TF 2.3', 'comp:lite', 'type:bug']\n",
      "['TF 2.0', 'type:build/install']\n",
      "['comp:data', 'type:feature']\n",
      "['TF 2.3', 'comp:lite', 'type:bug']\n",
      "['TF 2.3', 'comp:dist-strat', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.5', 'TFLiteConverter', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:lite', 'comp:micro', 'comp:micro:espressif', 'type:feature']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.3', 'comp:apis', 'stat:awaiting tensorflower', 'type:performance']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['cla: yes', 'comp:keras', 'prtype:bugfix', 'size:XS', 'stat:awaiting tensorflower']\n",
      "['TF 2.2', 'stat:awaiting tensorflower', 'subtype: ubuntu/linux', 'subtype:bazel', 'type:build/install']\n",
      "['TF 2.3', 'comp:ops', 'stat:awaiting tensorflower', 'type:docs-feature', 'type:support']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:performance']\n",
      "['TF 2.3', 'comp:apis', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['type:build/install', 'type:feature']\n",
      "['cla: yes', 'comp:keras', 'prtype:bugfix', 'size:XS', 'stat:awaiting tensorflower']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:lite', 'comp:micro', 'type:others']\n",
      "['TF 2.3', 'comp:data', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:lite', 'type:bug']\n",
      "['comp:runtime', 'type:performance']\n",
      "['TF 2.3', 'comp:xla', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:core', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TFLiteConverter', 'comp:lite', 'type:bug']\n",
      "['comp:keras', 'type:feature']\n",
      "['TF 2.3', 'comp:gpu', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['awaiting review', 'cla: yes', 'comp:core', 'size:L']\n",
      "['comp:dist-strat', 'type:performance']\n",
      "['TF 2.4', 'comp:gpu', 'subtype:windows', 'type:build/install']\n",
      "['TF 2.3', 'comp:dist-strat', 'stat:awaiting tensorflower', 'type:performance']\n",
      "['comp:lite', 'comp:micro', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TF 2.4', 'comp:data', 'stat:awaiting tensorflower', 'type:performance']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['cla: yes', 'size:XL', 'stat:awaiting tensorflower']\n",
      "['comp:core', 'type:feature']\n",
      "['TF 2.4', 'comp:apis', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.1', 'comp:gpu', 'type:bug']\n",
      "['comp:micro', 'comp:micro:arm', 'type:support']\n",
      "['TF 2.4', 'comp:core', 'stat:awaiting tensorflower', 'type:build/install']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'subtype:windows', 'type:build/install']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.4', 'comp:keras', 'type:bug']\n",
      "['TFLiteConverter', 'type:bug']\n",
      "['comp:lite', 'type:feature']\n",
      "['TF 2.3', 'comp:lite', 'stat:awaiting tensorflower', 'type:build/install']\n",
      "['comp:lite', 'type:feature']\n",
      "['TF 2.2', 'stat:awaiting tensorflower', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['cla: yes', 'comp:lite', 'comp:micro', 'prtype:bugfix', 'size:XL']\n",
      "['cla: yes', 'comp:lite', 'comp:micro', 'size:S']\n",
      "['TF 2.3', 'comp:data', 'type:bug']\n",
      "['TF 2.3', 'comp:data', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:lite', 'comp:micro', 'type:bug']\n",
      "['TF 2.3', 'comp:data', 'type:feature']\n",
      "['TF 2.4', 'comp:tensorboard', 'regression issue', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:ops', 'stat:awaiting tensorflower', 'type:performance']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:lite', 'comp:micro', 'stat:awaiting tensorflower', 'type:support']\n",
      "['awaiting review', 'cla: yes', 'comp:core', 'size:XS']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'stat:awaiting tensorflower', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['TF 2.3', 'comp:apis', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:dist-strat', 'type:bug']\n",
      "['TF 2.3', 'TFLiteConverter', 'comp:lite', 'type:support']\n",
      "['TF 2.2', 'comp:gpu', 'comp:keras', 'stat:awaiting tensorflower', 'type:support']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:build/install']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:tpus', 'type:bug']\n",
      "['TF 2.5', 'type:build/install', 'type:feature']\n",
      "['TF 2.3', 'comp:lite', 'type:support']\n",
      "['comp:lite', 'type:bug']\n",
      "['subtype:windows', 'type:build/install', 'type:feature']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:keras', 'stalled', 'stat:awaiting response', 'type:support']\n",
      "['TF 2.3', 'comp:xla', 'type:bug']\n",
      "['comp:lite', 'type:build/install']\n",
      "['TF 2.3', 'TFLiteConverter', 'comp:lite', 'type:support']\n",
      "['TF 2.3', 'comp:autograph', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:ops', 'type:support']\n",
      "['TF 2.3', 'comp:apis', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:support']\n",
      "['stat:awaiting tensorflower', 'subtype:windows', 'type:build/install']\n",
      "['comp:gpu', 'comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:data', 'type:feature']\n",
      "['TF 2.2', 'comp:model', 'type:support']\n",
      "['TF 2.3', 'comp:lite', 'type:bug']\n",
      "['TF 2.3', 'comp:keras', 'comp:xla', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:runtime', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TFLiteConverter', 'comp:lite', 'type:bug']\n",
      "['TF 2.3', 'comp:dist-strat', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['awaiting review', 'cla: yes', 'comp:core', 'size:L']\n",
      "['comp:lite', 'comp:micro', 'comp:micro:espressif', 'type:bug']\n",
      "['TF 2.4', 'comp:gpu', 'type:bug']\n",
      "['TF 2.3', 'comp:apis', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:lite', 'type:bug']\n",
      "['TF 2.3', 'comp:lite', 'type:support']\n",
      "['TF 2.3', 'comp:gpu', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:lite', 'comp:micro', 'type:feature']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:lite', 'type:bug']\n",
      "['cla: yes', 'comp:keras', 'prtype:bugfix', 'size:XS', 'stat:awaiting tensorflower']\n",
      "['TF 2.2', 'stat:awaiting tensorflower', 'subtype:macOS', 'type:build/install']\n",
      "['TF 2.4', 'comp:gpu', 'type:build/install']\n",
      "['comp:runtime', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:lite', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TF 2.3', 'comp:keras', 'type:bug']\n",
      "['cla: yes', 'size:XS', 'stat:awaiting response']\n",
      "['TF 2.3', 'comp:data', 'type:performance']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:performance']\n",
      "['comp:lite', 'type:support']\n",
      "['comp:gpu', 'type:support']\n",
      "['TF 2.3', 'subtype:bazel', 'type:build/install']\n",
      "['comp:lite', 'type:support']\n",
      "['comp:lite', 'type:support']\n",
      "['TF 2.3', 'comp:gpu', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:lite', 'type:bug']\n",
      "['comp:gpu', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:lite', 'type:bug']\n",
      "['TF 2.3', 'comp:gpu', 'stat:awaiting tensorflower', 'type:performance']\n",
      "['TF 2.3', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:gpu', 'comp:keras', 'stat:awaiting tensorflower', 'type:performance']\n",
      "['TF 2.3', 'comp:gpu', 'stat:awaiting tensorflower', 'type:performance']\n",
      "['TF 2.3', 'comp:lite', 'comp:micro', 'type:feature']\n",
      "['TF 2.3', 'comp:lite', 'comp:micro', 'type:support']\n",
      "['comp:lite', 'comp:micro', 'type:support']\n",
      "['TF 2.3', 'comp:gpu', 'stat:awaiting tensorflower', 'type:performance']\n",
      "['comp:lite', 'type:support']\n",
      "['TF 2.3', 'comp:gpu', 'type:bug', 'type:build/install']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.3', 'comp:lite', 'type:bug']\n",
      "['comp:gpu', 'type:feature']\n",
      "['comp:gpu', 'type:performance']\n",
      "['TF 2.3', 'comp:keras', 'comp:ops', 'type:bug']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['cla: yes', 'comp:keras', 'ready to pull', 'size:S']\n",
      "['comp:apis', 'type:feature']\n",
      "['comp:lite', 'type:docs-feature']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:lite', 'type:others']\n",
      "['comp:lite', 'type:bug']\n",
      "['TF 2.3', 'comp:keras', 'comp:ops', 'type:bug']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:gpu:tensorrt', 'comp:runtime', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.2', 'comp:lite', 'type:bug']\n",
      "['subtype: ubuntu/linux', 'type:build/install']\n",
      "['TF 2.3', 'TFLiteConverter', 'comp:lite', 'type:bug']\n",
      "['TF 2.3', 'comp:data', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.5', 'comp:lite', 'type:feature']\n",
      "['cla: yes', 'comp:lite', 'comp:micro', 'prtype:bugfix', 'size:M', 'stat:awaiting response']\n",
      "['TF 2.2', 'comp:ops', 'type:support']\n",
      "['comp:keras', 'stat:awaiting response', 'type:docs-feature']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:others']\n",
      "['TF 2.3', 'stat:awaiting tensorflower', 'subtype: raspberry pi', 'type:build/install']\n",
      "['TF 2.3', 'comp:lite', 'type:bug']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:apis', 'type:bug', 'type:docs-bug']\n",
      "['TF 2.2', 'comp:gpu', 'type:performance']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:dist-strat', 'type:support']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.3', 'comp:data', 'type:bug']\n",
      "['comp:lite', 'comp:micro', 'comp:micro:arm', 'type:feature']\n",
      "['TF 2.3', 'comp:lite', 'comp:runtime', 'type:build/install']\n",
      "['TF 2.3', 'comp:core', 'comp:dist-strat', 'comp:ops', 'type:bug']\n",
      "['comp:apis', 'type:support']\n",
      "['comp:lite', 'type:feature']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.3', 'comp:lite', 'type:bug']\n",
      "['type:docs-feature']\n",
      "['TF 2.3', 'comp:keras', 'type:bug']\n",
      "['awaiting review', 'cla: yes', 'comp:keras', 'size:XS']\n",
      "['TF 2.3', 'comp:keras', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:lite', 'comp:micro', 'type:bug']\n",
      "['TF 2.3', 'comp:gpu', 'type:bug']\n",
      "['comp:data', 'type:performance']\n",
      "['TF 2.3', 'comp:keras', 'comp:ops', 'type:bug']\n",
      "['cla: yes', 'comp:gpu:tensorrt', 'size:M', 'stat:awaiting response']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['cla: yes', 'comp:keras', 'ready to pull', 'size:XS']\n",
      "['TF 2.3', 'stat:awaiting tensorflower', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['TF 2.3', 'comp:lite', 'type:bug']\n",
      "['comp:micro', 'comp:micro:arm', 'type:build/install']\n",
      "['cla: yes', 'size:S']\n",
      "['comp:gpu', 'type:build/install']\n",
      "['TF 2.3', 'TFLiteConverter', 'comp:lite', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TF 2.3', 'stat:awaiting tensorflower', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['comp:gpu', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.2', 'comp:core', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.2', 'comp:dist-strat', 'stat:awaiting tensorflower', 'type:others']\n",
      "['TF 2.3', 'comp:keras', 'type:bug']\n",
      "['TF 2.3', 'TFLiteConverter', 'comp:lite', 'type:bug']\n",
      "['comp:dist-strat', 'type:feature']\n",
      "['comp:micro', 'stat:awaiting tensorflower', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:autograph', 'type:feature']\n",
      "['subtype:windows', 'type:build/install']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.3', 'comp:tpus', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:lite', 'type:performance']\n",
      "['TF 2.3', 'comp:tpus', 'comp:xla', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.2', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:keras', 'type:bug']\n",
      "['TF 2.1', 'stat:awaiting tensorflower', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['TF 2.3', 'comp:data', 'type:bug']\n",
      "['TF 2.3', 'comp:gpu', 'type:bug']\n",
      "['TF 2.3', 'comp:keras', 'type:bug']\n",
      "['TF 2.3', 'comp:lite', 'type:bug']\n",
      "['TF 2.3', 'comp:keras', 'type:bug']\n",
      "['comp:data', 'comp:tpus', 'type:performance']\n",
      "['TF 2.3', 'comp:core', 'comp:keras', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TF 2.3', 'comp:xla', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:gpu', 'comp:gpu:tensorrt', 'stat:awaiting tensorflower', 'type:docs-feature']\n",
      "['comp:keras', 'type:feature']\n",
      "['TF 2.3', 'comp:ops', 'stat:awaiting tensorflower', 'type:performance']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:ops', 'type:docs-bug']\n",
      "['comp:lite', 'comp:micro', 'type:bug']\n",
      "['cla: yes', 'size:L', 'stalled', 'stat:awaiting response']\n",
      "['stat:awaiting tensorflower', 'subtype: ubuntu/linux', 'type:build/install', 'type:docs-feature']\n",
      "['TF 2.3', 'comp:keras', 'comp:tpus', 'type:bug']\n",
      "['type:bug']\n",
      "['TF 2.3', 'comp:xla', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['comp:keras', 'stat:contributions welcome', 'type:feature']\n",
      "['TF 2.3', 'comp:gpu', 'comp:xla', 'stat:awaiting tensorflower', 'type:support']\n",
      "['comp:lite', 'type:support']\n",
      "['TF 2.3', 'TFLiteConverter', 'comp:lite', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:lite', 'type:bug']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.3', 'comp:keras', 'type:support']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:keras', 'stat:contributions welcome', 'type:feature']\n",
      "['TF 2.3', 'comp:autograph', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:keras', 'type:performance']\n",
      "['comp:micro', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:lite', 'comp:micro', 'type:feature']\n",
      "['comp:lite', 'comp:micro', 'type:feature']\n",
      "['TF 2.3', 'comp:lite', 'comp:runtime', 'type:bug']\n",
      "['cla: yes', 'size:M', 'stat:awaiting tensorflower']\n",
      "['TF 2.3', 'comp:keras', 'type:bug']\n",
      "['comp:lite', 'type:support']\n",
      "['TF 2.3', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:lite', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['cla: yes', 'comp:keras', 'size:M']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:support']\n",
      "['comp:lite', 'type:performance']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:lite', 'comp:micro', 'comp:micro:arm', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.3', 'comp:autograph', 'type:bug']\n",
      "['comp:lite', 'type:support']\n",
      "['TF 2.2', 'comp:gpu', 'type:support']\n",
      "['TF 2.3', 'comp:core', 'comp:ops', 'type:bug']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:lite', 'comp:micro', 'type:support']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['type:build/install', 'type:feature']\n",
      "['TF 2.2', 'comp:ops', 'stat:awaiting tensorflower', 'type:support']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:data', 'stat:contributions welcome', 'type:feature']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'TFLiteConverter', 'stat:awaiting tensorflower', 'type:performance']\n",
      "['comp:lite', 'type:bug']\n",
      "['comp:micro', 'type:feature']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:gpu', 'type:bug']\n",
      "['comp:lite', 'type:others']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:lite', 'comp:micro', 'comp:micro:espressif', 'type:feature']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.2', 'comp:gpu', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:gpu', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['cla: yes', 'comp:lite', 'comp:micro', 'comp:micro:arm', 'size:XL']\n",
      "['comp:lite', 'comp:micro', 'type:feature']\n",
      "['awaiting review', 'cla: yes', 'comp:keras', 'prtype:bugfix', 'size:S']\n",
      "['cla: yes', 'comp:keras', 'prtype:bugfix', 'ready to pull', 'size:XS']\n",
      "['comp:lite', 'type:bug']\n",
      "['TF 2.3', 'comp:lite', 'comp:micro', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.3', 'comp:keras', 'type:bug']\n",
      "['comp:xla', 'stat:awaiting tensorflower', 'type:others']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:lite', 'type:build/install']\n",
      "['TF 2.3', 'comp:tpus', 'type:bug']\n",
      "['cla: yes', 'size:L', 'stalled', 'stat:awaiting response']\n",
      "['comp:apis', 'type:feature']\n",
      "['comp:lite', 'type:bug']\n",
      "['TF 2.3', 'comp:lite', 'type:bug']\n",
      "['TF 2.3', 'comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['Fixed in Nightly', 'TF 2.3', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:xla', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:gpu', 'comp:ops', 'stat:awaiting tensorflower', 'type:performance']\n",
      "['TF 2.3', 'comp:lite', 'type:support']\n",
      "['comp:apis', 'type:others']\n",
      "['TF 2.3', 'TFLiteConverter', 'comp:lite', 'type:bug']\n",
      "['TF 2.3', 'comp:keras', 'type:performance']\n",
      "['TF 2.1', 'comp:micro', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:lite', 'type:build/install']\n",
      "['TF 2.3', 'comp:lite', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TF 2.2', 'TF 2.3', 'comp:gpu', 'comp:lite', 'type:support']\n",
      "['comp:lite', 'comp:micro', 'type:support']\n",
      "['TF 2.3', 'comp:keras', 'comp:tensorboard', 'type:bug']\n",
      "['TF 2.3', 'comp:keras', 'comp:tpus', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:keras', 'comp:tpus', 'type:support']\n",
      "['TF 2.3', 'comp:gpu', 'comp:keras', 'type:bug']\n",
      "['TF 2.4', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:keras', 'type:bug']\n",
      "['cla: yes', 'ready to pull', 'size:L']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.3', 'comp:keras', 'type:support']\n",
      "['TF 2.3', 'TFLiteConverter', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.4', 'stat:awaiting tensorflower', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['TF 2.3', 'comp:lite', 'comp:micro', 'type:performance']\n",
      "['TF 2.3', 'TFLiteConverter', 'type:bug']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:model', 'type:support']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:data', 'type:feature']\n",
      "['TF 2.3', 'comp:lite', 'type:feature']\n",
      "['TF 2.3', 'comp:lite', 'type:bug']\n",
      "['comp:keras', 'type:feature']\n",
      "['TF 2.1', 'comp:lite', 'type:bug']\n",
      "['cla: yes', 'size:L']\n",
      "['TF 2.1', 'comp:gpu', 'type:support']\n",
      "['TF 2.3', 'comp:apis', 'regression issue', 'type:bug']\n",
      "['TF 2.3', 'comp:tpus', 'type:bug']\n",
      "['comp:lite', 'type:support']\n",
      "['cla: yes', 'ready to pull', 'size:L']\n",
      "['subtype:windows', 'type:build/install']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:lite', 'comp:micro', 'type:bug']\n",
      "['TF 2.3', 'comp:data', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:lite', 'comp:micro', 'type:bug']\n",
      "['TF 2.3', 'comp:apis', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:lite', 'comp:micro', 'stat:awaiting response', 'type:bug']\n",
      "['TF 2.3', 'comp:core', 'regression issue', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:apis', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:lite', 'comp:micro', 'type:support']\n",
      "['TF 2.3', 'comp:ops', 'stat:awaiting tensorflower', 'type:performance']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['cla: yes', 'comp:mkl', 'ready to pull', 'size:M']\n",
      "['comp:apis', 'comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:ops', 'type:feature']\n",
      "['TF 2.3', 'TFLiteConverter', 'type:bug']\n",
      "['comp:data', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.3', 'comp:keras', 'type:bug']\n",
      "['TF 2.3', 'comp:data', 'type:bug']\n",
      "['cla: yes', 'size:M']\n",
      "['TF 2.3', 'comp:keras', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:lite', 'comp:micro', 'comp:micro:arm', 'type:bug']\n",
      "['TF 2.3', 'comp:xla', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:xla', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:xla', 'type:bug']\n",
      "['comp:runtime', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['awaiting review', 'cla: yes', 'comp:keras', 'size:XS']\n",
      "['TF 2.3', 'subtype:macOS', 'type:build/install']\n",
      "['comp:lite', 'type:support']\n",
      "['stat:awaiting tensorflower', 'type:build/install', 'type:support']\n",
      "['comp:lite', 'type:feature']\n",
      "['TF 2.3', 'comp:lite', 'type:bug']\n",
      "['TF 2.3', 'comp:lite', 'comp:micro', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:others']\n",
      "['TF 2.3', 'comp:keras', 'type:bug']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.3', 'comp:lite', 'type:bug']\n",
      "['TF 2.3', 'stat:community support', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['TF 2.2', 'comp:keras', 'type:bug']\n",
      "['TF 2.3', 'TFLiteConverter', 'comp:lite', 'type:bug']\n",
      "['TF 2.2', 'TF 2.3', 'comp:keras', 'type:bug']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.3', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.3', 'comp:keras', 'type:bug']\n",
      "['TF 2.3', 'TFLiteConverter', 'type:bug']\n",
      "['TF 2.3', 'type:build/install']\n",
      "['cla: yes', 'size:XL']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:performance']\n",
      "['TF 2.3', 'comp:apis', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:lite', 'type:bug']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TFLiteConverter', 'type:bug']\n",
      "['subtype: ubuntu/linux', 'type:build/install']\n",
      "['TF 2.3', 'comp:ops', 'type:bug']\n",
      "['comp:lite', 'comp:micro', 'comp:micro:arm', 'type:feature']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.3', 'subtype:windows', 'type:build/install']\n",
      "['comp:dist-strat', 'type:bug']\n",
      "['comp:runtime', 'type:build/install', 'type:feature']\n",
      "['TF 2.3', 'comp:lite', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:lite', 'type:support']\n",
      "['TF 2.3', 'comp:gpu', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:core', 'comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.3', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:data', 'stat:awaiting tensorflower', 'type:support']\n",
      "['awaiting review', 'cla: yes', 'comp:grappler', 'size:L']\n",
      "['TF 2.2', 'comp:gpu', 'comp:ops', 'stat:awaiting tensorflower', 'type:performance']\n",
      "['TF 2.3', 'comp:lite', 'type:bug']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:apis', 'type:performance']\n",
      "['cla: yes', 'comp:eager', 'size:M', 'stat:awaiting tensorflower']\n",
      "['TF 2.2', 'comp:ops', 'stat:awaiting tensorflower', 'type:performance']\n",
      "['comp:xla', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['TF 2.3', 'stat:awaiting tensorflower', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['comp:lite', 'type:feature']\n",
      "['comp:lite', 'type:bug']\n",
      "['TF 2.3', 'comp:core', 'comp:keras', 'stat:awaiting tensorflower', 'type:performance']\n",
      "['TF 2.3', 'comp:keras', 'comp:tpus', 'type:bug']\n",
      "['comp:keras', 'type:docs-bug']\n",
      "['comp:lite', 'type:performance']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['TF 2.3', 'comp:keras', 'type:support']\n",
      "['TF 2.3', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:data', 'type:performance']\n",
      "['TF 2.3', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:ops', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TF 2.3', 'comp:keras', 'type:bug']\n",
      "['TF 2.2', 'TFLiteConverter', 'comp:keras', 'type:bug']\n",
      "['TF 2.3', 'comp:gpu', 'type:bug']\n",
      "['TF 2.2', 'comp:core', 'type:bug']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:keras', 'type:feature']\n",
      "['comp:core', 'type:docs-feature']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:docs-feature']\n",
      "['TF 2.3', 'comp:keras', 'type:bug']\n",
      "['TF 2.3', 'comp:tensorboard', 'type:bug']\n",
      "['comp:tpus', 'type:support']\n",
      "['TF 2.3', 'comp:keras', 'type:bug']\n",
      "['type:feature']\n",
      "['TF 2.3', 'TFLiteConverter', 'comp:lite', 'type:support']\n",
      "['TF 2.3', 'regression issue', 'type:build/install']\n",
      "['comp:lite', 'type:support']\n",
      "['TF 2.3', 'comp:data', 'comp:gpu', 'type:bug']\n",
      "['comp:data', 'type:bug']\n",
      "['TF 2.2', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:lite', 'comp:micro', 'comp:micro:espressif', 'type:build/install']\n",
      "['comp:lite', 'type:bug']\n",
      "['comp:ops', 'type:feature']\n",
      "['TF 2.3', 'comp:data', 'type:bug']\n",
      "['comp:lite', 'type:support']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.3', 'comp:lite', 'type:bug']\n",
      "['TF 2.2', 'comp:lite', 'comp:micro', 'type:bug']\n",
      "['TF 2.3', 'comp:lite', 'type:bug']\n",
      "['TF 2.3', 'comp:lite', 'type:bug']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:lite', 'type:performance']\n",
      "['comp:micro', 'comp:micro:espressif', 'type:bug']\n",
      "['comp:lite', 'subtype:windows', 'type:build/install']\n",
      "['TF 2.3', 'comp:lite', 'type:performance']\n",
      "['TF 2.1', 'comp:signal', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:ops', 'type:docs-bug']\n",
      "['TF 2.2', 'comp:keras', 'type:bug']\n",
      "['subtype: ubuntu/linux', 'type:build/install']\n",
      "['TF 2.2', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.2', 'comp:gpu', 'comp:xla', 'stat:awaiting tensorflower', 'type:performance']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.3', 'comp:apis', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 1.14', 'stat:community support', 'subtype:windows', 'type:build/install']\n",
      "['TF 2.2', 'comp:lite', 'type:bug']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:lite', 'type:feature']\n",
      "['comp:micro', 'comp:micro:espressif', 'type:bug']\n",
      "['type:docs-bug']\n",
      "['TF 2.1', 'comp:gpu', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:lite', 'type:bug']\n",
      "['TF 2.3', 'comp:lite', 'type:bug']\n",
      "['TF 2.1', 'comp:ops', 'type:bug']\n",
      "['comp:apis', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:lite', 'type:build/install']\n",
      "['comp:tpus', 'stat:awaiting tensorflower', 'type:support']\n",
      "['comp:core', 'stat:awaiting response', 'type:docs-bug']\n",
      "['cla: yes', 'comp:core', 'ready to pull', 'size:M', 'stat:awaiting tensorflower']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:keras', 'type:bug']\n",
      "['TF 2.3', 'comp:core', 'stat:awaiting tensorflower', 'type:performance']\n",
      "['TF 2.3', 'comp:autograph', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:keras', 'regression issue', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.2', 'comp:lite', 'type:performance']\n",
      "['comp:lite', 'type:build/install']\n",
      "['comp:lite', 'comp:micro', 'type:bug']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['stat:awaiting tensorflower', 'type:build/install', 'type:docs-bug']\n",
      "['TF 2.1', 'TF 2.2', 'TF 2.3', 'comp:lite', 'type:bug']\n",
      "['comp:lite', 'comp:micro', 'type:build/install']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:mkl', 'stat:awaiting tensorflower', 'subtype:windows', 'type:build/install']\n",
      "['TF 2.2', 'comp:gpu', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:keras', 'comp:tensorboard', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TF 2.3', 'comp:keras', 'type:support']\n",
      "['TF 2.2', 'comp:dist-strat', 'type:bug']\n",
      "['TF 2.3', 'comp:lite', 'type:bug']\n",
      "['comp:lite', 'type:feature']\n",
      "['TF 2.2', 'comp:keras', 'type:bug']\n",
      "['TF 2.3', 'stat:awaiting tensorflower', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['TF 2.2', 'comp:lite', 'type:bug']\n",
      "['TF 2.2', 'comp:xla', 'type:performance']\n",
      "['TF 2.2', 'comp:runtime', 'comp:xla', 'type:bug']\n",
      "['comp:lite', 'comp:micro', 'comp:micro:espressif', 'type:bug']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['cla: yes', 'size:M']\n",
      "['cla: yes', 'size:M']\n",
      "['cla: yes', 'size:M']\n",
      "['cla: yes', 'size:M']\n",
      "['comp:apis', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.3', 'subtype:windows', 'type:build/install']\n",
      "['TF 2.1', 'comp:gpu', 'stat:awaiting tensorflower', 'type:support']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['cla: yes', 'size:XS', 'stat:awaiting tensorflower']\n",
      "['TF 2.2', 'comp:gpu', 'type:bug']\n",
      "['type:others']\n",
      "['comp:gpu', 'comp:keras', 'type:bug']\n",
      "['TF 2.3', 'comp:keras', 'regression issue', 'type:bug']\n",
      "['TF 2.3', 'subtype:windows', 'type:build/install']\n",
      "['TF 2.2', 'comp:lite', 'type:bug']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TF 2.3', 'comp:keras', 'type:performance']\n",
      "['TF 2.3', 'comp:keras', 'comp:ops', 'type:feature']\n",
      "['TF 2.3', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['TF 2.2', 'stat:awaiting tensorflower', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['TF 2.2', 'comp:lite', 'comp:micro', 'stat:awaiting tensorflower', 'type:support']\n",
      "['comp:data', 'type:feature']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.2', 'comp:keras', 'type:bug']\n",
      "['TF 2.2', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:gpu', 'comp:keras', 'stat:awaiting tensorflower', 'type:performance']\n",
      "['TF 2.2', 'comp:data', 'type:bug']\n",
      "['comp:autograph', 'comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:lite', 'type:build/install']\n",
      "['TF 2.2', 'subtype:windows', 'type:build/install']\n",
      "['comp:xla', 'type:docs-feature']\n",
      "['TF 2.2', 'comp:keras', 'type:bug']\n",
      "['TF 2.2', 'comp:autograph', 'type:bug']\n",
      "['TF 2.2', 'stat:awaiting tensorflower', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['cla: yes', 'comp:keras', 'size:M', 'stat:awaiting tensorflower']\n",
      "['TF 2.3', 'comp:runtime', 'type:bug']\n",
      "['TF 2.2', 'comp:data', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:ops', 'type:performance']\n",
      "['cla: yes', 'comp:keras', 'prtype:bugfix', 'size:S', 'stat:awaiting tensorflower']\n",
      "['comp:apis', 'type:feature']\n",
      "['TF 2.2', 'comp:signal', 'stat:awaiting tensorflower', 'type:performance']\n",
      "['comp:keras', 'type:feature']\n",
      "['TF 2.3', 'comp:lite', 'regression issue', 'type:bug']\n",
      "['comp:keras', 'type:bug']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['stat:awaiting tensorflower', 'type:build/install']\n",
      "['comp:lite', 'type:bug']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.2', 'comp:gpu', 'type:bug']\n",
      "['comp:ops', 'type:docs-bug']\n",
      "['comp:xla', 'type:support']\n",
      "['TF 2.2', 'comp:tpus', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.0', 'comp:keras', 'type:bug']\n",
      "['cla: yes', 'comp:keras', 'size:S', 'stat:awaiting tensorflower']\n",
      "['TF 2.2', 'comp:core', 'comp:ops', 'type:bug']\n",
      "['comp:dist-strat', 'type:feature']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:others']\n",
      "['comp:lite', 'type:bug']\n",
      "['TF 2.3', 'comp:lite', 'type:performance']\n",
      "['TF 2.2', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:keras', 'type:bug']\n",
      "['TF 2.2', 'TFLiteConverter', 'comp:lite', 'type:support']\n",
      "['TF 2.2', 'comp:apis', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TF 2.2', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.2', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['type:feature']\n",
      "['TF 2.2', 'comp:apis', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.2', 'comp:ops', 'type:bug']\n",
      "['comp:keras', 'type:performance']\n",
      "['TF 2.2', 'comp:xla', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:runtime', 'type:feature']\n",
      "['TF 2.2', 'comp:keras', 'type:bug']\n",
      "['comp:apis', 'type:feature']\n",
      "['TF 2.2', 'comp:gpu', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['type:bug']\n",
      "['TFLiteConverter', 'comp:lite', 'type:support']\n",
      "['TF 2.2', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.2', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.2', 'comp:gpu', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['awaiting review', 'cla: yes', 'comp:xla', 'prtype:bugfix', 'size:S']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.2', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.2', 'comp:keras', 'type:performance']\n",
      "['type:others']\n",
      "['comp:gpu', 'type:feature']\n",
      "['TF 2.3', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['TF 2.2', 'comp:keras', 'type:bug']\n",
      "['TF 2.2', 'comp:tfdbg', 'type:bug']\n",
      "['TF 2.2', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.2', 'comp:gpu', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['subtype:windows', 'type:build/install']\n",
      "['TF 2.2', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:apis', 'comp:gpu', 'type:performance']\n",
      "['TF 2.2', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:gpu', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.2', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.2', 'comp:data', 'type:support']\n",
      "['TF 2.2', 'comp:keras', 'stat:awaiting tensorflower', 'type:performance']\n",
      "['TF 2.2', 'comp:ops', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TF 2.2', 'comp:ops', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TF 2.3', 'comp:data', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'TFLiteConverter', 'comp:lite', 'type:support']\n",
      "['cla: yes', 'comp:gpu', 'size:S']\n",
      "['TF 2.2', 'comp:gpu', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:micro', 'type:bug']\n",
      "['comp:lite', 'type:support']\n",
      "['cla: yes', 'size:L', 'stat:awaiting tensorflower']\n",
      "['TF 2.2', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:lite', 'type:support']\n",
      "['comp:keras', 'type:feature']\n",
      "['TF 2.2', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.3', 'comp:runtime', 'type:bug']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.2', 'comp:keras', 'type:bug']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.3', 'comp:lite', 'type:bug']\n",
      "['TF 2.0', 'comp:gpu', 'comp:runtime', 'type:bug']\n",
      "['TF 2.2', 'comp:autograph', 'comp:keras', 'type:bug']\n",
      "['comp:lite', 'comp:micro', 'comp:micro:espressif', 'type:bug']\n",
      "['TF 2.3', 'comp:gpu', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TF 2.2', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.2', 'comp:keras', 'type:bug']\n",
      "['TF 2.2', 'comp:gpu', 'type:performance']\n",
      "['TF 2.2', 'comp:keras', 'type:performance']\n",
      "['comp:lite', 'type:feature']\n",
      "['TF 2.3', 'comp:grappler', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.2', 'comp:ops', 'type:bug']\n",
      "['TF 2.2', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['TF 2.2', 'subtype:windows', 'type:build/install']\n",
      "['TF 2.2', 'TFLiteConverter', 'type:bug']\n",
      "['comp:lite', 'subtype: raspberry pi', 'type:build/install']\n",
      "['TF 2.2', 'comp:lite', 'type:support']\n",
      "['TF 2.2', 'comp:tpus', 'type:bug']\n",
      "['TF 2.2', 'TFLiteConverter', 'comp:lite', 'regression issue', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 1.14', 'stat:awaiting tensorflower', 'subtype: raspberry pi', 'type:build/install']\n",
      "['TF 2.2', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.2', 'stat:awaiting tensorflower', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['comp:data', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.2', 'comp:apis', 'type:bug']\n",
      "['TF 2.2', 'TFLiteConverter', 'comp:lite', 'type:support']\n",
      "['comp:lite', 'comp:micro', 'stat:awaiting tensorflower', 'type:support']\n",
      "['stat:awaiting tensorflower', 'subtype:windows', 'type:build/install']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TF 2.2', 'TFLiteConverter', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:keras', 'type:bug']\n",
      "['comp:lite', 'type:build/install']\n",
      "['comp:lite', 'comp:micro', 'type:performance']\n",
      "['TF 2.2', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.0', 'comp:lite', 'comp:runtime', 'type:bug']\n",
      "['TF 2.2', 'stat:awaiting tensorflower', 'type:others']\n",
      "['TF 2.2', 'comp:keras', 'type:bug']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.2', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.2', 'comp:lite', 'stat:awaiting tensorflower', 'type:support']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.2', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.0', 'comp:micro', 'type:support']\n",
      "['comp:ops', 'type:docs-bug']\n",
      "['comp:ops', 'stat:contributions welcome', 'type:feature']\n",
      "['TF 2.2', 'comp:lite', 'comp:micro:espressif', 'type:bug']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:performance']\n",
      "['TF 2.2', 'comp:apis', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.1', 'comp:micro', 'type:bug']\n",
      "['comp:lite', 'type:performance']\n",
      "['TF 2.2', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.2', 'comp:keras', 'type:bug']\n",
      "['TF 2.2', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.2', 'comp:ops', 'stat:awaiting tensorflower', 'type:performance']\n",
      "['comp:lite', 'comp:micro', 'type:support']\n",
      "['TF 2.2', 'comp:ops', 'type:bug']\n",
      "['TF 2.2', 'stat:awaiting tensorflower', 'subtype:windows', 'type:build/install']\n",
      "['TF 2.2', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['cla: yes', 'comp:keras', 'ready to pull', 'size:XS']\n",
      "['TF 2.2', 'comp:lite', 'comp:micro', 'type:bug']\n",
      "['TF 2.1', 'comp:data', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TF 2.3', 'comp:mkl', 'type:bug']\n",
      "['TF 2.1', 'TFLiteConverter', 'comp:lite', 'type:bug']\n",
      "['TF 2.2', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['cla: yes', 'comp:eager', 'size:M', 'stat:awaiting tensorflower']\n",
      "['TF 2.0', 'comp:gpu', 'type:bug']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:gpu:tensorrt', 'type:bug']\n",
      "['TF 2.2', 'comp:gpu', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.2', 'comp:ops', 'type:bug']\n",
      "['comp:ops', 'type:bug']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:others']\n",
      "['TF 2.1', 'comp:lite', 'comp:micro', 'type:bug']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.1', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['TF 2.2', 'comp:ops', 'stat:awaiting tensorflower', 'type:performance']\n",
      "['TF 2.2', 'comp:keras', 'type:bug']\n",
      "['cla: yes', 'comp:gpu:tensorrt', 'size:XL']\n",
      "['TF 2.2', 'comp:apis', 'stat:awaiting tensorflower', 'type:support']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.2', 'stat:awaiting tensorflower', 'type:bug', 'type:others']\n",
      "['TF 2.2', 'comp:data', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:signal', 'type:docs-bug']\n",
      "['TF 2.2', 'comp:autograph', 'comp:core', 'comp:gpu', 'comp:runtime', 'type:docs-bug', 'type:performance']\n",
      "['comp:lite', 'comp:micro', 'type:bug']\n",
      "['cla: no', 'ready to pull', 'size:M']\n",
      "['TF 2.2', 'comp:gpu:tensorrt', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.2', 'comp:autograph', 'stat:awaiting tensorflower', 'type:performance']\n",
      "['TF 2.2', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.1', 'comp:keras', 'type:bug']\n",
      "['TF 2.2', 'comp:data', 'type:bug']\n",
      "['TF 2.2', 'comp:keras', 'type:bug']\n",
      "['TF 2.2', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['comp:signal', 'type:docs-bug']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.2', 'comp:gpu', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:apis', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:lite', 'type:feature']\n",
      "['TF 2.2', 'comp:keras', 'type:performance']\n",
      "['cla: yes', 'comp:gpu:tensorrt', 'size:XL']\n",
      "['comp:lite', 'type:feature']\n",
      "['TF 2.2', 'stat:awaiting tensorflower', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['TF 2.2', 'comp:apis', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['Fixed in Nightly', 'TF 2.2', 'comp:ops', 'type:bug']\n",
      "['comp:lite', 'comp:micro', 'type:support']\n",
      "['TF 2.2', 'stat:awaiting tensorflower', 'type:bug', 'type:performance']\n",
      "['TF 2.2', 'comp:keras', 'type:bug']\n",
      "['TF 2.2', 'TFLiteConverter', 'comp:lite', 'type:bug']\n",
      "['comp:lite', 'type:support']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TF 2.2', 'comp:lite', 'type:support']\n",
      "['comp:xla', 'type:feature']\n",
      "['comp:xla', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:lite', 'comp:micro', 'type:bug']\n",
      "['TF 2.2', 'comp:lite', 'type:bug']\n",
      "['comp:dist-strat', 'type:feature']\n",
      "['comp:apis', 'type:others']\n",
      "['comp:lite', 'type:feature']\n",
      "['TF 2.2', 'comp:keras', 'type:bug']\n",
      "['comp:core', 'type:others']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:lite', 'type:feature']\n",
      "['comp:lite', 'type:support']\n",
      "['type:build/install', 'type:feature']\n",
      "['TF 2.2', 'comp:dist-strat', 'type:feature']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:docs-feature']\n",
      "['comp:gpu', 'type:feature']\n",
      "['TF 2.1', 'comp:lite', 'comp:micro', 'type:support']\n",
      "['TF 2.1', 'comp:tensorboard', 'comp:tpus', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:lite', 'type:support']\n",
      "['TF 2.2', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.1', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.2', 'TFLiteConverter', 'comp:lite', 'type:support']\n",
      "['TF 2.3', 'TFLiteConverter', 'comp:lite', 'stat:awaiting tensorflower', 'type:support']\n",
      "['subtype:windows', 'type:build/install']\n",
      "['TF 2.2', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'TFLiteConverter', 'comp:lite', 'type:bug']\n",
      "['TF 2.2', 'comp:keras', 'type:bug']\n",
      "['TF 2.2', 'comp:keras', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TFLiteConverter', 'comp:lite', 'stat:awaiting tensorflower', 'type:support']\n",
      "['comp:lite', 'type:feature']\n",
      "['TF 2.2', 'comp:apis', 'type:bug']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:support']\n",
      "['awaiting review', 'cla: yes', 'comp:keras', 'size:S']\n",
      "['TF 2.2', 'comp:apis', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['subtype: ubuntu/linux', 'type:build/install', 'type:docs-bug']\n",
      "['TF 2.2', 'TFLiteConverter', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.2', 'comp:keras', 'comp:tpus', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.2', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.1', 'comp:keras', 'type:performance']\n",
      "['comp:apis', 'comp:core', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:ops', 'type:bug']\n",
      "['subtype:bazel', 'type:build/install', 'type:feature']\n",
      "['TF 2.2', 'TFLiteConverter', 'comp:lite', 'type:support']\n",
      "['TF 2.2', 'stat:awaiting tensorflower', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['TF 2.2', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:grappler', 'type:bug']\n",
      "['awaiting review', 'cla: yes', 'comp:keras', 'size:S']\n",
      "['comp:keras', 'stat:contributions welcome', 'type:feature']\n",
      "['TF 2.2', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['TF 2.2', 'comp:lite', 'type:bug']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['type:others']\n",
      "['TF 2.2', 'TFLiteConverter', 'comp:lite', 'type:support']\n",
      "['TF 2.2', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.1', 'comp:apis', 'type:bug']\n",
      "['TF 2.2', 'stat:awaiting tensorflower', 'subtype:centos', 'type:build/install']\n",
      "['comp:lite', 'type:bug']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.2', 'comp:lite', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.2', 'comp:runtime', 'type:bug']\n",
      "['TF 2.2', 'subtype:macOS', 'type:build/install']\n",
      "['comp:keras', 'type:docs-bug']\n",
      "['TF 2.1', 'TF 2.2', 'comp:ops', 'stat:awaiting response', 'type:bug']\n",
      "['cla: yes', 'size:M']\n",
      "['TF 2.2', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['TF 2.3', 'comp:lite', 'type:bug']\n",
      "['TF 2.3', 'comp:ops', 'type:bug']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.2', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:tensorboard', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.1', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.1', 'comp:keras', 'type:bug']\n",
      "['TF 2.2', 'comp:lite', 'type:bug']\n",
      "['TF 2.2', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.2', 'comp:lite', 'type:performance']\n",
      "['TF 2.1', 'comp:core', 'type:support']\n",
      "['TF 2.3', 'comp:lite', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:xla', 'type:bug']\n",
      "['comp:lite', 'type:feature']\n",
      "['TF 2.2', 'comp:tpus', 'type:bug']\n",
      "['cla: yes', 'comp:gpu', 'size:L', 'stat:awaiting tensorflower']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:docs-feature', 'type:feature']\n",
      "['TF 2.2', 'comp:ops', 'type:bug']\n",
      "['awaiting review', 'cla: yes', 'comp:keras', 'size:S']\n",
      "['TF 2.2', 'comp:keras', 'type:bug']\n",
      "['TF 2.2', 'comp:data', 'type:bug']\n",
      "['comp:keras', 'type:feature']\n",
      "['comp:data', 'type:support']\n",
      "['TF 2.2', 'comp:keras', 'type:bug']\n",
      "['comp:lite', 'comp:micro', 'type:support']\n",
      "['comp:lite', 'type:support']\n",
      "['comp:lite', 'comp:micro', 'type:build/install']\n",
      "['TF 2.2', 'comp:lite', 'type:support']\n",
      "['TF 2.2', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['comp:lite', 'type:support']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TF 2.3', 'comp:xla', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.1', 'comp:keras', 'type:bug']\n",
      "['comp:xla', 'type:bug']\n",
      "['TF 2.2', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.2', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.2', 'comp:gpu', 'type:bug']\n",
      "['TF 2.2', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.2', 'comp:xla', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:keras', 'comp:tensorboard', 'type:bug']\n",
      "['TF 2.2', 'stat:community support', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.2', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.2', 'comp:keras', 'stat:awaiting tensorflower', 'type:support']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.2', 'comp:runtime', 'type:bug']\n",
      "['TF 2.3', 'comp:autograph', 'comp:ops', 'type:bug']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:docs-feature']\n",
      "['TF 2.2', 'comp:keras', 'comp:xla', 'type:performance']\n",
      "['TF 2.2', 'comp:tensorboard', 'comp:xla', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.2', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:xla', 'type:bug']\n",
      "['comp:keras', 'type:feature']\n",
      "['TF 2.2', 'comp:dist-strat', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.2', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.2', 'comp:keras', 'stat:awaiting tensorflower', 'type:support']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'subtype: raspberry pi', 'type:build/install', 'type:docs-bug']\n",
      "['subtype: ubuntu/linux', 'type:build/install']\n",
      "['comp:keras', 'comp:tensorboard', 'type:feature']\n",
      "['TF 2.2', 'comp:ops', 'type:bug']\n",
      "['TF 2.1', 'comp:ops', 'type:performance']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['TF 2.2', 'comp:keras', 'comp:tensorboard', 'type:bug']\n",
      "['comp:apis', 'type:docs-feature']\n",
      "['TF 2.3', 'comp:tpus', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['TF 2.2', 'comp:data', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.2', 'comp:keras', 'regression issue', 'type:bug']\n",
      "['TF 2.1', 'comp:lite', 'type:bug']\n",
      "['TF 2.2', 'subtype:windows', 'type:build/install']\n",
      "['TF 2.2', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:lite', 'comp:micro', 'type:bug']\n",
      "['comp:keras', 'stat:awaiting response', 'type:docs-feature']\n",
      "['TF 2.2', 'comp:keras', 'type:performance']\n",
      "['TF 2.1', 'comp:gpu', 'type:performance']\n",
      "['TF 2.1', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:lite', 'type:build/install']\n",
      "['TF 2.2', 'comp:dist-strat', 'comp:keras', 'type:support']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.1', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.2', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:keras', 'type:docs-bug']\n",
      "['comp:lite', 'type:bug']\n",
      "['awaiting review', 'cla: yes', 'comp:keras', 'size:M']\n",
      "['comp:gpu', 'type:feature']\n",
      "['TF 2.1', 'TF 2.2', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.2', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.2', 'comp:mkl', 'subtype:macOS', 'type:build/install']\n",
      "['TF 2.1', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.2', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.2', 'comp:apis', 'comp:eager', 'comp:gpu', 'stat:awaiting tensorflower', 'type:performance']\n",
      "['TF 2.2', 'comp:keras', 'stat:awaiting tensorflower', 'type:performance']\n",
      "['TF 2.1', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['type:feature']\n",
      "['stat:awaiting tensorflower', 'subtype:bazel', 'type:build/install']\n",
      "['TF 2.2', 'comp:keras', 'type:performance']\n",
      "['TF 2.1', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.2', 'comp:tpus', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.2', 'comp:keras', 'stat:awaiting tensorflower', 'type:support']\n",
      "['comp:gpu', 'type:feature']\n",
      "['comp:gpu', 'stat:awaiting response', 'type:docs-bug']\n",
      "['TF 2.2', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['comp:ops', 'type:docs-bug']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TF 2.1', 'comp:gpu', 'comp:model', 'stat:awaiting tensorflower', 'type:performance']\n",
      "['comp:apis', 'type:docs-feature']\n",
      "['comp:apis', 'type:feature']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['TF 2.1', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.2', 'comp:keras', 'regression issue', 'type:bug']\n",
      "['TF 2.1', 'TF 2.2', 'comp:gpu', 'type:bug']\n",
      "['API review', 'cla: yes', 'prtype:bugfix', 'ready to pull', 'size:S', 'stat:awaiting response']\n",
      "['comp:micro', 'type:support']\n",
      "['TF 2.1', 'TF 2.2', 'comp:gpu', 'comp:keras', 'type:performance']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.2', 'TF 2.3', 'comp:keras', 'type:bug']\n",
      "['TF 2.1', 'subtype:windows', 'type:build/install']\n",
      "['TF 2.2', 'comp:data', 'type:bug']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.2', 'comp:keras', 'comp:tensorboard', 'type:bug']\n",
      "['TF 2.2', 'comp:lite', 'type:performance']\n",
      "['comp:lite', 'comp:micro', 'type:feature']\n",
      "['TF 2.1', 'comp:micro', 'type:support']\n",
      "['TF 2.2', 'TFLiteConverter', 'comp:lite', 'type:bug']\n",
      "['TF 2.1', 'comp:gpu', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['stat:awaiting tensorflower', 'type:build/install']\n",
      "['comp:data', 'type:feature']\n",
      "['TF 2.2', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:keras', 'type:others']\n",
      "['TF 2.0', 'stat:awaiting tensorflower', 'subtype:macOS', 'type:build/install']\n",
      "['cla: yes', 'size:M', 'stat:awaiting response']\n",
      "['TF 2.2', 'comp:autograph', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['comp:micro', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TF 2.2', 'comp:tensorboard', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.1', 'comp:autograph', 'type:bug']\n",
      "['TF 2.2', 'comp:tpus', 'type:bug']\n",
      "['TF 2.2', 'comp:dist-strat', 'type:bug']\n",
      "['TF 2.2', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['TF 2.1', 'comp:keras', 'type:bug']\n",
      "['TF 2.1', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['TF 2.2', 'stat:contributions welcome', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['TF 2.1', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.0', 'comp:lite', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['TF 2.1', 'TF 2.2', 'comp:keras', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TF 2.1', 'comp:data', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:lite', 'type:bug']\n",
      "['comp:runtime', 'type:feature']\n",
      "['TF 2.1', 'comp:keras', 'type:feature']\n",
      "['stat:community support', 'type:build/install', 'type:feature']\n",
      "['comp:ops', 'type:feature']\n",
      "['TF 2.1', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['type:build/install']\n",
      "['comp:keras', 'type:feature']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:lite', 'type:feature']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.2', 'comp:dist-strat', 'type:bug']\n",
      "['TF 2.2', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.0', 'comp:apis', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TF 2.2', 'TFLiteConverter', 'comp:lite', 'type:bug']\n",
      "['comp:ops', 'type:feature']\n",
      "['comp:apis', 'type:feature']\n",
      "['comp:dist-strat', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.2', 'comp:keras', 'stat:awaiting tensorflower', 'type:performance']\n",
      "['comp:keras', 'type:feature']\n",
      "['TF 2.1', 'comp:data', 'comp:keras', 'type:bug']\n",
      "['TF 2.1', 'TF 2.2', 'comp:runtime', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:lite', 'comp:micro', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.2', 'comp:apis', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.1', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.2', 'comp:micro', 'type:bug']\n",
      "['cla: yes', 'comp:keras', 'size:XS', 'stat:awaiting tensorflower']\n",
      "['TF 2.2', 'comp:runtime', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.1', 'comp:lite', 'type:bug']\n",
      "['TF 2.1', 'stat:awaiting tensorflower', 'subtype:windows', 'type:build/install']\n",
      "['comp:keras', 'type:docs-feature']\n",
      "['TF 2.1', 'comp:data', 'type:performance']\n",
      "['TF 2.1', 'TF 2.2', 'TFLiteConverter', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.1', 'TF 2.2', 'comp:tpus', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.2', 'stat:awaiting tensorflower', 'subtype: ubuntu/linux', 'subtype:macOS', 'type:build/install']\n",
      "['TF 2.1', 'comp:ops', 'type:performance']\n",
      "['TF 2.1', 'comp:ops', 'type:performance']\n",
      "['type:feature']\n",
      "['TF 2.2', 'comp:keras', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TF 2.2', 'comp:keras', 'type:bug']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.1', 'comp:ops', 'type:performance']\n",
      "['comp:keras', 'type:feature']\n",
      "['TF 2.1', 'comp:ops', 'type:bug']\n",
      "['TF 2.1', 'comp:mkl', 'stat:awaiting tensorflower', 'subtype:windows', 'type:build/install']\n",
      "['TF 2.1', 'TF 2.2', 'comp:apis', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.0', 'comp:lite', 'type:bug']\n",
      "['TF 2.1', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.1', 'TF 2.2', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.1', 'comp:tensorboard', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TF 2.1', 'TF 2.2', 'comp:tpus', 'type:bug']\n",
      "['TF 2.1', 'subtype:windows', 'type:build/install']\n",
      "['TF 2.1', 'type:bug']\n",
      "['comp:keras', 'type:feature']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:micro', 'type:build/install']\n",
      "['TF 2.1', 'comp:tensorboard', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:apis', 'stat:contributions welcome', 'type:feature']\n",
      "['TF 2.1', 'TF 2.2', 'comp:keras', 'type:bug']\n",
      "['stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.2', 'comp:tpus', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.1', 'comp:core', 'type:bug']\n",
      "['TF 2.3', 'comp:eager', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.2', 'comp:lite', 'type:performance']\n",
      "['comp:lite', 'type:feature']\n",
      "['TF 2.1', 'comp:gpu', 'type:build/install']\n",
      "['TF 2.2', 'comp:lite', 'type:bug']\n",
      "['comp:apis', 'type:feature']\n",
      "['type:docs-bug']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['awaiting review', 'cla: yes', 'comp:lite', 'comp:micro', 'prtype:bugfix', 'size:M']\n",
      "['TF 2.1', 'comp:lite', 'type:bug']\n",
      "['TF 2.1', 'comp:data', 'stat:awaiting tensorflower', 'type:performance']\n",
      "['TF 2.1', 'TF 2.2', 'comp:gpu', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:data', 'type:docs-bug']\n",
      "['comp:lite', 'comp:micro', 'comp:micro:espressif', 'type:support']\n",
      "['TF 2.1', 'TF 2.2', 'comp:xla', 'type:performance']\n",
      "['comp:lite', 'type:docs-bug']\n",
      "['comp:keras', 'type:feature']\n",
      "['comp:keras', 'type:bug']\n",
      "['comp:lite', 'type:support']\n",
      "['TF 2.1', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.1', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:ops', 'type:feature']\n",
      "['comp:lite', 'type:others']\n",
      "['comp:micro', 'type:bug']\n",
      "['comp:lite', 'type:support']\n",
      "['comp:keras', 'type:feature']\n",
      "['comp:lite', 'type:feature']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['TF 2.1', 'WIP', 'comp:autograph', 'comp:core', 'type:bug']\n",
      "['comp:lite', 'comp:micro', 'type:support']\n",
      "['comp:lite', 'type:feature']\n",
      "['comp:tensorboard', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TF 2.1', 'comp:lite', 'comp:micro', 'type:support']\n",
      "['TF 2.1', 'comp:lite', 'comp:micro', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.1', 'comp:lite', 'comp:micro', 'type:bug']\n",
      "['TF 2.2', 'comp:data', 'type:bug']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:support']\n",
      "['comp:lite', 'type:docs-bug']\n",
      "['TF 2.1', 'WIP', 'comp:autograph', 'comp:data', 'type:bug']\n",
      "['TF 2.1', 'TF 2.2', 'comp:apis', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.1', 'comp:gpu', 'type:bug']\n",
      "['TF 2.2', 'comp:xla', 'stat:awaiting tensorflower', 'type:build/install']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:lite', 'comp:micro', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:tensorboard', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['TF 2.1', 'comp:core', 'type:bug']\n",
      "['comp:dist-strat', 'comp:keras', 'type:bug']\n",
      "['TF 2.1', 'comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:lite', 'comp:micro', 'type:feature']\n",
      "['TF 2.1', 'comp:core', 'type:bug']\n",
      "['TF 2.2', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.2', 'comp:keras', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['type:feature']\n",
      "['TF 2.0', 'type:performance']\n",
      "['TF 2.2', 'comp:lite', 'type:bug']\n",
      "['TF 2.1', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['type:build/install', 'type:feature']\n",
      "['TF 2.0', 'comp:lite', 'type:performance']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TF 2.1', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.1', 'TFLiteConverter', 'comp:lite', 'type:bug']\n",
      "['TF 2.1', 'TF 2.2', 'comp:core', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:gpu', 'type:feature']\n",
      "['stat:awaiting tensorflower', 'subtype:centos', 'type:build/install']\n",
      "['TF 2.0', 'comp:micro', 'type:bug']\n",
      "['TF 2.0', 'comp:keras', 'comp:ops', 'stat:awaiting tensorflower', 'type:performance']\n",
      "['TF 2.1', 'TF 2.2', 'TF 2.3', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 1.15', 'TF 2.0', 'comp:apis', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TF 2.2', 'comp:runtime', 'type:bug']\n",
      "['comp:keras', 'type:docs-bug']\n",
      "['comp:keras', 'type:feature']\n",
      "['TF 2.1', 'comp:gpu', 'type:bug']\n",
      "['comp:ops', 'type:feature']\n",
      "['comp:keras', 'comp:lite', 'type:feature']\n",
      "['TF 2.1', 'comp:keras', 'type:bug']\n",
      "['TF 2.0', 'comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['cla: yes', 'comp:ops', 'size:S', 'stat:awaiting tensorflower']\n",
      "['TF 2.1', 'comp:keras', 'type:bug']\n",
      "['comp:data', 'stat:contributions welcome', 'type:feature']\n",
      "['comp:apis', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['stat:community support', 'type:feature']\n",
      "['comp:lite', 'type:bug']\n",
      "['TF 2.1', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.0', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['stat:awaiting tensorflower', 'type:build/install', 'type:feature']\n",
      "['comp:lite', 'comp:micro', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['type:others']\n",
      "['TF 2.1', 'comp:keras', 'type:bug']\n",
      "['TF 2.0', 'comp:lite', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:lite', 'comp:micro', 'stat:awaiting tensorflower', 'type:support']\n",
      "['comp:lite', 'type:support']\n",
      "['cla: yes', 'size:L']\n",
      "['TF 2.1', 'comp:apis', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:micro', 'type:build/install']\n",
      "['subtype: ubuntu/linux', 'type:build/install']\n",
      "['comp:data', 'type:docs-bug']\n",
      "['TF 2.1', 'comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.0', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.2', 'comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:lite', 'comp:micro', 'type:support']\n",
      "['TF 2.2', 'comp:lite', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TF 2.2', 'comp:lite', 'type:support']\n",
      "['comp:lite', 'type:bug']\n",
      "['comp:lite', 'comp:micro', 'type:bug']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.2', 'comp:keras', 'type:bug']\n",
      "['TF 2.1', 'comp:keras', 'type:docs-bug']\n",
      "['TF 2.1', 'comp:apis', 'comp:tpus', 'type:bug']\n",
      "['TF 2.0', 'stat:awaiting tensorflower', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['TF 2.3', 'comp:gpu', 'type:feature']\n",
      "['TF 2.1', 'comp:ops', 'type:performance']\n",
      "['comp:grappler', 'type:bug']\n",
      "['TF 2.2', 'comp:lite', 'comp:micro', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:runtime', 'type:docs-bug']\n",
      "['TF 2.1', 'comp:apis', 'type:feature']\n",
      "['TF 2.1', 'stat:community support', 'subtype:windows', 'type:build/install']\n",
      "['TF 2.1', 'subtype:windows', 'type:build/install']\n",
      "['comp:lite', 'comp:micro', 'type:bug']\n",
      "['comp:apis', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.2', 'comp:gpu', 'type:bug']\n",
      "['comp:keras', 'type:performance']\n",
      "['comp:ops', 'type:feature']\n",
      "['TF 2.0', 'type:build/install']\n",
      "['TF 2.1', 'comp:autograph', 'type:bug']\n",
      "['TF 2.2', 'TFLiteConverter', 'comp:lite', 'type:bug']\n",
      "['comp:keras', 'type:feature']\n",
      "['comp:runtime', 'type:performance']\n",
      "['TF 2.1', 'comp:apis', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.0', 'comp:ops', 'type:bug']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.0', 'comp:apis', 'type:bug']\n",
      "['TF 2.0', 'comp:lite', 'type:bug']\n",
      "['TF 2.0', 'comp:core', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.1', 'comp:apis', 'type:bug']\n",
      "['comp:lite', 'comp:micro', 'comp:micro:espressif', 'type:support']\n",
      "['comp:lite', 'comp:micro', 'type:bug']\n",
      "['TF 2.1', 'type:performance']\n",
      "['TF 2.1', 'TF 2.2', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.0', 'comp:keras', 'type:feature']\n",
      "['TF 2.1', 'comp:lite', 'type:feature']\n",
      "['TF 2.1', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['TF 2.1', 'comp:dist-strat', 'type:bug']\n",
      "['TF 2.1', 'comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.1', 'comp:lite', 'type:support']\n",
      "['TF 2.1', 'comp:apis', 'type:feature']\n",
      "['TF 2.0', 'comp:data', 'stat:awaiting tensorflower', 'type:performance']\n",
      "['TF 2.0', 'comp:keras', 'type:bug']\n",
      "['TF 2.1', 'TFLiteConverter', 'type:bug']\n",
      "['TF 2.2', 'comp:eager', 'comp:keras', 'type:bug']\n",
      "['TF 2.1', 'comp:data', 'type:performance']\n",
      "['TF 2.1', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.1', 'stat:awaiting tensorflower', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['TF 2.1', 'comp:lite', 'type:bug']\n",
      "['TF 1.15', 'TF 2.0', 'comp:dist-strat', 'type:bug']\n",
      "['TF 2.1', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.1', 'comp:apis', 'type:bug']\n",
      "['TF 2.1', 'comp:apis', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TF 2.1', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.1', 'comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.1', 'comp:data', 'type:bug']\n",
      "['TF 2.1', 'comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.0', 'comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:dist-strat', 'type:feature']\n",
      "['TF 2.1', 'comp:gpu', 'type:bug']\n",
      "['TF 2.0', 'comp:lite', 'type:bug']\n",
      "['comp:lite', 'comp:micro', 'type:bug']\n",
      "['TF 2.2', 'comp:keras', 'comp:tensorboard', 'type:bug']\n",
      "['TF 2.0', 'type:build/install']\n",
      "['TF 2.1', 'comp:keras', 'type:bug']\n",
      "['TF 2.1', 'comp:ops', 'type:bug']\n",
      "['TF 2.1', 'comp:keras', 'type:bug', 'type:performance']\n",
      "['TF 2.0', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['TF 2.0', 'comp:ops', 'comp:runtime', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.1', 'comp:autograph', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.1', 'comp:keras', 'stat:awaiting tensorflower', 'type:performance']\n",
      "['TF 2.1', 'comp:ops', 'type:feature']\n",
      "['TF 2.1', 'comp:ops', 'stat:contributions welcome', 'type:bug']\n",
      "['TF 2.1', 'comp:autograph', 'type:bug']\n",
      "['TF 2.1', 'comp:tensorboard', 'subtype:windows', 'type:build/install']\n",
      "['TF 2.1', 'comp:lite', 'type:bug']\n",
      "['TF 2.1', 'comp:apis', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.1', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.1', 'TFLiteConverter', 'comp:lite', 'type:bug']\n",
      "['TF 2.1', 'comp:ops', 'type:bug']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TF 1.15', 'TF 2.0', 'comp:tpus', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.1', 'comp:keras', 'type:bug']\n",
      "['TF 2.1', 'stat:awaiting tensorflower', 'subtype: raspberry pi', 'type:build/install']\n",
      "['comp:apis', 'comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.1', 'comp:lite', 'type:feature']\n",
      "['type:others']\n",
      "['TF 2.1', 'comp:apis', 'type:feature']\n",
      "['TF 2.1', 'comp:eager', 'comp:ops', 'type:performance']\n",
      "['TF 2.3', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['type:docs-feature']\n",
      "['comp:lite', 'comp:micro', 'type:feature']\n",
      "['TF 2.1', 'comp:gpu', 'comp:model', 'type:performance']\n",
      "['TF 2.1', 'comp:keras', 'type:docs-bug']\n",
      "['TF 2.1', 'comp:gpu:tensorrt', 'type:support']\n",
      "['TF 2.0', 'TF 2.2', 'comp:keras', 'type:performance']\n",
      "['TF 2.1', 'comp:runtime', 'type:performance']\n",
      "['comp:lite', 'comp:micro', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TF 2.0', 'comp:lite', 'comp:micro', 'type:support']\n",
      "['cla: yes', 'comp:keras', 'size:S', 'stat:awaiting response']\n",
      "['TF 2.1', 'TFLiteConverter', 'comp:lite', 'type:support']\n",
      "['TF 2.1', 'comp:tpus', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.1', 'comp:gpu:tensorrt', 'type:bug']\n",
      "['TF 2.1', 'comp:keras', 'type:bug']\n",
      "['stat:awaiting tensorflower', 'subtype:windows', 'type:build/install']\n",
      "['TF 2.1', 'comp:lite', 'subtype:windows', 'type:build/install']\n",
      "['TF 2.1', 'stat:awaiting tensorflower', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['TF 2.1', 'comp:dist-strat', 'type:bug']\n",
      "['TF 1.14', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.1', 'comp:data', 'type:bug']\n",
      "['TF 2.1', 'comp:apis', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.1', 'comp:gpu', 'type:bug']\n",
      "['TF 2.1', 'TFLiteConverter', 'comp:lite', 'type:bug']\n",
      "['cla: yes', 'comp:ops', 'prtype:bugfix', 'size:S', 'stat:awaiting tensorflower']\n",
      "['TF 2.3', 'comp:ops', 'type:bug']\n",
      "['comp:lite', 'comp:micro', 'comp:micro:espressif', 'type:support']\n",
      "['TF 2.1', 'comp:lite', 'stat:awaiting tensorflower', 'type:others']\n",
      "['TF 2.0', 'comp:ops', 'type:docs-feature']\n",
      "['TF 2.1', 'comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.1', 'comp:keras', 'type:bug']\n",
      "['TF 2.1', 'comp:gpu', 'type:performance']\n",
      "['TF 2.1', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['TF 2.1', 'comp:apis', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['TF 2.1', 'comp:keras', 'type:bug']\n",
      "['comp:lite', 'type:support']\n",
      "['TF 2.1', 'comp:dist-strat', 'type:bug']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.1', 'comp:data', 'stat:awaiting tensorflower', 'type:performance']\n",
      "['TF 2.1', 'TFLiteConverter', 'comp:lite', 'type:bug']\n",
      "['TF 2.1', 'comp:keras', 'type:bug']\n",
      "['TF 2.1', 'comp:ops', 'type:bug']\n",
      "['TF 2.1', 'comp:grappler', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.0', 'comp:keras', 'type:feature']\n",
      "['TF 2.1', 'comp:keras', 'type:bug']\n",
      "['TF 2.1', 'comp:lite', 'comp:micro', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TF 2.1', 'type:build/install']\n",
      "['TF 2.1', 'stat:awaiting tensorflower', 'subtype: raspberry pi', 'type:build/install']\n",
      "['TF 2.1', 'comp:keras', 'type:feature']\n",
      "['TF 2.0', 'comp:data', 'stat:awaiting tensorflower', 'type:performance']\n",
      "['TF 2.1', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.0', 'subtype:windows', 'type:build/install']\n",
      "['comp:keras', 'type:feature']\n",
      "['type:others']\n",
      "['TF 2.1', 'comp:ops', 'stat:awaiting tensorflower', 'type:performance']\n",
      "['TF 2.1', 'type:docs-bug']\n",
      "['TF 2.1', 'comp:keras', 'type:bug']\n",
      "['TF 2.1', 'comp:gpu', 'type:bug']\n",
      "['TF 2.1', 'subtype:macOS', 'type:build/install']\n",
      "['comp:micro', 'type:support']\n",
      "['TF 2.1', 'comp:keras', 'type:docs-bug']\n",
      "['TF 2.1', 'comp:gpu:tensorrt', 'type:bug']\n",
      "['TF 2.1', 'comp:gpu:tensorrt', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.1', 'comp:apis', 'comp:gpu:tensorrt', 'type:feature']\n",
      "['TF 2.3', 'comp:ops', 'type:bug']\n",
      "['TF 2.1', 'comp:lite', 'type:support']\n",
      "['TF 2.1', 'comp:lite', 'type:feature']\n",
      "['TF 2.1', 'comp:data', 'type:bug']\n",
      "['TF 2.1', 'comp:keras', 'type:feature']\n",
      "['TF 2.0', 'comp:keras', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TF 2.1', 'comp:gpu:tensorrt', 'type:bug']\n",
      "['TF 2.1', 'subtype: raspberry pi', 'type:build/install', 'type:feature']\n",
      "['TF 2.1', 'stat:awaiting tensorflower', 'subtype: raspberry pi', 'type:build/install']\n",
      "['comp:lite', 'comp:micro', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TF 2.1', 'comp:lite', 'comp:micro', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TF 2.1', 'comp:keras', 'type:bug']\n",
      "['TF 2.0', 'comp:data', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.1', 'comp:lite', 'type:bug']\n",
      "['awaiting review', 'cla: yes', 'comp:keras', 'size:M']\n",
      "['TF 2.1', 'comp:data', 'type:feature', 'type:performance']\n",
      "['TF 2.1', 'comp:ops', 'type:bug']\n",
      "['TF 2.1', 'comp:keras', 'type:bug']\n",
      "['TF 2.1', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['TF 2.1', 'comp:gpu:tensorrt', 'type:feature']\n",
      "['TF 2.0', 'comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.1', 'stat:awaiting tensorflower', 'type:build/install', 'type:feature']\n",
      "['TF 2.1', 'comp:data', 'type:bug']\n",
      "['stat:awaiting tensorflower', 'subtype:windows', 'type:build/install']\n",
      "['TF 2.1', 'comp:data', 'comp:gpu', 'type:bug']\n",
      "['TF 2.1', 'comp:apis', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['TF 2.0', 'comp:apis', 'type:performance']\n",
      "['type:build/install']\n",
      "['TF 2.1', 'comp:gpu', 'type:bug']\n",
      "['TF 2.1', 'comp:keras', 'type:bug']\n",
      "['TF 2.1', 'comp:keras', 'type:feature']\n",
      "['TF 1.15', 'TF 2.0', 'type:docs-bug']\n",
      "['TF 2.1', 'comp:keras', 'type:bug']\n",
      "['TF 2.1', 'comp:keras', 'type:bug']\n",
      "['TF 2.1', 'comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.1', 'comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['cla: yes', 'comp:ops', 'prtype:bugfix', 'size:XS', 'stat:awaiting tensorflower']\n",
      "['TF 2.1', 'comp:core', 'comp:gpu', 'comp:runtime', 'type:performance']\n",
      "['TF 2.0', 'comp:data', 'type:feature']\n",
      "['TF 2.0', 'stat:awaiting tensorflower', 'subtype:macOS', 'type:build/install']\n",
      "['comp:lite', 'comp:micro', 'type:feature']\n",
      "['TF 2.1', 'comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.1', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.0', 'comp:lite', 'stat:awaiting tensorflower', 'type:performance']\n",
      "['TF 2.1', 'comp:gpu', 'comp:gpu:tensorrt', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.1', 'comp:keras', 'type:feature']\n",
      "['TF 2.0', 'comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.1', 'comp:ops', 'type:feature']\n",
      "['TF 2.1', 'comp:core', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.1', 'comp:core', 'comp:ops', 'type:feature']\n",
      "['TF 2.1', 'comp:lite', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TF 2.1', 'comp:data', 'type:bug']\n",
      "['TF 2.1', 'comp:ops', 'type:bug']\n",
      "['TF 2.1', 'subtype:windows', 'type:build/install']\n",
      "['comp:lite', 'type:feature']\n",
      "['TF 1.15', 'stat:awaiting tensorflower', 'subtype:windows', 'type:build/install']\n",
      "['TF 2.1', 'comp:lite', 'type:bug']\n",
      "['TF 2.1', 'comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:lite', 'comp:micro', 'comp:micro:espressif', 'type:bug']\n",
      "['TF 2.1', 'comp:lite', 'type:bug']\n",
      "['TF 2.1', 'comp:ops', 'type:bug']\n",
      "['TF 2.1', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['TF 2.1', 'comp:ops', 'type:performance']\n",
      "['TF 2.0', 'subtype:windows', 'type:build/install']\n",
      "['TF 2.1', 'comp:tensorboard', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['stat:awaiting tensorflower', 'type:build/install']\n",
      "['comp:lite', 'comp:micro', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TF 2.1', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.1', 'comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['cla: yes', 'comp:keras', 'prtype:bugfix', 'size:XS', 'stat:awaiting tensorflower']\n",
      "['TF 2.0', 'comp:lite', 'type:support']\n",
      "['TF 2.1', 'comp:mkl', 'stat:awaiting tensorflower', 'subtype:windows', 'type:build/install']\n",
      "['TF 2.1', 'comp:keras', 'type:bug']\n",
      "['TF 2.0', 'comp:ops', 'type:performance']\n",
      "['TF 2.1', 'comp:gpu', 'type:bug']\n",
      "['TF 2.1', 'comp:runtime', 'type:bug']\n",
      "['TF 2.0', 'comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['stat:awaiting tensorflower', 'subtype:windows', 'type:build/install']\n",
      "['TF 2.1', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:lite', 'type:bug']\n",
      "['TF 2.1', 'subtype:windows', 'type:build/install']\n",
      "['TF 2.1', 'comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['type:feature']\n",
      "['TF 2.0', 'comp:apis', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.2', 'comp:data', 'type:bug']\n",
      "['TF 2.1', 'comp:lite', 'comp:micro', 'type:bug']\n",
      "['TF 2.1', 'comp:keras', 'type:bug']\n",
      "['TF 2.0', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:lite', 'comp:micro', 'type:bug']\n",
      "['TF 2.1', 'type:bug', 'type:build/install']\n",
      "['TF 2.1', 'subtype:centos', 'type:build/install']\n",
      "['TF 2.1', 'comp:keras', 'comp:tensorboard', 'type:bug']\n",
      "['TF 2.1', 'subtype:windows', 'type:build/install']\n",
      "['TF 2.1', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:lite', 'type:docs-bug']\n",
      "['TF 2.1', 'comp:keras', 'type:bug']\n",
      "['comp:lite', 'type:docs-bug']\n",
      "['comp:keras', 'type:feature']\n",
      "['TF 2.1', 'stat:awaiting tensorflower', 'subtype:windows', 'type:build/install']\n",
      "['comp:lite', 'comp:micro', 'type:support']\n",
      "['TF 2.1', 'comp:tpus', 'type:bug']\n",
      "['TF 2.1', 'comp:keras', 'type:bug']\n",
      "['stat:awaiting tensorflower', 'type:build/install']\n",
      "['TF 2.1', 'comp:keras', 'type:bug']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.1', 'TF 2.3', 'comp:keras', 'type:bug']\n",
      "['TF 2.1', 'comp:keras', 'type:bug']\n",
      "['TF 2.0', 'comp:lite', 'type:build/install']\n",
      "['TF 2.0', 'comp:ops', 'type:feature']\n",
      "['TF 2.0', 'comp:ops', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['TF 2.0', 'TF 2.1', 'comp:ops', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['TF 2.0', 'stat:awaiting tensorflower', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['TF 2.1', 'comp:keras', 'type:bug']\n",
      "['TF 2.0', 'comp:keras', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['TF 2.0', 'comp:keras', 'type:bug']\n",
      "['comp:lite', 'type:support']\n",
      "['TF 2.0', 'comp:data', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.0', 'comp:ops', 'type:docs-bug']\n",
      "['TF 2.0', 'comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.1', 'comp:keras', 'stat:awaiting tensorflower', 'type:performance']\n",
      "['TF 2.0', 'subtype:macOS', 'type:build/install']\n",
      "['TF 2.0', 'stat:awaiting tensorflower', 'subtype:centos', 'type:build/install']\n",
      "['TF 2.2', 'comp:core', 'type:bug']\n",
      "['comp:lite', 'type:bug']\n",
      "['comp:lite', 'type:bug']\n",
      "['TF 2.1', 'comp:runtime', 'type:bug']\n",
      "['TF 2.1', 'comp:gpu', 'comp:ops', 'type:bug']\n",
      "['TF 2.0', 'comp:keras', 'stat:awaiting tensorflower', 'type:support']\n",
      "[]\n",
      "['TF 2.1', 'comp:dist-strat', 'type:performance']\n",
      "['comp:lite', 'type:bug']\n",
      "['TF 2.0', 'subtype:windows', 'type:build/install']\n",
      "['TF 1.15', 'TF 2.0', 'comp:dist-strat', 'type:feature']\n",
      "['comp:lite', 'type:bug']\n",
      "['TF 2.1', 'type:support']\n",
      "['TF 1.15', 'TF 2.0', 'comp:keras', 'type:bug']\n",
      "['TF 2.1', 'comp:dist-strat', 'type:performance']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.0', 'comp:dist-strat', 'type:bug']\n",
      "['TF 2.0', 'comp:keras', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:lite', 'comp:micro', 'type:support']\n",
      "[]\n",
      "['TF 2.1', 'comp:lite', 'type:performance']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.0', 'stat:awaiting tensorflower', 'subtype:windows', 'type:build/install']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['TF 2.0', 'comp:apis', 'comp:keras', 'type:feature']\n",
      "['TF 2.0', 'comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:lite', 'type:support']\n",
      "['comp:micro', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TF 2.0', 'comp:lite', 'type:feature']\n",
      "['comp:lite', 'type:bug']\n",
      "['TF 2.1', 'comp:lite', 'type:bug']\n",
      "['TF 2.1', 'comp:lite', 'type:feature']\n",
      "['TF 2.0', 'subtype:windows', 'type:build/install']\n",
      "['TF 2.0', 'comp:lite', 'type:performance']\n",
      "['TF 1.13', 'subtype: ubuntu/linux', 'type:build/install', 'type:performance']\n",
      "['TF 2.1', 'comp:runtime', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.0', 'comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.1', 'comp:keras', 'type:docs-bug']\n",
      "['comp:micro', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TF 2.0', 'comp:keras', 'type:bug']\n",
      "['TF 2.0', 'comp:data', 'stat:contributions welcome', 'type:feature']\n",
      "['TF 2.0', 'comp:apis', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['comp:lite', 'comp:micro', 'stat:awaiting response', 'type:docs-bug']\n",
      "['comp:lite', 'comp:micro']\n",
      "['comp:micro']\n",
      "['TF 2.0', 'comp:ops', 'stat:awaiting response', 'type:docs-bug']\n",
      "['TF 2.0', 'comp:apis', 'type:docs-bug']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:support']\n",
      "['subtype: ubuntu/linux', 'type:build/install']\n",
      "['TF 2.3', 'comp:keras', 'type:performance']\n",
      "['TF 2.0', 'comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.0', 'comp:lite', 'type:bug']\n",
      "['TF 2.0', 'subtype:centos', 'type:build/install']\n",
      "['TF 2.1', 'comp:core', 'comp:ops', 'type:bug']\n",
      "['TF 2.1', 'comp:ops', 'type:bug']\n",
      "['TF 2.0', 'comp:eager', 'type:performance']\n",
      "['TF 2.2', 'comp:keras', 'type:bug']\n",
      "['comp:keras', 'stat:contributions welcome', 'type:bug']\n",
      "['comp:lite', 'type:support']\n",
      "['TF 2.0', 'stat:awaiting tensorflower', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['TF 2.0', 'comp:keras', 'type:docs-bug']\n",
      "['TF 2.0', 'comp:apis', 'stat:contributions welcome', 'type:feature']\n",
      "['TF 2.0', 'comp:model', 'type:feature']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TF 2.0', 'comp:keras', 'type:feature']\n",
      "['TF 2.1', 'stat:awaiting tensorflower', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['TF 1.15', 'TF 2.0', 'stat:awaiting tensorflower', 'subtype:bazel', 'type:build/install']\n",
      "[]\n",
      "['TF 2.0', 'comp:ops', 'type:bug']\n",
      "['TF 2.2', 'comp:keras', 'type:bug']\n",
      "[]\n",
      "['TF 2.2', 'comp:keras', 'type:bug']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'type:performance']\n",
      "['TF 2.0', 'comp:gpu', 'type:bug']\n",
      "['stat:awaiting tensorflower', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['TF 2.0', 'comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:lite']\n",
      "['comp:lite', 'stat:awaiting response', 'type:docs-bug']\n",
      "['TF 2.0', 'stat:awaiting tensorflower', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "[]\n",
      "['TF 2.0', 'comp:keras', 'type:docs-bug']\n",
      "['TF 2.0', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['TF 2.0', 'comp:gpu', 'type:bug']\n",
      "['TF 2.0', 'comp:autograph', 'type:bug']\n",
      "['TF 2.0', 'comp:gpu', 'comp:gpu:tensorrt', 'type:bug']\n",
      "['TF 2.0', 'comp:lite', 'type:support']\n",
      "['TF 2.0', 'comp:lite', 'type:bug']\n",
      "['TF 2.2', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.0', 'comp:lite', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.0', 'comp:keras', 'type:bug']\n",
      "['TF 1.15', 'TF 2.0', 'comp:keras', 'type:feature']\n",
      "['comp:lite', 'type:support']\n",
      "[]\n",
      "['TF 2.0', 'comp:lite', 'type:bug']\n",
      "['TF 2.0', 'comp:runtime', 'type:support']\n",
      "['TF 2.0', 'comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['subtype: ubuntu/linux', 'type:build/install']\n",
      "['TF 2.0', 'comp:lite', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TF 2.0', 'comp:ops', 'stat:awaiting response', 'type:docs-bug']\n",
      "['TF 2.0', 'comp:data', 'type:bug']\n",
      "['TF 2.0', 'comp:apis', 'type:support']\n",
      "['TF 2.0', 'comp:dist-strat', 'comp:tpus', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 1.14', 'subtype:windows', 'type:build/install']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "[]\n",
      "['TF 2.1', 'comp:core', 'comp:ops', 'comp:runtime', 'type:performance']\n",
      "['TF 2.2', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.0', 'comp:keras', 'type:bug']\n",
      "['TF 2.1', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['TF 2.0', 'comp:lite', 'type:feature']\n",
      "['TF 2.0', 'comp:lite', 'type:docs-bug']\n",
      "['TF 2.0', 'comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.0', 'comp:runtime', 'type:bug']\n",
      "['TF 2.0', 'stat:awaiting tensorflower', 'subtype:centos', 'type:build/install']\n",
      "['TF 2.0', 'comp:keras', 'type:bug']\n",
      "['TF 2.0', 'comp:lite', 'type:support']\n",
      "['TF 2.0', 'comp:lite', 'type:bug']\n",
      "['TF 2.0', 'comp:dist-strat', 'comp:tpus', 'type:bug']\n",
      "['TF 2.0', 'comp:ops', 'type:bug']\n",
      "['TF 2.0', 'comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.0', 'comp:gpu', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['TF 1.15', 'comp:dist-strat', 'comp:tpus', 'type:feature']\n",
      "['comp:lite']\n",
      "['TF 2.0', 'TF 2.1', 'TF 2.2', 'comp:keras', 'type:bug']\n",
      "['TF 2.0', 'comp:keras', 'comp:ops', 'type:bug']\n",
      "['TF 2.0', 'comp:lite', 'type:support']\n",
      "['TF 2.0', 'comp:lite', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.0', 'comp:gpu', 'comp:ops', 'type:bug']\n",
      "['TF 2.0', 'comp:keras', 'type:docs-bug']\n",
      "['stat:awaiting tensorflower', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['TF 2.2', 'comp:eager', 'type:bug']\n",
      "['TF 2.0', 'comp:lite', 'type:bug']\n",
      "['TF 2.0', 'comp:core', 'type:feature']\n",
      "['TF 2.1', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 1.15', 'TF 2.2', 'comp:ops', 'type:bug']\n",
      "['TF 2.0', 'comp:lite', 'type:bug']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.0', 'comp:eager', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.0', 'comp:lite', 'type:bug']\n",
      "['TF 2.0', 'stat:awaiting tensorflower', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['TF 2.0', 'comp:keras', 'type:bug']\n",
      "['TF 2.0', 'comp:apis', 'type:bug']\n",
      "['TF 2.0', 'comp:runtime', 'type:support']\n",
      "['TF 2.1', 'stat:awaiting tensorflower', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['TF 2.0', 'TF 2.2', 'comp:keras', 'type:bug']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TF 2.0', 'comp:apis', 'type:support']\n",
      "['TF 2.0', 'comp:gpu', 'comp:gpu:tensorrt', 'type:bug']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TF 2.0', 'comp:keras', 'type:feature']\n",
      "['TF 2.0', 'comp:gpu', 'type:bug']\n",
      "['TF 2.0', 'comp:keras', 'type:bug']\n",
      "['TF 2.0', 'comp:gpu', 'type:bug']\n",
      "['TF 2.0', 'comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.0', 'comp:lite', 'type:support']\n",
      "['TF 2.0', 'comp:gpu', 'comp:ops', 'type:bug']\n",
      "['TF 2.0', 'TF 2.2', 'comp:keras', 'type:bug']\n",
      "['TF 2.0', 'comp:data', 'type:feature']\n",
      "['TF 2.0', 'comp:keras', 'type:feature']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.0', 'comp:lite', 'type:feature']\n",
      "['TF 2.0', 'comp:keras', 'type:bug']\n",
      "['TF 2.0', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['TF 2.0', 'comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 1.15', 'comp:keras', 'stat:contributions welcome', 'type:bug']\n",
      "['TF 2.0', 'comp:mkl', 'stat:awaiting tensorflower', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['TF 2.0', 'comp:keras', 'comp:tensorboard', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.0', 'comp:keras', 'type:bug']\n",
      "['TF 2.0', 'comp:keras', 'stat:contributions welcome', 'type:docs-bug']\n",
      "['TF 2.0', 'type:build/install', 'type:docs-bug']\n",
      "['TF 2.0', 'comp:runtime', 'stat:community support', 'type:feature']\n",
      "['TF 2.0', 'comp:lite', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TF 2.0', 'comp:keras', 'type:feature']\n",
      "[]\n",
      "['TF 2.0', 'comp:lite', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:runtime', 'type:bug']\n",
      "['TF 2.2', 'comp:keras', 'type:bug']\n",
      "['TF 2.0', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.0', 'TF 2.1', 'TF 2.2', 'TF 2.3', 'comp:keras', 'type:bug']\n",
      "['TF 2.0', 'comp:apis', 'type:docs-bug']\n",
      "[]\n",
      "[]\n",
      "['type:bug']\n",
      "['TF 2.0', 'comp:keras', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['TF 2.0', 'comp:apis', 'type:feature']\n",
      "['comp:keras', 'type:feature']\n",
      "['TF 2.0', 'comp:apis', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['TF 2.0', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.0', 'comp:keras', 'stat:contributions welcome', 'type:bug']\n",
      "['TF 2.0', 'comp:keras', 'comp:model', 'type:feature']\n",
      "['TF 2.0', 'comp:keras', 'type:feature']\n",
      "['TF 2.0', 'comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.0', 'stat:awaiting tensorflower', 'subtype:windows', 'type:build/install']\n",
      "['TF 2.0', 'comp:lite', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:lite', 'comp:micro', 'type:bug']\n",
      "['TF 2.0', 'comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.0', 'comp:keras', 'type:feature']\n",
      "['TF 2.0', 'comp:dist-strat', 'type:performance']\n",
      "['TF 2.0', 'comp:lite', 'type:support']\n",
      "['TF 2.0', 'stat:awaiting tensorflower', 'subtype:windows', 'type:build/install']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:support']\n",
      "['comp:lite', 'type:support']\n",
      "['TF 2.0', 'type:build/install', 'type:feature']\n",
      "['TF 2.0', 'comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['cla: yes', 'prtype:bugfix', 'ready to pull', 'size:M']\n",
      "['TF 2.0', 'comp:dist-strat', 'comp:tpus', 'type:bug']\n",
      "['TF 2.0', 'comp:keras', 'comp:tensorboard', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.0', 'comp:keras', 'type:feature']\n",
      "['TF 2.0', 'comp:lite', 'type:feature']\n",
      "['TF 2.0', 'comp:gpu', 'type:feature']\n",
      "['comp:ops', 'type:feature']\n",
      "['TF 2.0', 'comp:lite', 'type:bug']\n",
      "['comp:lite', 'type:support']\n",
      "['comp:lite', 'comp:micro']\n",
      "['comp:lite']\n",
      "['TF 2.0', 'stat:awaiting tensorflower', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['TF 2.0', 'comp:keras', 'type:performance']\n",
      "['comp:lite', 'type:support']\n",
      "['TF 2.0', 'stat:contributions welcome', 'type:build/install', 'type:feature']\n",
      "['TF 2.0', 'comp:lite', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TF 2.0', 'comp:keras', 'type:support']\n",
      "['TF 2.0', 'comp:keras', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.0', 'comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 1.15', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['cla: yes', 'prtype:bugfix', 'size:S', 'stat:awaiting tensorflower']\n",
      "['TF 2.0', 'comp:ops', 'type:support']\n",
      "['TF 2.0', 'comp:lite', 'type:bug']\n",
      "['TF 2.0', 'stat:awaiting tensorflower', 'subtype:windows', 'type:build/install']\n",
      "['TF 2.0', 'comp:keras', 'type:bug']\n",
      "['comp:lite', 'type:bug']\n",
      "['TF 2.0', 'comp:tfdbg', 'type:support']\n",
      "['TF 2.0', 'comp:keras', 'comp:tensorboard', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.2', 'comp:autograph', 'comp:core', 'type:bug']\n",
      "['TF 2.2', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.0', 'comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.0', 'comp:tfdbg', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.0', 'subtype:bazel', 'type:build/install']\n",
      "['TF 2.0']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.0', 'comp:ops', 'type:feature']\n",
      "['TF 2.0', 'comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.0', 'comp:xla', 'subtype:windows', 'type:build/install']\n",
      "['TF 2.0', 'subtype:windows', 'type:build/install']\n",
      "['TF 2.0', 'comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "[]\n",
      "['TF 2.0', 'comp:data', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['TF 2.0', 'comp:lite', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TF 2.0', 'comp:tensorboard', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TF 2.0', 'comp:data', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.0', 'comp:apis', 'stat:contributions welcome', 'type:feature']\n",
      "['comp:runtime', 'type:support']\n",
      "['comp:lite', 'type:support']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TF 2.0', 'comp:data', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.0', 'comp:tfdbg', 'type:feature']\n",
      "['TF 2.0', 'comp:keras', 'type:bug']\n",
      "['comp:lite', 'type:feature']\n",
      "['TF 2.0', 'comp:gpu', 'comp:xla', 'type:bug']\n",
      "['comp:lite', 'type:support']\n",
      "['TF 2.2', 'comp:keras', 'comp:ops', 'type:bug']\n",
      "['TF 2.0', 'comp:keras', 'type:bug']\n",
      "['comp:lite', 'type:bug']\n",
      "['comp:lite', 'type:bug']\n",
      "['type:build/install', 'type:feature']\n",
      "['TF 2.0', 'comp:ops', 'type:feature']\n",
      "['TF 2.0', 'comp:apis', 'type:bug']\n",
      "['subtype:bazel', 'type:build/install']\n",
      "['TF 2.2', 'comp:autograph', 'stat:contributions welcome', 'type:bug']\n",
      "['TF 2.0', 'comp:keras', 'stat:contributions welcome', 'type:feature']\n",
      "['TF 2.0', 'comp:lite', 'stat:awaiting tensorflower', 'type:bug']\n",
      "[]\n",
      "['TF 1.14', 'TF 2.0', 'comp:lite', 'type:support']\n",
      "['TF 2.0', 'comp:autograph', 'comp:core', 'type:feature']\n",
      "['TF 2.2', 'comp:core', 'comp:data', 'comp:eager', 'comp:runtime', 'type:bug']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:bug']\n",
      "[]\n",
      "['TF 2.0', 'comp:ops', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['TF 2.0', 'comp:keras', 'type:bug']\n",
      "['comp:lite', 'type:bug']\n",
      "['subtype: ubuntu/linux', 'type:build/install']\n",
      "['TF 2.0', 'TF 2.3', 'comp:keras', 'type:bug']\n",
      "[]\n",
      "['TF 2.0', 'comp:dist-strat', 'comp:tpus', 'type:bug']\n",
      "['subtype: ubuntu/linux', 'type:build/install']\n",
      "['comp:keras', 'type:feature']\n",
      "['TF 2.0', 'comp:lite', 'type:support']\n",
      "['comp:dist-strat', 'type:performance']\n",
      "['TF 2.0', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "[]\n",
      "['TF 2.0', 'comp:dist-strat', 'type:bug']\n",
      "['comp:ops', 'type:bug']\n",
      "['type:build/install', 'type:docs-bug']\n",
      "['TF 2.0', 'comp:ops', 'type:bug']\n",
      "['TF 2.0', 'comp:apis', 'type:bug']\n",
      "['TF 1.14', 'comp:runtime']\n",
      "['TF 2.0', 'comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 1.14', 'comp:lite', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TF 2.0', 'comp:runtime', 'type:bug']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.0', 'comp:dist-strat', 'type:bug']\n",
      "['comp:lite', 'type:support']\n",
      "['comp:lite', 'type:performance']\n",
      "['cla: yes', 'size:S']\n",
      "['comp:ops', 'type:bug']\n",
      "['awaiting review', 'cla: yes', 'comp:ops', 'size:M']\n",
      "['comp:lite', 'type:docs-bug']\n",
      "['comp:data', 'type:feature']\n",
      "['TF 2.0', 'comp:apis', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:lite', 'type:support']\n",
      "['TF 2.0', 'comp:ops', 'comp:runtime', 'stat:contributions welcome', 'type:performance']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TF 2.0', 'comp:autograph', 'stat:contributions welcome', 'type:feature']\n",
      "['comp:lite', 'type:bug']\n",
      "['TF 2.0', 'comp:ops', 'type:feature']\n",
      "['TF 2.0', 'comp:ops', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['TF 2.0', 'comp:tensorboard', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:gpu', 'comp:gpu:tensorrt', 'type:bug']\n",
      "['comp:apis', 'type:support']\n",
      "['TF 2.0', 'comp:lite', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:lite', 'type:support']\n",
      "['subtype: ubuntu/linux', 'type:build/install']\n",
      "['TF 2.0', 'TF 2.1', 'TF 2.2', 'TF 2.3', 'comp:ops', 'type:bug']\n",
      "['TF 2.0', 'comp:gpu', 'type:feature']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['type:build/install', 'type:feature']\n",
      "['TF 2.0', 'comp:keras', 'type:feature']\n",
      "['TF 2.0', 'comp:ops', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['TF 2.0', 'comp:apis', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['TF 2.0', 'comp:ops', 'type:docs-bug']\n",
      "['stat:awaiting tensorflower', 'type:others']\n",
      "['TF 2.0', 'subtype:macOS', 'type:build/install']\n",
      "['TF 1.13', 'comp:gpu', 'comp:gpu:tensorrt', 'stat:contributions welcome', 'type:feature']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:lite', 'type:support']\n",
      "['TF 2.0', 'comp:keras', 'comp:tensorboard', 'type:feature']\n",
      "['TF 2.0', 'comp:data', 'type:bug']\n",
      "['TF 2.0', 'comp:keras', 'type:feature']\n",
      "['TF 1.13', 'comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 1.14', 'comp:ops', 'stat:contributions welcome', 'type:feature']\n",
      "['comp:gpu', 'type:feature']\n",
      "['TF 1.13', 'comp:keras', 'comp:tensorboard', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:gpu', 'comp:gpu:tensorrt', 'type:bug']\n",
      "['type:feature']\n",
      "['TF 2.3', 'comp:keras', 'stat:awaiting tensorflower', 'stat:contributions welcome', 'type:bug']\n",
      "['TF 1.12', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['TF 1.14', 'comp:ops', 'type:build/install']\n",
      "[]\n",
      "['comp:lite', 'type:performance']\n",
      "['TF 2.0', 'comp:apis', 'comp:dist-strat', 'stat:contributions welcome', 'type:bug']\n",
      "['stat:awaiting tensorflower', 'type:build/install']\n",
      "['comp:lite']\n",
      "['TF 1.15', 'TF 2.0', 'comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['subtype: raspberry pi', 'type:build/install']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:support']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['TF 2.0', 'subtype:windows', 'type:build/install']\n",
      "['TF 2.0', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['comp:apis', 'stat:contributions welcome', 'type:bug']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:lite', 'type:support']\n",
      "['comp:lite', 'type:support']\n",
      "['TF 2.0', 'comp:gpu', 'type:bug']\n",
      "['TF 2.2', 'comp:eager', 'comp:ops', 'type:bug']\n",
      "['comp:lite', 'type:support']\n",
      "['comp:gpu', 'type:support']\n",
      "['comp:runtime', 'type:docs-feature']\n",
      "['comp:lite', 'type:support']\n",
      "['TF 2.0', 'comp:dist-strat', 'type:bug']\n",
      "['TF 2.0', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:tpus', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['comp:lite', 'comp:model', 'type:support']\n",
      "['TF 2.0', 'comp:dist-strat', 'type:bug']\n",
      "['subtype:windows', 'type:build/install']\n",
      "['comp:runtime', 'type:support']\n",
      "['TF 2.0', 'comp:apis', 'type:feature']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:support']\n",
      "['stat:awaiting tensorflower', 'type:build/install', 'type:docs-bug']\n",
      "['stat:awaiting tensorflower', 'subtype: raspberry pi', 'type:build/install']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['TF 2.2', 'comp:eager', 'type:bug']\n",
      "['comp:ops', 'type:feature']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.0', 'comp:ops', 'stat:contributions welcome', 'type:feature', 'type:support']\n",
      "['subtype:windows', 'type:build/install']\n",
      "['comp:lite', 'comp:model', 'stat:awaiting tensorflower', 'type:support']\n",
      "['comp:ops', 'type:feature']\n",
      "['comp:ops', 'type:support']\n",
      "['type:feature']\n",
      "['subtype: ubuntu/linux', 'type:build/install']\n",
      "['comp:apis', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "[]\n",
      "['subtype:windows', 'type:build/install']\n",
      "['TF 1.14', 'comp:xla', 'type:bug']\n",
      "['comp:lite', 'type:bug']\n",
      "['comp:lite', 'type:support']\n",
      "['comp:ops', 'type:bug']\n",
      "['subtype:windows', 'type:build/install']\n",
      "['comp:ops', 'type:feature']\n",
      "['comp:lite', 'type:support']\n",
      "['comp:ops', 'type:feature']\n",
      "['comp:lite', 'type:support']\n",
      "['TF 1.13', 'comp:keras', 'type:feature']\n",
      "['comp:dist-strat', 'type:bug']\n",
      "['comp:runtime', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:lite', 'type:docs-bug']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:build/install', 'type:support']\n",
      "['comp:ops', 'stat:contributions welcome', 'type:feature']\n",
      "['comp:ops']\n",
      "['comp:apis', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:ops', 'type:feature']\n",
      "['comp:lite', 'type:support']\n",
      "['TF 2.0', 'comp:runtime', 'stat:awaiting tensorflower', 'type:bug']\n",
      "[]\n",
      "['comp:lite', 'type:bug']\n",
      "['comp:model', 'stat:awaiting tensorflower', 'type:support']\n",
      "['comp:keras', 'comp:tpus', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.0', 'comp:keras', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['comp:lite', 'type:feature']\n",
      "['TF 2.0', 'comp:data', 'type:docs-bug']\n",
      "['TF 2.0', 'comp:model', 'type:bug']\n",
      "['TF 2.0', 'comp:ops', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['TF 2.0', 'comp:ops', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['TF 2.0', 'comp:keras', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['TF 2.0', 'comp:ops', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['TF 2.0', 'comp:data', 'type:docs-bug']\n",
      "['TF 2.0', 'comp:tfdbg', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['TF 2.0', 'comp:data', 'type:docs-bug']\n",
      "['TF 2.0', 'comp:ops', 'type:docs-bug']\n",
      "['comp:gpu', 'type:feature']\n",
      "['TF 1.13', 'comp:gpu', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['TF 2.0', 'subtype:windows', 'type:build/install']\n",
      "[]\n",
      "['TF 2.0', 'comp:apis', 'type:docs-bug']\n",
      "['TF 2.0', 'comp:apis', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['TF 2.0', 'comp:keras', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['comp:apis', 'type:feature']\n",
      "['TF 2.0', 'comp:ops', 'type:feature']\n",
      "['TF 1.13', 'comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:lite', 'type:support']\n",
      "['TF 1.13', 'comp:model', 'type:support']\n",
      "['TF 2.0', 'comp:apis', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.0', 'subtype: raspberry pi', 'type:build/install']\n",
      "['TF 1.13', 'comp:tpus', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:support']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TF 2.0', 'comp:keras', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['stat:community support', 'stat:contributions welcome', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['TF 2.0', 'comp:data', 'type:docs-bug']\n",
      "['TF 2.0', 'comp:ops', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['TF 2.0', 'comp:data', 'type:docs-bug']\n",
      "['TF 2.0', 'comp:ops', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['TF 2.0', 'comp:ops', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['TF 2.0', 'comp:ops', 'type:docs-bug']\n",
      "['comp:ops', 'type:bug']\n",
      "['comp:xla', 'type:feature']\n",
      "['TF 1.12', 'comp:gpu', 'type:bug']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['comp:dist-strat', 'type:feature']\n",
      "['comp:lite']\n",
      "['comp:data', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TF 1.13', 'comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['TF 2.0', 'comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.0', 'comp:ops', 'type:docs-bug']\n",
      "['comp:lite', 'type:support']\n",
      "['comp:runtime', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TF 2.0', 'comp:keras', 'stat:contributions welcome', 'type:docs-bug']\n",
      "['comp:tfdbg', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.0', 'comp:data', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['TF 2.0', 'comp:ops', 'type:docs-bug']\n",
      "['TF 2.0', 'comp:ops', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['comp:apis', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['TF 2.0', 'comp:data', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['TF 2.0', 'comp:ops', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['comp:runtime', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.0', 'comp:data', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['TF 2.0', 'comp:apis', 'type:docs-bug']\n",
      "['TF 2.0', 'comp:data', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['TF 2.0', 'comp:ops', 'type:docs-bug']\n",
      "['TF 2.0', 'comp:ops', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['TF 2.0', 'comp:ops', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['TF 2.0', 'comp:ops', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['TF 2.0', 'comp:ops', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['TF 2.0', 'comp:ops', 'type:docs-bug']\n",
      "['TF 2.0', 'comp:ops', 'type:docs-bug']\n",
      "['comp:lite', 'type:support']\n",
      "['comp:runtime', 'type:support']\n",
      "['TF 2.0', 'comp:ops', 'type:docs-bug']\n",
      "['TF 1.13', 'comp:lite', 'type:bug']\n",
      "['TF 2.3', 'comp:ops', 'type:bug']\n",
      "['TF 1.13', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 1.13', 'comp:lite', 'type:bug']\n",
      "['TF 2.0', 'comp:apis', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:lite', 'type:bug']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['TF 2.0', 'comp:eager', 'comp:ops', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['TF 1.13', 'stat:contributions welcome', 'subtype:centos', 'type:build/install']\n",
      "['comp:gpu', 'stat:contributions welcome', 'type:feature']\n",
      "['comp:ops', 'type:feature']\n",
      "['stat:awaiting tensorflower', 'type:docs-bug', 'type:support']\n",
      "['TF 1.13', 'comp:apis', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 1.13', 'comp:lite', 'type:bug']\n",
      "['TF 2.0', 'comp:data', 'stat:contributions welcome', 'type:feature']\n",
      "['TF 2.0', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['TF 1.13', 'comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 1.13', 'comp:lite', 'type:bug']\n",
      "['comp:lite', 'type:support']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:support']\n",
      "['comp:gpu', 'comp:gpu:tensorrt', 'type:support']\n",
      "['TF 1.13', 'comp:grappler', 'subtype:macOS', 'type:build/install']\n",
      "['comp:gpu', 'comp:gpu:tensorrt', 'type:bug']\n",
      "['TF 1.13', 'comp:lite', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.0', 'comp:ops', 'stat:contributions welcome', 'type:feature']\n",
      "['TF 1.13', 'comp:lite', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 1.13', 'comp:lite', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:keras', 'type:bug']\n",
      "['TF 2.0', 'type:docs-bug']\n",
      "['comp:lite', 'type:bug']\n",
      "['TF 1.13', 'comp:ops', 'type:bug']\n",
      "['TF 1.13', 'comp:gpu', 'comp:runtime', 'type:support']\n",
      "['type:others']\n",
      "['TF 1.13', 'comp:eager', 'comp:ops', 'type:bug']\n",
      "['subtype:windows', 'type:build/install']\n",
      "['comp:lite', 'type:bug']\n",
      "['TF 2.0', 'comp:grappler', 'type:bug']\n",
      "['API review', 'TF 2.0', 'comp:apis', 'type:support']\n",
      "['comp:lite', 'type:bug']\n",
      "['TF 1.13', 'comp:tfdbg', 'stat:awaiting tensorflower', 'type:support']\n",
      "['comp:gpu']\n",
      "['TF 2.0', 'comp:grappler', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:ops', 'contrib', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:lite', 'type:feature']\n",
      "['comp:gpu', 'stat:contributions welcome', 'type:bug']\n",
      "['comp:ops', 'type:bug']\n",
      "[]\n",
      "['comp:grappler', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 1.12', 'comp:runtime', 'type:support']\n",
      "['subtype:macOS', 'type:build/install', 'type:feature']\n",
      "['comp:apis', 'type:feature']\n",
      "['comp:lite', 'type:bug']\n",
      "['comp:lite', 'type:bug']\n",
      "['TF 2.0', 'comp:runtime', 'type:bug']\n",
      "['TF 1.13', 'comp:keras', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:xla', 'subtype:windows', 'type:build/install']\n",
      "['comp:grappler', 'comp:lite', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.0', 'comp:tpus', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:runtime', 'type:feature']\n",
      "['TF 2.0', 'comp:ops', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['TF 2.0', 'comp:apis', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['TF 2.0', 'comp:apis', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:ops', 'type:feature']\n",
      "['contrib', 'stat:contributions welcome', 'type:feature']\n",
      "['comp:keras', 'comp:tpus', 'type:bug']\n",
      "['TF 2.0', 'comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:keras', 'type:bug']\n",
      "['comp:lite', 'type:bug']\n",
      "['TF 2.0', 'comp:ops', 'stat:awaiting tensorflower', 'type:docs-bug', 'type:feature']\n",
      "['comp:lite', 'type:feature']\n",
      "['TF 2.0', 'comp:keras', 'comp:tensorboard', 'type:feature']\n",
      "['TF 2.0', 'comp:keras', 'type:bug']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:gpu', 'type:bug']\n",
      "['awaiting review', 'cla: yes', 'size:S']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:support']\n",
      "['comp:runtime', 'type:bug']\n",
      "['comp:runtime', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:keras', 'type:bug']\n",
      "['comp:data', 'stat:contributions welcome', 'type:feature']\n",
      "['comp:runtime', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['type:feature']\n",
      "['comp:gpu', 'type:feature']\n",
      "['comp:ops', 'stat:contributions welcome', 'type:feature']\n",
      "['comp:apis', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:data', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['TF 1.13', 'comp:eager', 'type:bug']\n",
      "['comp:lite', 'type:feature']\n",
      "['comp:lite', 'type:support']\n",
      "['comp:mkl', 'stat:awaiting tensorflower', 'type:build/install']\n",
      "['comp:ops', 'type:bug', 'type:docs-bug']\n",
      "[]\n",
      "['stat:awaiting tensorflower', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:dist-strat', 'type:feature']\n",
      "['comp:dist-strat', 'type:feature']\n",
      "['comp:dist-strat', 'type:feature']\n",
      "['comp:lite', 'type:bug']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.2', 'comp:ops', 'type:bug']\n",
      "['subtype: raspberry pi', 'type:build/install']\n",
      "['comp:ops', 'type:support']\n",
      "['TF 2.0', 'comp:ops', 'type:feature']\n",
      "['comp:runtime', 'type:bug']\n",
      "['TF 2.0', 'comp:ops', 'stat:contributions welcome', 'type:feature']\n",
      "['comp:lite', 'type:feature']\n",
      "['stat:contributions welcome', 'subtype: ubuntu/linux', 'type:build/install', 'type:feature']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:lite', 'type:support']\n",
      "['comp:grappler', 'stat:contributions welcome', 'type:bug']\n",
      "['comp:lite', 'type:bug']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:docs-feature']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.0', 'comp:apis', 'type:bug']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:ops', 'type:bug']\n",
      "['comp:model', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.0', 'comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "[]\n",
      "['comp:tfdbg', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:support']\n",
      "['comp:grappler', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:ops', 'stat:contributions welcome', 'type:bug']\n",
      "['comp:gpu', 'stat:contributions welcome', 'type:bug']\n",
      "['comp:lite', 'type:feature']\n",
      "['comp:data', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:runtime', 'stat:awaiting tensorflower', 'subtype:windows', 'type:bug']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:ops', 'type:docs-bug']\n",
      "['comp:runtime', 'type:bug']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.0', 'comp:ops', 'type:feature']\n",
      "['TF 2.0', 'comp:data', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['comp:lite']\n",
      "['comp:ops', 'type:bug']\n",
      "['comp:lite', 'comp:signal', 'type:feature']\n",
      "['comp:tpus', 'contrib']\n",
      "['comp:ops', 'type:bug']\n",
      "['comp:apis', 'comp:runtime', 'type:bug']\n",
      "['comp:gpu', 'type:bug']\n",
      "['TF 1.12', 'comp:ops', 'type:bug']\n",
      "['comp:ops', 'type:bug']\n",
      "['subtype:windows', 'type:build/install']\n",
      "['comp:apis', 'type:bug']\n",
      "['comp:lite', 'type:support']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:support']\n",
      "['comp:lite', 'type:feature']\n",
      "['TF 2.0', 'comp:keras']\n",
      "['comp:ops', 'comp:signal', 'type:feature']\n",
      "['comp:keras', 'type:feature']\n",
      "['comp:gpu', 'comp:gpu:tensorrt', 'type:support']\n",
      "['contrib', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['comp:lite', 'comp:runtime', 'type:bug']\n",
      "['stat:awaiting tensorflower', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['type:docs-bug']\n",
      "['TF 2.0', 'comp:ops', 'type:bug']\n",
      "['TF 2.0', 'comp:model', 'type:feature']\n",
      "['comp:apis', 'contrib', 'type:bug']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:ops', 'type:bug']\n",
      "['comp:apis', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:lite', 'type:bug']\n",
      "['comp:apis', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['comp:data', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:dist-strat', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.0', 'type:feature']\n",
      "['TF 2.0', 'comp:model', 'type:docs-bug']\n",
      "['TF 2.0', 'comp:ops', 'type:docs-bug']\n",
      "['comp:ops', 'type:feature']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:runtime', 'type:feature']\n",
      "['comp:apis', 'type:bug']\n",
      "['comp:xla']\n",
      "['subtype: ubuntu/linux', 'type:build/install']\n",
      "['comp:tensorboard', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:grappler', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:runtime', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:ops', 'stat:contributions welcome', 'type:feature']\n",
      "['comp:lite', 'type:bug']\n",
      "['comp:apis', 'type:bug']\n",
      "['comp:grappler', 'comp:keras', 'type:bug']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TF 2.0', 'comp:keras', 'type:feature']\n",
      "['stat:awaiting tensorflower', 'type:bug', 'type:build/install']\n",
      "['comp:lite', 'comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['type:build/install', 'type:feature']\n",
      "['comp:runtime', 'type:support']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:support']\n",
      "['comp:lite', 'stat:awaiting tensorflower']\n",
      "['comp:ops', 'type:bug']\n",
      "[]\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:support']\n",
      "['TF 2.0', 'comp:keras', 'type:feature']\n",
      "['comp:runtime', 'type:feature']\n",
      "['comp:lite', 'type:bug']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:lite', 'type:bug']\n",
      "['comp:ops', 'type:bug']\n",
      "['type:feature']\n",
      "['comp:lite', 'type:bug']\n",
      "['comp:ops', 'type:bug']\n",
      "['comp:lite', 'type:build/install']\n",
      "['comp:lite', 'type:feature']\n",
      "['type:feature']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:ops', 'type:feature']\n",
      "['comp:lite', 'type:bug']\n",
      "['TF 1.15', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:apis', 'stat:awaiting tensorflower', 'type:support']\n",
      "['comp:apis', 'type:bug']\n",
      "['comp:ops', 'type:feature']\n",
      "['comp:lite', 'stat:contributions welcome', 'type:feature']\n",
      "['stat:awaiting tensorflower', 'type:others']\n",
      "['comp:apis', 'type:bug']\n",
      "['comp:tensorboard', 'contrib', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:support']\n",
      "['comp:ops', 'stat:awaiting response', 'type:docs-bug']\n",
      "['comp:grappler', 'type:bug']\n",
      "['TF 2.0', 'comp:tensorboard', 'type:feature']\n",
      "['TF 2.0', 'comp:tensorboard', 'type:feature']\n",
      "['TF 2.0', 'comp:tensorboard', 'type:bug']\n",
      "['TF 2.0', 'comp:tensorboard', 'type:feature']\n",
      "['TF 2.0', 'comp:tensorboard', 'type:feature']\n",
      "['comp:ops', 'type:bug']\n",
      "['TF 1.12', 'comp:lite', 'type:bug']\n",
      "['stat:contributions welcome', 'type:build/install', 'type:docs-bug']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:support']\n",
      "['subtype:centos', 'type:build/install']\n",
      "['comp:gpu', 'type:support']\n",
      "['comp:ops', 'type:bug']\n",
      "['TF 2.0', 'comp:keras', 'stat:contributions welcome']\n",
      "['TF 2.0', 'comp:keras', 'type:docs-bug']\n",
      "['comp:ops', 'type:bug']\n",
      "['comp:model', 'contrib', 'stat:awaiting tensorflower']\n",
      "['comp:ops', 'prtype:bugfix', 'stat:awaiting tensorflower']\n",
      "['comp:apis', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.3', 'comp:ops', 'type:bug']\n",
      "['comp:lite', 'type:bug']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:docs-bug', 'type:feature']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['comp:gpu', 'type:bug']\n",
      "['TF 2.0', 'comp:ops', 'good first issue', 'stat:contributions welcome', 'type:docs-bug']\n",
      "['TF 1.14', 'comp:keras', 'type:bug']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:apis', 'type:bug']\n",
      "['comp:apis', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['subtype:centos', 'type:build/install']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['stat:awaiting tensorflower', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['TF 1.13', 'comp:eager', 'type:bug']\n",
      "['comp:apis', 'comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:gpu', 'stat:contributions welcome', 'type:build/install']\n",
      "['contrib', 'stat:awaiting tensorflower', 'type:support']\n",
      "['stat:awaiting tensorflower', 'subtype: ubuntu/linux', 'type:build/install']\n",
      "['stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['comp:apis', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:apis', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:gpu', 'type:feature']\n",
      "['comp:ops', 'stat:contributions welcome', 'type:feature']\n",
      "['stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['comp:lite', 'type:bug']\n",
      "['comp:tfdbg']\n",
      "['comp:gpu', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:ops', 'type:bug']\n",
      "['comp:lite', 'type:bug']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.0', 'comp:apis', 'type:feature']\n",
      "[]\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:lite', 'type:feature']\n",
      "[]\n",
      "['TF 1.13', 'comp:xla', 'stat:awaiting tensorflower', 'type:build/install']\n",
      "['type:support']\n",
      "['comp:apis']\n",
      "['comp:keras', 'type:feature']\n",
      "['comp:keras']\n",
      "['TF 1.13', 'comp:keras', 'type:bug']\n",
      "['stat:contributions welcome', 'type:build/install', 'type:feature']\n",
      "['stat:contributions welcome', 'type:build/install']\n",
      "['comp:runtime', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:lite', 'type:docs-feature']\n",
      "['type:feature']\n",
      "['comp:lite', 'type:docs-bug']\n",
      "['comp:lite', 'type:bug']\n",
      "['comp:runtime', 'type:bug']\n",
      "['comp:runtime', 'stat:awaiting tensorflower']\n",
      "['comp:runtime', 'type:bug']\n",
      "['comp:lite', 'comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:lite', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:keras', 'type:bug']\n",
      "['comp:gpu', 'type:bug']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['comp:apis', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:apis', 'type:bug']\n",
      "['comp:apis', 'comp:lite', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:ops', 'stat:contributions welcome', 'type:feature']\n",
      "['stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.0', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 1.12', 'comp:ops', 'type:bug']\n",
      "['type:build/install', 'type:feature']\n",
      "['TF 1.13', 'comp:runtime', 'type:bug']\n",
      "['comp:tpus', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:lite', 'type:bug']\n",
      "['comp:ops', 'stat:contributions welcome', 'subtype: raspberry pi', 'type:bug']\n",
      "['TF 1.12', 'TF 1.13', 'subtype:windows', 'type:build/install']\n",
      "['TF 1.12', 'type:build/install']\n",
      "['comp:grappler']\n",
      "['comp:model', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:gpu']\n",
      "['TF 1.12', 'type:bug']\n",
      "['TF 2.0', 'comp:model', 'comp:tpus', 'type:feature']\n",
      "['TF 2.1', 'comp:dist-strat', 'comp:tpus', 'type:feature']\n",
      "['TF 2.1', 'comp:model', 'comp:tpus', 'type:feature']\n",
      "['comp:dist-strat']\n",
      "['comp:keras']\n",
      "['comp:lite']\n",
      "['comp:lite']\n",
      "['comp:runtime']\n",
      "['comp:tfdbg']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:ops', 'type:feature']\n",
      "['comp:gpu', 'comp:gpu:tensorrt', 'type:support']\n",
      "['TF 2.0', 'type:feature']\n",
      "['TF 2.0', 'type:feature']\n",
      "['TF 2.0', 'comp:tensorboard', 'type:feature']\n",
      "['TF 2.0', 'stat:contributions welcome', 'type:feature']\n",
      "['TF 2.1', 'comp:model', 'comp:tpus', 'type:feature']\n",
      "['TF 2.0', 'type:feature']\n",
      "['comp:ops', 'type:bug']\n",
      "['comp:ops', 'stat:contributions welcome', 'type:feature']\n",
      "['contrib', 'stat:contributions welcome']\n",
      "['comp:lite']\n",
      "['type:build/install']\n",
      "['comp:lite', 'subtype:bazel', 'type:build/install']\n",
      "['comp:xla', 'type:build/install']\n",
      "['comp:ops', 'stat:contributions welcome', 'type:feature']\n",
      "['comp:lite']\n",
      "['stat:community support', 'subtype:windows', 'type:build/install']\n",
      "['comp:keras', 'stat:contributions welcome', 'type:feature']\n",
      "['TF 1.13', 'contrib']\n",
      "['TF 1.13', 'type:build/install']\n",
      "['comp:apis', 'type:support']\n",
      "['comp:lite']\n",
      "['comp:lite', 'type:feature']\n",
      "['comp:lite']\n",
      "['comp:lite']\n",
      "['stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['comp:apis', 'type:support']\n",
      "['comp:apis', 'comp:dist-strat', 'contrib', 'stat:contributions welcome', 'type:bug']\n",
      "['type:feature']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['TF 2.0', 'comp:lite', 'type:feature', 'type:support']\n",
      "['comp:apis', 'stat:awaiting tensorflower', 'type:support']\n",
      "['stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['comp:lite', 'type:support']\n",
      "['subtype:windows', 'type:build/install']\n",
      "['comp:gpu', 'type:bug']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['type:feature']\n",
      "['comp:ops']\n",
      "['TF 1.13', 'comp:lite']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:apis', 'type:feature']\n",
      "['comp:tfdbg', 'stat:awaiting tensorflower']\n",
      "['TF 1.12', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:apis', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['comp:gpu', 'comp:gpu:tensorrt', 'stat:awaiting tensorflower', 'stat:community support', 'type:support']\n",
      "['comp:apis', 'type:bug/performance']\n",
      "['comp:apis', 'stat:awaiting tensorflower', 'type:support']\n",
      "['comp:ops', 'type:others', 'type:support']\n",
      "['comp:lite', 'stat:contributions welcome', 'type:build/install']\n",
      "['comp:lite', 'type:feature']\n",
      "['comp:runtime', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:keras', 'type:bug']\n",
      "['type:build/install', 'type:feature']\n",
      "['TF 2.0', 'TF 2.1', 'comp:ops', 'type:docs-bug']\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 1.12', 'type:feature']\n",
      "['TF 1.12', 'comp:dist-strat', 'comp:gpu', 'type:feature']\n",
      "['comp:eager', 'type:feature']\n",
      "[]\n",
      "['comp:runtime']\n",
      "['comp:ops', 'type:feature']\n",
      "['comp:data', 'type:bug']\n",
      "['comp:ops', 'type:bug']\n",
      "['comp:ops', 'type:feature']\n",
      "['comp:gpu', 'type:bug']\n",
      "['TF 1.12', 'comp:tpus', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:apis', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['TF 2.0', 'comp:gpu', 'type:bug']\n",
      "['type:build/install']\n",
      "['comp:lite', 'comp:model']\n",
      "['contrib', 'stat:contributions welcome']\n",
      "['comp:runtime', 'stat:awaiting tensorflower']\n",
      "['type:bug']\n",
      "['comp:dist-strat', 'stat:contributions welcome', 'type:feature']\n",
      "['comp:ops', 'type:feature']\n",
      "['comp:ops', 'type:support']\n",
      "['comp:apis', 'type:feature']\n",
      "['type:bug']\n",
      "['type:bug']\n",
      "['comp:ops', 'type:feature']\n",
      "['type:docs-bug', 'type:feature']\n",
      "['comp:lite']\n",
      "['comp:keras']\n",
      "['TF 1.12', 'comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:keras', 'comp:tpus', 'stat:awaiting tensorflower']\n",
      "[]\n",
      "['type:feature']\n",
      "[]\n",
      "[]\n",
      "['comp:dist-strat', 'stat:contributions welcome', 'type:feature']\n",
      "['type:bug']\n",
      "['type:bug']\n",
      "[]\n",
      "[]\n",
      "['comp:ops', 'type:feature']\n",
      "['comp:ops', 'type:feature']\n",
      "['TF 1.12', 'comp:ops', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['comp:tpus']\n",
      "['stat:awaiting tensorflower', 'type:build/install']\n",
      "['comp:apis', 'comp:model', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:data', 'stat:contributions welcome', 'type:feature']\n",
      "['comp:data', 'type:feature']\n",
      "['comp:keras']\n",
      "['stat:community support', 'stat:contributions welcome', 'type:build/install']\n",
      "['comp:tpus', 'stat:awaiting tensorflower']\n",
      "['comp:apis']\n",
      "['type:build/install']\n",
      "['comp:runtime', 'stat:awaiting tensorflower']\n",
      "['comp:gpu', 'stat:contributions welcome']\n",
      "['comp:ops', 'type:bug']\n",
      "['comp:keras', 'type:feature']\n",
      "['comp:ops']\n",
      "['comp:dist-strat']\n",
      "['comp:apis', 'type:others']\n",
      "['type:feature']\n",
      "['comp:runtime']\n",
      "['comp:runtime']\n",
      "['comp:ops', 'type:bug']\n",
      "['comp:ops', 'type:feature']\n",
      "['comp:apis', 'stat:awaiting tensorflower', 'type:docs-bug']\n",
      "['stat:awaiting tensorflower']\n",
      "['comp:ops']\n",
      "['comp:mkl', 'type:build/install']\n",
      "['stat:awaiting tensorflower', 'type:others']\n",
      "['stat:awaiting tensorflower']\n",
      "['comp:apis']\n",
      "['comp:lite']\n",
      "['type:docs-bug']\n",
      "['type:build/install']\n",
      "['type:bug']\n",
      "['stat:awaiting tensorflower', 'type:support']\n",
      "['type:feature']\n",
      "['type:bug']\n",
      "['type:bug']\n",
      "['stat:awaiting tensorflower']\n",
      "[]\n",
      "['comp:apis', 'type:feature']\n",
      "['comp:apis', 'stat:contributions welcome', 'type:docs-bug']\n",
      "['TF 1.12', 'comp:eager']\n",
      "['stat:awaiting tensorflower', 'type:others']\n",
      "['type:bug']\n",
      "['stat:awaiting tensorflower']\n",
      "['type:feature']\n",
      "['type:bug']\n",
      "['comp:lite', 'stat:contributions welcome', 'type:feature']\n",
      "['stat:awaiting tensorflower', 'type:feature']\n",
      "[]\n",
      "['type:build/install', 'type:feature']\n",
      "[]\n",
      "['comp:keras', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:gpu', 'comp:gpu:tensorrt', 'type:build/install']\n",
      "['comp:dist-strat', 'type:bug']\n",
      "['stat:community support']\n",
      "['type:build/install']\n",
      "['comp:xla', 'stat:awaiting tensorflower']\n",
      "['comp:model']\n",
      "['type:bug']\n",
      "[]\n",
      "['comp:xla', 'stat:awaiting tensorflower']\n",
      "['type:feature']\n",
      "['comp:tpus']\n",
      "['type:bug']\n",
      "['comp:tensorboard', 'stat:awaiting tensorflower']\n",
      "['WIP']\n",
      "['comp:ops', 'stat:contributions welcome']\n",
      "['comp:ops']\n",
      "['stat:awaiting tensorflower']\n",
      "['comp:ops']\n",
      "['comp:apis', 'type:feature']\n",
      "['comp:apis', 'type:feature']\n",
      "['comp:runtime']\n",
      "['comp:grappler', 'type:others']\n",
      "['comp:runtime']\n",
      "['stat:awaiting tensorflower', 'type:build/install']\n",
      "['stat:community support', 'type:build/install']\n",
      "[]\n",
      "['stat:community support']\n",
      "['stat:community support', 'stat:contributions welcome']\n",
      "['comp:keras']\n",
      "['stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:keras', 'stat:awaiting tensorflower']\n",
      "['type:build/install']\n",
      "['stat:awaiting tensorflower', 'type:bug']\n",
      "['type:build/install']\n",
      "[]\n",
      "['comp:ops', 'type:bug']\n",
      "['comp:dist-strat']\n",
      "['comp:mkl']\n",
      "['stat:awaiting tensorflower', 'type:others']\n",
      "['comp:keras']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "[]\n",
      "['comp:eager']\n",
      "['comp:keras', 'comp:runtime']\n",
      "['comp:ops', 'comp:signal', 'stat:contributions welcome', 'type:feature']\n",
      "['comp:apis', 'type:feature']\n",
      "['comp:model']\n",
      "['comp:model']\n",
      "['type:build/install']\n",
      "['comp:ops', 'type:feature']\n",
      "['comp:model']\n",
      "['comp:apis', 'type:feature']\n",
      "['comp:model']\n",
      "['comp:lite', 'type:build/install']\n",
      "['comp:runtime', 'type:bug']\n",
      "['comp:dist-strat', 'stat:contributions welcome']\n",
      "['comp:ops', 'type:bug']\n",
      "['comp:keras', 'stat:contributions welcome']\n",
      "['comp:keras', 'comp:model', 'stat:awaiting tensorflower']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "[]\n",
      "['comp:dist-strat']\n",
      "['comp:model']\n",
      "['stat:contributions welcome', 'type:docs-bug']\n",
      "['comp:ops', 'type:bug']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:ops']\n",
      "['comp:runtime', 'type:bug']\n",
      "['comp:apis', 'type:bug']\n",
      "['good first issue', 'stat:contributions welcome', 'type:feature']\n",
      "['type:build/install']\n",
      "[]\n",
      "['comp:dist-strat']\n",
      "[]\n",
      "['comp:gpu', 'comp:gpu:tensorrt']\n",
      "['comp:apis']\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:awaiting tensorflower']\n",
      "['type:build/install']\n",
      "[]\n",
      "[]\n",
      "['type:build/install']\n",
      "['stat:contributions welcome', 'type:docs-bug']\n",
      "[]\n",
      "['stat:community support', 'type:build/install']\n",
      "['stat:awaiting tensorflower']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['comp:lite', 'type:feature']\n",
      "['type:bug']\n",
      "[]\n",
      "[]\n",
      "['comp:dist-strat']\n",
      "['stat:contributions welcome']\n",
      "[]\n",
      "[]\n",
      "['type:build/install']\n",
      "['stat:awaiting tensorflower']\n",
      "[]\n",
      "[]\n",
      "['comp:gpu', 'stat:contributions welcome', 'type:bug']\n",
      "['comp:lite']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['type:build/install']\n",
      "[]\n",
      "['stat:community support', 'type:build/install']\n",
      "[]\n",
      "[]\n",
      "['comp:eager']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['stat:contributions welcome']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['comp:lite']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['stat:awaiting tensorflower', 'type:bug']\n",
      "[]\n",
      "['stat:awaiting tensorflower', 'type:bug']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:tensorboard', 'stat:contributions welcome', 'type:bug']\n",
      "['comp:ops', 'type:bug']\n",
      "[]\n",
      "['stat:awaiting tensorflower', 'type:bug']\n",
      "[]\n",
      "['stat:community support', 'type:build/install']\n",
      "[]\n",
      "[]\n",
      "['type:bug']\n",
      "[]\n",
      "['comp:keras', 'type:feature']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['stat:awaiting tensorflower']\n",
      "[]\n",
      "['comp:tensorboard']\n",
      "['stat:awaiting tensorflower', 'type:bug']\n",
      "[]\n",
      "['stat:awaiting tensorflower', 'type:docs-bug']\n",
      "[]\n",
      "['stat:awaiting tensorflower', 'type:feature']\n",
      "['WIP']\n",
      "[]\n",
      "['comp:data', 'stat:contributions welcome', 'type:docs-bug']\n",
      "['stat:community support']\n",
      "['comp:dist-strat']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['comp:apis', 'stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:build/install']\n",
      "['comp:ops', 'type:bug']\n",
      "['type:feature']\n",
      "['comp:ops', 'stat:community support', 'type:support']\n",
      "['stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:model', 'type:bug']\n",
      "['comp:runtime', 'type:support']\n",
      "['type:bug']\n",
      "['type:support']\n",
      "['comp:dist-strat', 'type:bug']\n",
      "['contrib', 'stat:contributions welcome']\n",
      "['contrib', 'stat:community support', 'stat:contributions welcome', 'type:feature']\n",
      "['comp:apis', 'comp:model', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:lite', 'type:feature']\n",
      "['comp:runtime', 'type:bug']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:apis', 'type:support']\n",
      "['type:others']\n",
      "['subtype: ubuntu/linux', 'type:bug']\n",
      "['comp:apis', 'type:bug']\n",
      "['comp:dist-strat', 'type:others']\n",
      "['type:support']\n",
      "['stat:contributions welcome', 'type:bug']\n",
      "['type:others']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:support']\n",
      "['type:support']\n",
      "['comp:runtime', 'type:bug']\n",
      "['comp:apis', 'stat:contributions welcome', 'type:feature']\n",
      "['comp:apis', 'type:bug']\n",
      "['stat:contributions welcome']\n",
      "['stat:community support']\n",
      "[]\n",
      "['comp:tensorboard', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['type:bug']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['stat:contributions welcome']\n",
      "[]\n",
      "['type:bug/performance']\n",
      "[]\n",
      "['stat:community support', 'type:build/install']\n",
      "['comp:apis', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:ops']\n",
      "['comp:ops', 'type:feature']\n",
      "['comp:ops', 'stat:awaiting tensorflower']\n",
      "['comp:apis', 'stat:awaiting tensorflower']\n",
      "['comp:ops', 'stat:awaiting tensorflower']\n",
      "['comp:apis', 'stat:awaiting tensorflower']\n",
      "['comp:apis', 'stat:contributions welcome']\n",
      "['comp:keras', 'stat:awaiting tensorflower']\n",
      "['comp:apis', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['stat:contributions welcome']\n",
      "['comp:apis', 'stat:awaiting tensorflower', 'type:bug']\n",
      "[]\n",
      "[]\n",
      "['comp:ops', 'stat:contributions welcome', 'type:feature']\n",
      "['comp:mkl', 'stat:contributions welcome', 'type:build/install']\n",
      "[]\n",
      "['comp:apis', 'stat:awaiting tensorflower']\n",
      "['comp:data', 'stat:awaiting tensorflower']\n",
      "['comp:ops', 'stat:contributions welcome', 'type:feature']\n",
      "['comp:lite', 'type:feature']\n",
      "['contrib']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['type:bug']\n",
      "['comp:apis', 'type:support']\n",
      "['stat:contributions welcome']\n",
      "['comp:model']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['comp:ops']\n",
      "['stat:contributions welcome']\n",
      "['comp:data', 'type:feature']\n",
      "[]\n",
      "[]\n",
      "['comp:model', 'type:feature']\n",
      "['contrib']\n",
      "['stat:community support', 'subtype:windows', 'type:build/install']\n",
      "['comp:data', 'type:support']\n",
      "['comp:ops', 'type:support']\n",
      "['comp:apis', 'type:support']\n",
      "[]\n",
      "['stat:awaiting tensorflower', 'type:feature']\n",
      "['type:support']\n",
      "[]\n",
      "['type:feature']\n",
      "['comp:apis', 'type:support']\n",
      "['comp:runtime', 'type:support']\n",
      "['comp:runtime', 'type:bug']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:awaiting tensorflower', 'type:bug', 'type:feature']\n",
      "['comp:ops', 'stat:contributions welcome', 'type:feature']\n",
      "['stat:awaiting tensorflower', 'type:others']\n",
      "['comp:runtime', 'type:support']\n",
      "['stat:contributions welcome', 'type:others']\n",
      "['type:others']\n",
      "['comp:ops', 'type:support']\n",
      "['comp:keras', 'type:bug']\n",
      "['comp:ops', 'type:feature']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['comp:ops', 'type:feature']\n",
      "['comp:ops', 'type:support']\n",
      "['comp:apis', 'stat:awaiting tensorflower', 'type:feature']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['comp:ops', 'stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:build/install']\n",
      "['comp:ops', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:awaiting tensorflower', 'type:feature']\n",
      "['type:support']\n",
      "['comp:ops', 'type:support']\n",
      "['comp:ops', 'type:support']\n",
      "['stat:community support', 'type:bug']\n",
      "['comp:ops', 'type:bug']\n",
      "['contrib', 'type:bug']\n",
      "['comp:ops', 'type:bug']\n",
      "['comp:keras', 'type:bug']\n",
      "[]\n",
      "['stat:awaiting tensorflower']\n",
      "[]\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome']\n",
      "[]\n",
      "['stat:community support', 'type:build/install']\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome']\n",
      "[]\n",
      "['stat:contributions welcome', 'type:docs-bug']\n",
      "['type:support']\n",
      "['stat:contributions welcome']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome']\n",
      "['stat:awaiting tensorflower']\n",
      "[]\n",
      "['stat:contributions welcome']\n",
      "[]\n",
      "['comp:apis', 'type:feature']\n",
      "['type:bug']\n",
      "['type:feature']\n",
      "['stat:awaiting tensorflower']\n",
      "['stat:contributions welcome']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['stat:community support']\n",
      "['stat:contributions welcome']\n",
      "['stat:awaiting tensorflower', 'type:docs-bug']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['stat:contributions welcome']\n",
      "[]\n",
      "['stat:community support']\n",
      "[]\n",
      "[]\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "[]\n",
      "['stat:awaiting tensorflower', 'type:bug']\n",
      "['comp:tpus', 'stat:awaiting tensorflower', 'type:bug']\n",
      "['WIP', 'type:bug']\n",
      "['stat:community support', 'type:bug']\n",
      "[]\n",
      "['stat:awaiting tensorflower']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['type:support']\n",
      "[]\n",
      "['stat:awaiting tensorflower', 'type:bug']\n",
      "[]\n",
      "['stat:contributions welcome']\n",
      "['type:bug']\n",
      "[]\n",
      "[]\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:bug']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['comp:lite', 'type:feature']\n",
      "[]\n",
      "[]\n",
      "['comp:eager']\n",
      "[]\n",
      "['type:bug']\n",
      "['type:feature']\n",
      "['stat:awaiting tensorflower', 'type:bug']\n",
      "[]\n",
      "['type:support']\n",
      "['stat:community support']\n",
      "['stat:contributions welcome']\n",
      "[]\n",
      "['type:feature']\n",
      "['type:feature']\n",
      "[]\n",
      "['stat:awaiting tensorflower']\n",
      "['type:support']\n",
      "['stat:contributions welcome']\n",
      "[]\n",
      "[]\n",
      "['stat:community support', 'stat:contributions welcome', 'type:build/install']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "[]\n",
      "['type:feature']\n",
      "[]\n",
      "[]\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome']\n",
      "[]\n",
      "['comp:signal']\n",
      "['type:feature']\n",
      "['stat:contributions welcome']\n",
      "['type:build/install']\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "[]\n",
      "['stat:community support']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['stat:awaiting tensorflower']\n",
      "[]\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:community support', 'type:feature']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['type:feature']\n",
      "[]\n",
      "['type:feature']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['type:support']\n",
      "['type:feature']\n",
      "['stat:awaiting tensorflower']\n",
      "[]\n",
      "['stat:awaiting tensorflower']\n",
      "['type:feature']\n",
      "['stat:contributions welcome']\n",
      "['stat:community support', 'type:support']\n",
      "['stat:awaiting tensorflower']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['stat:contributions welcome']\n",
      "[]\n",
      "['stat:awaiting tensorflower', 'type:bug']\n",
      "['type:bug']\n",
      "['stat:awaiting tensorflower']\n",
      "['type:feature']\n",
      "[]\n",
      "['stat:awaiting tensorflower']\n",
      "['stat:awaiting tensorflower']\n",
      "['stat:contributions welcome', 'type:bug']\n",
      "[]\n",
      "['stat:awaiting tensorflower']\n",
      "['stat:awaiting tensorflower']\n",
      "['stat:contributions welcome']\n",
      "[]\n",
      "['stat:community support', 'type:build/install']\n",
      "[]\n",
      "['stat:contributions welcome']\n",
      "[]\n",
      "['comp:apis']\n",
      "['type:bug']\n",
      "[]\n",
      "['stat:contributions welcome']\n",
      "['stat:community support']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "[]\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "[]\n",
      "['stat:contributions welcome', 'type:bug']\n",
      "[]\n",
      "['stat:community support', 'stat:contributions welcome', 'type:build/install']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome']\n",
      "['type:bug']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:docs-bug']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "[]\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:community support']\n",
      "[]\n",
      "['stat:community support']\n",
      "['stat:contributions welcome', 'type:build/install']\n",
      "['type:bug']\n",
      "['stat:contributions welcome']\n",
      "[]\n",
      "['stat:community support', 'stat:contributions welcome', 'type:bug']\n",
      "[]\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome']\n",
      "[]\n",
      "['stat:awaiting tensorflower', 'type:bug']\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "[]\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome', 'type:build/install']\n",
      "['type:feature']\n",
      "['comp:signal']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "[]\n",
      "['stat:awaiting tensorflower']\n",
      "['stat:contributions welcome']\n",
      "['type:feature']\n",
      "['stat:contributions welcome', 'type:build/install']\n",
      "['stat:contributions welcome', 'type:bug']\n",
      "['type:bug']\n",
      "['stat:contributions welcome']\n",
      "['stat:awaiting tensorflower', 'type:feature']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:community support']\n",
      "['stat:community support', 'stat:contributions welcome']\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "[]\n",
      "['stat:community support', 'type:build/install']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:community support', 'stat:contributions welcome']\n",
      "['comp:lite']\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome']\n",
      "[]\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome', 'type:bug']\n",
      "['stat:community support', 'type:build/install']\n",
      "['comp:ops', 'stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome']\n",
      "['stat:awaiting tensorflower']\n",
      "['stat:contributions welcome']\n",
      "['stat:community support', 'stat:contributions welcome']\n",
      "['stat:community support', 'type:build/install']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:community support', 'stat:contributions welcome']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:community support']\n",
      "['stat:community support']\n",
      "['comp:signal', 'stat:community support']\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome', 'type:build/install']\n",
      "['type:feature']\n",
      "['stat:contributions welcome', 'type:bug']\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome']\n",
      "[]\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome']\n",
      "[]\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:bug']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:community support', 'type:build/install']\n",
      "['stat:contributions welcome']\n",
      "['stat:community support', 'type:build/install']\n",
      "['stat:contributions welcome']\n",
      "['comp:eager', 'stat:awaiting tensorflower']\n",
      "['stat:community support', 'type:feature']\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:community support']\n",
      "['stat:contributions welcome', 'type:bug']\n",
      "['stat:contributions welcome']\n",
      "['stat:community support']\n",
      "['stat:community support']\n",
      "['stat:community support', 'type:bug']\n",
      "['stat:contributions welcome', 'type:build/install']\n",
      "['stat:contributions welcome']\n",
      "['stat:community support']\n",
      "['comp:signal', 'stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "[]\n",
      "[]\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome', 'type:support']\n",
      "['stat:contributions welcome', 'type:build/install']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome']\n",
      "['stat:community support']\n",
      "[]\n",
      "['stat:awaiting tensorflower', 'type:bug']\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:community support']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome']\n",
      "['stat:community support']\n",
      "['stat:contributions welcome']\n",
      "['stat:community support']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:community support']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['type:bug']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:build/install']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:bug']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:community support', 'type:build/install']\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "[]\n",
      "['stat:community support']\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:bug']\n",
      "['stat:community support']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:bug']\n",
      "['stat:contributions welcome']\n",
      "['stat:community support', 'type:support']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['type:feature']\n",
      "['stat:contributions welcome']\n",
      "['type:feature']\n",
      "['stat:awaiting tensorflower', 'type:feature']\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:build/install']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:community support', 'type:support']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:bug']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:community support', 'type:feature']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:bug']\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome']\n",
      "[]\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome', 'type:build/install']\n",
      "['stat:contributions welcome', 'type:bug']\n",
      "['stat:contributions welcome', 'type:bug']\n",
      "['stat:contributions welcome', 'type:bug']\n",
      "['stat:contributions welcome', 'type:docs-bug']\n",
      "['stat:contributions welcome', 'type:build/install']\n",
      "['stat:contributions welcome', 'type:bug']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:bug']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:bug']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:bug']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome', 'type:bug']\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome', 'type:bug']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:build/install']\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['type:bug']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:bug']\n",
      "['stat:contributions welcome', 'type:build/install']\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome']\n",
      "['comp:signal', 'type:performance']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:support']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome', 'type:bug', 'type:docs-bug']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome', 'type:feature']\n",
      "['stat:contributions welcome']\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(labels_list)):\n",
    "    print(labels_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving and loading a model using tf.keras and CategoricalCrossentropy with from_logits=True restores an incorrect model.\n",
      "\n",
      "\"import tensorflow\" configures Python logging if tensorboard is not installed\n",
      "\n",
      "tf-nightly-gpu builds from February are crashing on 'tf.keras model.fit()' for Nvidia RTX 3090; January builds performance is slower than 2080ti on : 2Layer Bidirectional LSTM + 3 Dense Layers\n",
      "\n",
      "Addition of TanhExp activation function - issue #45929\n",
      "\n",
      "TFLM: Reduce memory usage for some types\n",
      "\n",
      "Large overhead when py_function returns tuple of lists\n",
      "\n",
      "Tensorflow WARNING: tensorflow:Gradients do not exist for variables\n",
      "\n",
      "Flatbuffers Checksum Issue Building With Make\n",
      "\n",
      "RuntimeError: Mixing different tf.distribute.Strategy objects while using tf.distribute.MirroredStrategy()\n",
      "\n",
      "Correct range of input values for MobileNet\n",
      "\n",
      "Suspected duplicate code in resize_bilinear_op.cc\n",
      "\n",
      "Problem with TensorFlow model example with custom Sin operator\n",
      "\n",
      "minimize total loss without step loss\n",
      "\n",
      "unable to train model by using TPU(free TPU)\n",
      "\n",
      "Error \"failed to connect to all addresses\" when iterating dataset on TPU with Colab or Kaggle\n",
      "\n",
      "issue with tf.data.experimental.rejection_resample method\n",
      "\n",
      "TF-TRT: batch dimension is failing in FasterRCNN\n",
      "\n",
      "how to set C++ standard version for tensenflow as another bazel project dependency\n",
      "\n",
      "Batch Normalization fails as kernel constraint for Conv layers when using mixed precision\n",
      "\n",
      "[_Derived_]RecvAsync is cancelled Error when training with LSTM on tf-gpu \n",
      "\n",
      "Empty Interpreter Input/Output Buffer Pointers\n",
      "\n",
      "Tensorflow Addons connected components not working properly\n",
      "\n",
      "Keras model saving erroring: TypeError: get_config() missing 1 required positional argument: 'self'\n",
      "\n",
      "Lr workaround for wrapped  optimizers\n",
      "\n",
      "custom op on gpu/dsp\n",
      "\n",
      "Quantize for pooling op\n",
      "\n",
      "Modify Transpose kernel to work in TFLu\n",
      "\n",
      "\"ValueError: No gradients provided \" error in TF v2.4.1, works fine on v2.3.1\n",
      "\n",
      "fix erroneous assign in Env::StartTransaction()\n",
      "\n",
      "First step towards prototyping v2 of TFLM integration with external IDEs.\n",
      "\n",
      "AttributeError: module 'tensorflow' has no attribute 'Session' with tensorflow 2.4.1\n",
      "\n",
      "tensorflow-gpu 2.2.0 doesn't recognize Nvidia MX130 GPU\n",
      "\n",
      "add MNIST training to `mnist_grad_test`\n",
      "\n",
      "Make the output for model summary wrap\n",
      "\n",
      "Correctly close file descriptors when loading saved models\n",
      "\n",
      "Building TensorFlow Lite - macos_arm64 support using cmake\n",
      "\n",
      "Distributed computing, extreme distribution latency \n",
      "\n",
      "Remove unnecessary model parsing from keras.model.load_model\n",
      "\n",
      "Add link to presentation, minor fixes\n",
      "\n",
      "Model.predict accepts tensors of incorrect rank\n",
      "\n",
      "Make libtensorflow_jni target compatible with Tensorflow Text\n",
      "\n",
      "Failed to run models with libtensorflow_jni and tensorflow text together\n",
      "\n",
      "Cannot import name 'boosted_trees_test' from 'tensorflow_estimator.python.estimator.canned' \n",
      "\n",
      "Simplify Huber Loss implementation\n",
      "\n",
      "Normalization layer supresses ValueError when model is called with bad input shape\n",
      "\n",
      "Pass for Dialect conversion from Tflite to TF  \n",
      "\n",
      "Disable unroll batch matmul pass\n",
      "\n",
      "SELU activation in TFlite micro\n",
      "\n",
      "Add thread name in the error message when thread creation fails.\n",
      "\n",
      "NotFoundError: It shows libcudart.so.8.0, but I am using CUDA 10.0.\n",
      "\n",
      "Support all fp types in GPU SparseTensorDenseMatMul\n",
      "\n",
      "Remove unit8 support from kernels prior to adding new optimized implementations\n",
      "\n",
      "Changes to MklConvFwdPrimitiveFactory to support Arm Compute Library backend\n",
      "\n",
      "TFLM project generation support (version 2)\n",
      "\n",
      "Add axis argument\n",
      "\n",
      "No algorithm worked on Ubuntu 20.04 LTS with CUDA 11.0 and Tensorflow 2.4.1\n",
      "\n",
      "Add support for probability/thresholds in meanIoU\n",
      "\n",
      "Different outputs from GradientTape and CropAndResizeGradImage \n",
      "\n",
      "How to run a model training command in redhat linux slurm file?\n",
      "\n",
      "Layer .input_shape and .output_shape wrongly claim \"The layer has never been called\"\n",
      "\n",
      "Tensorflow and flask has an error loading model.\n",
      "\n",
      "Compile TensorFlow Lite for iOS Simulator on Apple Silicon\n",
      "\n",
      "Unable to build TFLite for Microcontrollers on Mac OS \n",
      "\n",
      "[XLA:GPU] Updated hlo_to_llvm_ir to also emit PTX and re-enable tests using it.\n",
      "\n",
      "Add missing closing backtick\n",
      "\n",
      "BestExporter exports worse model when training tasks restart.\n",
      "\n",
      "EagerTensor implements `__len__` but not `len()`.\n",
      "\n",
      "Did not open image after detecting object using tensorflow2\n",
      "\n",
      "tf.errors.UnknownError should include __cause__\n",
      "\n",
      "TypeError: tensor_equals() missing 1 required positional argument: 'other'\n",
      "\n",
      "Libcudnn Major Version Variable attached\n",
      "\n",
      "[XLA:GPU] Build failure\n",
      "\n",
      "Port micro op EXPAND_DIMS for it to pass the tests\n",
      "\n",
      "ModifyGraphWithDelegate(delegate) running very slow (ubuntu with USB accelerator)\n",
      "\n",
      "Generating the descriptors of constant shape\n",
      "\n",
      "Fixed docstring formatting for api_docs\n",
      "\n",
      "tf_upgrade_v2: UnicodeDecodeError: 'charmap' codec can't decode byte 0x98 in position 385: character maps to <undefined>\n",
      "\n",
      "True loss values\n",
      "\n",
      "Tensorflow is ignoring `tf.config.set_soft_device_placement(True)`\n",
      "\n",
      "Add a status badge for Arduino examples\n",
      "\n",
      "metrics values don't match between custom metric function and model.fit callbacks\n",
      "\n",
      "Getting ERROR: Could not find a version that satisfies the requirement tensorflow==2.4.1\n",
      "\n",
      "Any suggestion to use tensorflow feasibly?\n",
      "\n",
      "Ampere bfloat16 support\n",
      "\n",
      "Support tf.IsFinite lowering\n",
      "\n",
      "The comparing with memory by 1080 ti and 3090\n",
      "\n",
      "tflite interpreter on android crashes on a model downloaded from tfhub\n",
      "\n",
      "[Kernel C API] Fix inplacement issue in C API\n",
      "\n",
      "ImportError traceback in VScode\n",
      "\n",
      "fix Windows build errors\n",
      "\n",
      "Custom objects in tf.keras have conflicting interface\n",
      "\n",
      "In-batch-negatives and distributed traning\n",
      "\n",
      "Assert if y_pred and y_true have same shape in categorical_accuracy()\n",
      "\n",
      "Give Grappler hints about function XLA compilation so it can selectively apply compatible optimizations\n",
      "\n",
      "Calling model.test_on_batch after model.evaluate returns corrupted values for the loss and the metrics\n",
      "\n",
      "SparseTensorDenseMatMul adjoint_a takes transpose, not adjoint\n",
      "\n",
      "Slower on a Nvidia 1080TI GPU than on CPU \n",
      "\n",
      "ERROR: Didn't find op for builtin opcode 'ADD' version '1'\n",
      "\n",
      "Adabound feature request \n",
      "\n",
      "memory increasing slowly and largely at the beginning of model.fit() in tf.keras\n",
      "\n",
      "tf.nn.depth_to_space with NCHW argument works in TF 2.3 but fails in TF 2.4\n",
      "\n",
      "Unable to convert mT5 model to tflite (tensorflow.GraphDef exceeds maximum protobuf size of 2GB)\n",
      "\n",
      "Getting negative loss function in Autoregressive\n",
      "\n",
      "TPU with bfloat16 does not support tf.image.resize with nearest\n",
      "\n",
      "[tf.data] refactor serialization tests based on CheckpointTestBase in kernel_tests\n",
      "\n",
      "Add cmake build for tflite label_image example.\n",
      "\n",
      "[MLIR] Add concatenateOp lowering from lmhlo to Affine.\n",
      "\n",
      "[tf.data] move map, map_and_batch serialization tests to kernel_tests\n",
      "\n",
      "CUDA 11.2/TF 2.5.0 Failed to load libcudnn.so.8\n",
      "\n",
      "TypeError: Cannot convert a symbolic Keras input/output to a numpy array.\n",
      "\n",
      "How to fix 'Tidx' type in TensorFlowOpLayer (Inconsistent values for attr 'Tidx' DT_FLOAT vs. DT_INT32 while building NodeDef)\n",
      "\n",
      "Need a easy way to let different weights have different LR in Keras.\n",
      "\n",
      "Building v2.3.1 from sources with avx512 support on a system without avx512\n",
      "\n",
      "TopKV2 InvalidArgumentError: k must be scalar\n",
      "\n",
      "Remove incorrect test in batch_to_space_nd.cc and space_to_batch_nd.cc\n",
      "\n",
      "Add compatability for h5py 3\n",
      "\n",
      "`saved_model_cli show` report a error when apply it to a bert fine-tuned model\n",
      "\n",
      "Remove unnecessary bool conversion in basic test.\n",
      "\n",
      "Cant compile because there is no cuda.h\n",
      "\n",
      "Can the GradCAM being integrate to savedModel format?\n",
      "\n",
      "micro: port op LOG_SOFTMAX from lite\n",
      "\n",
      "micro: port op CUMSUM from lite\n",
      "\n",
      "tf.keras.layers.ZeroPadding2D crashes (segfault)\n",
      "\n",
      "The Example Code in tf.feature_column.numeric_column is not Executable/Complete/Self-Sufficient\n",
      "\n",
      "`tf.keras.backend` python api docs don't include most symbols exported\n",
      "\n",
      "Tensorflow 2.4.1 build failing with MKL enabled\n",
      "\n",
      "whether tensorflow2.0 suppert bert pretraining?\n",
      "\n",
      "\"classes\" not working on flow_from_dataframe\n",
      "\n",
      "[INTEL MKL] Refactoring code related to compiling oneDNN with threadpool.\n",
      "\n",
      "Why is there no model parallelism api？\n",
      "\n",
      "tf.raw_ops.Send/Recv fail within MultiWorkerMirroredStrategy\n",
      "\n",
      "HDF5 locking issues when using concurrent.futures.ProcessPoolExecutor\n",
      "\n",
      "tf.convert_to_tensor crashes(segfault) \n",
      "\n",
      "Rename models directory to examples\n",
      "\n",
      "Export tensorflow-lite target in CMake build\n",
      "\n",
      "LinearOperatorKronecker for SparseTensors\n",
      "\n",
      "Minimal example that exercises the tf.function(experimental_implements) functionality?\n",
      "\n",
      "BoostedTreeClassifier does not support EmbeddingColumns : EmbeddingColumns has no attribute 'dtype'\n",
      "\n",
      "Successive prediction (loop) in keras model generate NaN values\n",
      "\n",
      "Keras layers do not track tf.Module (not conforming to SOLID principles)\n",
      "\n",
      "(Numpy v1.20+ compat.): Cannot convert a symbolic Tensor (sequential_1/random_rotation_1/rotation_matrix/strided_slice:0) to a numpy array\n",
      "\n",
      "EfficientnetB1 :ValueError: You are trying to load a weight file containing 185 layers into a model with 184 layers \n",
      "\n",
      "EfficientnetB1 :ValueError: You are trying to load a weight file containing 185 layers into a model with 184 layers \n",
      "\n",
      "[Grappler] Optimise branches with constant predicates. \n",
      "\n",
      "tensorflow is shown on listing packages using pip list or conda list but you can't import in Python\n",
      "\n",
      "Same issue happened even with using  pip install https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-2.3.0-cp38-cp38-macosx_10_14_x86_64.whl  as mentioned in https://www.tensorflow.org/install/pip#package-location  tensorflow is shown on listing packages using pip list or conda list but you can't import in Python\n",
      "\n",
      "CUDA_ERROR_ILLEGAL_ADDRESS after upgrading form TF 2.3.1 to 2.3.2 on Windows 10\n",
      "\n",
      "Switch gcs to modular file system on tensorflow/io\n",
      "\n",
      "Illegal instruction (core dumped)\n",
      "\n",
      "Cannot assign a device for operation using Google Cloud TPU\n",
      "\n",
      "TFL: Update detection_postprocess kernel\n",
      "\n",
      "Cannot convert a symbolic Tensor (gru/strided_slice:0) to a numpy array.\n",
      "\n",
      "add tflite_micro with cmake files and zephyr modules\n",
      "\n",
      "add cmake build support for tflite_micro\n",
      "\n",
      "ParameterServerStrategy supporting GPU workers\n",
      "\n",
      "CMSIS-NN fully_connected kernel does not support null pointer bias\n",
      "\n",
      "How do I profile layers in tensorflow instead of ops? Is there a way to profile only layers in tensorflow?\n",
      "\n",
      "Failed to convert QAT model to tflite when I use tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8\n",
      "\n",
      "Add GPU support for SparseToDense Op\n",
      "\n",
      "Add NFNets to keras.applications\n",
      "\n",
      "Why I cannot using TensorArray.gather() in @tf.function?\n",
      "\n",
      "Building TFLite WHL on Pi Zero (armv6l) - \"warning: requested alignment 16 is larger than 8 [-Wattributes]\"\n",
      "\n",
      "[XLA] Support QuantizeAndDequantizeV4\n",
      "\n",
      "Keras cannot restore custom functions for subclassed Model, but for subclassed Layer custom functions can be restored\n",
      "\n",
      "Long prediction time when using tensorflow lite optimize\n",
      "\n",
      "LSTM - Issue with model fitting using ragged tensor\n",
      "\n",
      "Add reminder on MacOS Catalina\n",
      "\n",
      "I have problem with python code in google colab\n",
      "\n",
      "DSP HiFi4 support (delegate) for TF-Lite\n",
      "\n",
      "TypeError if set the weights to the current weights via `set_weights`\n",
      "\n",
      "ValueError: No gradients provided for any variable: ['conv1_conv/kernel:0', 'conv1_conv/bias:0', 'conv2_block1_preact_bn/gamma:0', 'conv2_block1_preact_bn/beta:0', 'conv2_block1_1_conv/kernel:0',\n",
      "\n",
      "Tensorflow Lite QAT: Double QUANT/DEQUANT in half-quantized and failure in full-quantized model conversion\n",
      "\n",
      "tensorflow.python.framework.errors_impl.NotFoundError: undefined symbol: _ZN10tensorflow8OpKernel11TraceStringB5cxx11EPNS_15OpKernelContextEb\n",
      "\n",
      "Support for tf.where without x and y arguments in XLA\n",
      "\n",
      "TFLite converter produces wrong output shape_signature if RNN / LSTM output layer in model (becomes all unknown dimensions)\n",
      "\n",
      "\"No matching distribution found for tensorflow\" when installing with pip on macOS 11\n",
      "\n",
      "Training with MultiWorkerMirroredStrategy for machines with heterogeneous number of GPUs\n",
      "\n",
      "[MLIR] Remove hardcoded element type in lowering of SpaceToBatchND\n",
      "\n",
      "Multi-thread support for TF2 ParameterServerStrategy\n",
      "\n",
      "Non exact-zero value for fake quantizer ops\n",
      "\n",
      "Bidirection Masked LSTM breaks in graph mode.\n",
      "\n",
      "Error with running tensorflow after installation\n",
      "\n",
      "tf.keras.layers.Embedding updating embeddings of nearby indices even though they were not updated in the code\n",
      "\n",
      "Add tf.nn.log_gaussian_loss\n",
      "\n",
      "Cannot import Tensorflow Lite model with an LSTM layer to Android Studio\n",
      "\n",
      "Conv2D operator not XLA compliant with Windows 10\n",
      "\n",
      "Crash when creating Dataset from RaggedTensor, parallel function calls\n",
      "\n",
      "unique operation on strings returns bogus indices\n",
      "\n",
      "'An error ocurred while starting the kernel' while training on gpu in Spyder!\n",
      "\n",
      "tensorflow_model_server  taking a lot of time in deploying model\n",
      "\n",
      "Need to get the version info from the model\n",
      "\n",
      "EfficientNet models from TensorFlow.Keras not being reproducible on GPU\n",
      "\n",
      "`ValueError: None values not supported` when using the Estimator API\n",
      "\n",
      "How to compute gradient of output wrt input in Tensorflow 2.0 when there are multi-labels?\n",
      "\n",
      "Slow as_numpy_iterator() and StringLookup.adapt() with make_csv_dataset()\n",
      "\n",
      "Can't compile Tensorarray with XLA.\n",
      "\n",
      "Modernize TF_CPP_VMODULE handling using absl\n",
      "\n",
      "TFLite GPU Delegate Fails \n",
      "\n",
      "Failed to build Tensorflow Lite for Win32 using CMake\n",
      "\n",
      "Update input parameter validation for advanced activation layers.\n",
      "\n",
      "Error while training a model using spark and elephas estimator- Please suggest\n",
      "\n",
      "Compiled metrics cannot deal with dictionaries in update_state\n",
      "\n",
      "Keras fails to load saved model / properly infer dtypes in `tf.math.maximum`\n",
      "\n",
      "std::partial_sort in detection_postprocess kernel is not stable\n",
      "\n",
      "Mixed precision : Numerical instability with instance normalization layer\n",
      "\n",
      "ERROR: Could not find a version that satisfies the requirement tensorflow ERROR: No matching distribution found for tensorflow\n",
      "\n",
      "ValueError: Cannot add function '__inference_Dataset_map_<lambda>_12' because a different function with the same name already exists.\n",
      "\n",
      "Hi I am trying to compile on windows and get errors\n",
      "\n",
      "Installing tensorflow 2 on raspberry pi 3 model B\n",
      "\n",
      "How to use tensor on the tensorflow lite  like tensorflow tensor operation? For example, tensor ascend dimension ,reduction dimension and tensor  slice. Thanks.\n",
      "\n",
      "com_github_apple_swift_swift_protobuf sha256 mismatch in tensorflow/workspace.bzl\n",
      "\n",
      "Incorrect console print output while printing tensor generated from tf.image.grayscale_to_rgb api in Tensorflow 1.2\n",
      "\n",
      "[tflite] Problem reading multi-input tflite model with Java\n",
      "\n",
      "Can't set empty layer name\n",
      "\n",
      "Does tf.keras.callbacks.Callback work with 3 Dimensions?\n",
      "\n",
      "Fix `tf.keras.initializers.get`: convert provided type object to object\n",
      "\n",
      "Flex Delegate bazel build for C api\n",
      "\n",
      "installation issue in intel core2 cpu \n",
      "\n",
      "Custom Model Data Cardinality Check Ambiguous\n",
      "\n",
      "tf.keras.experimental.CosineDecay() get wrong step after tf.keras.Model.load_weight()\n",
      "\n",
      "Extend tensorflow/examples for raspberry_pi\n",
      "\n",
      "TF Lite benchmark NOT_EQUAL node takes up a lot of time.\n",
      "\n",
      "TF Lite Benchmark randomly throws errors.\n",
      "\n",
      "micro: port operator FLOOR_MOD kernel from lite with test\n",
      "\n",
      "Do not rely on ADL when invoking std::sort\n",
      "\n",
      "tf.All support as TFlite buildin op\n",
      "\n",
      "tf.keras.metrics.Metric not returning actual result value over batches\n",
      "\n",
      "Fix LayerNormalization on CPU\n",
      "\n",
      "RaggedTensor support for sparse_categorical_crossentropy.\n",
      "\n",
      "tf.keras.metric.Metric.reset_states() fails if state contains variables with rank greater than zero\n",
      "\n",
      " SavedModel restored in Graph mode under MirroredStrategy crashes global_variables_initializer\n",
      "\n",
      "KerasClassifier, GridSearchCV ignore with tf.device('cpu:0')\n",
      "\n",
      "Fix a bug where reordering tf.Transpose is not done correctly\n",
      "\n",
      "No gradients provided for any variable, check your graph for ops that do not support gradients, between variables. Problem with tf.cast op\n",
      "\n",
      "Add GPU implementation for Unique[WithCounts] ops\n",
      "\n",
      "Multiple GPU tests run redundantly\n",
      "\n",
      "Tensorlist bug fixes\n",
      "\n",
      "Micro benchmarks is excluded for cortex_m_corstone_300_makefile.inc\n",
      "\n",
      "Some tests are excluded for cortex_m_corstone_300_makefile.inc\n",
      "\n",
      "QAT model to TFLite strict int8 quantisation - big performance gap\n",
      "\n",
      "TF 2.4 tf.data cached shuffle 35% slower compared to TF 2.3\n",
      "\n",
      " Request for better installation instructions for TF on Ampere GPUs!\n",
      "\n",
      "please update the release with lib file assets\n",
      "\n",
      "Fixed person_detection_example for esp32\n",
      "\n",
      "tf.function Out of Memory on non first call\n",
      "\n",
      "CUDA_ERROR_NO_BINARY_FOR_GPU on A100 when compiled with CC 7.0\n",
      "\n",
      "Prefer std::copy_n to naive for loop\n",
      "\n",
      "Allow cloning of tf.keras.Model subclass\n",
      "\n",
      "c_api_distributed_test creates huge amount of threads and segfaults\n",
      "\n",
      "Upload sdists to PyPI\n",
      "\n",
      "TF_SessionRun with multiple inputs gives Segmentation Fault\n",
      "\n",
      "TF 2.4.1 with python 3.8.7 (apple silicon with rosetta) terminated by SIGILL on import\n",
      "\n",
      "TF-TRT Dynamic shape mode test for Softmax op converter\n",
      "\n",
      "shared_embeddings columns BUG: not updated when BP in Estimator which will cause lower metrics in model\n",
      "\n",
      "SIGSEGV - error when using large convolution - GPU\n",
      "\n",
      "Conv2D feeding into LSTM breaks model for inference\n",
      "\n",
      "Setting class_weight in model.fit() with tf.data.Dataset causes error\n",
      "\n",
      "post_training_integer_quantization issue\n",
      "\n",
      "tf.strings.split cannot handle some characters\n",
      "\n",
      "text_dataset_from_directory doesn't work if there isn't at least one subdirectory\n",
      "\n",
      "Some Optimizers Don't Work on GPU With Embedding Layers\n",
      "\n",
      "Distributed mode is giving lower accuracy than standalone mode\n",
      "\n",
      "Simplify tf.keras.activations.softmax codepath\n",
      "\n",
      "Construct sparse tensor using keras inputs (keras symbolic tensor)\n",
      "\n",
      "RNN + CUDA crashing with a generator\n",
      "\n",
      "[TF Lite] How to use XNNPACK delegate on Windows?\n",
      "\n",
      "Import tensorflow -> Illegal instruction\n",
      "\n",
      "tf.nn.local_response_normalization outputs NaN\n",
      "\n",
      "`tf.nn.weighted_moments` outputs NaN when `axes=[]` and `x` is complex\n",
      "\n",
      "[TFL] Fix unbounded activation clip range.\n",
      "\n",
      "Run MIRNet and get Segment Fault on Raspberry Pi Model 3B\n",
      "\n",
      "Stateful global metrics with multi-device distribution\n",
      "\n",
      "STM32F746NG Hello World LCD Display not working correctly\n",
      "\n",
      "Hard to use a lstm tflite model.\n",
      "\n",
      "Documentation about convolution padding computation is missing\n",
      "\n",
      "Is there a error \"DCHECK((ref_.store(0), true));\"  of RefCounted::Unref() function.\n",
      "\n",
      "Build with clang support is failing\n",
      "\n",
      "tensorflow not working with RTX 3070 cuda 11.1+cudnn 8.0 and 11.2+cudnn 8.1\n",
      "\n",
      "Fix crash with tf.transpose when a is complex and conjugate is True\n",
      "\n",
      "How to build tensorflow when making small changes?\n",
      "\n",
      "ValueError: ssd_mobilenet_v2 is not supported. See `model_builder.py` for features extractors compatible with different versions of Tensorflow\n",
      "\n",
      "Optimised Images\n",
      "\n",
      "Integrate CUDNN v8 frontend API for convolution\n",
      "\n",
      "Move Tensorflow Lite packages off of Bintray / JCenter due to deprecation of Bintray\n",
      "\n",
      "tf.data function mapping slower when using tf.GradientTape\n",
      "\n",
      "Problems with tf.keras.metrics.CategoricalAccuracy?\n",
      "\n",
      "Bug reported: Tensorflow GPU version(Cuda) may cause thread blocking when calling scripts across languages.  C#.\n",
      "\n",
      "TensorFlow with DLPack invokes unexpected data transfers\n",
      "\n",
      "How to use tflite on bert\n",
      "\n",
      "Unable to create functional model as slice from internal layer output if it is a Sequential model\n",
      "\n",
      "tf.keras.backend.random_normal segfault \n",
      "\n",
      "tf.nn.depth_to_space crash(aborts) with block_size is large\n",
      "\n",
      "`tf.random.learned_unigram_candidate_sampler` crashes(abort) when `range_max` is large\n",
      "\n",
      "Possible bug: tf2 raise `OverflowError: Python int too large to convert to C long` for `feature_column.crossed_column`\n",
      "\n",
      "Training is different depending on loss function being decorate with tf.function or not\n",
      "\n",
      "[quantization] Post-training quantization using TFLiteConverter isn't working in TF 2.3\n",
      "\n",
      "MeanIoU not ignoring background\n",
      "\n",
      "tf.nn.atrous_conv2d crashes(aborts) when rate is large value\n",
      "\n",
      "tf.keras.layers.UpSampling2D crashes(aborts) when size is large\n",
      "\n",
      "tf.keras.layers.RepeatVector crashes(aborts) when n is large\n",
      "\n",
      "`tf.sparse.eye` crashes(aborts) when num_rows contains large number\n",
      "\n",
      "tf.keras.backend.tile crash(aborts) when n is large\n",
      "\n",
      "tf.quantization.fake_quant_with_min_max_vars(_gradient) crash(abort)\n",
      "\n",
      "tf.summary.create_file_writer aborts \n",
      "\n",
      "tf.nn.softmax_cross_entropy_with_logits and tf.keras.backend.categorical_crossentropy crash(abort)\n",
      "\n",
      "Tokenizer doesn't work with string tensors (AttributeError)\n",
      "\n",
      "libtensorflowlite_flex_jni.so not found when using tensorflow-lite.aar\n",
      "\n",
      "tf.ragged.range and  tf.range crash (abort) when `start` is large\n",
      "\n",
      "`tf.random.fixed_unigram_candidate_sampler` crashes(abort) when `vocab_file` is invalid\n",
      "\n",
      "tf.sparse.segment_sqrt_n/sum crashes(abort) when segment_ids is large uint\n",
      "\n",
      "Custom \"Best\" Metric Not Tracking Best Accuracy\n",
      "\n",
      "Automatically treat dataclasses as pytrees\n",
      "\n",
      "tf.transpose crashes(abort) if `a` is complex\n",
      "\n",
      "tf.image.resize/resize_with_crop_or_pad/pad_to_bounding_box/extract_glimpse crash(abort)\n",
      "\n",
      "tf.keras.backend.arange crash (abort) when start is large\n",
      "\n",
      "tf.math.segment_max/min/mean/sun/prod crashes(aborts) when segment_ids is large\n",
      "\n",
      "`tf.math.floormod` crashes(floating point exception) when x is large\n",
      "\n",
      "Tensorflow lite return zero in the third running time.\n",
      "\n",
      "No UInt8 support for logistic operator in TFLu\n",
      "\n",
      "micro: port operator FLOOR_DIV kernel from lite with test\n",
      "\n",
      "TFLu: Add UInt8 support for logistic operator\n",
      "\n",
      "RuntimeError: Unable to create link (name already exists)\n",
      "\n",
      "Crash when Dense 2D without Bias on Android GPU Delegate\n",
      "\n",
      "[Graph C API] Util API\n",
      "\n",
      "[Graph C API] Registration API\n",
      "\n",
      "[Keras Fit & Compile] Horovod worker desynchronization - Potentially deadlock training TF 2.4\n",
      "\n",
      "Add missing import to compile micro kernels on Windows.\n",
      "\n",
      "InaccessibleTensorError in custom Model using add_loss and build\n",
      "\n",
      "LU solve output shape incorrect for wildcard batched inputs\n",
      "\n",
      "Training freezed at last batch of first epoch , not starting validation generator\n",
      "\n",
      "Windows GUI application: ptxas.exe spawns a blank console window\n",
      "\n",
      "Cannot use load_model when using a layer with dynamic = True and saving to hdf5 format\n",
      "\n",
      "experimental apis in iOS pod is missing.\n",
      "\n",
      "the label_smoothing argument for CE loss in keras\n",
      "\n",
      "Hi everyone, I want to look at the person_detect.tflite model you mentioned in Arduino IDE ESP32 person detection code, I want to look at the input layer of it, my beetle classifier has 96 by 96 and is grayscale so it is 1 x 96 x96 x1, but I am not sure if I can modify the person detection code to detect beetle or cat and dog TFlite converted to c model. Would you please provide the person_detect.tflite model available online? \n",
      "\n",
      "[docker]  ERROR executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc\n",
      "\n",
      "Use vectorized load/store in GatherOpKernel\n",
      "\n",
      "TypeError: __array__() takes 1 positional argument but 2 were given\n",
      "\n",
      "Lower ResizeNearestNeighbor when half_pixel_centers=true\n",
      "\n",
      "backing_device changes depending on the tensor's dtype\n",
      "\n",
      "Missing CI for OPTIMIZED_KERNEL_DIR=cmsis_nn with MVEI extension\n",
      "\n",
      "To save subclassed Keras model from_config method is mandatory\n",
      "\n",
      "On Windows, full-integer TFLite conversion is broken; `schema_py_generated.py` is empty\n",
      "\n",
      "TFLite converter does not convert dilated conv to single conv op when spatial dimension is dynamic\n",
      "\n",
      "ValueError: Failed to parse the model: pybind11::init(): factory function returned nullptr. when convert and quantize tf model\n",
      "\n",
      "tf.cond within vectorized_map results in unknown output shape\n",
      "\n",
      "edgetpu_compiler: \"Model not Quantized\" despite being quantized\n",
      "\n",
      "Expose TString related C API #46803\n",
      "\n",
      "Expose TString related C API\n",
      "\n",
      "Get path from environment CUDA_PATH \n",
      "\n",
      "GPU Underutilized\n",
      "\n",
      "TFlite conversion wierd Java Script error\n",
      "\n",
      "C++ compilation of rule failed - Error tensorflow building from source\n",
      "\n",
      "[Intel MKL] Fix shape inference for Quantized{Conv2D/DepthwiseConv2D}-like operations\n",
      "\n",
      "Deprecation warnings Model.state_updates when calling keras model `predict` after `get_weights`, in static execution\n",
      "\n",
      "Inconsistency of computations with different batch size\n",
      "\n",
      "RFC: Port 16x8 TensorFlow Lite operators to TensorFlow Lite for Microcontrollers\n",
      "\n",
      "C API to efficiently share weights and access tensor data during inferencing\n",
      "\n",
      "Add placement API for C interface - control subset of devices to make visible for model\n",
      "\n",
      "Performance and functional parity of C and Python API for the graph mode\n",
      "\n",
      "How to install Tensorflow on AIX7.2 server\n",
      "\n",
      "Pass kwargs in apply_gradients to wrapped optimizer\n",
      "\n",
      "tf.linalg.triangular_solve adjoint option description is wrong\n",
      "\n",
      "An error when loading quantized model \n",
      "\n",
      "error: 'tfl.max_pool_2d' op quantization parameters violate the same scale constraint\n",
      "\n",
      "Recommend replacement list for some incompatible ops in TFLite\n",
      "\n",
      "Correctly set out-of-boundary values.\n",
      "\n",
      "Implementation of Ordinary Differential Equations\n",
      "\n",
      "Loading and stitching TF graphs has tf.Variable conflict even if using the name scope in loading\n",
      "\n",
      "Distributed training for transformer model.\n",
      "\n",
      "Converting TF2 Object detection API model to frozen graph\n",
      "\n",
      "fix conjugate-transpose for matrices of certain sizes (issue #19200)\n",
      "\n",
      "Saved_model should be able to save any CompositeTensor signature.\n",
      "\n",
      "Support request for tanh activation function during quantization aware training (TensorFlow Model Optimization)\n",
      "\n",
      "Some **function** symbols are exported as DATA from prebuilt Windows CPU tensorflow.dll\n",
      "\n",
      "What is the accuracy of PoseNet??\n",
      "\n",
      "FR for keras.preprocessing.image_dataset_from_directory label_mode parameter to accept enum as input\n",
      "\n",
      "different result on dict of tensors by getting `_enable_dict_to_input_mapping` in tf.python.keras.engine.functional.py\n",
      "\n",
      "Adjust types of loop counters\n",
      "\n",
      "[WIP] Use BlasLtMatmul APIs in matmul_op_impl\n",
      "\n",
      "*** The TAGS command line option is no longer supported in the TFLM Makefile..\n",
      "\n",
      "tf wide and deep model save error\n",
      "\n",
      "Typo Recognize Flowers with TensorFlow Lite on Android\n",
      "\n",
      "Add relevant shape check for tf.reshape to prevent crash\n",
      "\n",
      "enable tf.shape on ragged keras input tensor\n",
      "\n",
      "Issue with stateful metrics initialization in ProgbarLogger inside the model.evaluate() function\n",
      "\n",
      "Allow reusing existing tensors for outputs of TF_SessionRun()\n",
      "\n",
      "Using the function signatures loaded from a SavedModel requires the original trackable object be kept in scope.\n",
      "\n",
      "Tesnorflow Sparse operations are slow - Comparison between Dense, SparseTensor and raw_ops sparse \n",
      "\n",
      "tf.data.Dataset.list_files is increadibly slow on Tensorflow 2.3.1\n",
      "\n",
      "floating point exception in tf.signal.stft \n",
      "\n",
      "tf.keras.backend.constant abortion\n",
      "\n",
      "tf.sequence_mask abortion when lengths contains large value\n",
      "\n",
      "fixed the output description of bucketized column to one-hot encoded value\n",
      "\n",
      "tf.keras.backend.reshape abortion when shape contain large values\n",
      "\n",
      "Time issue in micro_speec\n",
      "\n",
      "micro_speech: ESP32 Audio Codec Chip not initialized\n",
      "\n",
      "When ryzen build tf_to_gpu_binary.exe failed: error executing command error\n",
      "\n",
      "Integer quantization converts bias of 32-bit float type in conv2d to 8-bit int type on TFLite \n",
      "\n",
      "softmax_cross_entropy_with_logits - wrong backprop for this operation\n",
      "\n",
      "Old Lookup Bug still active?\n",
      "\n",
      "Memory Leak in VGG16 based model\n",
      "\n",
      "Loading a model with a Lambda layer causes a 'str' object is not callable exception\n",
      "\n",
      "Guide for porting reference ops to TFLite micro\n",
      "\n",
      "Ignoring visible gpu device (device: 0, name: GeForce GTX 780M compute capability: 3.0) with Cuda compute capability 3.0. The minimum required Cuda capability is 3.5.\n",
      "\n",
      "tf.repeat fails\n",
      "\n",
      "Reloading weights give small difference\n",
      "\n",
      "Unable to tf.saved_model.load() from a trained Keras model\n",
      "\n",
      "tf.raw_ops.ImageProjectiveTransformV3 interpolates pixels that are outside image boundary (instead of using `fill_value`)\n",
      "\n",
      "Remove py_proto_library Macro from tensorflow/core/platform/default/build_config.bzl\n",
      "\n",
      "Missing GPU op for zeros_like for RaggedTensorVariant, error occurs when Ragged Tensor fed thru tf.map_fn\n",
      "\n",
      "Linux C library for TensorFlow 2.4.1 contains macOS dylib files\n",
      "\n",
      "NaN occurs when building with Dropout, Conv2DTranspose, LeakyReLU, ELU, PReLU, Flatten and Dense\n",
      "\n",
      "UnimplementedError | InvalidArgumentError when running tf.nn.conv2d with data_format NCHW\n",
      "\n",
      "tf.reduce_max returns wrong answers on large tensors e.g., (2048,2048,1024)\n",
      "\n",
      "TFLite outputs different inference results for the same input\n",
      "\n",
      "custom hardware support for tensorflow\n",
      "\n",
      "ExponentialMovingAverage Does Not Work Under tf.function (TF 2.4)\n",
      "\n",
      "Trilinear interpolation in UpSampling3D\n",
      "\n",
      "Broken link in doc\n",
      "\n",
      "Race condition in port picker leads to test failures\n",
      "\n",
      "_pywrap_tensorflow_internal.pyd is to big after PyInstaller pack\n",
      "\n",
      "tf.keras.preprocessing.image_dataset_from_directory includes hidden dirs as classes\n",
      "\n",
      "How to get the walltime of tensorflow trace?\n",
      "\n",
      "Misleading warning when loading weights from hdf5 format weights\n",
      "\n",
      "Tensorboard not loading scalars with reload arrow button\n",
      "\n",
      "tflite model can not inference when use tensorflow op\n",
      "\n",
      "macOS Target //tensorflow/tools/pip_package:build_pip_package failed to build in debug mode\n",
      "\n",
      "How the quantization on BERT\n",
      "\n",
      "tf.Variable throws TypeError on conversion to typed numpy ndarray\n",
      "\n",
      "[Intel MKL] Fixing threadpool bug\n",
      "\n",
      "Auto-generated cortex-m0 hello world make project failing to link\n",
      "\n",
      " An error occurs when inferring by loaded pretrained model\n",
      "\n",
      "Fix an issue for multi CUPTI sessions\n",
      "\n",
      "Upgrade to CUDNN RNN v8 APIs\n",
      "\n",
      "Go: Deallocate large TF_TString on Tensor finalization\n",
      "\n",
      "tf.nn.convolution crashes Floating-point exception when filters has 0 in shape\n",
      "\n",
      "Add Windows build to nightly libtensorflow C packages\n",
      "\n",
      "Extracting item from a list with tf.function() :  TypeError: list indices must be integers or slices, not Tensor\n",
      "\n",
      "mkl_layout_pass_test fails due to optional BFLOAT16 support in MKL/oneDNN\n",
      "\n",
      "save custom training loop with keras API (train_step)\n",
      "\n",
      "CrossShardOptimizer must be used for model training on TPUs\n",
      "\n",
      "Very slow tf.TensorArray and a possible bug when used with range()\n",
      "\n",
      "Make tf.image.resize compatible with XLA compilation\n",
      "\n",
      "Tensorflow 2.4 about 10% slower than 2.3 for training RNN\n",
      "\n",
      "Unable to selectively build TensorFlow Lite with Docker\n",
      "\n",
      "Tracing of tf function is stuck when the function contains custom CRF layer that wraps tfa crf functions inside\n",
      "\n",
      "Add tf.io.encode_raw\n",
      "\n",
      "micro: port op BATCH_MATMUL from lite\n",
      "\n",
      "Docker image tensorflow:latest-gpu-jupyter - Setting a password doesn't work\n",
      "\n",
      "TfLite converter crashes when quantization is enabled\n",
      "\n",
      "Issues when Cross-building TFLite GPU Delegate and loading the built so file in Python\n",
      "\n",
      "Add option to write tensor to raw byte string\n",
      "\n",
      "Converted models gives wrong outputs\n",
      "\n",
      "Efficient and simple encoding for large tensors (such as `encode_raw`) for the use with tensorflow serving needed\n",
      "\n",
      "(Donkey Car: Epoch failure during train command) Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized.\n",
      "\n",
      "[TFLite] OpenCL strange performance compared to OpenGL\n",
      "\n",
      "InvalidArgumentError: 5 nodes in a cycle \n",
      "\n",
      "Addition of Pull request template\n",
      "\n",
      "Memory leak in Conv2D/Activation on GPU\n",
      "\n",
      "tensorflow.io.gfile.GFile: utf-8 can't decode byte 0xe9 (with both 'r' and 'rb' arguments), while normal open can read the file\n",
      "\n",
      "Tensorflow Lite OpenCL delegate outputs NaN's when CPU works fine\n",
      "\n",
      "tensorflow/core/kernels/sparse_matmul_op_test segfaults due to wrong alignment\n",
      "\n",
      "Sparse IndexedSlices warning due to tf.gather() and LossScaleOptimizer\n",
      "\n",
      "[INTEL MKL] Refactor mkl_relu_ops files, and add accuracy and performace UT.\n",
      "\n",
      "Two SavedModel roundtrips lose track of a Resource's initializer and its assets\n",
      "\n",
      "Import Issue of cross-built tflite package\n",
      "\n",
      "TensorRT converter fails for CombinedNonMaxSuppression\n",
      "\n",
      "Add SkipLine to BufferedInputStream\n",
      "\n",
      "Make tf.image.resize compatible with XLA compilation\n",
      "\n",
      "Optimize RunLineHelper in BufferedInputStream\n",
      "\n",
      "Spurious results from a full-integer TensorFlow Lite model\n",
      "\n",
      "explicit ValueError for built-in variations\n",
      "\n",
      "Passing \"Accuracy\" to model.compile() parameter \"metrics\" (instead of \"accuracy\") returns 0 / epoch without throwing an error\n",
      "\n",
      "Missing include files in libtensorflow-cpu-windows-x86_64-2.4.0.zip\n",
      "\n",
      "[tf-nightly] `keras.callbacks.EarlyStopping` stops training immediately after the first epoch even if monitored score has improved\n",
      "\n",
      "loss function input \"y_pred\" should be full list of model-outputs\n",
      "\n",
      "Remove no_cuda_on_cpu_tap tag and fix tests\n",
      "\n",
      "Timeseries example does not work with Preprocessing layers\n",
      "\n",
      "InceptionResNetV2: BatchNormalization layer does not work outside during validation/evaluation\n",
      "\n",
      "Speedup SpaceToDepth\n",
      "\n",
      "tf.math.tanh makes silently crash Python\n",
      "\n",
      "New feature request:implementation of _get_control_flow_context function\n",
      "\n",
      "DataFrameIterator is not exported\n",
      "\n",
      "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type list).\n",
      "\n",
      "Training loss not decreasing if both mixed-precision and XLA/JIT is enabled (with deep model)\n",
      "\n",
      "Support for LSTM and GRU \n",
      "\n",
      "Internal error: Failed to apply delegate: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors\n",
      "\n",
      "Wrong warning when a saved model (from TF2.4) is loaded using tf-nightly \n",
      "\n",
      "Add support for converting mixed_float16 models to TF_lite\n",
      "\n",
      "Speedup ResizeNearestNeighborGrad.\n",
      "\n",
      "Failure to train distributed with strategy mirrored, custom training, functions supplied in class\n",
      "\n",
      "LayerNormalization crashes on empty inputs when run on CPU\n",
      "\n",
      "micro_speech example: FeatureProvider tries to read audio data that is in the future\n",
      "\n",
      "custom normalizer_fn in tf.feature_column.numeric_column & tf.Keras Denselayer API results in failure when loading saved model.\n",
      "\n",
      "tflite custom Bazel build for wasm target \n",
      "\n",
      "Duplicate classes (com.google.flatbuffers) when flatbuffers is already in project\n",
      "\n",
      "tf.keras.callbacks.BaseLogger() is broken since \"metrics\" is no longer a key in Callback.params dict\n",
      "\n",
      "Missing headers (TensorFlow C, 2.4.0, Windows)\n",
      "\n",
      "Minor Bug in estimator/tensorflow_estimator/python/estimator/canned/dnn.py\n",
      "\n",
      "Issue with generating examples for loaded estimator when running with tf.function\n",
      "\n",
      "3D SparseTensor matrix multiplication with 2D Tensor :InvalidArgumentError: Tensor 'a_shape' must have 2 elements [Op:SparseTensorDenseMatMul]\n",
      "\n",
      "Remove cusparseDnVecDescr_t from cusparse_10_1.inc\n",
      "\n",
      "TF ConvertedModel: Invoke fails with \"Node number X (CONCATENATION) failed to prepare\" error\n",
      "\n",
      "tensorflow.lite.python.convert.ConverterError: <unknown>:0: error: loc(\"batch_normalization/moving_mean\"): is not immutable, try running tf-saved-model-optimize-global-tensors to prove tensors are immutable \n",
      "\n",
      "Speedup DepthToSpace\n",
      "\n",
      "Add TFLite metadata support to TensorFlowLiteSwift\n",
      "\n",
      "Unable to build debug version\n",
      "\n",
      "tensorflow/c/eager/c_api_test fails to find GPU implementation of MatMulOp\n",
      "\n",
      "Add IRFFT operator to TF Lite\n",
      "\n",
      "tf.io.gfile.GFile does not raise an error when given a directory\n",
      "\n",
      "why ExtractVolumePatches uses different implements for ThreadPoolDevice and GPUDevice.\n",
      "\n",
      "ESP32 example creation fails for person_detection example\n",
      "\n",
      "tf.experimental.numpy fails with plt.hist\n",
      "\n",
      "Are mean and standard deviation values always 127.5 and 127.5\n",
      "\n",
      "Micro: port op GATHER_ND from Lite\n",
      "\n",
      "Normalisation layer save standard deviation instead of variance\n",
      "\n",
      "Micro: port op EXPAND_DIMS from Lite\n",
      "\n",
      "small discrepancies between images loaded with `tf.keras.preprocessing.image_dataset_from_directory()` and `tf.keras.preprocessing.image.load_img()`\n",
      "\n",
      "Cannot Convert SentencePieceTokenizer to TensorRT\n",
      "\n",
      "TensorBoard callback with update_freq='batch' fails when fitting multiple times\n",
      "\n",
      "Unexpected Events CUDA_ERROR_ILLEGAL_ADDRESS and CUDA_ERROR_LAUNCH_FAILED\n",
      "\n",
      "Different results for add_loss() within TimeDistributed for eager vs. compiled execution of keras Layer\n",
      "\n",
      "Add Sony Spresense target\n",
      "\n",
      "Implement a simple way to load a TF dataset from a Spark dataset and a simple way to convert a TF dataset to a Spark dataset\n",
      "\n",
      "`sample_weight` does not work for multi-output (multi-task) models\n",
      "\n",
      "Unable to read pb file in OpenCV or Tensorflow in Python\n",
      "\n",
      "[TFLM] Added support for optimized quantize kernel for CEVA-BX1 and CEVA-SP500 cores\n",
      "\n",
      "DepthwiseConv2D is slower than Conv2D\n",
      "\n",
      "micro: port operator LEAKY_RELU kernel from lite with test\n",
      "\n",
      "micro: port operator ADD_N kernel from lite with test\n",
      "\n",
      "micro: prepare to port operator ADD_N kernel from lite with test\n",
      "\n",
      "ButchNormalization fails when nvidia GPU is used and the training size and the batch size is \"well set\"\n",
      "\n",
      "compute gradient error:  'KerasTensor' object has no attribute '_id',   (tensorflow 2.4.0)\n",
      "\n",
      "Add support for padding and cropping to tf.keras.layers.experimental.preprocessing.Resizing\n",
      "\n",
      "Generalising class weights for all label ranks\n",
      "\n",
      "Publishing Tensorflow wheel package for s390x architecture\n",
      "\n",
      "Fix SparseDenseCwiseMulOrDivGrad scalar case\n",
      "\n",
      "AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fdb3138d5f8>>\n",
      "\n",
      "Failed to build with CUDA 11.1 + TensorRT 7.2\n",
      "\n",
      "Add GPU kernel for SparseFillEmptyRows[Grad]\n",
      "\n",
      "Test TensorFloat32 with conv2d\n",
      "\n",
      "[PluggableDevice] Enable DefaultDevice for ops who only have host code.\n",
      "\n",
      "[PluggableDevice] Allow quantized type registration.\n",
      "\n",
      "train_on_batch is much slower than custom training loop on GPU\n",
      "\n",
      "GPU-delegate null EGLDisplay: lack of Wayland support?\n",
      "\n",
      "micro: port op ADD_N from lite\n",
      "\n",
      "micro: port op LEAKY_RELU from lite\n",
      "\n",
      "Model loaded from a SavedModel format in a distribution strategy has weights whose names are not unique\n",
      "\n",
      "TensorFlow and TensorFlowLite model produces different results\n",
      "\n",
      "Fix bool Where accumulation bug\n",
      "\n",
      "java.lang.IllegalArgumentException: Internal error: Cannot create interpreter: Didn't find op for builtin opcode 'CONV_2D' version '5'\n",
      "\n",
      "Android: Instantiating TFLite Interpreter with GPU delegate takes 2.5 seconds.\n",
      "\n",
      "Small last batches for TripletSemiHardLoss\n",
      "\n",
      "Will TensorFlow update new optimizer?: Adabelief\n",
      "\n",
      "[Build] Building on Windows 10 fails with Bad address error\n",
      "\n",
      "Add ReadVariableOp gradient to C++\n",
      "\n",
      "C++ Gradient for ReadVariableOp\n",
      "\n",
      "Resource exhausted:  OOM when allocating tensor with TF 2.4.0\n",
      "\n",
      "TFlite interpreter stuck after call to invoke()\n",
      "\n",
      " Request: offer image-enlarging library that uses AI (TensorFlow)\n",
      "\n",
      "Exported TFLite Inference Graph (Step 1), but can't Convert to TFLite (Step 2)\n",
      "\n",
      "Expose class_id parameter on subclasses of SensitivitySpecificityBase\n",
      "\n",
      "When can we expect Tensorflow builds with Cuda 11.1 or Cuda 11.2?\n",
      "\n",
      "call() missing 1 required positional argument\n",
      "\n",
      "Big SparseTensor constant and Datasets either turn into a large graph or a slow execution.\n",
      "\n",
      "ESP32-EYE person_detection example will not build\n",
      "\n",
      "[RNN] only LSTM not working\n",
      "\n",
      "benchmark_model build failed\n",
      "\n",
      "tensorflow.math.abs crashes Python on Windows\n",
      "\n",
      "Assigning values to variables in Tensorflow graphs using the C API\n",
      "\n",
      "Cannot quantize part of a model\n",
      "\n",
      "How the parameters of decay_rate & decay_steps are taken into consideration while computing InverseTimeDecay()\n",
      "\n",
      "Make `TextVectorization` support pickling\n",
      "\n",
      "Support Relu activation function with CUDA cuDNN gpu accleeration  instead of just tanh\n",
      "\n",
      "Modularize and consolidate the Docker images for downstream usage\n",
      "\n",
      "Error during converting the OpenNMT-tf model to TensorFlow Lite\n",
      "\n",
      "Bug: Converting quantized Keras models to TFLite\n",
      "\n",
      "Ability to parse non-Example protos without Eager Execution\n",
      "\n",
      "Pylint upgrade 2.6.2\n",
      "\n",
      "How to run tensorflow 2.4.0 on ARM Mac using Rosetta 2\n",
      "\n",
      "Tensorflow crashes when running the profiler with CUDA 11\n",
      "\n",
      "map_fn can't generate ragged tensor on arbitrary dimension\n",
      "\n",
      "I need to pass a buffer in the tensorFlow library when the buffer is of the type byteArray\n",
      "\n",
      "micro: port op DEPTH_TO_SPACE from lite\n",
      "\n",
      "class_id support for PrecisionAtRecall, RecallAtPrecision, and related metrics\n",
      "\n",
      "Cross-compilation for Raspberry Pi fails on macOS\n",
      "\n",
      "Error on TPU V3-8 with XLA: RPC failed with status = \"Unavailable: Socket closed\"\n",
      "\n",
      "TFlite Poor Performance on Pixel 3 CPU when channel not multiple of 4\n",
      "\n",
      "Weight Normalization layer doesn't work with mixed precision\n",
      "\n",
      "Incorrect conda dependency information for tensorflow-gpu\n",
      "\n",
      "Bug in Transpose Convolution Padding\n",
      "\n",
      " When I use AndroidStudio to import Image Segementation TensorsorflowLite model, there is a problem...\n",
      "\n",
      "java.lang.NoClassDefFoundError: Failed resolution of: Lorg/tensorflow/lite/support/image/ColorSpaceType\n",
      "\n",
      "micro: port operator DIV kernel from lite with test\n",
      "\n",
      "TF 2.4.0 crashes on startup with IndexError assuming that sys.argv[0] exists when it may be hosted by C++ (regression from 2.3.1)\n",
      "\n",
      "How to access operations of the tf graph from a keras model? (for tf 2.4.0)\n",
      "\n",
      "cannot execute exe of tensorflow py compiled with pyinstaller\n",
      "\n",
      "Unrecognized DataType enum value 68 and crash in malloc\n",
      "\n",
      "TF 2.4: Metrics not recognized as stateful in ProgbarLogger callback\n",
      "\n",
      "TensorflowLite Android OpenCL delegate may produce invalid Conv2D result\n",
      "\n",
      "Timeseries example does NOT address the core of prediction\n",
      "\n",
      "tf.data.Data.as_numpy_iterator() creates unnecessary copies\n",
      "\n",
      "on_epoch_end() being called multiple times when workers > 0\n",
      "\n",
      "Performance discrepency in an integer quantized model \n",
      "\n",
      "TFLiteTransferConverter (tfltransfer) for model personalization cannot convert a model on 'keras_model_head.py': AttributeError: 'list' object has no attribute 'values' line 51\n",
      "\n",
      "Tensorflow operations : Invalid data type according to Tensorflow Profiler\n",
      "\n",
      "[C++][FATAL]Calling FreezeSavedModel in cc/tools/saved_model.h causes protobuf::FatalException\n",
      "\n",
      "standalone pip package for tf.io.gfile.GFile\n",
      "\n",
      "Index used for OOV 'UNK' token from Tokenizer has a breaking change from 2.1.4 to 2.3.0.\n",
      "\n",
      "Modular filesystem inconsistent rename_file behaviour\n",
      "\n",
      "saved_model.save write .pb with read only permissions that can't be removed even with admin permissions\n",
      "\n",
      "Graidents for weights in tf.keras.layers.GRUCell not being computed when using tf.estamtor\n",
      "\n",
      "TanhExp activation function\n",
      "\n",
      "Linking issue: undefined reference to `tflite::micro::GetTensorShape SparkfunEdge\n",
      "\n",
      "FullyConnected reference_integer_ops miscalculate output data due to endianness issue on s390x\n",
      "\n",
      "Could not find device for node: {{node FloorMod}} = FloorMod[T=DT_HALF]\n",
      "\n",
      "TF 2.4 met 'TypeError: can't pickle _thread.lock objects' where 2.3 did not.\n",
      "\n",
      "[Intel MKL] unit test for bfloat16 on addN, relu, gelu etc\n",
      "\n",
      "Model with custom metrics broken if saved and reloaded\n",
      "\n",
      "Number of Delegated Nodes\n",
      "\n",
      "Fix #45839: tf.distribute.CollectiveAllReduceStrategy can't load model from checkpoint\n",
      "\n",
      "Missing Documentation for compiling binaries in c++\n",
      "\n",
      "keras Callbacks without _supports_tf_logs update separate `logs` \n",
      "\n",
      "TF 2.4.0 twice slowlier than 2.3.1\n",
      "\n",
      "Cannot quantize only part of a QAT model for running on both Android CPU/DSP\n",
      "\n",
      "Model.test_on_batch reset_metrics incorrect\n",
      "\n",
      "Feature request - testing_split in ImageDataGenerator\n",
      "\n",
      "Fix audio bugs in the Arduino micro_speech example\n",
      "\n",
      "SavedModel FROM OFFICAL DOCS always returns the same class( use OFFICAL DOCS' CODE TO PREDICT)\n",
      "\n",
      "TPU guide doesn't work, but used to - `Op type not registered 'DecodeImage' in binary`\n",
      "\n",
      "[Intel MKL] Enabling few optimizations with native format\n",
      "\n",
      "Changing kBufferAlignment to 8 from 16 for SIMD extensions fixes TfLite micro_allocator_test\n",
      "\n",
      "[Intel MKL] Enabling quantized matmul with native format\n",
      "\n",
      "Tensorflow 2.3.0 MKL Intel AVX Binary Issue\n",
      "\n",
      "TPU compile fail using the new 2.4.0 version\n",
      "\n",
      "Inexact numeric jacobian causes test failures\n",
      "\n",
      "Failed to apply delegate: TfLiteGpuDelegate Init: MUL: Expected a 3D tensor of shape HxWxC or a 4D tensor of shape 1xHxWxC but got 98x8 (Android)\n",
      "\n",
      "Why model can't converge when batch size > 1?\n",
      "\n",
      "[INTEL MKL] Replace dead Switch nodes with Identity\n",
      "\n",
      "micro: port op SPACE_TO_DEPTH from lite\n",
      "\n",
      "Official Sample C++ Test Custom Op Build Fails With TF 2.4 Using VS2019\n",
      "\n",
      "[INTEL MKL] Convert necessary PlaceholderWithDefault or Placeholder to Constant f…\n",
      "\n",
      "MHLO/LHLO Fusion lower\n",
      "\n",
      "Shape of model output can differ from target data shape\n",
      "\n",
      "saved_model does not support RaggedTensors\n",
      "\n",
      "Keras add_loss()/kernel_regularizer incompatability\n",
      "\n",
      "[C++] tensorflow::ops::Unstack Documentation and Implementation Mismatch\n",
      "\n",
      "[C++] Cannot Construct tensorflow::Status with std::string\n",
      "\n",
      "The problem with the application performing speech command recognition on the mobile phone\n",
      "\n",
      "Invalid index list in batch_scatter_ops_test.py \n",
      "\n",
      "[PluggableDevice] PluggableDevice mechanism implementation\n",
      "\n",
      "`Model` subclasses cannot be loaded without \"losing\" their class. \n",
      "\n",
      "[Documentation bug] Format issue in document of `tf.raw_ops.For`\n",
      "\n",
      "Floating point exception in tf.truncatemod when x is boundary value\n",
      "\n",
      "Segmentation fault in tf.histogram_fixed_width\n",
      "\n",
      "Progress bar silence during validation for model.fit()\n",
      "\n",
      "int8 quantization with int16 activations support for TFMicro and Cortex-M\n",
      "\n",
      "Support channel wise fill value\n",
      "\n",
      "SVD Function has (very) different GPU and CPU outputs \n",
      "\n",
      "micro: port op FLOOR_MOD from lite\n",
      "\n",
      "Passing tensorflow::ops::DecodeJpeg::Attrs to the tensorflow::ops::DecodeJpeg constructor causes std::logic_error\n",
      "\n",
      "Tensorflow compile failure on Power9/PPC64LE, how to pass compile flags to NVCC?\n",
      "\n",
      "TF Lite for MCU porting, why not provide the generating the static library flow.\n",
      "\n",
      "Different fill value for different channel with ImageProjectiveTransformV3\n",
      "\n",
      "Model training stalls forever after just a few batches. \n",
      "\n",
      "Dot layer incomplete description\n",
      "\n",
      "Request for a tutorial on building C++ API not pure C\n",
      "\n",
      "micro: Port TRANSPOSE from lite to micro\n",
      "\n",
      "micro: port ops BATCH_TO_SPACE_ND and SPACE_TO_BATCH_ND from lite \n",
      "\n",
      "Unable to compile the TFLite for Microcontrollers C++ Library on Raspberry Pi\n",
      "\n",
      "MeanIoU: Support y_pred passed as a logits tensor or probability tensor\n",
      "\n",
      "StringLookup on GPU and disable_eager_mode gives incosistent results\n",
      "\n",
      "ERROR: flatbuffers::flatbuffer_version_string Multiply defined Global Symbol\n",
      "\n",
      "Tensorflow 2.4 takes 3 seconds per epoch during training versus 1 second with TensorFlow 2.3\n",
      "\n",
      "Multi-GPU selection in map_fn and vectorized_map\n",
      "\n",
      "Out of memory in some tests due to GPU memory limit confusion\n",
      "\n",
      "No registered 'ResourceScatterNdUpdate' OpKernel for 'GPU' \n",
      "\n",
      "Test failures with \"OpError not raised\"\n",
      "\n",
      "Seg faults in various tests\n",
      "\n",
      "Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string\n",
      "\n",
      "micro: port op FLOOR_DIV from lite\n",
      "\n",
      "Mask RCNN tflite inference on android\n",
      "\n",
      " 2.4.0 sess.run freezing in Windows 10 with RTX 3090\n",
      "\n",
      "micro: port operator FILL from lite\n",
      "\n",
      "Add classifier_activation param to applications\n",
      "\n",
      "TFLM: Adding TANH op optimized for CEVA-BX1\n",
      "\n",
      "activation, kernel, bias are all attributes in Dense\n",
      "\n",
      "Is input_shape on Dense for public use?\n",
      "\n",
      "ERROR: An error occurred during the fetch of repository 'com_google_protobuf'\n",
      "\n",
      "Help with object detection project\n",
      "\n",
      "The current TensorFlow lite version cannot be built with Visual Studio 2017.\n",
      "\n",
      "metrics  in the Classification on imbalanced data tutorial \n",
      "\n",
      "TFLM: Adding ported and optimized operations for CEVA-BX1\n",
      "\n",
      "TF-TRT Test ConvertQuantize in dynamic shape mode\n",
      "\n",
      "CancelledError: [_Derived_]RecvAsync is cancelled. \n",
      "\n",
      "Keras doesn't release the memory after fit (single thread and multiprocessing)\n",
      "\n",
      "Value 'sm_86' is not defined for option 'gpu-name'\n",
      "\n",
      "Inconsistent results when using tf_upgrade_v2 script\n",
      "\n",
      "\"SAME\" padding for avg_pool2d yields different results than explicitly 0-padded input with \"VALID\"\n",
      "\n",
      "Error when building tensorflow from source\n",
      "\n",
      "save_model/load_model pair does not preserve model's metric\n",
      "\n",
      "Add RaggedTensor support to tf.stop_gradient and tf.reduce_logsumexp\n",
      "\n",
      "Using mixed precision causes incorrect loss, metric values\n",
      "\n",
      "Cannot load SavedModel when model trained with tfa.optimizers.NovoGrad: ValueError: Shapes (3, 8) and () are incompatible\n",
      "\n",
      "lstm related tests are failing due to unprovided optional inputs are not read correctly as nullptr on s390x machine\n",
      "\n",
      "Tensorflow does not use the GPU during training with eager execution, despite manual activation\n",
      "\n",
      "Bug in tensorflow.python.ops.parallel_for.gradients.batch_jacobian()\n",
      "\n",
      "save/load pair do not preserve `trainable` attribute\n",
      "\n",
      "TF2 GPUs on Kubernetes\n",
      "\n",
      "[Intel MKL] Enabling QuantizeV2 with native format\n",
      "\n",
      "[Intel MKL] Enabling quantized pooling ops with native format\n",
      "\n",
      "Same code - difference result from PC and jetson nano\n",
      "\n",
      "TF-TRT Dynamic Shapes Feature Tracker\n",
      "\n",
      "[Intel MKL] Enabling Dequantize op with native format\n",
      "\n",
      "[Intel MKL] Enabling QunatizedConcatV2 with native format\n",
      "\n",
      "tf.nn larger support for RaggedTensor\n",
      "\n",
      "Raspberry Pi 4 camera feed doesn't appear\n",
      "\n",
      "TFlite fails to run on GPU with - ERROR: TfLiteGpuDelegate Prepare: No shader implementation for reduce_sum\n",
      "\n",
      "How to unset changes made by tf.debugging.set_log_device_placement?\n",
      "\n",
      "Cannot convert model: Exception has occurred: ValueError\n",
      "\n",
      "\"keras predict\" uses one more batch than specified by \"steps\" argument with generator x\n",
      "\n",
      "recorded_allocation parameters for quantized tensors are incorrect for BE machines\n",
      "\n",
      "Segmenation fault when saving StringLookup layer\n",
      "\n",
      "Source compilation fails on Musl system on multiple places\n",
      "\n",
      "Did not get operators or tensors in subgraph 1. when using tf.lite.Interpreter\n",
      "\n",
      "tf.keras.model.Model-subclassed models do not serialise tf.function-ed methods\n",
      "\n",
      "micro: port op DIV from lite\n",
      "\n",
      "Tensorflow Estimator passes train data through some weird normalization before entering net\n",
      "\n",
      "XLA dumped llvm IR: how to convert the ir to assembly code?\n",
      "\n",
      "Optimize MutableDenseHashTable's memory usage and performance in case of large value_shape\n",
      "\n",
      "Op to convert numeric tensor to string type faster \n",
      "\n",
      "Compilation Problems\n",
      "\n",
      "Support RaggedTensors in Keras losses (at least MSE, CE, SCE)\n",
      "\n",
      "[RNN] tensorflow/lite/kernels/fully_connected.cc:119 is_optional_bias_int != true error in quantized LSTM model\n",
      "\n",
      "Tensorflow-Python Environment Assertion Error \n",
      "\n",
      "nnapi delegate and cpu output with my QAT int8 model got Inconsistent result\n",
      "\n",
      "Converted TensorFlow Lite model is not a valid model in Android Studio\n",
      "\n",
      "TimeDistributed Layer Applied to Masked Layer (Functional API) not working under TF 2.2+\n",
      "\n",
      "TFLite Converter for BiLSTM based architecture\n",
      "\n",
      "\"bluepill\" target build fails if repository pathname contains spaces\n",
      "\n",
      "[TFLite] Add TABLE operator for LUT-based operators lowering\n",
      "\n",
      "Fix Const op tensor_content on s390x during save/load\n",
      "\n",
      "Update _inferred_steps when recreating iterator.\n",
      "\n",
      "Post-training quantization same model get different result for tf.concat\n",
      "\n",
      "RuntimeError: Quantization not yet supported for op: 'DEQUANTIZE', after QAT quantize_apply()\n",
      "\n",
      "tf.keras.preprocessing.image.random_shear raises TypeError when it is mapped to a tf dataset.\n",
      "\n",
      "tf.keras.preprocessing.image.random_shift consistently failing in graph mode\n",
      "\n",
      "Memory leak using nested name_scope in eager mode\n",
      "\n",
      "TFLite: Did not get operators, tensors, or buffers in subgraph 0. Error running on Android\n",
      "\n",
      "TFlite gets the incorrect value dividing by zero or computing tf.log(x)\n",
      "\n",
      "micro: port op FILL from lite\n",
      "\n",
      "error in converting ssdlite_mobilenet_v2_coco_2018_05_09 tflite_graph.pb to tflite\n",
      "\n",
      "I am trying to convert yolov3 to a full uint8 tflite model after, i got this error : Quantized not yet supported for op : 'EXP'\n",
      "\n",
      "Fix can't get a replica variable bug when use recompute_grad\n",
      "\n",
      "tflite 2.4 build benchmark_model error on ubuntu 16.04 (cmake)\n",
      "\n",
      "load_model in tf>=2.3.0 raises TypeError when loading a model saved in tf 2.2.0\n",
      "\n",
      "Tensorflow gpu does not work with RTX 3000 series card.\n",
      "\n",
      "Initial step for TensorBoard Callback\n",
      "\n",
      "What is the difference between Keras Attention and “Scaled dot product attention” as in the TF Transformer tutorial\n",
      "\n",
      "ReduceLROnPlateau forced casting on integer learning rates\n",
      "\n",
      "SeparableConv documention missing argument constraint\n",
      "\n",
      "Barrier in implementing demo: SHAPE -> STRIDED_SLICE impossible?\n",
      "\n",
      "Universal support for bfloat16 data, ie saving results of float32 operations as bfloat16 to memory by truncating\n",
      "\n",
      "Validation data generator yields 2 times, but should only 1\n",
      "\n",
      "Fail to find the dnn implementation while using recurrent layers\n",
      "\n",
      "Resource exhausted: MemoryError: Unable to allocate\n",
      "\n",
      "Trying to deploy hello_world on KW41Z (M0) device\n",
      "\n",
      "Unable to restore a layer of class TextVectorization - Text Classification \n",
      "\n",
      "Building tests fails on POWER\n",
      "\n",
      "DataLossError: Unable to parse tensor from stored proto when loading saved large tf dataset.\n",
      "\n",
      "need a  quantized model for posenet tensorflow lite\n",
      "\n",
      "Groups parameter of Conv2d and Conv2dTranpose ('deconvolution') not Working ?\n",
      "\n",
      "disable \"Successfully opened dynamic library libcudart.so\" logger\n",
      "\n",
      "Pretrained Xception takes more memory than an InceptionResNetV2, despite having half of the parameters\n",
      "\n",
      "Tensorflow 2.3.0 Bazel 3.1.0 compile ERROR\n",
      "\n",
      "model.inputs and model.outputs are None when creating a sub-classed model\n",
      "\n",
      "model doesn't learn when using shuffle='batch'\n",
      "\n",
      "Didn't find op for builtin opcode 'GATHER' version '1'\n",
      "\n",
      "Add uint8_t -> int8_t requant method\n",
      "\n",
      "TFLite Interpreter crashes with null TfLiteDelegatePtr\n",
      "\n",
      "Disable _FusedConv2D generation in grappler if XLA is on (GPU only).\n",
      "\n",
      "EarlyStopping with baseline doesn't set weights\n",
      "\n",
      "ESP32 LyraT: Audio data/I2S issue\n",
      "\n",
      "how to modify the input shape of existing model file again？\n",
      "\n",
      "Serializing EagerTensors in model.save()\n",
      "\n",
      "Errors when loading a model having keras.Lambda\n",
      "\n",
      "Lamda Layer seems to ignore output_shape parameter\n",
      "\n",
      "How can I build tensorflow C++ API on x86 platform\n",
      "\n",
      "TPUEstimator does not update meta-weights\n",
      "\n",
      "GFile does not create file when nothing is written.\n",
      "\n",
      "SubProcess ended with return code: 4294967295 with tensorflow-gpu 2.4.0rc cuda11.0 cudnn8.0.2 on windows 10\n",
      "\n",
      "mbed build broken with upgrade to Python 3.9 as part of CI\n",
      "\n",
      "TFLite - Java - Executing models on JVM not Android\n",
      "\n",
      "Allow --cpu=x64_arm64_windows as a compilation target on Windows\n",
      "\n",
      "TFLu: Port squeeze op from TFLite\n",
      "\n",
      "[TF2.4rc2] linalg.svd missing kernel for DT_HALF\n",
      "\n",
      "Metadata writer for custom TFLite object detection model\n",
      "\n",
      "Missing wheels tagged for macosx_11_x\n",
      "\n",
      "[INTEL MKL]  Added mkl_einsum op\n",
      "\n",
      "GPU and CPU utilization slashed after first epoch using tf.data and tf.keras\n",
      "\n",
      "Perhaps TF graph connectivity issues in keras-based Model containing Masking layers\n",
      "\n",
      "Intermediate-level guide in Time Series Forecasting example\n",
      "\n",
      "[Intel MKL] Enabling Quantized Conv ops in native format mode\n",
      "\n",
      "[Intel MKL] Changes in common files to enable MKL Quantized ops with native format\n",
      "\n",
      "C and Java perfomance differences under Android\n",
      "\n",
      "AttributeError: 'TensorArray' object has no attribute 'mark_used' with tf.function\n",
      "\n",
      "Adding a Lite flag to tf.keras.applications.EfficientNetBX\n",
      "\n",
      "Create makefile targets to expose file lists to enable separate scripting\n",
      "\n",
      "Create makefile targets to expose file lists to enable separate scripting\n",
      "\n",
      "Support SparseSegmentSum and SparseSegmentSumWithNumSegments on GPU\n",
      "\n",
      "Management of Gpu memory by fit method to avoid memory errors\n",
      "\n",
      "Model weights using the HDF5 format are not saved\n",
      "\n",
      "Deploy micro_speech to ESP32 , is running ,but do not get any output,how to modify?where is wrong\n",
      "\n",
      "Quantization differences from TensorFlow 1.9 to 2.3\n",
      "\n",
      "Numpy and dataframe converter\n",
      "\n",
      "KeyError when taking derivative to input of Conv2D with tf.function only\n",
      "\n",
      "build tensorflow_cc failed with cycle dependency graph\n",
      "\n",
      "Add support for ragged tensors to tf.keras.layers.experimental.preprocessing.Resizing\n",
      "\n",
      "OP_REQUIRES failed at conv_ops.cc:1106 : Not found: No algorithm worked!\n",
      "\n",
      "tensorflow/lite/micro/tools/make/Makefile:403: *** Something went wrong with the flatbuffers download: .  Stop.\n",
      "\n",
      "Wheel repairing broken when using tensorflow/tensorflow:custom-op-gpu-ubuntu16 docker image\n",
      "\n",
      "Request implementation of SSE regularization\n",
      "\n",
      "Reduce on MirroredVariable\n",
      "\n",
      "execution_profile_test_with_xla_hlo_profile_cpu fails on s390x\n",
      "\n",
      "Building r2.3 branch fails on Windows 10 x64\n",
      "\n",
      "Cannot use Hexagon delegate in Oneplus 6t. Failed to fetch Hexagon NN version. Not working since 18-Nov-2020.\n",
      "\n",
      "Test linking fails due to missing ldl\n",
      "\n",
      "Cannot run GPU on tensorflow Docker Image with Tensorflow 2.3.1\n",
      "\n",
      "TF Lite issue when loading a saved TF Lite model on platforms with different endianness\n",
      "\n",
      "Unable to perform subsampling using tf.data.Dataset map() method.\n",
      "\n",
      "I strongly suspect that we are going to want a dedicated tosa-opt of some form eventually, but we don't need to start there if it is not convenient. We will eventually have a dedicated/minimal binary for importing from TFLite Flatbuffers at least, and I'm keeping an eye on how we can keep the build boundaries sufficient to meet that end.\n",
      "\n",
      "TF_RegisterLogListener in C API does not seem to work\n",
      "\n",
      "Resource exhausted error when running tf.keras.Model.predict on large tensor\n",
      "\n",
      "xrt ops use bfc_allocator instead\n",
      "\n",
      "Incorrect error handling for invalid input names on SavedModel export\n",
      "\n",
      "Changing self properties does not trigger retracing\n",
      "\n",
      "Error LNK2005 when linking in debug mode.\n",
      "\n",
      "Quantization op for Reshape is not supported in keras\n",
      "\n",
      "[Intel MKL] Fuse group of primitive ops for batch normalization to Fu…\n",
      "\n",
      "tflite_runtime got installed successfully, however getting error while importing interpreter.\n",
      "\n",
      "RuntimeError: tensorflow/lite/kernels/concatenation.cc:76 t->dims->data[d] != t0->dims->data[d] (32 != 1)Node number 23 (CONCATENATION) failed to prepare. Node number 30 (WHILE) failed to invoke.\n",
      "\n",
      "Performance issue with 3070\n",
      "\n",
      "Dataset.unbatch() sets cardinality to -2 when batch remainder is not explicitly dropped\n",
      "\n",
      "[INTEL MKL]Register Eigen bf16 kernels with NoOps to make bf16 kernels(which In clear and infer list) can go through auto-mixed-precision pass.\n",
      "\n",
      "[Intel MKL] Updating mkl_layout_test to test for native format mode (Part 1)\n",
      "\n",
      "Problems to convert a tensorflow model to TFLite\n",
      "\n",
      "Slowdown training LSTM on TensorFlow 2.4+\n",
      "\n",
      "Issue in Bazel build for CUDA 11.1\n",
      "\n",
      "Avoid asymmetric quantization and uint8 support in Tensorflow Lite Micro as much as possible.\n",
      "\n",
      "Multiple models sequentially with TFLu\n",
      "\n",
      "RecursionError with `dynamic=True` when using a `Lambda` layer \n",
      "\n",
      "Boolean_mask can't be compiled by XLA on CPU\n",
      "\n",
      "tflite: why using mali gpu will cause memory leak?\n",
      "\n",
      "yolov3-tiny tflite model of shape [1, 2535, 85] representation\n",
      "\n",
      "Make tf.debugging.assert_shapes support agnostic comparisons\n",
      "\n",
      "tf.debugging.assert_shapes inconsistent behaviour for scalars\n",
      "\n",
      "Enable TensorFloat32 with XLA\n",
      "\n",
      "TF Dataset performance regression / best practices for data augmentation on the accelerator\n",
      "\n",
      "No Operation named [input] in the Graph running in React-Native\n",
      "\n",
      "Confusing results from tf.math.mod. An issue with precision?\n",
      "\n",
      "NotFoundError when using an optimizer on complex variables\n",
      "\n",
      "Optimize Tensorflow for Cortex-R MCUs\n",
      "\n",
      "postnet  android\n",
      "\n",
      "How to pass the seq length in the LSTMs in tfLite\n",
      "\n",
      "what is the difference between tf.nn.fixed_unigram_candidate_sampler and  tf.random.log_uniform_candidate_sampler\n",
      "\n",
      "[TFLite] Quantization and unfolding of the ADD_N operator\n",
      "\n",
      "TFLite: GPU delegate (OpenCL) not copying output data from GPU to CPU.\n",
      "\n",
      "Undefined symbols for architecture arm64 _TFE_*\n",
      "\n",
      "TFRecord add total_num\n",
      "\n",
      "ambiguity of data type of weight\n",
      "\n",
      "set_visible_devices with MirroredStrategy causes NCCL warnings\n",
      "\n",
      "Exception: <unknown>:0: error: loc(\"batch_normalization/moving_mean\"): is not immutable, try running tf-saved-model-optimize-global-tensors to prove tensors are immutable\n",
      "\n",
      "TF Lite Micro : Hosted Model: \"Didn't find op for builtin opcode 'SQUEEZE'\"\n",
      "\n",
      "Efficient Ragged Tensor support on TFlite\n",
      "\n",
      "EfficientNet loading slowly from SavedModel format\n",
      "\n",
      "tf.keras.preprocessing.image_dataset_from_directory ignores labels passed in 'labels' argument\n",
      "\n",
      "Fix class_names in image_dataset_from_directory\n",
      "\n",
      "ERROR: /data/workspace/willy_sung/tensorflow/tensorflow/python/tools/BUILD:282:1 \n",
      "\n",
      "tf.random.log_uniform_candidate_sampler gives undesired true class\n",
      "\n",
      "Memory leak with keras metrics\n",
      "\n",
      "Java API load model core dumped\n",
      "\n",
      "Loading images with non-inferred labels\n",
      "\n",
      "Add support for CUDA 11.1 on Windows 10 for the 8.6 compute capability\n",
      "\n",
      "Fix wrong path separator in error message when saved model does not exist\n",
      "\n",
      "Wrong path separator in error message when saved model does not exist (in keras.models.load_model)\n",
      "\n",
      "Discussion of Apollo3 TFLu PRs\n",
      "\n",
      "Bug/Issue tf.data.Dataset for Keras multi-input model\n",
      "\n",
      "TFLite TensorArrayScatterV3 failed (not found)\n",
      "\n",
      "LeakSanitizer: detected memory leaks\n",
      "\n",
      "//tensorflow/compiler/xla/service/cpu:vectorized_reduce_with_no_vector_registers_test fails on s390x \n",
      "\n",
      "On-disk cache for compiled functions\n",
      "\n",
      "[TFLite] TF MatMul operator converted to an unsupported TFL 16x16 FullyConnected operator with 16x8 post-training quantization\n",
      "\n",
      "tf concatination layers compatible with tfp\n",
      "\n",
      "Actor-critic model with LSTM layers runs out of GPU memory eventhough it shouldn't.\n",
      "\n",
      "Support string sorting\n",
      "\n",
      "support SparseToDenseOp on GPU\n",
      "\n",
      "Training takes forever when applying distribute strategy\n",
      "\n",
      "Incorrect Cupti dll name on W10\n",
      "\n",
      "Memory leak in TF 2.3.1 when using Multi-worker training\n",
      "\n",
      "How can I use micro_mutable_op_resolver.h instead of all_ops_resolver.h to run TensorFlow Lite for Microcontrollers exapmle?\n",
      "\n",
      "Bad performance of tf.data.Dataset API when dealing with windowed inputs / timeseries problems / 3D arrays\n",
      "\n",
      "BUG: Keras SaveModel does not properly save optimizer state\n",
      "\n",
      "tf.linalg.pinv also for complex matrices\n",
      "\n",
      "[Go] package go-gettable, update wrappers, fix tests\n",
      "\n",
      "Version requirements for Python dependencies too strict\n",
      "\n",
      "test with random fail on graph mode\n",
      "\n",
      "Tensorflow 2+ framework class exception\n",
      "\n",
      "Move EthosU custom op out of AllOpsResolver\n",
      "\n",
      "ambiguous template instantiation error with Eigen code\n",
      "\n",
      "cuDNN LSTMs not utilized after loading a model using `tf.saved_model.load`\n",
      "\n",
      "Tensorflow 2.3.1 is not compatible with cuda 10.2 on Windows\n",
      "\n",
      "Different behavior of model.predict and .call if training=True at dropout layer with pre-set seed\n",
      "\n",
      "[TF 2.4] KerasTensor breaks typing compatibility\n",
      "\n",
      "Convert Tensorflow model to TFLite model - problem with boxes after quantization\n",
      "\n",
      "Make TensorFlow Lite available as Swift Package Manager package or XCFramework\n",
      "\n",
      "tensorflow lite Makefile issue : nnapi_delegate_provider.cc is not compiled and nnapi cannot be used from command line of benchmarck_model\n",
      "\n",
      "Functionality of writing back Tflite model with multiple subgraphs that is currently loaded in the Tflite Interpreter.\n",
      "\n",
      "fatal error: tensorflow/core/framework/types.pb.h: No such file or directory\n",
      "\n",
      "Layer.add_loss has wrong behaviour when building Sequential model in for loop\n",
      "\n",
      "next() not raising StopIteration in tf.python.keras.preprocessing.image.DirectoryIterator\n",
      "\n",
      "Cadence xtensa_hifi kernels update\n",
      "\n",
      "Add more requant methods\n",
      "\n",
      "Dataset.from_tensor_slices regression?\n",
      "\n",
      "TFRecordWriter create in parent process can't work properly in child process\n",
      "\n",
      "Enforce single optimized kernel implementation\n",
      "\n",
      "from_tensor_slices not compatible with sparse data\n",
      "\n",
      "Error using TensorBoard callback in graph mode in TF 2.4\n",
      "\n",
      "tf.convert_to_tensor is slow when converting list of numpy arrays\n",
      "\n",
      "tf.keras.model fit() CUDA crash when using generator inputs\n",
      "\n",
      "Does TensorFlow Micro support self define output type for each layer?\n",
      "\n",
      "Add use_default_shell_env = True to all ctx.actions.run_shell rules\n",
      "\n",
      "No Keras callback for sumary database writer\n",
      "\n",
      "Cannot export Keras sub-classed model with 2 args as a SavedModel\n",
      "\n",
      "tensorflow/go@v2.3.1: but does not contain package go/core/protobuf/for_core_protos_go_proto\n",
      "\n",
      "Unable to parse stateful RNN in C API of Tensorflow 2.3.1\n",
      "\n",
      "attr return values handled differently than dict by tf.distribute.Strategy.run\n",
      "\n",
      "TypeError: call() missing 2 required positional arguments: 'features' and 'hidden'\n",
      "\n",
      "tf.device scope not working correctly\n",
      "\n",
      "Building TensorFlow Lite library with tensorflow ops failed\n",
      "\n",
      "tensorflow.keras.callback.TensorflowCallback turns off recording for tf.summary, and stops other callbacks from recording summaries\n",
      "\n",
      "TPUStrategy.run fails on non-primary thread with \"No OpKernel was registered\", \"TPUReplicatedInput\"\n",
      "\n",
      "Support Python 3.9\n",
      "\n",
      "TFlite Custom trained model Issue (IndexError: invalid index to scalar variable)\n",
      "\n",
      "ByteBuffer is not a valid flatbuffer model\n",
      "\n",
      "Add WinRT (UWP) as a supported platform\n",
      "\n",
      "GradCAM and nested models\n",
      "\n",
      "Possible memory leak in tf.keras.layers.experimental.preprocessing.TextVectorization\n",
      "\n",
      "Compiler Test cases with tf-mlir-translate pass/crash with specific build flag on s390x architecture\n",
      "\n",
      "tfl_quantizer\n",
      "\n",
      "[ssd mobileNet v2 model] ValueError: Failed to parse the model: pybind11::init(): factory function returned nullptr.\n",
      "\n",
      "Cannot save a subclassed keras model that relies on AutoGraph\n",
      "\n",
      "OP_REQUIRES failed at constant_op.cc\n",
      "\n",
      "Model not deterministic, even though os.environ['TF_DETERMINISTIC_OPS'] = '1' is set\n",
      "\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: 'func' argument to TF_GraphCopyFunction cannot be null\n",
      "\n",
      "How are batch gradients computed on embedding layers?\n",
      "\n",
      "TF Nightly fails to find most recent cusolver64\n",
      "\n",
      "GPU kernel for SparseSegmentReduction ops.\n",
      "\n",
      "tf.data.Dataset.from_tensor_slices requests same shape tensors\n",
      "\n",
      "the moving_mean and moving_variance in batchnormalization going to very big after 20 epoch \n",
      "\n",
      "How to get detailed performance profiling results of TFLite for Apple Metal on iOS devices?\n",
      "\n",
      "`LLVM ERROR: Cannot select` issue when compiling LLVM module on s390x architecture\n",
      "\n",
      "Tensorflow randomly crashes running multiple consecutive images through classifier model TF_SessionRun() in c api\n",
      "\n",
      "Enable control of use of caching in tf.keras.preprocessing.image_dataset_from_directory\n",
      "\n",
      "Unreferenced buffers in flatbuffer\n",
      "\n",
      "CTC Error crashes on empty GPU batch\n",
      "\n",
      "Add gpu implementation of ResourceSparseApplyAdadelta Op for issue #2314 \n",
      "\n",
      "Problems generating the Hello-World-Project von ESP32\n",
      "\n",
      "Tensorflow auto killed my training processing.\n",
      "\n",
      "Concrete Function output shape sometimes changes after save/load cycle\n",
      "\n",
      "[TFLite] [GPU delegate] DataLayout::DHWC4 and ObjectType::OPENGL_SSBO input does not work\n",
      "\n",
      "TF Lite GPU delegate - Selection of GPU device\n",
      "\n",
      "Non-deterministic results if using Functional model creation style on GPU\n",
      "\n",
      "Models for person_detect used in micro vision demo of Tensorflow Lite\n",
      "\n",
      "Build up new Optimizer\n",
      "\n",
      "Quantized MobileBERT and ALBERT models performing very poorly\n",
      "\n",
      "Add extra warning info for lr\n",
      "\n",
      "Building TF-2.2 OpenCL triSYCL-1.2 on macOS-10.15 with GCC-4.9 and Bazel-2.0.0 Failed\n",
      "\n",
      "tensorflow-nightly-gpu looking for cusolver64_10.dll on a cuDNN 11.1 installation\n",
      "\n",
      "missing folder absl? \n",
      "\n",
      "Converted TFLite model accuracy drop\n",
      "\n",
      "training keras Sequential model with python generator raise error: AttributeError: 'tuple' object has no attribute 'rank'\n",
      "\n",
      "update grpc to v1.32.0 to fix val.val issues while compiling TFS from source\n",
      "\n",
      "Tensorflow dataset is not faster with multiple cores\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is slow when using tf.keras.utils.Sequence with large Numpy arrays\n",
      "\n",
      "TensorFlow Lite Error: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. TensorFlow Lite Error: Node number 0 (FlexConv3D) failed to prepare.  Fatal error: Failed to allocate memory for input tensors.\n",
      "\n",
      "How to force to run a function on GPU\n",
      "\n",
      "tensorflow/tools/pip_package/BUILD:66:1 C++ compilation of rule '//tensorflow/python:bfloat16_lib' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command\n",
      "\n",
      "why the scales in quantization are floats not in the format of m/2^n\n",
      "\n",
      "opcode 'SUB' version '3' in tfllite_runtime on Windows\n",
      "\n",
      "Embedding layer with NaN input different behaviour on CPU vs GPU\n",
      "\n",
      "TensorFlow Lite Crashed when calling void tflite::reference_ops::Gather<long, int>\n",
      "\n",
      "Internal: No unary variant unary_op function found for unary variant op enum: 1 Variant type_name: RaggedTensorVariant for device type: GPU\n",
      "\n",
      "TFLite model weight shape is different from original model\n",
      "\n",
      "tensor.numpy() from GPU model prediction very slow\n",
      "\n",
      "Cannot access a resource variable from a custom op on Mac and Windows\n",
      "\n",
      "tf.keras.layers.Embedding forces CPU placement if eager execution is enabled and GPUs are present.\n",
      "\n",
      "mixed_float16 + example code = horrible performance on a GeForce GTX 1660 Ti\n",
      "\n",
      "ESP32: Register_RESIZE_NEAREST_NEIGHBOR not a member \n",
      "\n",
      "Not able to convert model to c array after quantization\n",
      "\n",
      "Missing compiler flags for some Cortex-M target architectures\n",
      "\n",
      "cpu cores not fully utilized causing gpu bottleneck\n",
      "\n",
      "bool disable_per_channel: per-channel vs. per-layer quantization\n",
      "\n",
      "cuda_11.1.0_456.43_win10 + cudnn-11.1-windows-x64-v8.0.4.30 + master branch= some errors\n",
      "\n",
      "keras.models.load_model() checking for keras configuration mismatch\n",
      "\n",
      "Running TFlite on mobile device GPU fails  - Ops not supported\n",
      "\n",
      "Optimize GPU memory consumption: Decrease heap usage at the beginning of the training and allow GPU to use 100% fragmentation.\n",
      "\n",
      "3080 & 3090 coumpute capability 86 degraded performance after some updates\n",
      "\n",
      "Unable to fit against and batch ragged output in Keras.\n",
      "\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "\n",
      "Make fname optional in get_file\n",
      "\n",
      "Dealing with `from tensorflow.contrib import ...` in  tf_upgrade_v2 \n",
      "\n",
      "Add tflite_runtime 2.3.1 wheels to Python quickstart\n",
      "\n",
      "Feature request: ResNet34\n",
      "\n",
      "How to remove unused operators from tensorflowlite_c.so to reduce it's size?\n",
      "\n",
      "How do we know the compatibility of flatbuffers version in SchemaGenerated.h \n",
      "\n",
      "`size` argument of TensorArray works only when specified by pythonic int but tf.Tensor doesn't\n",
      "\n",
      "tf.keras.layers.experimental.preprocessing.TextVectorization doesn't raise error when  len(vocabulary_list)<=max_tokens<len(vocabulary_list) + 2\n",
      "\n",
      "TensorFlow C API vs. TensorRT: \"Op type not registered 'TRTEngineOp' in binary running on ...\"\n",
      "\n",
      "[TfLite] gpu gl delegate: reduce_mean([1,2]) returns average of first row only \n",
      "\n",
      " go get github.com/tensorflow/tensorflow/tensorflow/go throwing an error\n",
      "\n",
      "full integer quantization error\n",
      "\n",
      "dataset windows don't work in batches\n",
      "\n",
      "[TFLite] Built-in way to run the reference kernels with the Python API\n",
      "\n",
      "Fix compilation of xtensa_hifi activations kernel\n",
      "\n",
      "different result from tf.image.resize in tensorflow 2\n",
      "\n",
      "How to pass mutable tensors to a Custom Op in Python?\n",
      "\n",
      "Densenet (Floating point models) reported accuracy is much lower than the original paper\n",
      "\n",
      "Unable to build TensorFlow targeting Raspberry Pi\n",
      "\n",
      "In TFnighty TFLite  Interpreter missing setUseNNAPI \"wrapper\" method  to call Interpreter.Options.setUseNNAPI\n",
      "\n",
      "TFLite not respecting ByteBuffer's limit\n",
      "\n",
      "ValueError: The same saveable will be restored with two names: layer_with_weights-1/_table/.ATTRIBUTES/table\n",
      "\n",
      "Comparison of gpu acceleration between gtx 1660ti(6g video memory) and quadro gv100(32g video memory)\n",
      "\n",
      "Regularization Loss is getting added twice if model is encapsulated in another model\n",
      "\n",
      "remote devices in cluster not visible in TF 2\n",
      "\n",
      "Method to get k smallest elements from a tensor\n",
      "\n",
      "Memory leak with tf.data.Dataset.map\n",
      "\n",
      "Enable profiling in release builds\n",
      "\n",
      "How to build tflite C++ library for ios from tf 2.3 onwards?\n",
      "\n",
      "tf.function and PerReplica tf.Variable does not work together.\n",
      "\n",
      "Masking scores before Softmax in Attention discussed here - https://www.tensorflow.org/tutorials/text/nmt_with_attention\n",
      "\n",
      "support Quantization Aware Training for Conv2dTranspose->BathcNorm->Activation\n",
      "\n",
      "[EfficientNetB7] validation not getting correct accuracy\n",
      "\n",
      "please expose uniform_full_int on tf.random\n",
      "\n",
      "TFLite Interpreter Crashes when running with TF SELECT op\n",
      "\n",
      "Missing motivation for TF hacking in Pix2Pix Generative tutorial\n",
      "\n",
      "Using Orthogonal initializer in dense layer with backend set to fp16 gets stuck\n",
      "\n",
      "Allow custom image size for NASNet models\n",
      "\n",
      "Keras Functional API does not support tf.custom_gradient() with tf.numpy_function()\n",
      "\n",
      "Optimized cadence kernels need to use the new TfLiteEvalTensor API\n",
      "\n",
      "\"device kernel image is invalid\" when using pre-built linux+gpu c library\n",
      "\n",
      "Prefetch to GPU: prefetch_to_device does not do anything (from_generator), tf.data.Dataset API unclear\n",
      "\n",
      "Per Example Gradients fail with LSTM\n",
      "\n",
      "[TF-TRT] Cast now supports more conversions if TRT >= 7.0.0.11\n",
      "\n",
      "No way to initialize Interpreter with Model reference in TensorFlowLiteSwift\n",
      "\n",
      "Update Keras 2D convolution docs\n",
      "\n",
      "Using SYSTEM cURL (build against OpenSSL) conflicts with BoringSSL\n",
      "\n",
      "TFLite: Getting error \"Expected bias tensor to be a vector\" when trying to convert and quantize a model\n",
      "\n",
      "Wrong linker flags in Makefile for ARM Compiler\n",
      "\n",
      "Fix tf.function speacial casing for forwardporp (placeholder issue)\n",
      "\n",
      "CUDA driver is not load in docker container\n",
      "\n",
      "TFLite ConverterError: Node has inputs from different frames\n",
      "\n",
      "Go installation error\n",
      "\n",
      "Access to GPU memory usage\n",
      "\n",
      "Unexpected output shape when trying to convert to Frozen Graph using convert_variables_to_constants_v2\n",
      "\n",
      "RPC retries do not work on Istio\n",
      "\n",
      "Horovod fails to train Keras Preprocessing IntegerLookup Layer\n",
      "\n",
      "TF2.3 converter.convert ValueError: Input 0 of node StatefulPartitionedCall/functional_1/resnet50/conv1_bn/AssignNewValue was passed float from Func/StatefulPartitionedCall/input/_5:0 incompatible with expected resource.\n",
      "\n",
      "Enable multiple input pipelines with a single machine, MultiWorkerMirroredStrategy's experimental_distribute_datasets_from_function\n",
      "\n",
      "Serial printing breaks after updating Mbed OS\n",
      "\n",
      "Support tf.keras.metrics.MeanIoU on TPU\n",
      "\n",
      "Possibility to compile python code when @tf.function is used in TensorFlow 2.x \n",
      "\n",
      "cuda_configure.bzl: wrong cuda_config.cudnn_version cause wrong cudnn_headers list\n",
      "\n",
      "Include support for the Constant-Q transform.\n",
      "\n",
      "Google Collab error for TPU - UnavailableError: {{function_node __inference_train_function_99378}} failed to connect to all addresses \n",
      "\n",
      "Interpreter takes too much time to  return results while similar iOS app makes it 10 times faster\n",
      "\n",
      "embedding_lookup cause ran out of memory \n",
      "\n",
      "bitwise op with 1 msb throws in tf.function\n",
      "\n",
      "Saving and loading Keras model with RNN layer that uses both multiple input and constants will result in ValueError when using model.predict()\n",
      "\n",
      "third_party/gpus/find_cuda_config.py is unable to detect cudnn version and causing `./configure` to fail\n",
      "\n",
      "Dataset.from_generator() returns a Dataset which freezes execution upon iteration\n",
      "\n",
      "UnknownError: Failed to get convolution algorithm. \n",
      "\n",
      "training.tracking.data_structures.List is not properly serialized / deserialized\n",
      "\n",
      "Segmentation fault (core dumped)\n",
      "\n",
      "Keras Backend ones_like with Lambda is not serializable\n",
      "\n",
      "Variable and Slow InfeedEnqueueTuple on TPUv3-8\n",
      "\n",
      "Batch Renormalization via Keras Layer not working with TF 2+ and tf.function\n",
      "\n",
      "XLA Compilation Bug\n",
      "\n",
      "Docs: provide CUDA/CuDNN/TensorRT version information for PyPI packages\n",
      "\n",
      "Masking support for multiple values.\n",
      "\n",
      "ParseTensor (tf.io.parse_tensor) is not vectorized - Vectorizing via tf.vectorized_map uses while_loop\n",
      "\n",
      "Multiple instances of custom metrics don't behave as expected with multiple output model when evaluating.\n",
      "\n",
      "More details and examples for tf.image.generate_bounding_box_proposals\n",
      "\n",
      "Remove CMSIS Core include from all Makefiles\n",
      "\n",
      "[Draft] Adding Multi device iterator Serialization and De-serialization\n",
      "\n",
      "Add Ubuntu 20.04 installation instructions\n",
      "\n",
      "InvalidArgumentError with `Conv1D` which has the same `dilation_rate` as the batch size, using `tf.keras.Model.fit`, on TPU\n",
      "\n",
      "[TransformGraph & TF2 SavedModel] - Missing API\n",
      "\n",
      "XLA Compilation does not work with Embeddings Layer\n",
      "\n",
      "Wrong maximum value passed in the positional encoding for the Transformer model\n",
      "\n",
      "tf.keras.layers.experimental.preprocessing.CategoryEncoding: is it possible to apply it to a nD tensor?\n",
      "\n",
      "IteratorGetNext: unsupported op: No registered 'IteratorGetNext' OpKernel for XLA_GPU_JIT devices compatible with node {{node IteratorGetNext}} \t.  Registered:  device='XLA_GPU'\n",
      "\n",
      "TFLite android image classification demo failed on samsung note10 whose soc is snapdragon 855\n",
      "\n",
      "Core dumped when invoking TFLite model converted using latest nightly TFLite converter (2.4.0dev2020929)\n",
      "\n",
      "null pointer dereference Error in TF2.3.0 with runforMultipleInputOutput\n",
      "\n",
      "A new feature in tf.roll\n",
      "\n",
      "Custom loss function is not working\n",
      "\n",
      "LookupError when computing nested gradient with UpSampling2D\n",
      "\n",
      "Keras EarlyStopping callback on val_auc running mysteriously\n",
      "\n",
      "k hot encoding for tf.keras.preprocessing.image.DirectoryIterator\n",
      "\n",
      "tf.cond throws AssertionError when computing its gradient twice\n",
      "\n",
      "TF2 Memory Leak when fitting multiple models in Docker\n",
      "\n",
      "Feature request: MCUXpresso IDE integration\n",
      "\n",
      "Feature request: STM32CubeIDE integration\n",
      "\n",
      "Compiling Person Detection Model for zephyr_riscv\n",
      "\n",
      "Metal delegate Crash with C++ interface\n",
      "\n",
      "[PluggableDevice] Add device alias support\n",
      "\n",
      "Debugger V2 not working.  Invalid argument:  DebugNumericSummaryV2Op requires tensor_id to be less than or equal to (2^53). Given tensor_id:26\n",
      "\n",
      "Example of inferencing a Tensorflow lite model with parsing_serving_input_receiver_fn using C++ API\n",
      "\n",
      "tfio.audio.AudioIOTensor doesn't prepare data in time in loop.\n",
      "\n",
      "RuntimeError: Fill only currently supports int32, int64, float32, bool, string for input 1, got 9.Node number 412 (FILL) failed to invoke.\n",
      "\n",
      "WorkerTrainingState Test and Bugfix\n",
      "\n",
      "RaggedTensor raises error with Keras model\n",
      "\n",
      "Tensorflow lite(ssd_mobilenet_v2_fpnlite_640*640)  Android APP detect nothing\n",
      "\n",
      "Training loop from scratch for multi-out model - how to calculate loss and update it.\n",
      "\n",
      "TensorBoard callback doesn't update step properly\n",
      "\n",
      "CMSIS-NN: Add support for dilation\n",
      "\n",
      "Spurious tf.function retracing warnings, when developing Keras layer in colab\n",
      "\n",
      "Cannot use Hexagon delegate in Samsung S20 ultra. Failed to fetch Hexagon NN version. \n",
      "\n",
      "Problem In tensorflow-gpu with error \"Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.20GiB with freed_by_count=0.\"\n",
      "\n",
      "Problem with custom grad with multiple external variables\n",
      "\n",
      "\"IndexError: list index out of range\" when load EfficientDet SavedModel with tf.keras\n",
      "\n",
      "keyword benchmark broken for bluepill with TAGS=cmsis-nn\n",
      "\n",
      "keras.layers.experimental.preprocessing.Discretization fails for mixed precision training\n",
      "\n",
      "Official Tensorflow build for s390x CPU\n",
      "\n",
      "einsum for SparseTensors\n",
      "\n",
      "Feature Request: Support sparse to dense with max_length input\n",
      "\n",
      "Add support for dataset to pandas dataframe\n",
      "\n",
      "Reusing Keras.Model errors\n",
      "\n",
      "Significant accuracy drop during conversion of DistillBert.\n",
      "\n",
      "TFLiteCCoreML Delegate Runtime Crash (Espresso::ANERuntimeEngine overflow error)\n",
      "\n",
      "Missing TRANSPOSE Op Kernel\n",
      "\n",
      "Additive Attention Layer is not really Additive Attention\n",
      "\n",
      "[ROCm] Incorrect path to clang. \n",
      "\n",
      "\"Cadence processor cores only\" license restriction\n",
      "\n",
      "Feature Request: Support for ragged argmin/argmax\n",
      "\n",
      "Feature request: Test in cl_build for the esp32\n",
      "\n",
      "Complete functionality of SparseTensor.__mul__\n",
      "\n",
      "top_k crashes for certain large structures\n",
      "\n",
      "nan is clipped to the upper\n",
      "\n",
      "Support RESIZE_BILINEAR in TFLu\n",
      "\n",
      "Negate Kernel for int8 is yet to be implemented\n",
      "\n",
      "Fixes #43200 - TensorBoard histogram breaks on unsupported weight dtypes\n",
      "\n",
      "Fixes #42872: map_to_outputs_names always returns a copy\n",
      "\n",
      "Got Segmentation fault when calling tflite::Model::UnPack() \n",
      "\n",
      "Using keras GRUCell layer in TFLM\n",
      "\n",
      "tf.keras.Model with multiple outputs, using fit() with a generator dataset passes y to loss with wrong shape\n",
      "\n",
      "XLA future\n",
      "\n",
      "Cant save keras RNN model with custom cell whose call function accepts constants\n",
      "\n",
      "Unable to build tensorflowlite.dll in Windows with FLEX delegate support - library limit of 65535 objects exceeded\n",
      "\n",
      "distributed_dataset tensorflow.python.framework.errors_impl.UnavailableError: Socket closed\n",
      "\n",
      "Pad() gradient\n",
      "\n",
      "Support AWS IAM roles when using TensorFlow file_io\n",
      "\n",
      "Tensorflow lite speech sample is not detecting silence (wrong detections)\n",
      "\n",
      "Tensorflow Lite: iOS 14 breaks the NPU Delegate\n",
      "\n",
      "Standard Deviation calculation Pytorch vs TF, how to set unbiased\n",
      "\n",
      "vectorized_map fails if function contains an abs of a complex128\n",
      "\n",
      "Crash when attempting to use xla.conv with complex inputs\n",
      "\n",
      "Kernel crash with complex matrices on Windows\n",
      "\n",
      "edgeTPU compiler error on TF2.3 quantized model\n",
      "\n",
      "My doc build is picking up methods not in the advertised API\n",
      "\n",
      "TensorFlow Lite (2.3.0) Converter Quantization Incompatibility for Coral Edge TPU Compiler\n",
      "\n",
      "TF 2.3 - loading of saved_model from disk with ragged=True input is slower than ragged=False\n",
      "\n",
      "Interpreter AllocateTensors() does not finish or report error\n",
      "\n",
      "libtensorflow_inference.so: protobuf failed to link __android_log_write and dl_iterate_phdr\n",
      "\n",
      "Didn't find op for builtin opcode 'CONV_2D' version '5'\n",
      "\n",
      "running TF operations on mobile GPU - error conversion to dynamic graph yields issues while running on mobile GPU\n",
      "\n",
      "micro speech example not working with STM32F769\n",
      "\n",
      "tf.keras.callbacks.TensorBoard histogram summary breaks on tf.bool layer weights\n",
      "\n",
      "tf.keras SparseCategoricalCrossentropy with sample_weight on TPU: Error DenseToDenseSetOperation (No registered 'DenseToDenseSetOperation)\n",
      "\n",
      "error when trying to modify code to train  on tpu's.\n",
      "\n",
      "NotFoundError:  No algorithm worked! when using Conv2D\n",
      "\n",
      "Segfault with tf.linalg.cholesky\n",
      "\n",
      "SyncBatchNormalization has NaN losses with channels-first format\n",
      "\n",
      "Uses of the new rocm skip test decorator.\n",
      "\n",
      "`validation_split` support for RaggedTensors\n",
      "\n",
      "[EfficientNet] Understanding architectural implementation decision for the Squeeze and Excitation phase\n",
      "\n",
      "Tensorflow 2.3.0 TfLite converter adds ExpandDims\n",
      "\n",
      "Generating ICU normalization_data.c and icu_conversion_data.c on s390x architecture\n",
      "\n",
      "[TFLu] int8 ops slower than f32\n",
      "\n",
      "TF lite fails conversion with post train int quantization, converts fine without: RuntimeError: Mismatch between number of weight maxs and channels\n",
      "\n",
      "Have similar default values for min_delta in ReduceLROnPlateau and EarlyStopping callbacks\n",
      "\n",
      "Allow symmetric TFLite quantization (no zero point/scale only) \n",
      "\n",
      "Understanding the Difference between Normal Training vs eager mode training in TFOD API v2\n",
      "\n",
      "RaggedTensor support for model output\n",
      "\n",
      "tf.data.experimental.make_csv_dataset, currently only supports one label; However, most datasets can have more than one label\n",
      "\n",
      "Support INT16 quantisation for RESIZE_NEAREST_NEIGHBOR\n",
      "\n",
      "interperter with GPU Delegate blocks Android from rendering on Main thread.\n",
      "\n",
      "Duplicate code in `tf.python.keras.engine.training.Model.evaluate()`, `train_on_batch()` and `test_on_batch()`\n",
      "\n",
      "TFLite model gives random outputs when running on Android\n",
      "\n",
      "C++ canonicalize_function_inputs WIP\n",
      "\n",
      "How to use unified memory in TF2.1\n",
      "\n",
      "Pylint incorrectly identifies tensorflow public API functions in tensorflow 2.2+\n",
      "\n",
      "Error \"failed to connect to all addresses\" when training on TPU with Colab\n",
      "\n",
      "Generate sparse tensor in tflite\n",
      "\n",
      "XLA Asynchronous compilation\n",
      "\n",
      "Could not load dynamic library 'cupti64_110.dll'; dlerror: cupti64_110.dll not found\n",
      "\n",
      "Using Intermediate (non-Input) Layer as input to secondary network\n",
      "\n",
      "Arduino tensorflow\n",
      "\n",
      "Dataset.from_generator breaks on last element\n",
      "\n",
      "tflite(micro) pointer-arithmetic overflow left unhandled in TfLiteStatus SimpleMemoryAllocator::EnsureHeadSize\n",
      "\n",
      "Go implementation does not provide SummaryImage/SummaryAudio\n",
      "\n",
      "Micro speech example not working on Sparkfun Edge\n",
      "\n",
      "Loading a TF1 Protocol buffer does not work in TF versions 2.2.0 and above\n",
      "\n",
      "tf.nn.max_pool2d: No support for tf.Tensor input for ksize\n",
      "\n",
      "Did not find a parser for CONV_2D with latest repo download\n",
      "\n",
      "Extremely slow eigendecomposition compared to numpy/scipy.\n",
      "\n",
      "tf.io.gfile.glob hangs if gcs directory contains a directory named '/'.\n",
      "\n",
      "[Intel MKL] unit test for bfloat16 on matmul.\n",
      "\n",
      "tf.nn.ctc_greedy_decoder: self define the output node's name\n",
      "\n",
      "New tf.image.resize method: Maximum Value\n",
      "\n",
      "Converter spews out alphanumeric symbols\n",
      "\n",
      "How to save tf.data.Dataset object?\n",
      "\n",
      "Failed to convert SparseTensor to Tensor\n",
      "\n",
      "bug when using \"num_parallel_calls\" when mapping dataset to tfa function\n",
      "\n",
      "C++ FunctionSpec() - all methods besides canonicalize_function_inputs\n",
      "\n",
      "nan gradient issue\n",
      "\n",
      "Uninitialized memory access of per-channel params\n",
      "\n",
      "NMS support in XLA\n",
      "\n",
      "TensorArray issues in XLA\n",
      "\n",
      "control flow support issue in TensorFlow XLA\n",
      "\n",
      "Tensorflow model dump in c++ during inference\n",
      "\n",
      "ValueError: Unable to save the object (Saving a Keras.model)\n",
      "\n",
      "Group Conv3D malloc abort trap 6 \n",
      "\n",
      "Add a warning when fitting models loaded from SavedModels\n",
      "\n",
      "org.tensorflow:tensorflow:jar:2.3.0 not found via Maven\n",
      "\n",
      "tflite delagte to nn accelerator\n",
      "\n",
      "which tensorflow version starts to support XLA compiler?\n",
      "\n",
      "libtensorflowlite XNNPACK support beyond f32\n",
      "\n",
      "tensorflowlite v2.3 + XNNPACK run into error {ModifyGraphWithDelegate is disallowed}\n",
      "\n",
      "STM32F407G Build Error - AddBuiltin() mismatch in arguments\n",
      "\n",
      "Unable to find the tf.raw_ops.mfcc python code in tensorflow\n",
      "\n",
      "Error on doing regression with weighted time steps using sample_weight_mode='temporal'\n",
      "\n",
      "ModelCheckpoint behavior doesn't change when changing save_weights_only from True to False for custom tf.keras.Model\n",
      "\n",
      "non_max_suppression outputs each ROI's assignment to non-max supressed bbox\n",
      "\n",
      "Use of GPU delegate increases inference runtime in Android's Native environment (C++) but not using Java API\n",
      "\n",
      "error: undefined reference to '__umoddi3'\n",
      "\n",
      "DataHandler intermittent InvalidArgumentError for large input dimension\n",
      "\n",
      "Translataion bug in tf.math.reduce_variance and tf.keras.backend.var\n",
      "\n",
      "Cannot build network with a dict input_spec\n",
      "\n",
      "individual neuron pruning (similar to tfkerassurgeon)\n",
      "\n",
      "Unrecognized command line -msse3 while cross-compiling tensorflow to aarch64 targets\n",
      "\n",
      "ImageDataGenerator should have a verbosity argument\n",
      "\n",
      "adapt() method of a CategoryEncoding layer returns the wrong shape when fitted to Integer list not starting at 0\n",
      "\n",
      "Cannot convert a Tensor of dtype resource to a NumPy array\n",
      "\n",
      "Could not find TensorFlow Java 2.3.0 release in maven central\n",
      "\n",
      "Resnet gradients cpp\n",
      "\n",
      "Slow training speed of  tensorflow.keras.layers.SeparableConv2D and tensorflow.keras.layers.DepthwiseConv2D\n",
      "\n",
      "from_dlpack unable to process arrays with column-major strides\n",
      "\n",
      "[tflite] Conv2DTranspose output difference between mobile CPU and mobile GPU \n",
      "\n",
      "Running the Smart Replies Model in TFLite Python\n",
      "\n",
      "Cannot convert predict function of LinearRegressor\n",
      "\n",
      "error when installing for go from source.\n",
      "\n",
      "Size 1 must be non-negative error when using boolean mask tensorflow\n",
      "\n",
      "Add 'Error' prefix to TF_LITE_REPORT_ERROR\n",
      "\n",
      "CoordConv\n",
      "\n",
      "CUDA 11.0 Build with tensorflow master on Win 10 : Link error\n",
      "\n",
      "Memory leak when using MultiWorkerMirroredStrategy for distributed training\n",
      "\n",
      "Request for Prebuilt C++ Library libtensorflow_cc.so (probably from CI artifacts)\n",
      "\n",
      "Mismatch in number of weights when loading quantized model (activation layer)\n",
      "\n",
      "I've got a build error for \"Build a handwritten digit classifier app with TensorFlow Lite\"\n",
      "\n",
      "[Bug] apply_gradients core dump when variable shape is [0] on GPU\n",
      "\n",
      "Support for batch to single element conversion\n",
      "\n",
      "Unexpected behavior , none persistent errors and, Loop execution was cancelled.\n",
      "\n",
      "Resource exhausted: OOM while using shuffle()\n",
      "\n",
      "[Grappler] Improve HoistCWiseUnaryChainsStage to support Reshape\n",
      "\n",
      "Tensor Cores no performance improvement CUBLAS tf.matmul()\n",
      "\n",
      "TFLite Inference Runtime Error with Models Containing Conv2dLstm\n",
      "\n",
      "StringLookup layer adds None to list of saved model variables which breaks hub.KerasLayer\n",
      "\n",
      "Mixed precision training on TensorFlow 2.3 doesn't speed-up training progress as TensorFlow 2.2 does\n",
      "\n",
      "CallCounter C++ version\n",
      "\n",
      "tf.argmin across longer axis significantly slower than tf.reduce_min\n",
      "\n",
      "Discrepancy between available operation semantics documentation and documentation of `tf2xla.python.xla.conv`\n",
      "\n",
      "Missing headers when using the C++ library\n",
      "\n",
      "TFLite - enable XNNPACK with dynamic shapes of tensors\n",
      "\n",
      "Application crash when using SetNumThreads from tflite::impl::Interpreter\n",
      "\n",
      "Tensorflow 2.3.0 is much slower than PyTorch 1.4.0 in backward propagation. (20 times slower)\n",
      "\n",
      "Keras 'Random' preprocessing layers do not work on cloud TPU\n",
      "\n",
      "Should the custom loss function in Keras return a single loss value for the batch or an arrary of losses for every sample in the training batch?\n",
      "\n",
      "Setting multithreading  for tensorflowlite does not work！\n",
      "\n",
      "Keras Model accepts unnamed \n",
      "\n",
      "Failed to build v2.3.0 from source in debug mode\n",
      "\n",
      "Convert savedmodel to h5 Keras format - AttributeError: 'AutoTrackable' object has no attribute 'summary'\n",
      "\n",
      "```tf.stack``` returns different shapes when run in eager and graph mode\n",
      "\n",
      "`tf.data.experimental.sample_from_datasets` imports all the dataset into GPU memory\n",
      "\n",
      "Segmentation fault in tf.image.combined_non_max_suppression\n",
      "\n",
      "parallel_for: No converter defined for UniqueWithCounts, SegmentSum, Range, Where, Tile in tf.vectorized_map()\n",
      "\n",
      "tf.keras.layers.Conv1D fails for RaggedTensor input\n",
      "\n",
      "ConverterError: input resource[0] expected type resource != float, the type of while_model_embed_gather_resource_0[0]\n",
      "\n",
      "Cannot initialize variables on GPU with keras?\n",
      "\n",
      "Pybind11 exception with tensorflow 2.2 env in python cpp communication\n",
      "\n",
      "Adding Conv3DLSTM and depth-wise separable Conv3D layers to tensorflow.keras API\n",
      "\n",
      "Add Object Detection EfficientDet/Retinanet Model(s) to tf.keras.applications\n",
      "\n",
      "Undocumented S3_DISABLE_MULTI_PART_DOWNLOAD variable and its behavior\n",
      "\n",
      "Image-Scaling Attacks: Discuss threat and mention secure scaling options to prevent attacks\n",
      "\n",
      "`metrics=['accuracy']` works, `metrics=['Accuracy']` gives `ValueError: Shapes (None, 10) and (None, 1) are incompatible`\n",
      "\n",
      "Embedding for Embedding Projector works on one computer, not on another.\n",
      "\n",
      "The estimated size for TPUEstimatorSpec.predictions is too large. \n",
      "\n",
      "Exception when concatenating empty flattened layer\n",
      "\n",
      "Changing hyper parameters during training based on scalars\n",
      "\n",
      "Error converting multilingual universal sentence encoder to TFLite. Input 1 of node StatefulPartitionedCall was passed float from statefulpartitionedcall_args_1:0 incompatible with expected resource.\n",
      "\n",
      "Googleapis no longer usable as a system lib with ignored com_github_googleapis_googleapis\n",
      "\n",
      "Inference for two or more neural networks on gpu\n",
      "\n",
      "Indexing into EagerTensor returns all zeros (on Windows w/ CUDA)\n",
      "\n",
      "tf.data.experimental.dense_to_ragged_batch fails with inputs from generator with unspecified shape in TF 2.3\n",
      "\n",
      "Issue with iou_threshold in non_max_suppression\n",
      "\n",
      "Deploy micro_speech to ESP32 using ESP IDF\n",
      "\n",
      "Different inference values tensorflow vs tflite model\n",
      "\n",
      "CQT implementation - Feature request\n",
      "\n",
      "data.Dataset.as_numpy_iterator is non-reentrant compared to tensorflow data iterator.\n",
      "\n",
      "How to load a tflite model directly from a link or aws s3?\n",
      "\n",
      "Ragged Tensors in Keras Model Output\n",
      "\n",
      "How to convert a image caption model to tensorflow Lite model?ValueError: Python inputs incompatible with input_signature:\n",
      "\n",
      "Input and output tensors of converted TFLite model do not accord with trained model in TF\n",
      "\n",
      "tensorflow lite inference via c++ : interpreter->output_tensor(0) gives wrong tensor value\n",
      "\n",
      "Huge CPU, GPU output diff after updates to fully_connected GpuDelegate gl kernel.\n",
      "\n",
      "Predict on batch being able to do the prediction with inputs of different first shapes\n",
      "\n",
      "XNNPACK delegate performs much slower than default TFLite backend if multi-threading is configured according to documentation\n",
      "\n",
      "Helloworld example on esp32 not working\n",
      "\n",
      "Unable to build tflite aar file on windows\n",
      "\n",
      "There is memory leak in Invoke() of gpu delegate (OpenCL) in Tensorflow lite v2.3.0\n",
      "\n",
      "tf.signal.inverse_stft segfault when frame_length is a large value\n",
      "\n",
      "tf.convert_to_tensor and tf.constant ignoring tf.device\n",
      "\n",
      "ValueError when using dictionary format for input of optimizer options in Tensorflow 2.2.0 Keras models\n",
      "\n",
      "version 2.3.0 build error: user_ops_gen_cc: undefined symbol\n",
      "\n",
      "Testing Operator \"raw_ops.ApplyAdam\" \n",
      "\n",
      "Reduction is average step time when disabling certain HLO passes\n",
      "\n",
      "Add random shear to keras.layers.experimental.preprocessing\n",
      "\n",
      "tf.nn.space_to_depth and tf.nn.depth_to_space crashes (segfault) in `NCHW_VECT_C ` mode with certain input\n",
      "\n",
      "DLL load failed for Tensorflow-GPU==1.14, with CUDA 10.0\n",
      "\n",
      "[TFlite]: Failed to instantiate the interpreter with a StyleGAN2 generator model\n",
      "\n",
      "Not every keras.metrics.* accept from_logits=True\n",
      "\n",
      "While Tensorflow usually uses GatherV2, TFLite only supports Gather\n",
      "\n",
      "Error when trying to run MobileNetV2 on esp32 (abort)\n",
      "\n",
      "Update TensorFlow docs for a11y\n",
      "\n",
      "Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED\n",
      "\n",
      "Input shape for converted tflite model is wrong\n",
      "\n",
      "Convert saved model - issue generated shapes\n",
      "\n",
      "floating point exception in `tf.nn.atrous_conv2d` when there is 0 in filters.shape\n",
      "\n",
      "tf.nn.ctc_beam_search_decoder expects output of softmax whereas documentation says it expects logits\n",
      "\n",
      "Keras plot_model bug when rendering to svg\n",
      "\n",
      "XNNPack delegate undefined reference\n",
      "\n",
      "Docs on using TPUs with custom training loop can be misleading\n",
      "\n",
      "blank_index in tf.nn.ctc_loss and tf.nn.ctc_beam_search_decoder has different default value\n",
      "\n",
      "Make Transactional API methods pure virtual\n",
      "\n",
      "Add __name__ property to optimizer classes\n",
      "\n",
      "TimeDistributed layer does not (always) propagate mask\n",
      "\n",
      "Slow iteration/converting of tensors to vector<float> via Python C API\n",
      "\n",
      "autograph fails inside keras model train_step including a for loop over a tensor\n",
      "\n",
      "tf.keras.models.load_model unable to load model in TF 2.3.0 - \"int() argument must be a string, a bytes-like object or a number, not 'NoneType' \"\n",
      "\n",
      "plot_model not connecting layers between models correctly\n",
      "\n",
      "[Tensorflow Lite] MobileBert Inferencing Latency on mobile device significantly slower than expected\n",
      "\n",
      "cross-compile Inference Diff tool\n",
      "\n",
      "Hello World example project's generated binary is empty for Bluepill\n",
      "\n",
      "Universal Multiple Optimizer Wrapper with Layer Assignment\n",
      "\n",
      "[Docs] RAM usage when building from source\n",
      "\n",
      "TensorFlow Lite causes segementation faults in C++ when creating threads if fully statically linked\n",
      "\n",
      "Binary won't build for hello world.\n",
      "\n",
      "Keras custom model shape inference does not work with model.fit() for hub modules\n",
      "\n",
      "comp:mkl\n",
      "\n",
      "tf.constant causes CUDA_ERROR_TOO_MANY_PEERS error with more than 10 GPUs\n",
      "\n",
      "hang in recursive call of google::protobuf::DescriptorPool::FindFileByName\n",
      "\n",
      "Add deterministic tf.image.crop_and_resize backprop\n",
      "\n",
      "Stale(?) references to activation histograms\n",
      "\n",
      "Sparse input name missing in exported model\n",
      "\n",
      "ValueError: Graph disconnected: cannot obtain value for tensor Tensor…The following previous layers were accessed without issue:\n",
      "\n",
      "Large batch size with dense layers will fail all_reduce occasionally\n",
      "\n",
      "Benchmark application with `use_xnnpack=true`: ModifyGraphWithDelegate is disallowed when graph is immutable.\n",
      "\n",
      "Looking for Post-training Quantization for int 16 in TF2/TF Lite\n",
      "\n",
      "Model performs differently for model.fit and custom training loop\n",
      "\n",
      "tensorflow 2.3.0: target 'grpc++_public_hdrs' not declared in package when build from source\n",
      "\n",
      "tflite on Android: Didn't find op for builtin opcode 'CONV_2D' version '5'\n",
      "\n",
      "AOT compiled graph is 2-7x slower than Python\n",
      "\n",
      "Unresolved symbol EigenMatMulF64 when linking a compiled graph with XLA AOT runtime\n",
      "\n",
      "Error with TFlite hello world example on ESP-EYE\n",
      "\n",
      "TF 2.3 broken hierarchical functional model loading (e.g. HAN) [ValueError: Unknown layer: Functional]\n",
      "\n",
      "[Wsign-compare] warning resolutions, by directory, 15\n",
      "\n",
      "[Wsign-compare] warning resolutions, by directory, 14\n",
      "\n",
      "[Wsign-compare] warning resolutions, by directory, 13\n",
      "\n",
      "[Wsign-compare] warning resolutions, by directory, 12\n",
      "\n",
      "Support outputting tree leaves in BoostedTreesClassifier\n",
      "\n",
      "Windows import library is missing sufficient symbols to use C++ API\n",
      "\n",
      "Get deadlock after Restoring SavedModelBundle(cuda10.1, cudnn7.6.3, trt6.0, Tesla T4 GPU)\n",
      "\n",
      "Add \"CIRCULAR\" mode padding to tensorflow.pad()\n",
      "\n",
      "Try RecursivelyCreateDir Except os.makedirs solution to Create Missing Path\n",
      "\n",
      "CUDA_ERROR_ILLEGAL_ADDRESS in toy training example\n",
      "\n",
      "Pylint E1121 caused by arguments to reshape method.\n",
      "\n",
      "tf.keras.Sequential() fails\n",
      "\n",
      "Keras Callbacks logs / numpy_logs not in sync\n",
      "\n",
      "TF r2.3 Bad Address build issue on windows\n",
      "\n",
      "java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: OpenCL library not loaded - dlopen failed: library \"libOpenCL-pixel.so\" not found Falling back to OpenGL TfLiteGpuDelegate Invoke: Write to buffer failed. Source data is larger than buffer. \n",
      "\n",
      "Is this a functionality that is possible?\n",
      "\n",
      "TF 2.3 training slowed down by 15% compared to 2.2\n",
      "\n",
      "Support for ragged tensor targets (variable-length y)\n",
      "\n",
      "Golang Tensorflow v2.3.0 installation fails\n",
      "\n",
      "compile tensorflow fails with error: SWIGing tensorflow/python/tensorflow.i ... swig failed: error executing command\n",
      "\n",
      "RuntimeError: external/org_tensorflow/tensorflow/lite/core/subgraph.cc:1044 required_bytes != bytes (602112 != 150528)\n",
      "\n",
      "S3 ParseURI supporting query parameters\n",
      "\n",
      "the shared network inside a compiled keras model got modified after resetting the trainable attribute of the shared network\n",
      "\n",
      "Loss follows Learning Rate trend when using tf.keras.optimizers.schedules.LearningRateSchedule\n",
      "\n",
      "sampled_softmax_loss weights and logits don't get gradients\n",
      "\n",
      "ResourceExhaustedError (out of memory) when training a Saved Model\n",
      "\n",
      "Adagrad colocation fitting error\n",
      "\n",
      "SavedModel with dictionary/list data member\n",
      "\n",
      "Build TFLite with Flex Ops without Bazel \n",
      "\n",
      "Build windows x86 c++ library failed\n",
      "\n",
      "Please list XLA flags\n",
      "\n",
      "Can't change model attribute inside @tf.function with custom training loop.\n",
      "\n",
      "Autograph fails to convert nested **if-else** in a for loop\n",
      "\n",
      "Building tensorflow 2.2 fails\n",
      "\n",
      "Run entire epoch as compiled tf.function in model.fit()\n",
      "\n",
      "OP_REQUIRES failure in c++ LoadSavedModel with conv2d w/ bias into an Add\n",
      "\n",
      "BoostedTreesClassifier only supports dictionary-based dataset\n",
      "\n",
      "High CPU memory usage when calling GradientTape's gradient() when using multiple threads/cores\n",
      "\n",
      "Try to fix bce loss check\n",
      "\n",
      "Golang: SessionOptions & ConfigProto opaqueness & inaccessibility\n",
      "\n",
      "tf.signal.dct very slow as compared to scipy.dct even on GPU\n",
      "\n",
      "Keras Metric Multiple Outputs / Inputs\n",
      "\n",
      "tf.lite.TFLiteConverter crashes when converting Keras model\n",
      "\n",
      "assert padding in {'same', 'valid', 'full'} AssertionError\n",
      "\n",
      "Weighted CE and BCE\n",
      "\n",
      "Git Pull doesn't work in building from source for GPU\n",
      "\n",
      "BroadcastTo\n",
      "\n",
      "Allow slots to be initialized with different shapes + values than primary var in Keras optimizer\n",
      "\n",
      "Problems with Transformations when trying to model.fit() my 2D CNN Model, leading to two exceptions\n",
      "\n",
      "Maybe incorrect Brier score calculation\n",
      "\n",
      "[XLA] Unintuitive behavior of jit_scope\n",
      "\n",
      "Logging of \"TensorFlow with TPUs\" example in Google Colab is happening Twice\n",
      "\n",
      "Seed for dropout in LSTM - Difference in model(X) and model.predict(X)\n",
      "\n",
      "Fix binary crossentropy double epsilon\n",
      "\n",
      "tf.function breaks the gradient tape when looping over datasets\n",
      "\n",
      "When dose Tensorflow add elasticity function in worker？\n",
      "\n",
      "pyspark + keras as pandas_udf does not work properly\n",
      "\n",
      "Camera does not resume when activity is resumed - TFLite Android flower-classification codelab\n",
      "\n",
      "tensorflow lite performs linear relation between batch size and inference time\n",
      "\n",
      "keras.Model.save_weights is overwriting all_model_checkpoint_paths\n",
      "\n",
      "CategoryEncoding not working when loading model again\n",
      "\n",
      "model.export(export_dir='.')  does not export labels nor model.\n",
      "\n",
      "How to load tensorflow 2.0 savedbundle file  by using Java\n",
      "\n",
      "tf.io.TFRecordWriter cannot work with gs:// or ram:// at Google Colab TPU\n",
      "\n",
      "KeyError: 'name' when try to load saved model with tf.keras.models.load_model()\n",
      "\n",
      "Suggestions For Tensorflow v3\n",
      "\n",
      "Any way to specific op name when I use saved_model\n",
      "\n",
      "Potential overflow in tf.broadcast_to\n",
      "\n",
      "Does TensorFlow Profiler step-time consider overlapping?\n",
      "\n",
      "XLA NMS bug\n",
      "\n",
      "Implement LSH-based Methods for Enhanced CPU-Only Performance\n",
      "\n",
      "tf.keras.preprocessing.image.apply_affine_transform shifts tx as height and ty as width\n",
      "\n",
      "TF to s3a:// access needed\n",
      "\n",
      "tf.random.categorical overflow before OOM on GPU\n",
      "\n",
      "macos not raising OOM error\n",
      "\n",
      "I'm trying to convert a custom model on inception V3 + SSD and running into the same issue. Any workarounds to get the model to tflite format until this is officially fixed?\n",
      "\n",
      "LUP factorization (tensorflow.linalg.lu) throws \"Input is not invertible\" error.\n",
      "\n",
      "tf.keras cannot weight classes when using multiple outputs\n",
      "\n",
      " LSTM crash randomly on Windows 10: Failed to call ThenRnnBackward with model config\n",
      "\n",
      "complex support for tf.math.argmin and tf.math.argmax\n",
      "\n",
      "Tensorflow model benchmarking with keras model file\n",
      "\n",
      "tf.math.argmin and tf.math.argmax don't support complex\n",
      "\n",
      "Python Keras code out of memory for no apparent reason\n",
      "\n",
      "[MLIR] HLO to LHLO conversion and fusion\n",
      "\n",
      "Built-in Method to check if a GPU is already Reserved\n",
      "\n",
      "Error: #include nested too deeply (libtensorflow_cc.so)\n",
      "\n",
      "pylint errors on master branch\n",
      "\n",
      "Debugging predictions step with tf.debugging.experimental.enable_dump_debug_info throws an error.\n",
      "\n",
      "exponential_avg_factor not in Op FusedBatchNormV3: model deployed using Golang Tensorflow v2.0.2\n",
      "\n",
      "Windows - tensorflow.python.framework.errors_impl.UnknownError: Failed to rename: \n",
      "\n",
      "Deprecation of validation_data in tensorflow.keras.Callbacks\n",
      "\n",
      "Not able to use libtensorflow-lite.a static library in Android studio for including headers.\n",
      "\n",
      "twice embedding_lookup‘ result is the same as once embedding_lookup,but gradients not\n",
      "\n",
      "Why TF-OP runs slower than pure API?\n",
      "\n",
      "Keras model.compile(..., metrics=[\"accuracy\"]) no longer introspects loss function in TF 2.2\n",
      "\n",
      "Using HIP instead of CUDA (work on all graphics card)\n",
      "\n",
      "Unable to load custom model when using tf.keras.callbacks.ModelCheckpoint\n",
      "\n",
      "Shuffling then zip tf.data.Dataset\n",
      "\n",
      "Performance issue when calling Keras conv2D / gen_nn_ops.conv2d with different sizes\n",
      "\n",
      " Quantization Aware Training with tf.GradientTape gives Error in TensorFlow2.0\n",
      "\n",
      "Quantization Aware Training with tf.GradientTape gives Error in TensorFlow2.0\n",
      "\n",
      "Tensorflow default supports dynamic preprocessing (e.g. masking).\n",
      "\n",
      "[RNN] Converting network with LSTM layer to int8 sefaults whole python\n",
      "\n",
      "Improve ROCm's sqrt and rsqrt for std::complex.\n",
      "\n",
      "Incorrect gradient for ctc_loss on GPU when using logit_length\n",
      "\n",
      "User defined color in tf.image.resize_with_crop_or_pad\n",
      "\n",
      "TFLite micro: Is the flatbuffer allowed to be placed in flash in all cases?\n",
      "\n",
      "How to check programmatically TF-Lite Ops supported\n",
      "\n",
      "Type inference using type annotations\n",
      "\n",
      "The `**kwargs` in `tf.keras.Model` does not accept any argument\n",
      "\n",
      "Unknown call to ResizeNearestNeighbor?\n",
      "\n",
      "Support for DepthwiseConv2D with output channel less than input channel\n",
      "\n",
      "Tensorflow profiler crashes when using string categorical layer\n",
      "\n",
      "TF2 add a reduce or aggregation key to tf.scatter_nd\n",
      "\n",
      "Error when running inference on CPU using C++ bindings: Not found: Container localhost does not exist\n",
      "\n",
      "Masked Pooling and Convolution\n",
      "\n",
      "Differences in using tf.keras.Model.fit() API and writing training loop from scratch (simple regression model)\n",
      "\n",
      "Spectral Normalization\n",
      "\n",
      "DataType error: DataType 6 is not recognized in Java (version 2.4.0)\n",
      "\n",
      "C++ Program crashed while running tensorflow 2.0.0 with cuda 10.0 on Tegra Tx2\n",
      "\n",
      "Autograph applied to Keras Custom Loss during Eager Execution\n",
      "\n",
      "Bad input tensor parameters in model\n",
      "\n",
      "failed to query event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure tensorflow/stream_executor/dnn.cc:613] CUDNN_STATUS_INTERNAL_ERROR\n",
      "\n",
      "Inconsistent regularization loss computation when used with pretrained models. \n",
      "\n",
      "The order of weights is changed for array of layers inside sub classed model\n",
      "\n",
      "Tensorflow GPU needs to wait a few minutes each time before computing\n",
      "\n",
      "Predict is slow on first call when using variable batch_size\n",
      "\n",
      "Support strings as a first-class input/output data type in Swift\n",
      "\n",
      "Grappler errors on LSTM jacobian when `experimental_use_pfor=False`\n",
      "\n",
      "tf.io.decode_image rotates the images\n",
      "\n",
      "nan bug running automatic differentiation\n",
      "\n",
      "tf.keras.layers.Permute documentation is misleading, error messages even more so\n",
      "\n",
      "Debug .dll build with CUDA support on Windows fails to link\n",
      "\n",
      "Error Converting ssd_mobilenet_v2_oid_v4_2018_12_12 To Tflite\n",
      "\n",
      "[RPI Zero] ModuleNotFoundError: No module named 'tflite_runtime'\n",
      "\n",
      "Artistic Style Transfer Android Demo missing \"Style blending\" code\n",
      "\n",
      "Socket closed when \"LayerNormalization\" was used for tensors of 3 or more dimensions with TPU in colab.\n",
      "\n",
      "Converting RESIZE_NEAREST_NEIGHBOR\n",
      "\n",
      "Requirement already up to date when running pip install --upgrade tensorflow. Stuck at version 1.14.0\n",
      "\n",
      "Zero AUROC when validation set is only comprised of positive labels\n",
      "\n",
      "Relink `/usr/local/lib/libtensorflow_framework.so.2' with `/lib/x86_64-linux-gnu/libz.so.1' for IFUNC symbol `crc32_z'\n",
      "\n",
      "tf.data, construct a batch with different data? Are there easy ways to do it? TensorFlow1.15\n",
      "\n",
      "tf.feature_column.shared_embeddings cannot be saved\n",
      "\n",
      "Edge TPU Compiler Failed to Compile\n",
      "\n",
      "How to load tensorflow lite during the reboot process \n",
      "\n",
      "Build from Souce Failed Dependencies cudnn_stub.cc\n",
      "\n",
      "Add a method to save and load the optimizer.\n",
      "\n",
      "TF Lite Interpreter: Segmentation Fault\n",
      "\n",
      "Error when loading a Subclass model with tf.keras.Sequential blocks inside \n",
      "\n",
      "Error when building tflite 2.3.0-rc0 metal delegate on macOS\n",
      "\n",
      "FP32 model performance is better than quant model using TFLite on raspi 3 for mobilenetv2 \n",
      "\n",
      "Error while saving a model with one layer having a list of one ragged item\n",
      "\n",
      "Segmentation Fault tflite::ops::builtin::transpose_conv::ResizeCol2ImTensor\n",
      "\n",
      "ModuleNotFoundError: No module named 'tensorflow.compiler.tf2tensorrt'\n",
      "\n",
      "Error when saving weights in h5 format for layer with nested layers\n",
      "\n",
      "'fit' takes a dataset with increasing batch size\n",
      "\n",
      "“Layer is not connected” issue while accessing intermediate layer from custom callback if model is built by sub-classing\n",
      "\n",
      ".pb object detection model with 1 output tensor conversion to 4 output tensors\n",
      "\n",
      "Keras optional Input with default value\n",
      "\n",
      "SavedModel of Keras Model does not save the trainable status\n",
      "\n",
      "Run micro speech example on Apollo 2\n",
      "\n",
      "Query result not show aliased functions\n",
      "\n",
      "End2end Transformer Training by Using Ragged Tensor \n",
      "\n",
      "ESP32 doenst run correct anymore - Person Detection with Esp32 Camera\n",
      "\n",
      "ssd_mobilenet_v2_mnasfpn_shared_box_predictor_320x320_coco_sync_2020_05_18.tar.gz error run in android\n",
      "\n",
      "High memory consumption with model.fit in TF 2.x\n",
      "\n",
      "Saved_model exported from TrtGraphConverterV2 demands an extra \"global_step\" as input, while original saved_model does not\n",
      "\n",
      "tflite micro mbed image recognition example accuracy difference for custom model\n",
      "\n",
      "TFLite quant model run on the android phone have no speed up than float model\n",
      "\n",
      "Inconsistent prediction by model using Attention layer\n",
      "\n",
      "KeyError: 'Failed to format this callback filepath: \"checkpoint_5000/checkpoint_{epoch:02d}_{batch:04d}\". Reason: \\'batch\\''\n",
      "\n",
      "Segmentation fault in keras.backend.temporal_padding\n",
      "\n",
      "Process killed on tf.dynamic_partition\n",
      "\n",
      "Where is imgae_recognition_model.cc in image_recognition_experimental?\n",
      "\n",
      "tf.GradientTape.batch_jacobian fails on compiled tf.function due to assert op\n",
      "\n",
      "Building C API under Windows 10 fails\n",
      "\n",
      "Bug when serializing optimizer with `tf.keras.utils.serialize_keras_object` but works fine with `tf.keras.optimizers.serialize`\n",
      "\n",
      "Remove unused TensorBoard._epoch\n",
      "\n",
      "TFLu wrong predictions for optimized model\n",
      "\n",
      "issue with parallelizing dataset\n",
      "\n",
      "[TF 2.3.0] [Intel MKL] OMP Threads are created with variable number when we compile with XLA flag ON\n",
      "\n",
      "Post-training quantization\n",
      "\n",
      "Dot model generation not working when input_shape is not specified (Maybe just missing error message) \n",
      "\n",
      "tf.function ensures type annotations and input_signature match\n",
      "\n",
      "CUDA illegal error access error when running distributed mixed precision\n",
      "\n",
      "Add \"align_corners\" and \"half_pixel_centers\" as argument to tf.image.resize \n",
      "\n",
      "[TFTRT] Possible issue with Conv2D converter without `is_dynamic_op= True`\n",
      "\n",
      "LSTM loss and accuracy values differ greatly between GPU and CPU implementation\n",
      "\n",
      "tf.io.gfile.GFile truncates gzipped files loaded from Google Cloud buckets\n",
      "\n",
      "Cannot test op using mutlithreading\n",
      "\n",
      "Question: fake-quantize layers are also called from TF-Lite\n",
      "\n",
      "Could not detect op for DEPTHWISE_CONV_2D\n",
      "\n",
      "tf.image.random_flip_left_right should be also to apply to a list of tsor\n",
      "\n",
      "Jetson Nano build fails with missing \"cudnn_version\" string (default=7)\n",
      "\n",
      "Gradient calculation of tf.reduce_prod runs in CPU, inducing performance impact\n",
      "\n",
      "keras.Model using EmbeddingColumn with pre-trained ckpt fail to be reconstructed without the original ckpt\n",
      "\n",
      "TF-TRT slice op converter dynamic shape\n",
      "\n",
      "tf.nn.ctc_beam_search_decoder does not pick path with highest probability at next time step\n",
      "\n",
      "Import .tflite model into Tensorflow graph\n",
      "\n",
      "TF Unit test flaky in haswell/broadwell machines\n",
      "\n",
      "ValueError: return statements are not supported within a TensorFlow loop.\n",
      "\n",
      "What exactly does `tf.signal.fft` compute?\n",
      "\n",
      "looping over tf.range in tf.function is slower than looping over range\n",
      "\n",
      "Didn't find op for builtin opcode 'LOGISTIC' version '1'\n",
      "\n",
      "Type annotations for tf dtypes\n",
      "\n",
      "saved_model_cli convert tensorrt adds unknown inputs to serving signature\n",
      "\n",
      "[TF 2.2] Elapsed time of ConcreteFunction becomes shorter when printing loss\n",
      "\n",
      "Keras Model produces nan on fit\n",
      "\n",
      "Loss function not accessible when overriding optimizer_v2.OptimizerV2\n",
      "\n",
      "random in tf.data.Dataset.map is not random if not coming from tensorflow\n",
      "\n",
      "keras.models.clone_model ignores input_tensors for functional models\n",
      "\n",
      "If TMP is not set, then the build system defaults to the wrong path\n",
      "\n",
      "Dynamical Tensor (and EagerTensor) slice assignment\n",
      "\n",
      "tf.signal.rfft(2d/3d) documentation refers `Tcomplex` and `input` as arguments.\n",
      "\n",
      "tf.signal.irfft(2d/3d) documentation refers `input` and `Treal` as arguments.\n",
      "\n",
      "Trying to dynamically change weighted connections between training examples\n",
      "\n",
      "The GPUs hang when split a log_prob and gradient computation across a number of GPU devices\n",
      "\n",
      "JAVA DataType support float16?\n",
      "\n",
      "TFLite General OpenGL delegate \n",
      "\n",
      "TF 2.2 GPU memory usage regression on model.predict() with big inputs\n",
      "\n",
      "TF-TRT Improve reshape op converter\n",
      "\n",
      "Feature request: facemesh example for tensorflow lite\n",
      "\n",
      "[TF2.2] Build libtensorflow_cc.so for C++ APIs\n",
      "\n",
      "Combining multiple savedmodels into one SavedModel\n",
      "\n",
      "segfault during tf.image.non_max_suppression_padded\n",
      "\n",
      "Building binaries for Arduino fails, fatal error: PDM.h\n",
      "\n",
      "High memory use / leak in model.fit\n",
      "\n",
      "Keras callback stops receiving some params with tensorflow 2.2.0\n",
      "\n",
      "[RNN] LSTM and Bidir layers can't be converted in a TFLite model\n",
      "\n",
      "Android-PoseNet is not accurate when person is laying down\n",
      "\n",
      "SavedModel does not work correctly with signatures that use structured inputs or outputs\n",
      "\n",
      "tf.GatherV2 is always not support \n",
      "\n",
      "tfcompile for non-x86-64 CPU\n",
      "\n",
      "tfcompile AOT with quantization\n",
      "\n",
      "Support for use of tensorflow_probability.distributions.Distribution instances in model.fit(...)\n",
      "\n",
      "Using own dataset in microspeech project doesn't work \n",
      "\n",
      "Tensorflow Lite subtraction in TOCO quantized model takes longer than all the convolutions in a MobileNet v3 model\n",
      "\n",
      "Gradient compression\n",
      "\n",
      "Tensorflow Java API reliability/stability guaranties\n",
      "\n",
      "Add arm64 third-party CI\n",
      "\n",
      "TF 2.2.0: Error with model.fit; class_weight is only supported for Models with a single output.\n",
      "\n",
      "On the endianness of TensorProto::tensor_content\n",
      "\n",
      "Please Remove Keyword check by append **kwargs in the tf.keras.Model\n",
      "\n",
      "when support tflite op: SparseFillEmptyRows\n",
      "\n",
      "Codelab: Recognize Flowers with TensorFlow Lite on Android - emulator crashes\n",
      "\n",
      "CUDNN8 + CUDA 10.2 build?\n",
      "\n",
      "MirroredStrategy preventing use of cuDNN GRU implementation\n",
      "\n",
      "AbstractRNNCell documentation\n",
      "\n",
      "Usage and signature of Model.train_step() is unclear\n",
      "\n",
      "Feature Request: API to access peak memory usage of a function call that uses TF\n",
      "\n",
      "ilsvrc:imagenet_accuracy_eval crossed compiled for Pixel4 fails on TF2.1.0's TFlite MobileNetV3 model\n",
      "\n",
      "TPUStrategy does not export graph to TensorBoard while TPUEstimator does it\n",
      "\n",
      "Image Encoding/Decoding and B64 Encoding/Decoding Not Working\n",
      "\n",
      "file_hash not validated after downloading in Keras.utils.data_utils.get_file().\n",
      "\n",
      "Bug with keras applications preprocess_input method\n",
      "\n",
      "Seg fault while post quantizing my model\n",
      "\n",
      "TensorFlow Lite Post-training Integer Quantization predicts identical value always\n",
      "\n",
      "Symbol file not loaded when I use C API\n",
      "\n",
      "Backward LSTM behavior mismatch\n",
      "\n",
      "We cannot duplicate the value since it's not constant. Failed to duplicate values for the stateful op.\n",
      "\n",
      "Cannot resume training using model.save and load_model()\n",
      "\n",
      "Eager Function Inputs cannot be Keras Symbolic Tensors error when using Intermediate layers in custom loss function\n",
      "\n",
      "Options for tflite_convert\n",
      "\n",
      "Can I inference models with XNNPACK backend through tensorflow lite Python APIs?\n",
      "\n",
      "File system scheme 's3' not implemented\n",
      "\n",
      "Decoding output of object detection mobile_ssd_v2_float_coco.tflite \n",
      "\n",
      "Update nasnet.py, Add `classifier_activation` arg\n",
      "\n",
      "Floating point exception occurred in tf.nn.fractional_max_pool\n",
      "\n",
      "Documentation instructions on installing tensorflow with CUDA support doesn't work\n",
      "\n",
      "AutoGraph saved model with uint8 input will not convert to tflite\n",
      "\n",
      "Keras layer on TPU, Cannot assign a device for operation RandomUniform\n",
      "\n",
      "learned_unigram_candidate_sampler may enter infinite loop when range_max is a very large number\n",
      "\n",
      "model.predict is much slower on TF 2.1+\n",
      "\n",
      "Collect runtime information when using tf.function\n",
      "\n",
      "Difference of 0.5 factor compared to Research Paper in Soft NMS implementation\n",
      "\n",
      "Support including tensorflow directly in bazel WORKSPACEs\n",
      "\n",
      "Functional Keras API not getting converted using TFLiteTransferConverter in Model Personalization\n",
      "\n",
      "Tensorflow v2.2 build fails with cuda 10.2 TensorRT 7.0.0.11-1\n",
      "\n",
      "Tensor had Inf values (maybe from TensorArray)\n",
      "\n",
      "how could tf.image.extract_patches with dynamic kernel size?\n",
      "\n",
      "remove training nodes from freeze_graph failure using TransformGraph.\n",
      "\n",
      "Use new param log_all in CSVLogger to log all elements.\n",
      "\n",
      "Provide compatibility with previous Keras-Preprocessing API.\n",
      "\n",
      "\"Inconsistent CUDA toolkit path: /usr vs /usr/lib\" when running ./configure\n",
      "\n",
      "Rolled RNN cannot be converted to INT8\n",
      "\n",
      "Allow overriding the build method for Metrics\n",
      "\n",
      "Would be better to replace the signature of ReadNBytes to not have `size_t` as the type of the last argument.\n",
      "\n",
      "Very slow quantized tflite model\n",
      "\n",
      "Always exceeds 10% of free system memory.\n",
      "\n",
      "tf.summary.flush segfaults when writer is not a valid tf.summary.SummaryWriter object\n",
      "\n",
      "build failed on CentOS-7 @flatbuffers//:flatc' #1660 \n",
      "\n",
      "GPU delegate not as accurate as NNAPI reference  on 16float models on Style Transfer example. tensorflow-lite\n",
      "\n",
      "Make possible loading model with custom gradient\n",
      "\n",
      "tf.ragged.constant does not detect dense dimensions\n",
      "\n",
      "TFLite: ELU activation not supported on GPU delegate\n",
      "\n",
      "adding_an_op compile fail Windows unistd.h\n",
      "\n",
      "ERROR: /tensorflow_src/tensorflow/core/kernels/BUILD:4135:1: C++ compilation of rule '//tensorflow/core/kernels:cwise_op' failed (Exit 4)\n",
      "\n",
      "Timeseries forecasting tutorial has bias issues\n",
      "\n",
      "tf.broadcast_to abort() and core dump when given shape value overflows int32\n",
      "\n",
      "Type annotations for tf.saturate_cast & tf.constant ops\n",
      "\n",
      "No Debug Symbols when using per_file_copt option\n",
      "\n",
      "TFLite Converter not quantizing supported op weights\n",
      "\n",
      "tf.random.uniform unexpected behaviour for unknown shape\n",
      "\n",
      "[Feature Request] Logging of validation metrics when using validation_freq > 1\n",
      "\n",
      "Dynamically added layers do not show up in custom model layers member\n",
      "\n",
      "Design a generic type Python API for the hparams plugin\n",
      "\n",
      "layers.set_weights() not working with tf.keras.optimizers.Ftrl\n",
      "\n",
      "Lambda layer does not compute masked values properly\n",
      "\n",
      "RNN unrolled, cannot be converted using from_keras_model\n",
      "\n",
      "Concatenate ReLU and Batch Norm error\n",
      "\n",
      "Tensorflow Lite GPU delegate using C++ thread detach on Galaxy Tab S6 is 2.5x slower than without a thread detach\n",
      "\n",
      "tf.io.gfile / GCS fails to work on OpenSUSE\n",
      "\n",
      "DefaultQuantParamsPass doesn't work correctly if bias constant has multiple users\n",
      "\n",
      "[MLIR/XLA] Invalid IR passes verification\n",
      "\n",
      "TFLite: Support grouped convolutions\n",
      "\n",
      "Training error on Cloud TPUs\n",
      "\n",
      "[ROCm] Adding a GPU kernel for dropout\n",
      "\n",
      "custom operators by reference in tf2.2.0\n",
      "\n",
      "Link error for libtensorflow_cc.so when using LoadSavedModel\n",
      "\n",
      "Clarify documentation of output shape from DepthwiseConv2D\n",
      "\n",
      "broadcasting behavior in tensorflow keras layers vs base tensorflow operators\n",
      "\n",
      "Suboptimal execution order of parallel map calls for tf.data\n",
      "\n",
      "Correct way of using tf.keras.layers.experimental.preprocessing layers under strategy scope\n",
      "\n",
      "Custom dataset op encounters refcount error\n",
      "\n",
      "Conv3D operations are not using tensor cores with mixed float16 policy\n",
      "\n",
      "micro_speech example broken\n",
      "\n",
      "how to check quantization method applied\n",
      "\n",
      "TF Lite wheel is not supported on platform including ARMv7 and python 3.6.5\n",
      "\n",
      "Convert keras model to quantized tflite lost precision\n",
      "\n",
      "Can't build tensorflow from source with docker image\n",
      "\n",
      "per-channel quantization\n",
      "\n",
      "Incorrect Layer Normalization description\n",
      "\n",
      "XLA compilation requires that operator arguments that represent shapes or dimensions be evaluated to concrete values at compile time\n",
      "\n",
      "Keras: Regularizer not saved for Lambda layers\n",
      "\n",
      "[MLIR] xla hlo -> lhlo conversion issue when operand is a constant tensor\n",
      "\n",
      "Incompatible shapes using `sample_weight` in Graph execution\n",
      "\n",
      "Add n-dimensional tensor support for tf.unique and tf.unique_with_counts\n",
      "\n",
      "MeanSquaredError truncated to 32-bit precision when using 64-bit\n",
      "\n",
      "'too many indices for array' exception depending on tensor length when masking a tensor\n",
      "\n",
      "sys.exc_info() can't put to queue in MultiProcessRunner\n",
      "\n",
      "device_spec from_string method not accepting some inputs\n",
      "\n",
      "`tf.keras.models.clone_model` does not support custom model\n",
      "\n",
      "XLA compilation not working in Windows 10 Pro\n",
      "\n",
      "Graph visualization failed, in Graph mode [TF 2.2, TB 2.2.1]\n",
      "\n",
      "Build tensorflow from source errors\n",
      "\n",
      "Passing initial_epoch parameter to callbacks' self.params in tf.keras.model.fit\n",
      "\n",
      "Flops calculation in tensorflow 2.x version.\n",
      "\n",
      "Hashing functions for tf.string\n",
      "\n",
      "Tensorflow keras fit - accuracy and loss both increasing drastically\n",
      "\n",
      "Stop .flow_from_dataframe from printing\n",
      "\n",
      "CUDA compiling include file path failure inside tensorflow python installation directory in Windows 10\n",
      "\n",
      "tf.queue.FIFOQueue throws NotFoundError on enqueue / dequeue if created in graph mode\n",
      "\n",
      "no specification for blank index for ctc_batch_cost\n",
      "\n",
      "ResourceExhaustedError with a RNN compiled with XLA\n",
      "\n",
      "Unable to log scalar summaries in XLA\n",
      "\n",
      "In tensorflow, for custom layers that need arguments at instantialion, does the get_config method need overriding?\n",
      "\n",
      "Failed to legalize operation 'xla_hlo.return'\n",
      "\n",
      "How to get the learning rate in optimizer during training?\n",
      "\n",
      "[TF.Distribute] ctx.all_reduce is failed in the codition depends on layer-intenal variable.\n",
      "\n",
      "Custom Keras model adds extra dimension to scalar inputs\n",
      "\n",
      "Example of using SavedModel\n",
      "\n",
      "RPi Zero Build instructions not working\n",
      "\n",
      "Failed to bazel build //third_party/gpus/cuda/...\n",
      "\n",
      "tf.keras/Tensorboard callback does not implement activation histogram (and says it does)\n",
      "\n",
      "Non-deterministic behaviour: tf.math.unsorted_segment_sum uses CUDA Atomic Operations\n",
      "\n",
      "`Tensor` slicing is ~300x slower than `numpy` slicing\n",
      "\n",
      "Documentation of accepted datatypes for `validation_data` in keras.Model.fit is incorrect. \n",
      "\n",
      "Keras Tensorboard Callback conflicts with Lambda Callback writing summaries\n",
      "\n",
      "Is there any good detailed  description of GraphDef and Saved Model ?\n",
      "\n",
      "TPU PyFunction results in UnavailableError: failed to connect to all addresses\n",
      "\n",
      "tfa ReduceLROnPlateau callback from Tf keras is not recognizing cohen kappa metrics direction in 'Auto' mode\n",
      "\n",
      "`Model.predict(...)` seems incapable of handling two inputs when using the tf.data-API\n",
      "\n",
      "Weird block of RNN in TF2.2\n",
      "\n",
      "tf.lite.experimental.load_delegate throws exception with try except block\n",
      "\n",
      "TF 2.2: Build failure on Win10 (Bad address issue)\n",
      "\n",
      "tf.keras.callbacks.ModelCheckpoint only saves weights although save_weights_only=False\n",
      "\n",
      "AllocateTensors fails in hello world example\n",
      "\n",
      "Update \"Custom Training\" section for `Model.train_step`\n",
      "\n",
      "Why is TF 2.2 much slower than TF 2.0?\n",
      "\n",
      "How to speed up text generation in TensorFlow reference example notebook?\n",
      "\n",
      "Checkpoint is not work properly\n",
      "\n",
      "no such package '@androidsdk//com.android.support'\n",
      "\n",
      "How to implement data-dependent initialization with MultiGPU/TPU [Tensorflow >= 2.x]\n",
      "\n",
      "Debug keras code on Graph mode.\n",
      "\n",
      "Successive STFT transforms increases signal amplitude\n",
      "\n",
      "Repost of keras-team/keras #13118.\n",
      "\n",
      "tf.keras.Model docs missing get_weights method and metrics property\n",
      "\n",
      "native crash when using GpuDelegate on Float16 nn from java android.\n",
      "\n",
      "Add __reduce_ex__ to Keras Model to enable copy.deepcopy and pickle\n",
      "\n",
      "Memory Usage of a model\n",
      "\n",
      "tf.keras.layers.Lambda can't infer output dimension of tf.slice\n",
      "\n",
      "tensorflow.keras deadlock when using model's predict() in parallel\n",
      "\n",
      "undeclared identifier 'ABSL_FALLTHROUGH_INTENDED'\n",
      "\n",
      "Training-Time updated custom Layer attributes are NOT saved\n",
      "\n",
      "Model saved with Lambda layer with a ragged input breaks when reloading\n",
      "\n",
      "Different training performance in eager, model.fit and estimator.train\n",
      "\n",
      "Training produces Out Of Memory error with TF 2.2.0 but works with TF 1.14\n",
      "\n",
      "Multithreaded Function Stops after Model is Instantiated\n",
      "\n",
      "Recursive support for tf.io.gfile.glob\n",
      "\n",
      "Suggest to the user to define a cache volume for bazel\n",
      "\n",
      "Much worse performance when using mixed precision training (using tensorflow.keras policy)\n",
      "\n",
      "tf.convert_to_tensor throws ValueError for tf.float64 tensor and dtype=tf.float32 instead of silently casting\n",
      "\n",
      "TPU issue using Colab - NotFoundError: 'AnonymousSeedGenerator' is neither a type of a primitive operation nor a name of a function registered in binary running on n-c0cd0e2d-w-0. Make sure the operation or function is registered in the binary running in this process. [Op:TakeDataset]\n",
      "\n",
      "Weights for model net_10 have not yet been created. Weights are created when the Model is first called on inputs or `build()` is called with an `input_shape`.\n",
      "\n",
      "[feature request] - please consider adding in gpumlib features to tensorflow\n",
      "\n",
      "Documentation for ensuring CUPTI for Profiling is Misleading\n",
      "\n",
      "Undefined symbol in debug mode\n",
      "\n",
      "tf.signal.rfft documentation refers to Tcomplex as an argument \n",
      "\n",
      "Construct tf.SparseTensor with tf.int32 dense_shape\n",
      "\n",
      "Add support of float in lite/kernels/kernel_util.cc#CalculateActivationRangeQuantized\n",
      "\n",
      "Slow memory copy on Jetson Nano\n",
      "\n",
      "Using the code from the official api documentation, I cannot pass the feature column to the keras model\n",
      "\n",
      "Causal padding for tfp.layers.Convolution1DFlipout\n",
      "\n",
      "Keras Conv3DTranspose inconsistency between padding, output_shape and output_padding\n",
      "\n",
      "Deceleration of fit by 10 times, after tf.keras.models.load_model('model.h5')\n",
      "\n",
      "Keras Model.evaluate progress bar doesn't work in graph mode in TF2.2\n",
      "\n",
      "CuDNN LSTM failed with large batch size\n",
      "\n",
      "Add missing double overloads for GetAttr, GetNodeAttr, and TryGetNodeAttr\n",
      "\n",
      "Has someone ported tensorflow example codes to anyother microcontroller?\n",
      "\n",
      "Upgrading from TF 2.1 to 2.2 gives 12% slowdown and 23% memory increase\n",
      "\n",
      "tensorflow.keras behaves wrongly in autoencoder setup compared to keras\n",
      "\n",
      "Categorical CrossEntropy returning wrong values\n",
      "\n",
      "tensorflow.lib missing some symbols on linking in a application\n",
      "\n",
      "tf.data.Dataset doesn't handle namedtuples properly\n",
      "\n",
      "Add support for more dimensions -  ImageDataGenerator's .flow_from_directory()\n",
      "\n",
      "TensorFlowOpLayer messes up the TensorBoard graphs / grouping with TensorBoard\n",
      "\n",
      "TFlite model performance issues on Snapdragon 855+ - using benchmark_model\n",
      "\n",
      "add support of reduce max to tensorflow lite micro\n",
      "\n",
      "Is the TFLite-Micro Library Thread-safe?\n",
      "\n",
      "[RNN]Failed to do full integer quantization and got error: Failed to parse the model: pybind11::init(): factory function returned nullptr.\n",
      "\n",
      "TF 2.1.0 doesn't recognize GPU, PyTorch does\n",
      "\n",
      "Java Module support\n",
      "\n",
      "Unstack a ragged tensor stacked by the tf data api\n",
      "\n",
      "fit function doesn't throw any error or warning if validation_data is a list instead of tuple but shows unexpected validation logs\n",
      "\n",
      "AdditiveAttention score calculation\n",
      "\n",
      "Build failure in tensorflow/tensorflow:devel docker container on MacOS Catalina 10.15.4\n",
      "\n",
      "Add blank_index parameter for ctc_greedy_decoder\n",
      "\n",
      "InaccessibleTensorError: The tensor 'Tensor(\"Tile:0\", shape=(None, 3), dtype=float32)' cannot be accessed here: it is defined in another function or code block.\n",
      "\n",
      "control_dependencies with assert_equal\n",
      "\n",
      "TFLite-Micro Operation Support for \"Dropout\"\n",
      "\n",
      "tf.Module.name_scope overrides parent scopes\n",
      "\n",
      "train_on_batch fails with MirroredStrategy\n",
      "\n",
      "Unintended tf.distribute.ReplicaContext.merge_call behavior on TPU\n",
      "\n",
      "Inconsistency in MirroredStrategy evaluation results for batch dependent metrics\n",
      "\n",
      "Remove GCC_HOST_COMPILER_PREFIX as it may be out of sync with GCC_HOST_COMPILER_PATH\n",
      "\n",
      "\"OverflowError: cannot serialize a bytes object larger than 4 GiB\" when using model.fit([...], use_multiprocessing=True) on custom generator\n",
      "\n",
      ".bazelversion and configure.py conflicts\n",
      "\n",
      "Upgrade/Provide Docker Images based on Ubuntu 20.04 LTS\n",
      "\n",
      "tf.keras.losses.categorical_crossentropy and binary_crossentropy (and other losses) only works for channels_last layout networks\n",
      "\n",
      "Cannot build TFLite with OpenCL SYCL support\n",
      "\n",
      "Inputs mismatch in model.predict does not raise Error\n",
      "\n",
      "Tensorflow writes events file in TMPDIR with unbounded size\n",
      "\n",
      "tensorflow/lite/kernels/gather.cc:80 0 <= axis && axis < NumDimensions(input). tflite for android\n",
      "\n",
      "C API Release\n",
      "\n",
      "tf.keras.metrics.MeanIoU API is practically unusable without a threshold\n",
      "\n",
      "TensorFlow can't be build for PS4 using Orbis LLVM compiler\n",
      "\n",
      "Rank-k cholesky up/downdates\n",
      "\n",
      "Loss over a padded and masked sequence\n",
      "\n",
      "Adding Tab press for showing available methods and autocompletion for objects\n",
      "\n",
      "TF 2.2 - Train_step output control\n",
      "\n",
      "Using dynamic=True in a keras Metric constructor should run the metric methods in eager mode\n",
      "\n",
      "Implementation of custom operations in the layers\n",
      "\n",
      "Add Bluetooth support to send Box Locations array\n",
      "\n",
      "Improve training runtime reporting during .fit() for different modes.\n",
      "\n",
      "google.protobuf.message.DecodeError With Multi-GPU and Mirrored Strategy on 2.2rc3 (and 2.2rc4)\n",
      "\n",
      "Error in Keras Save with TF Lookup\n",
      "\n",
      "Serializing a tensor and writing to tf.train.Example from within a graph\n",
      "\n",
      "Failure to convert model to tflite on local laptop\n",
      "\n",
      "GPU Kernel for SparseFillEmptyRows OP\n",
      "\n",
      "tf.nn.depthwise_conv2d with rank=1 kernels (separable filters)\n",
      "\n",
      "tensorflowrun for distributed training (MultiWorkerMirroredStrategy & ParameterServerStrategy)\n",
      "\n",
      "For the same model Keras trains about 4x slower compared to Estimators.\n",
      "\n",
      "Training Property on Keras Layers\n",
      "\n",
      "impl.OpError: file is too short to be an sstable & DataLossError: Checksum does not match- TensorFlow 2.1.0\n",
      "\n",
      "C-API input tensor type of all run functions\n",
      "\n",
      "Micro kernel elementwise math ops issues (abs, sin, cos, log, sqrt, tanh)\n",
      "\n",
      "Loading a saved multi-input model\n",
      "\n",
      "Can't load saved keras model.h5\n",
      "\n",
      "TFLMicro mul kernel input shapes broadcasting\n",
      "\n",
      "Completed validation_data entry for .fit()\n",
      "\n",
      "Thread hang when setting inter_op_parallelism_threads=1\n",
      "\n",
      "EdgeTPU compiler creates a model with different behaviour\n",
      "\n",
      "hlo_algorithm_blacklist.cc compilation fails\n",
      "\n",
      "validation_data documentation entry incomplete\n",
      "\n",
      "Using tf.data.Dataset has big overhead\n",
      "\n",
      "Tensorflow lite Quantization aware training in Keras FAIL\n",
      "\n",
      "Tensorflow TPU - ValueError: No gradients provided for any variable\n",
      "\n",
      "Discrepancy of soname for libtensorflow_framework.[dylib|so] on macOS and Linux\n",
      "\n",
      "Sampling uniforms is slow\n",
      "\n",
      "unique_with_counts is quite slow\n",
      "\n",
      "Qualify TensorFlow Docker Hub image as Docker Certified\n",
      "\n",
      "Adding regularization losses with the `model.add_loss` method doesn't seem to do the intended thing\n",
      "\n",
      "K.in_train_phase() broken in eager mode when used outside of Lambda layer\n",
      "\n",
      "Migrate mobilenet_v3 from keras-team/keras-applications to tensorflow/tensorflow/python/keras/applications/\n",
      "\n",
      "Distribution algorithm has limited performance on CPU\n",
      "\n",
      "Fit an Use the Keras scikit-learn wrapper with Generator dataset\n",
      "\n",
      "tf.ragged.stack may return tf.Tensor instead of tf.RaggedTensor\n",
      "\n",
      "[CPU] Best CPU Build found with MKL config in bazel build BUT with MKL Disabled in script, especially on LSTM, can't undestand why\n",
      "\n",
      "Batch normalization performs different in tf.kera.Model and tf.estimator.Estimator.\n",
      "\n",
      "Quantized Conv2D op gives different result in TensorFlow and TFLite\n",
      "\n",
      "Weights mismatch in model.load_weights does not rise a ValueError\n",
      "\n",
      "tf.keras.utils.get_file inconsistent behavior with keras.utils.get_file\n",
      "\n",
      "Tensorboard projector does not display logits when using TPUEstimator\n",
      "\n",
      "AutoGraph and tf.function are not working for TPU\n",
      "\n",
      "What is the minimum subset of cuda toolkit to install for tensorflow gpu to work?\n",
      "\n",
      "Some similar Docker images don't have the same image ID\n",
      "\n",
      "[Keras] Support multiple validation sets in Model.fit\n",
      "\n",
      "[TFlite dynamic range quantization]How can I disable dynamic quantization of activations? in inference?\n",
      "\n",
      "Considerations about micro_speech example\n",
      "\n",
      "No default summary writer available when using tf.py_function with autograph\n",
      "\n",
      ".NET Language Bindings\n",
      "\n",
      "ConvRNN2D.build() - TypeError: can only concatenate list (not \"tuple\") to list\n",
      "\n",
      "Gradient checkpointing for TF keras models\n",
      "\n",
      "TPU PyFunction results in UnavailableError: failed to connect to all addresses\n",
      "\n",
      "Matching Keras Backend functions\n",
      "\n",
      "tf.name_scope has no effect when used with tf.cond and autograph\n",
      "\n",
      "tf.name_scope with spaces does not raise ValueError in eager execution \n",
      "\n",
      "tflite gpu delegate create and load model use v2 api is very slow compare with v1 api(10x) why ?\n",
      "\n",
      "Generate  C++ code from TensorFlow Lite metadata\n",
      "\n",
      "Build Tensorflow with triSYCL?\n",
      "\n",
      "TFLite MaxPool2D GPU op seems to modify source tensor\n",
      "\n",
      "mixed precision for non-Keras TensorFlow scripts\n",
      "\n",
      "[ko][zh-cn] How to get chinese/korean fonts to work in matplotlib + Colab?\n",
      "\n",
      "ResNeXt seems to be missing\n",
      "\n",
      "[TFLite] Fully quantize the uint8 concatenation kernel to make it pure integer\n",
      "\n",
      "TFLite Concatenation Fails on GPU delegate\n",
      "\n",
      "Tensorflow hangs, requires reboot\n",
      "\n",
      "NaN's in saved model only when training with tensorflow-gpu\n",
      "\n",
      "Does `tf.constant()` waste memory? What is the alternative?\n",
      "\n",
      "Can I use other Microcontrollers other than which are specified by tensorflow?\n",
      "\n",
      "[2.2] XLA requires 2x GPU memory with sparse_categorical_crossentropy\n",
      "\n",
      "Unable To Find Relevant Documentation For Quantization from python/tf/dtype/DType \n",
      "\n",
      "Make it simpler to write custom metrics! \n",
      "\n",
      "In ModelCheckpoint, filepath is not accepting batch as formatting parameter.\n",
      "\n",
      "tf2+yolov3 Convert to tflite quession\n",
      "\n",
      "clip_by_value handles python float and numpy float differently\n",
      "\n",
      "The output of BatchNormalization may contain Nan under certain parameters\n",
      "\n",
      "Resize Second Derivatives\n",
      "\n",
      "Roadmap for publishing TFLite runtime on PyPi?\n",
      "\n",
      "compilation error\n",
      "\n",
      "Image segmentation model\n",
      "\n",
      "Make \"history\" return by fit store the initial losses\n",
      "\n",
      "[TFLITE] Modelmaker and code generator for boundingRect/Cuboid + keypoints\n",
      "\n",
      "Memory leak when try to use Metal delegate with “TFLGpuDelegateWaitTypeAggressive” option\n",
      "\n",
      "FTRL maths displayed messy\n",
      "\n",
      "tf.function decorated functions fail in graph mode if any of the branches of conditionals would be invalid at runtime\n",
      "\n",
      "C API for TensorFlow Lite for Microcontrollers (micro)?\n",
      "\n",
      "Tflite GPU Delegate for Jetson TK1 Platform 32bit K1 GPU\n",
      "\n",
      "Tensorboard exception with non-ascii character username on windows\n",
      "\n",
      "hard_swish(x) layer in TFLM app crash\n",
      "\n",
      "problem with keras.applications models if pooling != None\n",
      "\n",
      "Didn't find op for builtin opcode 'QUANTIZE' version '2'\n",
      "\n",
      "tensorflow.python.framework.errors_impl.UnimplementedError: File system scheme 'gs' not implemented (file: 'gs://tfds-data/datasets/mnist')\n",
      "\n",
      "tflite buffer from GPU\n",
      "\n",
      "More clarity on TFLite + GPU\n",
      "\n",
      "Gradient not registered in autograph mode with tf.data.Dataset loop\n",
      "\n",
      "tf.train.Checkpoint complain that tf.train.ExponentialMovingAverage instance is not a trackable object\n",
      "\n",
      "Simple graph invoking tf.complex() doesn't work on GPU, but works on CPU\n",
      "\n",
      "xla_build CMakeLists.txt - multi platforms\n",
      "\n",
      "Broadcasting sparse indices\n",
      "\n",
      "Not converting pb to tflite using tflite_convert with some custom ops\n",
      "\n",
      "Better documentation for the Embeddings Projector, especially graphs/edges?\n",
      "\n",
      "Tensorflow Datasets with string inputs do not preserve data type\n",
      "\n",
      "Keras `model_from_json` ignores distribution strategy\n",
      "\n",
      "[TF 2.1] Jacobian / Hessian with Conv1D layers\n",
      "\n",
      "tflm vc compilation error and how to fix\n",
      "\n",
      "Wrong x_lerp computation in \"ResizeBilinearKernel_faster\" and \"ResizeBilinearKernel\"\n",
      "\n",
      "Cannot save to SavedModel when custom layers accepts args and kwargs\n",
      "\n",
      "Strange behavior of ODE solving toy model\n",
      "\n",
      "Add MPI cluster resolver\n",
      "\n",
      "The docker image provided to compile tensorflow custom ops is too big.\n",
      "\n",
      "mobilenet_v2_140_224/classification: Cannot copy between a TensorFlowLite tensor with shape [1, 120] and a Java object with shape [1, 10, 4]\n",
      "\n",
      "TFLite Failed to build gpu delegate on Linux\n",
      "\n",
      "Update NCCL version to the latest release v2.6.4-1\n",
      "\n",
      "Performance issue with cross compiled benchmark_model and label_img\n",
      "\n",
      "TFLite Conversion: ValueError: This converter can only convert a single ConcreteFunction. Converting multiple functions is under development. \n",
      "\n",
      "Keras model compiled with custom loss raises \"object has no attribute '__name__'\" error.\n",
      "\n",
      "Cannot convert lstm with \"stateful=True\" to tflite\n",
      "\n",
      "tf.lookup.StaticHashTable: Cannot convert a Tensor of dtype resource to a NumPy array\n",
      "\n",
      "How to limit GPU memory usage in java?\n",
      "\n",
      "Master build failed with user local specific GCC\n",
      "\n",
      "ERROR: Did not get operators, tensors, or buffers in subgraph 0.\n",
      "\n",
      "Embedding layer ~10x slower in tf.function\n",
      "\n",
      "TF2.2.0rc2: exception if shape of input to tf.linag.set_diag not fully specified \n",
      "\n",
      "Add a Mutable Hash Table of Ragged Tensors \n",
      "\n",
      "estimator.train_and_evaluate() sometimes throws errors during training \n",
      "\n",
      "Failed to find bogomips warning\n",
      "\n",
      "in nmt_with_attention, the gru in decoder is not connected, last step state is not passed to this step\n",
      "\n",
      "register keras serializables is module_objects\n",
      "\n",
      "TensorFlow hangs: 0% GPU, 1 CPU core @ 100% in cuModuleUnload\n",
      "\n",
      "Named Dimensions\n",
      "\n",
      "Request to have ConvLSTM2D for TFLite\n",
      "\n",
      "Missing Trainable Variables and Variables\n",
      "\n",
      "pixelwise-loss weight map part 2\n",
      "\n",
      "Close (and reopen new) summary files at regular intervals\n",
      "\n",
      "tensorflow.keras.constraints.RadialConstraint causes exception when training\n",
      "\n",
      "tf.data.Dataset should support attr classes instances as values\n",
      "\n",
      "Add GPU-deterministic back-prop for fused softmax/cross-entropy ops\n",
      "\n",
      "[PROPOSAL] Go back to cmake and drop support to bazel -- a real memory and time abusing toolkit for building medium size projects\n",
      "\n",
      "example person_detection_test  can not make form source code of tensorflow\n",
      "\n",
      "tf.gather Converting sparse IndexedSlices warning\n",
      "\n",
      "Dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory;\n",
      "\n",
      "Provide a tensorflow maven package for AWS Lambda or lighter builds in general\n",
      "\n",
      "TFLM: add run-time check that statically allocated buffers are sufficiently large\n",
      "\n",
      "Test deterministic cuDNN CTC loss\n",
      "\n",
      "Gradient Compute Error in Embedding Layers\n",
      "\n",
      "Custom Implementation for FusedBatchNormV3\n",
      "\n",
      "implicit conversion from 'int' to 'float' changes value from INT_MAX to INT_MAX+1\n",
      "\n",
      "Use only tensorflow-lite-gpu aar file\n",
      "\n",
      "Direct feed from GPU memory in Java\n",
      "\n",
      "Allow users to specify method name when using tf.saved_model.save\n",
      "\n",
      "/bin/sh: 1: python: not found error when generate projects\n",
      "\n",
      "Failed to build the master branch\n",
      "\n",
      "Also show \"Available Since\" for each API in tf docs\n",
      "\n",
      "PiecewiseConstantDecay Keras Learning Rate Scheduler not compatible with XLA Compilation\n",
      "\n",
      "Unable to access files on S3 with tf.io.gfile.GFile\n",
      "\n",
      "TextVectorizationLayer supporting custom padding token\n",
      "\n",
      "flatbuffer.shape() might return nullptr\n",
      "\n",
      "Unable to cross compile TFLite for Raspberry Pi Zero  using Docker image nightly-develop\n",
      "\n",
      "java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors\n",
      "\n",
      "segfault in FlexDelegate on android\n",
      "\n",
      "Unable to build micro_speech for Sparkfun Edge\n",
      "\n",
      "tf.sparse.reorder() produces fully-unknown shaped outputs from partially-unknown shaped placeholder inputs\n",
      "\n",
      "KerasClassifier.score is ... broken!?\n",
      "\n",
      "Documentation corresponding to Arguments for Pre-Trained Models inside <tf.keras.applications> is missing\n",
      "\n",
      "[tf.estimator] - `tf.estimator.Head` do not work with TPUs\n",
      "\n",
      "Unable to successfully build a devel-cpu.Dockerfile for arm32v7/arm64v8\n",
      "\n",
      "Large Model Support\n",
      "\n",
      "Binary add op BF16 has lower performance than FP32\n",
      "\n",
      "Graph Transform Tool remove_nodes is unable to remove Switch nodes but Identity nodes\n",
      "\n",
      "Using experimental_new_converter generates opcode that is not found in kernel\n",
      "\n",
      "Guide for Distributed TF with c++\n",
      "\n",
      "Update vocabulary and add a new embeddings for the new token as you see them in training time\n",
      "\n",
      "Anconda Tensorflow-GPU 2.1.0 cudart64_101.dll not found\n",
      "\n",
      "Compile error on Windows\n",
      "\n",
      "MicroInterpreter error: Seg Fault, *** stack smashing detected *** inside MicroInterpreter::Invoke() method\n",
      "\n",
      "**debug feature** for debugging numeric instability, like nan or infinity\n",
      "\n",
      "Copying tensors to GPU fails non-deterministically\n",
      "\n",
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.112000). Check your callbacks.\n",
      "\n",
      "Allow tf.image.random_crop to return multiple crops\n",
      "\n",
      "Debundled protobuf broken\n",
      "\n",
      "tf.function retracing even with input_signature\n",
      "\n",
      "Cannot convert model containing categorical_column_with_vocabulary_list op\n",
      "\n",
      "[Feature Request][Keras] Allow loss function using multiple tensors as input\n",
      "\n",
      "Code improvements results from Cppcheck\n",
      "\n",
      "tf.Module does not update state when a trackable attribute is reassigned a nontrackable\n",
      "\n",
      "GradientTape.jacobian works for batch shape 0 when `experimental_use_pfor=True` but not when `experimental_use_pfor=False`\n",
      "\n",
      "[Feature Request] Keras backend support jax\n",
      "\n",
      "min_epsilon of fused_batch_norm is incorrect\n",
      "\n",
      "Failed to convert object detection model to tensorflow lite\n",
      "\n",
      "corrupted tfrecord error should also have the info of file name\n",
      "\n",
      "SavedModelEstimator not found in TF 2\n",
      "\n",
      "TFLite Micro Speech not detecting audio on ESP-EYE\n",
      "\n",
      "STM32F746NG Hello World example fails in /tensorflow/lite/micro/kernels/cmsis-nn/conv.cc\n",
      "\n",
      "import tensorflow is very slow\n",
      "\n",
      "Gradients do not exist for variables after tf.concat().\n",
      "\n",
      "Partial shapes for keras.InputLayer with sparse=True \n",
      "\n",
      "problem with batch processing for tflite in android java\n",
      "\n",
      "2.1.0 build error due to pybind11\n",
      "\n",
      "CUDA_ERROR_LAUNCH_TIMEOUT when using NCCL with MultiWorkerMirroredStrategy on multi-node systems\n",
      "\n",
      "shape asserts to check internal code correctness\n",
      "\n",
      "Getting std::bad_alloc in Android - StyleTransfer\n",
      "\n",
      "Cannot create feature_column from sparse tensor of feature counts\n",
      "\n",
      "TFRecord dataset reader causes excessive page faults\n",
      "\n",
      "tf.Keras BatchNormalization layer causing impossible loss/accuracy in GAN training\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stateful Keras RNN Conversion\n",
      "\n",
      "Get different results in eager and graph mode when I use tf.keras.Reduction.NONE on Loss object.\n",
      "\n",
      "Memory Leak in tf.data.Dataset.from_generator\n",
      "\n",
      "Predict API does not work with VarLenFeature when batch inputs from different size\n",
      "\n",
      "Build TF 2.1.0 with RHEL6/RHEL7\n",
      "\n",
      "[tflite][java] has_rank_one_input_condition error when use runForMultipleInputsOutputs()\n",
      "\n",
      "grpc+verbs fail with tensorflow/core/common_runtime/process_state.cc:128] Check failed: 0 == cpu_allocators_.size() (0 vs. 1)\n",
      "\n",
      "ModelCheckpoint does not save the model when `_is_graph_network` is False\n",
      "\n",
      "TF saved model Assertion Error ( Called a function referencing variables which have been deleted )\n",
      "\n",
      "tf.train.FloatList() on a tensor takes too long\n",
      "\n",
      "log(diag_part(x)) vs diag_part(log(x)) difference with Cholesky gradients\n",
      "\n",
      "Add Octave Convolution Layers to tensorflow keras layers\n",
      "\n",
      "tf.data.experimental.ignore_errors unintuitive behavior\n",
      "\n",
      "Cumulative decaying sum\n",
      "\n",
      "Dose TensorFlow has tf.sparse.segment_max? how to code it by customer?\n",
      "\n",
      "large model parallelism on gpu causing oom\n",
      "\n",
      "TF_FORCE_GPU_ALLOW_GROWTH=false Triggers cuDNN Error\n",
      "\n",
      "Android model personalization crash with keras model\n",
      "\n",
      "Concat op not quantized \n",
      "\n",
      "\"Another profiler is running\" in TF 2.2.0rc0\n",
      "\n",
      "C-API timeline for TensorFlow 2\n",
      "\n",
      "Custom Pearson Correlation Loss Taking Unfeasible Values During Training\n",
      "\n",
      "tf.numpy_function with lambda function is much slower starting from TF 1.15\n",
      "\n",
      "Excessive memory consumption and preparation runtime of tf.keras.backend.max in custom layer with masking\n",
      "\n",
      "Configurable attribute \"srcs\" doesn't match this configuration\n",
      "\n",
      "No gradient defined for op\n",
      "\n",
      "map_fn + @tf.function + tf.nn.conv2d throws error when strides to conv2d are supplied in the elems argument for map_fn\n",
      "\n",
      "Resource temporarily unavailable, ENOMEM on CPU with memory rlimit\n",
      "\n",
      "Increase BiasGrad size:  \"BiasGrad requires tensor size <= int32 max\"\n",
      "\n",
      "Python crashes when computing max/min of a complex tensor\n",
      "\n",
      "InaccessibleTensorError in graph mode with for loop itteration (different sized tensors) ( tf.image.resize)\n",
      "\n",
      "AttributeError: module 'tensorboard' has no attribute 'lazy'\n",
      "\n",
      "Encountered unresolved custom op: BatchMatMulV2.\n",
      "\n",
      "tf.Graph.get_tensor_by_name does not work as expected in TF2\n",
      "\n",
      "TensorFlow 2.1.0: _FallbackException: This function does not handle the case of the path where all inputs are not already EagerTensors\n",
      "\n",
      "FasterRCNN_resnet101 .tflite conversion\n",
      "\n",
      "Inconsistent device placement when using nondifferentiable_batch_function and GPU\n",
      "\n",
      "Model customization in iOS\n",
      "\n",
      "feature request: add tf_random_seed argument for TPUConfig\n",
      "\n",
      "ResNet models in tf.keras.applications contain a bias term which should not be there.\n",
      "\n",
      "Raspberry Pi 3: ERROR: tensorflow-2.1.0-cp35-none-linux_armv7l.whl is not a supported wheel on this platform.\n",
      "\n",
      "Method Chaining in Tensorflow\n",
      "\n",
      "CombinedNonMaxSuppression is not whitelisted operator\n",
      "\n",
      "Cert error for https://download.tensorflow.org/\n",
      "\n",
      "tf.estimator.SummarySaverHook should support native V2 symbols.\n",
      "\n",
      "Memory leak activated when setting a seed in TensorFlow 2 (2.0 & 2.1) and using eager execution.\n",
      "\n",
      "Warning/Error not present when not using the result of write in a TensorArray\n",
      "\n",
      "Community feedback for translated landing pages\n",
      "\n",
      "TFLite Micro: SUB Op Support\n",
      "\n",
      "Docker Tensorflow GPU 5x times slower than TF CPU\n",
      "\n",
      "guarantees for the logs argument in keras Callbacks\n",
      "\n",
      "Could not find any TF GPUs when I use trt_convert\n",
      "\n",
      "tf2.0 when use large input dims in tf.keras.layers.Embedding layer under eager mode, it turns to be very slow\n",
      "\n",
      "Thread leakage in TF2.1\n",
      "\n",
      "Problem with Himax HM01B0 camera on the Sparkfun Edge board\n",
      "\n",
      "Error on ESP32\n",
      "\n",
      "Added ResNext models\n",
      "\n",
      "list of operators for which you will need custom implementations: StatelessWhile, TensorListFromTensor, TensorListReserve, TensorListStack, While.\n",
      "\n",
      "\"Could not append to the internal temporary file\" when writing checkpoints to GCP during TPU training\n",
      "\n",
      "TensorRT 6/7 converter crash with tf 2.1.0\n",
      "\n",
      "Using Precison metric in compile method raises shape mismatch error\n",
      "\n",
      "I have to install TF 1.8!\n",
      "\n",
      "Tensorflow Workspace not configured for android build\n",
      "\n",
      "v2.1.0 build fails with TensorRT 6 and 7\n",
      "\n",
      "MultiWorkerMirroredStrategy OOM - Ubuntu 18.04 \n",
      "\n",
      "The output of the Conv2D layer in backend TensorFlow is different from that in Theano and CNTK\n",
      "\n",
      "reshuffle_each_iteration=False ignored on validation (tf.dataset + Keras)\n",
      "\n",
      "Feature suggestion: Ability to add copyright notice to saved model\n",
      "\n",
      "Hang on out of memory error\n",
      "\n",
      "RNN unrolled, wrong quantization graph\n",
      "\n",
      "Modified reduce_variance for extra optimization\n",
      "\n",
      "TopK result always sort when k==num_cols\n",
      "\n",
      "Unable to implement \"Hello_world\" example on esp32\n",
      "\n",
      "RuntimeError: Encountered unresolved custom op: TensorListFromTensor.Node number 892 (TensorListFromTensor) failed to prepare.\n",
      "\n",
      "[Doc] Laplacian for Deep Dream 2.0\n",
      "\n",
      "Proposal: Modify tf.math.reduce_variance so that it is compatible with ragged tensors\n",
      "\n",
      "Shape issues in keras.metrics.sparse_top_k_categorical_accuracy with multiple dimensions\n",
      "\n",
      "`recompute_grad` does not save memory and is incompatible with graph mode\n",
      "\n",
      "CUDA broken on docker image tensorflow/tensorflow:devel-gpu\n",
      "\n",
      "Gradient checkpointing: Wrap `tf.keras.Model` or `tf.keras.layers.Layer` in `tf.recompute_gradients()`\n",
      "\n",
      "Cannot save model when not giving a name in Layer.add_weight call\n",
      "\n",
      "Java.lang.ArrayIndexOutOfBoundsException: length=66049; index=66049\n",
      "\n",
      "MultiWorkerMirroredStrategy failed to run after a few batches occasionally\n",
      "\n",
      "DepthwiseConv1D: implementation, check and warning message\n",
      "\n",
      "Memory leak in finalizing GeneratorDataset iterator\n",
      "\n",
      "input_details shape is mismatching when converting Estimator to tflite\n",
      "\n",
      "ValueError: Unable to save the object ListWrapper(...) (a list wrapper constructed to track trackable TensorFlow objects) when calling the method tf.keras.Model.save_weights\n",
      "\n",
      "tensorflow.python.ops.linalg.sparse.matmul crashes when inputs have different rank, but only if either is a CSRSparseMatrix\n",
      "\n",
      "tensorflow/core/grappler/optimizers/meta_optimizer.cc:561] layout failed: Invalid argument: MutableGraphView::SortTopologically error:\n",
      "\n",
      "Feature Request: Pickle an entire model.\n",
      "\n",
      "tf.keras.layers.ReLU() drops tensor._keras_mask (sample_weight) unintentionally.\n",
      "\n",
      "How to quant TFLite model in int8 format\n",
      "\n",
      "DockerHub tag 2.1.0-py3 includes GPU support\n",
      "\n",
      "Tensorflow Lite Build Problem - Specialization for Eigen::internal::scalar_logistic_op<float> Removed\n",
      "\n",
      "Keras with multiple outputs: cannot evaluate a metric without associated loss\n",
      "\n",
      "Memory leak using generators with tf.data.experimental.AUTOTUNE\n",
      "\n",
      "import_graph_def  with cond nodes fails in eager/tf.function contexts\n",
      "\n",
      "Pip package building fail on third party NASM linking \n",
      "\n",
      "Feature Request: General Purpose Metrics Callback\n",
      "\n",
      "TF 2.0: Spyder auto complete and go to definition not working. Please fix it!\n",
      "\n",
      "Severe slowdown with 1 BN step in while_v2\n",
      "\n",
      "Add \"See also\"  references\n",
      "\n",
      "TF 2.1/tf.keras AdaDelta optimizer: default epsilon and learning rate values\n",
      "\n",
      "Blas GEMM launch failed\n",
      "\n",
      "Building libtensorflow for v2.1.0 fails on mac with unrecognized flag \"-fno-constant-cfstrings\"\n",
      "\n",
      "posenet negative output scores\n",
      "\n",
      "Add more information about eager/graph context for Keras layer.call()\n",
      "\n",
      "Image Classification pretrained model breaks for batch size 1 with BaseCollectiveExecutor::StartAbort error\n",
      "\n",
      "Bug for TF2.x + TensorRT(7) failing when minimum_segment_size=2\n",
      "\n",
      "Allow TrtGraphConverterV2 to accept Frozen Graph input as well as saved_model\n",
      "\n",
      "BUG: tf.random.normal() has a fixed value in eager mode (TF2.0)\n",
      "\n",
      "Error TFLite converting tensorflow graph generated with config file with experimental_new_converter = True\n",
      "\n",
      "RNN support for Tensorflow Lite Micro\n",
      "\n",
      "Dataset padded_batch fails with InvalidArgumentError\n",
      "\n",
      "tf.keras.layers.Softmax should respect image_data_format by default\n",
      "\n",
      "Custom training loss with custom gradients\n",
      "\n",
      "[TF2] TRT Engine Ops are not garbage collected, resulting in incorrect reuse\n",
      "\n",
      "Add a raspberry pi docker image\n",
      "\n",
      "Pip package for tensorflow 2.1 for Raspberry Pi (python 3.7)\n",
      "\n",
      "generate_projects target in the tf micro makefile fails for some target architectures\n",
      "\n",
      "Hello World example for STM32F746 make error\n",
      "\n",
      "Custom layer weights all have the same name by default\n",
      "\n",
      "IndexError: list index out of range when using tf.dataset and keras for autoencoder\n",
      "\n",
      "Tensorflow lite NnApiDelegate crashes on Pixel 3a XL\n",
      "\n",
      "Make callbacks serializable\n",
      "\n",
      "Dataset.shuffle leads to worse training performance due to chunked processing\n",
      "\n",
      "tf.signal.inverse_stft does not reconstructs the original signal\n",
      "\n",
      "tf.saved_model.save can not save a model containing DenseHashTable\n",
      "\n",
      "Buid failed in redhat system..\n",
      "\n",
      "TensorRT plugins\n",
      "\n",
      "Heatmap function\n",
      "\n",
      "Building a static library for windows with visual studio.\n",
      "\n",
      "tf.data.Dataset unusable with steps_per_epoch standard training loop\n",
      "\n",
      "Failed build on Windows with clang-cl\n",
      "\n",
      "wrong error message from tf.data.Dataset when GPU OOM\n",
      "\n",
      "Misleading documentation for hook parameters in estimators evaluate, predict, train methods\n",
      "\n",
      "Loading Tensorflow models from disk is instant while loading from gcs is very slow\n",
      "\n",
      "Cannot install tensorflow go\n",
      "\n",
      "How can I clear GPU memory in tensorflow 2?\n",
      "\n",
      "tf.keras.Sequential ignores the outer name scope(s) when built proactively\n",
      "\n",
      "Keras - Supporting load/save models and weights to Google Storage\n",
      "\n",
      "Broken outbound links from index pages for older TF API\n",
      "\n",
      "Keras.gradients() returns None in loss function\n",
      "\n",
      "[TF 2.1] Error when converting LSTM model to a frozen graph using convert_variables_to_constants_v2()\n",
      "\n",
      "Callbacks should be able to access the training and validation data\n",
      "\n",
      "More advanced ways of collecting metrics in tf.keras while training models\n",
      "\n",
      "embedding_lookup_sparse divide by zero fixed\n",
      "\n",
      "tf.function using higher GPU memory than normal python function\n",
      "\n",
      "Parallel Write TFRecords Shards\n",
      "\n",
      "Got Warning : Disabling AVX support in Mac OSX Catalina\n",
      "\n",
      "OpenBLASS for efficient Matrix multiplications?\n",
      "\n",
      "Ability to calculate projected memory usage for a given model\n",
      "\n",
      "How do I create multiple custom AUC metrics, one for each of the outputs, in TensorFlow?\n",
      "\n",
      "test that the full precision model runs on the CPU and GPU, but it takes 800ms to initialize on the GPU, but it only takes a few milliseconds to initialize on the CPU, how should I optimize\n",
      "\n",
      "TensorRT native segment lookup error in calibration mode\n",
      "\n",
      "tf.keras loss/metric argument/return specification\n",
      "\n",
      "Decouple RaggedStructure from RaggedTensor\n",
      "\n",
      "Support tf.SparseTensor and tf.RaggedTensor in tf.py_function\n",
      "\n",
      "Support nested structure as return value in tf.py_function.\n",
      "\n",
      "Support setting shapes directly in tf.py_function\n",
      "\n",
      "TFLite FP16 network Fully connected layer failing on gpu in android\n",
      "\n",
      "Check failed: 0 <= new_num_elements\n",
      "\n",
      "GradientTape does not track intermediate variables inside a tf.function\n",
      "\n",
      "TF 2.1 libtensorflow library does not build under Windows\n",
      "\n",
      "TF-Lite example label Image w shared library\n",
      "\n",
      "external/pybind11\\include/pybind11/pybind11.h(139): error C2783: “std::tuple<pybind11::detail::type_caster<pybind11::handle,void>>::tuple(void) noexcept(<expr>)”: 未能为“__formal”推导 模板 参数\n",
      "\n",
      "tf.lite.converter.convert() Erroe\n",
      "\n",
      "Keras: expose loss metric of singular output\n",
      "\n",
      "Unable to build \"Hello World\" for ESP32\n",
      "\n",
      "TFLite new quantizer brokes ALBERT model\n",
      "\n",
      "AttributeError: 'Tensor' object has no attribute 'log_prob'\n",
      "\n",
      "Build fails with messages including \"undefined reference to symbol 'acos@@GLIBC_2.2.5' \", \"Linking of rule '@flatbuffers//:flatc' failed\"\n",
      "\n",
      "Memory leaks when doing a random tf.op on a previously converted to NumPy Tensor in TF2.1 \n",
      "\n",
      "Tensorflow 2.0.0 cpu, import error: DLL load failed\n",
      "\n",
      "Tensorflow.summary.histogram produces wrong output\n",
      "\n",
      "Make TensorFlow build with system packages\n",
      "\n",
      "TFLite for Microcontrollers: Compilation issues for operators for bare metal\n",
      "\n",
      "Multiple outputs from a keras model\n",
      "\n",
      "Sampled Softmax for Tensorflow Keras\n",
      "\n",
      "Fix: Can't set None on TextVectorization layer's split parameter problem\n",
      "\n",
      "TFLite Android Model Benchmark Tool -- results not showing up in adb logcat\n",
      "\n",
      "Failed build on windows wtih MKL\n",
      "\n",
      "Simple keras model, Model.fit() does not learn unless experimental_run_tf_function=False at compile\n",
      "\n",
      "Possible mem leak in tf.random.categorical\n",
      "\n",
      "Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n",
      "\n",
      "Loading a tf.Module with tf.saved_model.load missing attributes\n",
      "\n",
      "The summary method could be changed for improved understandability and readability\n",
      "\n",
      "AOT tests fail on Windows (MSVC 2019)\n",
      "\n",
      "model.fit requests and leaks extra data from validation_data generator than the specified validation_steps\n",
      "\n",
      "Error while generating TensorflowLite file\n",
      "\n",
      "windows build failed with master branch\n",
      "\n",
      "Allow relative min_delta for EarlyStopping\n",
      "\n",
      "Add python dev (alpha version) for testing and building in CI\n",
      "\n",
      "Support for saved_model (tf2.*) in tf_compile\n",
      "\n",
      "tf.data.Dataset.from_generator converts input argument types implicitly instead of just forwarding\n",
      "\n",
      "Problems when compiling with Clang\n",
      "\n",
      "Dataset with Keras Functional Model: tuple index out of range uin steps_per_epoch\n",
      "\n",
      "global numbering scheme of  tf.keras.layers.BatchNormalization layers seems like a wrong strategy\n",
      "\n",
      "STM32F7 Hello World example fails for arm_cmplx_mag_squared_q10p6.c\n",
      "\n",
      "Benchmark test failed on windows\n",
      "\n",
      "Linking error when building Tensorflow 2.1\n",
      "\n",
      "TensorBoard callback without profile_batch setting cause Errors: CUPTI_ERROR_INSUFFICIENT_PRIVILEGES and CUPTI_ERROR_INVALID_PARAMETER\n",
      "\n",
      "tensorflow/python/eager/BUILD:14:1: in cc_library rule //tensorflow/python/eager:pywrap_tfe_lib: cycle in dependency graph\n",
      "\n",
      "Same op sequence is computed multiple times\n",
      "\n",
      "Batching images first or formatting them first?\n",
      "\n",
      "Threading + tf.keras.layers.Input produces TypeError: 'NoneType' object is not iterable\n",
      "\n",
      "Typo Error in `tflite_c02_transfer_learning.ipynb`\n",
      "\n",
      "[Feature Request] MultiHeadAttention ops and Layer\n",
      "\n",
      "hlo_algorithm_blacklist.cc fails compilation\n",
      "\n",
      "Failed to allocate memory for tensor_info, 1320 bytes required\n",
      "\n",
      "TPU InternalError with TF 2.1.0 on Google Colab (Assigned device does not have registered OpKernel support for _Arg node iteratorgetnext_iterator)\n",
      "\n",
      "model._function_kwargs is silently ignored\n",
      "\n",
      "ERROR: ~/....../tensorflow/python/keras/api/BUILD:130:1: Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v2 failed\n",
      "\n",
      "A Subclass of Custom Loss Function Is not Trackable as an Argument in Checkpoint\n",
      "\n",
      "Error when running gmake for Tensorflow Lite Micro\n",
      "\n",
      "A strange numerical computation bug for the simple dense layer\n",
      "\n",
      "Recurrent Dropout is Wrong\n",
      "\n",
      "TFLite C/C++ library header file installation\n",
      "\n",
      "Define variable shape at restore/load, allow direct restoring of variables prior to calling __build__ (non-lazy variable loading from checkpoint)\n",
      "\n",
      "Explanation regarding `seed` parameter\n",
      "\n",
      "assign() got an unexpected keyword argument 'validate_shape'\n",
      "\n",
      "Cannot build from source, get error: command succeeded, but there were errors parsing the target pattern\n",
      "\n",
      "InvalidArgumentError if np.ndarray is registered as Sequence type\n",
      "\n",
      "Malformed `TRAINABLE_ATTRIBUTE_NOTE` in batch normalization documentation\n",
      "\n",
      "EarlyStopping should restore weights on end of training, not end of epoch\n",
      "\n",
      "TFLite GPU execution failed\n",
      "\n",
      "OutOfRangeError Unknown Error when extracting particular zip file\n",
      "\n",
      "How many items does tf.gradients return?\n",
      "\n",
      "erfcx special function\n",
      "\n",
      "Unable to build hello_world for sparkfun_edge\n",
      "\n",
      "GPU memory on Bahdanau Attention\n",
      "\n",
      "tensorflow 2.0.0 crashes with protobuf 3.10.1 on macOS\n",
      "\n",
      "Can anyone build a optimized tensorflow-gpu-v2.0 based on cuda10.0 and cudnn7.6(centos7) for me? \n",
      "\n",
      "Deadlock on recursive tf.function-decorated function\n",
      "\n",
      "TFLite GPU Delegate on iOS: failed assertion `Cannot create a buffer of zero length.\n",
      "\n",
      "Error: Not in GZIP format\n",
      "\n",
      "`files_io.get_matching_files` fails for valid filenames that contain globs\n",
      "\n",
      "non_max_suppression is full of bugs!\n",
      "\n",
      "Error while trying to serilize Image Captioning keras model '_UserObject' object is not callable'\n",
      "\n",
      "Flag --incompatible_no_implicit_file_export will break TensorFlow in a future Bazel release\n",
      "\n",
      "Bijectors are orders of magnitude slower in tf2.1 autograph distributed mirrored single-gpu mode\n",
      "\n",
      "Cannot build gl_delegate of android\n",
      "\n",
      "tensorflowlib for 2.0\n",
      "\n",
      "Enable SO_REUSEPORT option in tensorflow training server\n",
      "\n",
      "TensorflowLite gives wrong results when use GPU delegate\n",
      "\n",
      "Tensorflow Python modules import the same module under multiple different names\n",
      "\n",
      "You tried to call `count_params` on z_input, but the layer isn't built. You can build it manually via: `z_input.build(batch_input_shape\n",
      "\n",
      "AutoGraph is compiled 5x slower in TF2.x Multi-GPU Distributed Mirrored Strategy\n",
      "\n",
      "ValueError: name for name_scope must be a string when Building up a Custom Model\n",
      "\n",
      "Optimizer within Estimator computes incorrectly small gradient when used with MirroredStrategy\n",
      "\n",
      "Error: Cannot convert 'auto' to EagerTensor of dtype float\n",
      "\n",
      "TensorFlow Lite Micro fully connected int8 test passes illegal filter offset\n",
      "\n",
      "runnin magic_wand on efr/efm32 microcontroller\n",
      "\n",
      "unbounded memory leak in tf.io.gfile.isdir()\n",
      "\n",
      "Android TFlite inconsistent performance when app is not in focus.\n",
      "\n",
      "TF-Lite Micro: Selectively omit data types at compile-time\n",
      "\n",
      "tensorflow build fails\n",
      "\n",
      "Flag --incompatible_restrict_string_escapes will break TensorFlow in Bazel 1.2.1\n",
      "\n",
      "Flag --incompatible_use_platforms_repo_for_constraints will break TensorFlow in Bazel 1.2.1\n",
      "\n",
      "Flag --incompatible_load_cc_rules_from_bzl will break TensorFlow in Bazel 1.2.1\n",
      "\n",
      "Flag --incompatible_disable_target_provider_fields will break TensorFlow in Bazel 1.2.1\n",
      "\n",
      "Flag --incompatible_no_implicit_file_export will break TensorFlow in Bazel 1.2.1\n",
      "\n",
      "Flag --incompatible_load_python_rules_from_bzl will break TensorFlow in Bazel 1.2.1\n",
      "\n",
      "Flag --incompatible_disallow_empty_glob will break TensorFlow in Bazel 1.2.1\n",
      "\n",
      "add_weight with None name generate a graph that it's not possible to save by checkpoint manager\n",
      "\n",
      "Return some sort of verifying data structure for load_weights for tf.keras models\n",
      "\n",
      "tflite.load_delegate() failed when running Demo API on Google Coral mini PCIe\n",
      "\n",
      "macOS compiling: hidden symbol `__dso_handle' isn't defined and undefined reference to `operator delete(void*)'\n",
      "\n",
      "TensorImage grayscale (single channel) image support \n",
      "\n",
      "Object detection producing incorrect results on mobile (ios)\n",
      "\n",
      "Softmax activations don't get converted to Softmax TFLite operator if ndim > 2\n",
      "\n",
      "Failed to convert weights to 8 bit precision: \"Quantize weights tool only supports tflite models with one subgraph\" \n",
      "\n",
      "Windows build failed - Internal compiler error Visual Studio 2017 - FAILED: Build did NOT complete successfully\n",
      "\n",
      "tensorflow-lite-gpu.aar built from source is much slower than downloaded prebuilt library\n",
      "\n",
      "-D_GLIBCXX_USE_CXX11_ABI=1 increases a lot RAM usage\n",
      "\n",
      "SavedModelBundle.getSignatures() does not return outputs in correct order.\n",
      "\n",
      "tf.where should have optional dtype parameter\n",
      "\n",
      "Replacement for experimental_run_tf_function after removal from tf.keras.Model.compile \n",
      "\n",
      "STM32F7-Disco Hello World example fails to build\n",
      "\n",
      "Adam implementation differs from paper (applies bias B_2 correction to \\epsilon)\n",
      "\n",
      "Start background prefetching without calling next on dataset iterator\n",
      "\n",
      "Add usage examples for tf.audio APIs\n",
      "\n",
      "Explain how int8 input and output quantization conversion works in TensorFlow Lite\n",
      "\n",
      "TensorFlow Lite Micro MaxPool kernel needs int8 support\n",
      "\n",
      "Calculate arena size for TensorFlow Lite Micro models\n",
      "\n",
      "Bug in documentation of tf.while_loop.  parallel_iterations doesn't seem to affect performance.\n",
      "\n",
      "No example provided for using tf.nn.ctc_loss\n",
      "\n",
      " AndroidRuntime: Node number 1 (SPLIT) failed to prepare.\n",
      "\n",
      "Use bazel to build tensorflow has some error\n",
      "\n",
      "Memory leaks when using tf.keras.metrics update_states in multi-threads.\n",
      "\n",
      "TF2.0: No way to create local variables in tensorflow 2.0\n",
      "\n",
      "ValueError: This converter can only convert a single ConcreteFunction. Converting multiple functions is under development.\n",
      "\n",
      "Build error due to missing dependency declarations in double-conversion\n",
      "\n",
      "Gradient of matmul in while_loop works when run eagerly but not as tf.function\n",
      "\n",
      "Derivative of pow(x, y) can give nan when x=0 for higher order derivatives\n",
      "\n",
      "Memory leak with tf.py_function in eager mode using TF 2.0\n",
      "\n",
      "Attention/AdditiveAttention Issue\n",
      "\n",
      "Inconsistent loading of saved model\n",
      "\n",
      "How to convert ByteBuffer to Bitmap Image?\n",
      "\n",
      "_pywrap_tensorflow_internal.so: cannot open shared object file: No such file or directory\n",
      "\n",
      "Document about masking does not have previous masks information\n",
      "\n",
      "Add K Medoids Estimator to tf canned estimators\n",
      "\n",
      "EfficientDet: Are you going to implement this in the object detection API?\n",
      "\n",
      "DepthwiseConv in lite/kernels/internal/reference/depthwiseconv_float.h can be optimized better\n",
      "\n",
      "Keras MobilenetV3 weights \n",
      "\n",
      "Does TensorFlow 2.x still need the bazel build option of --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" ?\n",
      "\n",
      "Support EIGEN_USE_MKL_ALL macro for building tensorflow\n",
      "\n",
      "Saving custom tensor fails\n",
      "\n",
      "tf.execute_volume_patches misses one dimension when used with placeholders and None\n",
      "\n",
      "TF2.0 - Cannot convert EagerTensor to datatype and casting\n",
      "\n",
      "Segmentation fault and corrupted output during inference with large 3D U-Net\n",
      "\n",
      "layer.output raises AttributeError because inbound nodes lost after call to activation function\n",
      "\n",
      "Bogus cache keys in Network._output_shape_cache can cause wrong output shapes (and gradual but unbounded memory growth)\n",
      "\n",
      "Can not run tensorflow gpu 2.0 with cuda 10.0 on arm64 boards (jetson nano)\n",
      "\n",
      "Build issue \"None of the libraries match their SONAME\" (libcudnn)\n",
      "\n",
      "Keras' predict method should be compatible with TensorFlow Probability\n",
      "\n",
      "MLIR-based convertor fails to convert Universal Sentence Encoder model\n",
      "\n",
      "Multiple inputs for iOS benchmark app\n",
      "\n",
      "Will TF2.0 build with Python3 ONLY, without Python2? \n",
      "\n",
      "tensorflow/workspace: re2 dependency does not use release/master branch\n",
      "\n",
      "Missing information: embedding_lookup automatically returns 0 for an out-of-vocab index\n",
      "\n",
      "Docker build for TensorFlow GPU taking too long(Up 45 hours!) \n",
      "\n",
      "explicit GPU device placement failed due to runtime error \"unknown CPU\"\n",
      "\n",
      "Can't use assert statement inside tf.function\n",
      "\n",
      "Can not convert a TF2 saved model to a TensorRT engine and save it.\n",
      "\n",
      "Complete support for LSTM/GRU for TFLite\n",
      "\n",
      "toco_from_protos: not found - breaking\n",
      "\n",
      "Keras model pickle-able but tf.keras model not pickle-able\n",
      "\n",
      "Size and CombinedNMS Op support request for tflite\n",
      "\n",
      "keras Model with a dictionary as output: compile and fit expect layers names instead of the output dictionary's keys\n",
      "\n",
      "Keras scikit-learn wrapper not compatible with keras functional model\n",
      "\n",
      "Missing Operations for TfLite GPU Delegate\n",
      "\n",
      "GPU unit-test infrastructure breakage (`Dockerfile.gpu`, `gpu/run_py3_core.sh`) \n",
      "\n",
      "[tflite] Output difference for simple MobileNetV2 model\n",
      "\n",
      "Shape_refiner.cc documentation\n",
      "\n",
      "Support exporting the transform graph from tf.Transform together with the model graph defined using Keras to SavedModel in TensorFlow 2.0\n",
      "\n",
      "Build a static tensorflow on Alpine linux \n",
      "\n",
      " whether tflite can not support the two operator ( MaxPool2D and QUANTIZE) in TensorFlow 2.0.0 when running the mode in embed freeRTOS system?\n",
      "\n",
      "Documentation Error on nn.ctc_loss\n",
      "\n",
      "Tensorflow 2.0 does not iterate through entire dataset when tf.keras.model.fit is called\n",
      "\n",
      "Filter outputs for available estimators\n",
      "\n",
      "experimental_connect_to_cluster hangs\n",
      "\n",
      "undefined reference to `_imp__TF_Version' when compiling in C\n",
      "\n",
      "tf.keras fit is incompatible with tf.function\n",
      "\n",
      "TensorFlow Lite schema updater loses floating-point precision\n",
      "\n",
      "[TF2.1] Performance: Control flow and scalar ops 225x slower than raw Python and 24000x slower than C++\n",
      "\n",
      "tf.keras computes incorrect loss values with Masking\n",
      "\n",
      "ListWrapper does not support insert method for nested lists of layers\n",
      "\n",
      "Build failure: undefined reference to protobuf symbols #34117 closed issue still occurring on r2.1\n",
      "\n",
      "Add GPU delegate support to the TFLite experimental c api\n",
      "\n",
      "Model mismatch between create_low_latency_conv_model and cnn-one-fstride4\n",
      "\n",
      "Have the count mode passed to the callbacks\n",
      "\n",
      "How to limit the cpu core number in Tensorflow 2.0\n",
      "\n",
      "C++ compilation of rule '//tensorflow/cc:cc_op_gen_main' failed (Ex it 1)\n",
      "\n",
      "tensorflow.python.keras.testing_utils.layer_test breaks when a (custom) layer is returning a list/tuple of tensor \n",
      "\n",
      "Error converting universal sentence encoder to TFLite with new converter. Failed to find function '__inference_pruned_1633'\n",
      "\n",
      "TFLite SIGILL on Invoke()\n",
      "\n",
      "TPUStrategy broken in TF2 Keras\n",
      "\n",
      "tf.io.gfile.glob does not list all files in a Google Cloud Storage bucket\n",
      "\n",
      "Why tf.concat not support multiply types\n",
      "\n",
      "Error Massage: tensorflow.python.framework.errors_impl.UnknownError:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n",
      "\n",
      "ImageDataGenerator does not work with tpu\n",
      "\n",
      "TFLite allocate tensors fails: (CONCATENATION) failed to prepare after input shape resize\n",
      "\n",
      "[TF 2.0] Nested Gradient Tape - unconnected graphs\n",
      "\n",
      "AutoCastVariable.assign returns wrapped variable instead of casted version\n",
      "\n",
      "tf.signal.mfccs_from_log_mel_spectrograms caused internal error in converted TensorFlow Lite model\n",
      "\n",
      "Complex dtype support tflite converter\n",
      "\n",
      "Collective AllGather Fails to Collect Tensors in Multi-(TF)Task Between-Graph Distributed Execution\n",
      "\n",
      "Behaviour of the loss function with sample_weights (keras)\n",
      "\n",
      "Error on compiling from source\n",
      "\n",
      "Brackets in directory for tf.train.CheckpointManager in TF2.0\n",
      "\n",
      "C++ API producing incorrect model metaparams\n",
      "\n",
      "tf.function should trace the additional attributes of a tensor\n",
      "\n",
      "A custom RNN cell does not support a high-order state_size\n",
      "\n",
      "tf.image.non_max_suppression bug when iou_threshold=0.0\n",
      "\n",
      "Cannot convert models to the *.tflite format with GRU/GRUCells\n",
      "\n",
      "Check failed: dim_size >= 1 (0 vs. 1)\n",
      "\n",
      "Eager context seems contradicting the distributed TensorFlow concept\n",
      "\n",
      "STM32F7-Disco Hello World example fails to build\n",
      "\n",
      "build tf2.0 with gcc4.8.5\n",
      "\n",
      "keras.backend.function with learning phase gives AttributeError\n",
      "\n",
      "OutOfRangeError when training estimator with filter in the input_fn\n",
      "\n",
      "Problem with the Run API  in  C++\n",
      "\n",
      "TF2.1 build from source failure\n",
      "\n",
      "ValueError: Unknown layer: DenseFeatures for tensorflow2.0\n",
      "\n",
      "How to add custom ops to Tensorflow Lite Swift/ObjC ? \n",
      "\n",
      "Bug in LSTMBlockCellBpropWithCUDA when using peephole connections\n",
      "\n",
      "TensorRT Segmentation Fault During Conversion\n",
      "\n",
      "Failed to load delegate from libedgetpu.so.1.0\n",
      "\n",
      "Named dictionary inputs and outputs for tf.keras.Model\n",
      "\n",
      "DatasetVariantWrapper \"No unary variant device copy function found\"\n",
      "\n",
      "Cannot load saved model fitted via csv dataset\n",
      "\n",
      "When i try run the  `tf.keras.layers.Bidirectional` on my windows system, it turns out CancelledError!  \n",
      "\n",
      "[TF 2.0] Add batched gradient function to GradientTape\n",
      "\n",
      "Work group size selection in OpenGL\n",
      "\n",
      "ResourceApplyGradientDescent happens on CPU?\n",
      "\n",
      "tf.keras.model_to_estimator() fails with custom loss/custom metrics\n",
      "\n",
      "Support key mapping in TFRecord\n",
      "\n",
      "Please let load_weights load only weights as its name describes\n",
      "\n",
      "Issues converting recurrent ML-Agents model to TFLite (Tensorflow 1.15)\n",
      "\n",
      "TFLite 5D Tensors with the OpenCL Back-End\n",
      "\n",
      "Keras .fit() yields incorrect results when using a custom loss function\n",
      "\n",
      "Update `nogpu` tag references to be `no_gpu` instead?\n",
      "\n",
      "Errors when instantiating multiple tf.keras models on different threads concurrently\n",
      "\n",
      "Keras load weights fails to load model from directory containing [[\n",
      "\n",
      "Bazel build does not pick up correct compiler include paths\n",
      "\n",
      "Cannot use dictionary embeddings metadata for keras callbacks\n",
      "\n",
      "Bidirectional LSTM fail on TF2.0\n",
      "\n",
      "Insert Windows command line in the tutorials\n",
      "\n",
      "'Copy link to this section' feature not working for certain sections.\n",
      "\n",
      "Porting gradients to C++ from python\n",
      "\n",
      "TF Real Op Not Supported for TFLite when generating MFCCs\n",
      "\n",
      " Support sparse inputs for Embedding layer\n",
      "\n",
      "Build breaks: The value 'REPOSITORY_NAME' has been removed in favor of 'repository_name()', please use the latter (https://docs.bazel.build/versions/master/skylark/lib/native.html#repository_name).\n",
      "\n",
      "Fail to invoke tflite_runtime.interpreter\n",
      "\n",
      "C API: Session Options for prediction on GPU\n",
      "\n",
      "Optimizer state gets automatically restored when loading weights from checkpoint and doesnt change when you compile the model\n",
      "\n",
      "tf.image.ssim_multiscale does not work in tf-2.0.0\n",
      "\n",
      "Embedding Layer's mask operation with LSTM Layer Gives Wrong Results When Using a GPU\n",
      "\n",
      "Missing TF 2.0 low-level API guide\n",
      "\n",
      "Pull Requests: Map needed for request workflow\n",
      "\n",
      "Pull Requests: Status information should be available\n",
      "\n",
      "Pull Requests: Trusted committers should be able to approve\n",
      "\n",
      "[TF 2.0 API Docs] tf.keras.layers.simpleRNN\n",
      "\n",
      "tfp.sts.AutoregressiveStateSpaceModel noise mean should be settable\n",
      "\n",
      "What is the right way to use coverage.py with Tensorflow?\n",
      "\n",
      "Inclusion of a model re-training example\n",
      "\n",
      "Dense variable constraints are not allowed with sparse gradients\n",
      "\n",
      "Resize layers in model producing LookupError when computing gradients\n",
      "\n",
      "Pretrained Inception V4 to Keras Application Folder\n",
      "\n",
      "add a new class for realizing Early Stopping\n",
      "\n",
      "GradientTape: Allow to execute backward functions on same device as forward functions\n",
      "\n",
      "Unsure as to compile options for Intel i7 930, Bloomfield CPU for when rebuilding Tensorflow from source\n",
      "\n",
      "TensorFlow Lite Support for LSTM Models\n",
      "\n",
      "tflite:experimental:micro:riscv: the default build target is wrong.\n",
      "\n",
      "tf.keras.callbacks.ReduceLROnPlateau -  min_delta parameter should be percentage, not absolute  \n",
      "\n",
      "Allow Keras ModelCheckpoint to save the best N checkpoints\n",
      "\n",
      "Error: No OpKernel was registered to support Op 'NcclAllReduce'\n",
      "\n",
      "tf-lite 2.0 python\n",
      "\n",
      "massive c++14 warning during build\n",
      "\n",
      "Arduino compilation fail : error: macro \"max\" requires 2 arguments, but only 1 given  max(0.0f),\n",
      "\n",
      "OUT_OF_MEMORY error on Mali GPU.\n",
      "\n",
      "C binding for tensorflow 2.0\n",
      "\n",
      "about tf.gradients() API\n",
      "\n",
      "Fix behavior difference between tf.io.GFile and python file for utf8\n",
      "\n",
      "tf.compat.v1.wrap_function throws error when creating variable in tf.distribute scope\n",
      "\n",
      "Tracing of first batch does not collect compute events\n",
      "\n",
      "TimeseriesGenerator for labeled time-series such as sensor data\n",
      "\n",
      "Support for Mediapipe tflite ops.\n",
      "\n",
      "TF2.0 for compute capability 3.0\n",
      "\n",
      "Finish making strided_slice equivalent to numpy's strided_slice\n",
      "\n",
      "tf.lite operators missing for `tf.nn.ctc_loss`: `Empty`, `InplaceAdd` and `While`\n",
      "\n",
      "Using GPU for Tflite on Android\n",
      "\n",
      "TFLite-micro: AllocateTensors produces HardFault even for small models\n",
      "\n",
      "Undefined symbols for architecture arm64:   \"NewGpuDelegate(GpuDelegateOptions const*)\n",
      "\n",
      "internal compiler error: in assign_temp, at function.c:968\n",
      "\n",
      "tf2.0 takes more than twice memory than 1.14\n",
      "\n",
      "tfliteGpuDelegate Invoke write to buffer failed source data is larger than buffer\n",
      "\n",
      "Docker Image with a pre-built Tensorflow-gpu with a compute capability 3.0 GPUs\n",
      "\n",
      "[TF 2.0] Custom objects\n",
      "\n",
      "Cannot train canned estimators in multiple estimator.train() calls when using tf.keras.optimizers or tf.optimizers\n",
      "\n",
      "keras optimizer `apply_gradients` arg `grads_and_vars` has wrong type in documentation\n",
      "\n",
      "TF lite implementation for RandomStandardNormal\n",
      "\n",
      "tf.sparse.sparse_dense_matmul can't broadcast as tf.linalg.matmul does.\n",
      "\n",
      "With lazy_import tensorflow cannot be imported\n",
      "\n",
      "Fix discrepancy between tf.io.gfile.mkdir and os.mkdir's created mode\n",
      "\n",
      "tf_upgrade_v2 is not reporting issues related to invalid imports in TensorFlow 2\n",
      "\n",
      "Convert to tflite will change output order for models with multiple outputs\n",
      "\n",
      "An application using tensorflow_cc.dll crashes whenever the Tensorflow API is invoked.\n",
      "\n",
      "Keras does not verify supports_masking\n",
      "\n",
      "need some Ops in RNN\n",
      "\n",
      "redzone_checker appears a lot while profiling tf-2.0\n",
      "\n",
      "Embedding visualization in tf.keras / TensorFlow 2.0\n",
      "\n",
      "AutoGraph returns an empty array when I use a for-loop and TensorArray\n",
      "\n",
      "tf.compat.v1.keras.backend.get_session gives erroneous deprecation warning\n",
      "\n",
      "add expm_multiply functionality\n",
      "\n",
      "Warning on why compute devices aren't being used \n",
      "\n",
      "Tensorflow 2.0 - bazel build from source fails (can't exec cc1plus)\n",
      "\n",
      "Prebuilt libtensorflow (C API)\n",
      "\n",
      "EdgeTPU library is not updating for coral dev board \n",
      "\n",
      "optimize gfile for sequential reading\n",
      "\n",
      "Will DepthwiseConv2D support non-square Strides in the future? Ex: `strides=(1,3)`\n",
      "\n",
      "ERROR: C:/tensorflow/tensorflow/compiler/xla/service/gpu/BUILD:1314:1: C++ compilation of rule '//tensorflow/compiler/xla/service/gpu:cudnn_fused_conv_rewriter' failed (Exit 2): python.exe failed: error executing command\n",
      "\n",
      "Cannot link to built tensorflow_cc.lib with Visual C++ -- undefined symbols; r2.0\n",
      "\n",
      "Automatic gradient for numpy functions/external functions\n",
      "\n",
      "TensorFlow 2.x Java/Maven release\n",
      "\n",
      "Documentation for streaming training data from disk.\n",
      "\n",
      "BatchMatMul for TFLite\n",
      "\n",
      "How to explitly save the values of every buffer in a tflite model?\n",
      "\n",
      "Tensorflow 2.0 custom_scalar plugin for tensorboard\n",
      "\n",
      "model.fit with tf.data.Dataset.from_generator can't infer shape\n",
      "\n",
      "Let us decide the index of the blank in tf.nn.ctc_greedy_decoder \n",
      "\n",
      "How to get cublas handle to run cublas function?\n",
      "\n",
      "The use of tflite Model of C3D Network in Android \n",
      "\n",
      "RPi0W - no go\n",
      "\n",
      "Re-emerged Issue #31509 - BaseCollectiveExecutor::StartAbort Out of range:\n",
      "\n",
      "TF 2.0 Feature: Flops calculation\n",
      "\n",
      "Inconsistency between keras model.predict() and model.call()\n",
      "\n",
      "When does tensorflow lite support 3d cnn?\n",
      "\n",
      "device_lib.list_local_devices() InvalidArgumentError: Invalid device ordinal value (1). Valid range is [0, 0].\n",
      "\n",
      "A question about tensorflow lite\n",
      "\n",
      "ValueError when passing tensors to keras subclass model calls\n",
      "\n",
      " keras.metrics.Accuracy != keras.metrics.accuracy\n",
      "\n",
      "TensorFlow Lite save  from_session error\n",
      "\n",
      "Converting unsupported operation ??\n",
      "\n",
      " Support for 32 bits architecture #32315 \n",
      "\n",
      "Use tf.Tensor as input to tf.io.gfile.GFile\n",
      "\n",
      "python kernel restart when training xgboost estimator\n",
      "\n",
      "Bazel workspace.bzl requires a dependency on rules_closure\n",
      "\n",
      "[autograph List comprehension issue] OperatorNotAllowedInGraphError error in Tensorflow 2.0\n",
      "\n",
      "Improving compatibility with Scikit-learn\n",
      "\n",
      "[r2.0.0-rc1] Converting to TFLite format: Invalid quantization params for op RESHAPE\n",
      "\n",
      "SIGSEGV\n",
      "\n",
      "TF Micro requires CONV_2D version '2' when applying quantization\n",
      "\n",
      "[TF 2.0] Feature request: let autograph accept collection inputs\n",
      "\n",
      "Calling tf.function from tf.py_function in dataset.map hangs.\n",
      "\n",
      "Cannot cross-compile ARM64 on Mac host\n",
      "\n",
      "Int32 overflow in sparse_reshape on Windows\n",
      "\n",
      "Possible issue in tf.scatter_nd documentation.\n",
      "\n",
      "Keras 'fit_generator' - Zip object is not considered generator or sequence in \n",
      "\n",
      "[lite/micro] Huge size of libtensorflow-microlite.a for arm\n",
      "\n",
      "Build fails on latest master with clang and CUDA\n",
      "\n",
      "compute_mask not called when custom layer does not alter inputs and mask is not None\n",
      "\n",
      "Support for 32 bits architecture\n",
      "\n",
      "TPU with tensorflow 2.0 -- 'DeleteIterator' OpKernel missing\n",
      "\n",
      "Please modify cuda-bin in cuda_configure.bzl to copy just needed files instead of directory\n",
      "\n",
      "SpatialDropout2D\n",
      "\n",
      "Keras to tflite. Multiple outputs all given name Identity\n",
      "\n",
      "Mirror Strategy slow down by adding GPUs\n",
      "\n",
      "Custom loss with extra argument in TF 2.0\n",
      "\n",
      "Cannot seek on write only tf.gfile.GFile\n",
      "\n",
      "tf2.0.0-rc0: tensor shape check failed when using tf.function in tf.distribute.Strategy scope.\n",
      "\n",
      "Initial read on nonexistent tf.gfile.GFile in w+ mode crashes\n",
      "\n",
      "Documentation for ./configure environment variables\n",
      "\n",
      "[TF 2.0] tf.assert_equal([], [1.0]) doesn't raise error\n",
      "\n",
      "Linear RAM memory increase with Dataset and Estimator with epoch loops\n",
      "\n",
      "How to create Python extension module that uses TensorFlow C API?\n",
      "\n",
      "please add tensor factorization\n",
      "\n",
      "Enabling model training on Android \n",
      "\n",
      "TF_CPP_MIN_LOG_LEVEL does not work with TF2.0 dev20190820\n",
      "\n",
      "Tensorflow lite: micro_vision missing model data\n",
      "\n",
      "Tensorflow2.0 distributed training gives error :- A non-DistributedValues value 8 cannot be reduced with the given reduce op ReduceOp.SUM.\n",
      "\n",
      "Using libtensorflow-core.a to enable training on Android \n",
      "\n",
      " TensorFlow Lite GPU on Android slower than CPU\n",
      "\n",
      "Change all 'hashlib.md5' to 'hashlib.sha1'\n",
      "\n",
      "FIPS enable computers fail due to md5 use\n",
      "\n",
      "random crop edge case\n",
      "\n",
      "Guide is needed for transform of models suitable for tensorflow lite with gpu delegate\n",
      "\n",
      "delete and append to Tfrecords: Tensorflow\n",
      "\n",
      "Make ConcreteFunction Compatible with Pickle\n",
      "\n",
      "Only the first 30 ops will run on the GPU, and the remaining 1 on the CPU\n",
      "\n",
      "Tensorflow 2.0 is much slower than pytorch for large matrix assignment\n",
      "\n",
      "[issue]Pb convert to tflite :how to calculate output min max of concatenation layer\n",
      "\n",
      "[TF 2.0] allow tf.function input_signature to be specified by annotations\n",
      "\n",
      "TensorFlow Lite BroadcastTo\n",
      "\n",
      "tf.shape() for RaggedTensor is raising an exception\n",
      "\n",
      "Interrelations of collections and scope counts are not clear from documentation\n",
      "\n",
      "Ploting Gradients to Tensorboard and Console\n",
      "\n",
      "saved_model_cli convert hub module to tensorRT model\n",
      "\n",
      "Can't translate saved model to MLIR\n",
      "\n",
      "Adding speech output for tensorflowlite apps\n",
      "\n",
      "python API debugging best practices\n",
      "\n",
      "tensorflow 1.14 build failed \n",
      "\n",
      "SIGABRT on `tf.image.encode_png` with empty tensor\n",
      "\n",
      "DeprecationWarning: the imp module is deprecated\n",
      "\n",
      "TensorflowLite Runtime Installation Doesn't provide Interpreter Package\n",
      "\n",
      "Tensorflow library query API\n",
      "\n",
      "Change `kernel_initializer` in some `tf.keras` layers for improved performance\n",
      "\n",
      "[TF 2.0 API Docs] tf.tuple\n",
      "\n",
      "[TF.2.0 API Docs] tf.train.list_variables\n",
      "\n",
      "[TF 2.0 API Docs] tf.io.extract_jpeg\n",
      "\n",
      "google.protobuf.json_format.MessageToDict incorrectly decodes bytes_list\n",
      "\n",
      "Branch r2.0 Failed Source Build on macOS Mojave\n",
      "\n",
      "Make TF-TRT  input and output  semantic same as  TF \n",
      "\n",
      "Incorrect predictions of Mobilenet_V2\n",
      "\n",
      "Can not load tflite model\n",
      "\n",
      "un-deprecate `write_grads` for `fit`\n",
      "\n",
      "404 Error during oxford-pets tutorial dataset loading\n",
      "\n",
      "Feature Request: Capability to fix the seed for all keras kernel initializers\n",
      "\n",
      "Can't set tf.keras.backend.variable(trainable=False)\n",
      "\n",
      "tensorflow polyfill operation\n",
      "\n",
      "caching cuDNN CNN kernel choices\n",
      "\n",
      "Can't set sprite in Keras Tensorboard Callback\n",
      "\n",
      "TensorRT Slowdown Native->FP32 and FP16->INT8; File Size Increase\n",
      "\n",
      "Name Scope Automatic Handling According to the Model Architectures blocks\n",
      "\n",
      "Sample weights being expanded to match y_true/y_pred before they're sliced using a class_id in keras metrics.\n",
      "\n",
      "does pip wheels support hdfs on default?\n",
      "\n",
      "Segfault in custom op with TensorFlow 1.14, Python 3.6 and GCC 5.\n",
      "\n",
      "Potential bugs found with static analysis\n",
      "\n",
      "TF-Lite micro low performance\n",
      "\n",
      "[TF 2.0 nightly] tf.keras.estimator.model_to_estimator with strategy=tf.distribute.MirroredStrategy() -> Method requires being in cross-replica context, use get_replica_context().merge_call()\n",
      "\n",
      "TensorFlow C API Nightly URLs\n",
      "\n",
      "CPU.ModifyGraphWithDelegate is disallowed when graph is immutable. Delegate should run on the same thread where it was initialized.Node number 64 (TfLiteGpuDelegate) failed to invoke.ed to invoke.\n",
      "\n",
      "Standard way of sharing architectures?\n",
      "\n",
      "ffmpeg not found when build in Raspberry Pi 3 B+ Raspbian 10\n",
      "\n",
      "Can not Exporting a GraphDef from file\n",
      "\n",
      " mismatch in the description of BatchDot and TensorFlow's implementation. \n",
      "\n",
      "Tensorflow 2.0 - Build From Source not creating Python Wrappers such as _pywrap_tensorflow_internal.lib\n",
      "\n",
      "Feedback on Tensorflow 2.0 beta - documentation issues?\n",
      "\n",
      "Memory Leak in `tf.estimator.experimental.InMemoryEvaluatorHook`\n",
      "\n",
      "fit_generator with use_multiprocessing=True causes memory usage grow (leakage) on TF 1.13+\n",
      "\n",
      "Unable to find implementation of Select TensorFlow operators\n",
      "\n",
      "2019-07-17 12:43:20.046568: F tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:180] An array, resize/ResizeNearestNeighbor, still does not have a known data type after all graph transformations have run.\n",
      "\n",
      "[TF 2.0] Can not compile model more than once without running out of memory\n",
      "\n",
      "Cryptic error message when assigning to a variable in a tf.function\n",
      "\n",
      "MAXIMUM and RESIZE_NEAREST_NEIGHBOR not available for tensorflow-lite-gpu\n",
      "\n",
      "why GetLocalGPUInfo() using cuda runtime api, I guess it should use cudart_stub instead.\n",
      "\n",
      "Unable to find documentation for framework/ common run time files\n",
      "\n",
      "TOCO unable to convert unsupported operation using --allow_custom_ops\n",
      "\n",
      "Low GPU usage of RNN layers under MirroredStrategy\n",
      "\n",
      "Duplicate variables for models whose layers share weights\n",
      "\n",
      "TPUEstimator function requires `train_batch_size` to be set when `use_tpu` is True\n",
      "\n",
      "About Quantization Full Integer: Test with Yolo Tiny v2\n",
      "\n",
      "[Mirrored Strategy] You dataset iterator ran out of data; interrupting training. \n",
      "\n",
      "VS linkage fails with \"unresolved external symbol\"\n",
      "\n",
      "MSVC compiler gives warning C4190 when include Tensorflow header\n",
      "\n",
      "Argument to force CuDNN implementation for tf.keras.layers.LSTM\n",
      "\n",
      "How can i make sure the hash is consistent when saving the same model?\n",
      "\n",
      "Custom OP Documentation: `-D_GLIBCXX_USE_CXX11_ABI=0` not required anymore?\n",
      "\n",
      "Hwloc mirror - download issue\n",
      "\n",
      "TensorflowLite model for On-Device Speech Recognizer\n",
      "\n",
      "tensorflow 2.0 variable slice assign_add not supported\n",
      "\n",
      "ctc_* api support custom blank_index\n",
      "\n",
      "Unable to build micro_speech for bluepill target (region `FLASH' overflowed by 9656 bytes)\n",
      "\n",
      "Memory Saving Gradients for TF2\n",
      "\n",
      "Build doesn't find files that exist\n",
      "\n",
      "Bad inference on mobilenet quantized graph\n",
      "\n",
      "get_variable style tf.Variable initialization\n",
      "\n",
      "Compiling custom GPU operation on Microsoft Windows 10\n",
      "\n",
      "[Question] Adding new hardware device support\n",
      "\n",
      "third_party/repo.bzl file silently failed to apply patch\n",
      "\n",
      "Need better documentation for BestExporter\n",
      "\n",
      "tf.keras.experimental.export_saved_model in multi-gpu mode doesn't work\n",
      "\n",
      "The shared library of C++ API lacks of operation symbols on windows.\n",
      "\n",
      "`xla.compile` + `tf.function` lose information about compile-time constants\n",
      "\n",
      "Tensorflow Lite conversion misshapes bias vector of FullyConnected\n",
      "\n",
      "tensorflow:libtensorflow_cc.so fails to build for --cpu=armeabi-v7a\n",
      "\n",
      "Unexpected `tf.cast` behavior between signed and unsigned integers\n",
      "\n",
      "error C2280: 'tensorflow::FunctionLibraryDefinition &tensorflow::FunctionLibraryDefinition::operator =(const tensorflow::FunctionLibraryDefinition &)': attempting to reference a deleted function\n",
      "\n",
      "make tf.image ops support tf.float16 in tensorflow 2.0\n",
      "\n",
      "Getting `bad_alloc` in allocateTensors \n",
      "\n",
      "How to write the  .bmp images from tensors to disk after resizing ?\n",
      "\n",
      "TFLite GPU works slower than CPU\n",
      "\n",
      "tf.keras.BatchNormalization ignores Mask when calculating Mean/Variance\n",
      "\n",
      "About `Evaluator` in TF_CONFIG\n",
      "\n",
      "CPU memory is not released after calling sess.run and closing session\n",
      "\n",
      "TypeError: Generator object is not an iterator\n",
      "\n",
      "Multiple duplicate tflite android example projects\n",
      "\n",
      "Can build tflite with xcode ?\n",
      "\n",
      "Support Sparse Tensors in py_function\n",
      "\n",
      "Tensorflow Bitcast - float32 to int32 to float32?\n",
      "\n",
      "input_fn recall hook for estimator api\n",
      "\n",
      "ParseSequenceExample should return Ragged Tensor or have an option for this\n",
      "\n",
      "FlexAudioSpectrogram ops is not supported by the tflite interpreter\n",
      "\n",
      "Many context switches / Many threads even if threading is limited\n",
      "\n",
      "build for Windows always fail\n",
      "\n",
      "Android performance: CPU affinity\n",
      "\n",
      "\"Node 'Const' is not unique\" Using C++ api on windows to freeze graph\n",
      "\n",
      "Keras Colab TPU Error when compiling and fitting a pre-trained model in 1.14\n",
      "\n",
      "[TF 2.0 API Docs] tf.keras.backend.count_params\n",
      "\n",
      "TensorFlow Lite Op Request\n",
      "\n",
      "TFRecord guide doesn't show how to serialize and parse tensors\n",
      "\n",
      "I think Windows performance might be poor due to WDDM? tensorflow-gpu\n",
      "\n",
      "[TF 2.0 API Docs] tf.errors.DeadlineExceededError\n",
      "\n",
      "[TF 2.0 API Docs] tf.dynamic_stitch\n",
      "\n",
      "[TF 2.0 API Docs] https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/MaxPool1D\n",
      "\n",
      "[TF 2.0 API Docs] tf.dynamic_partition\n",
      "\n",
      "[TF 2.0 API Docs] tf.data.experimental.rejection_resample\n",
      "\n",
      "[TF 2.0 API Docs] tf.debugging.assert_type\n",
      "\n",
      "[TF 2.0 API Docs] tf.data.experimental.prefetch_to_device\n",
      "\n",
      "[TF 2.0 API Docs] tf.errors.ResourceExhaustedError\n",
      "\n",
      "Bigger Than Memory ops should automatically fallback to RAM and/or Disc in tf-gpu\n",
      "\n",
      "GPU Support - Shared Object Exclusion \n",
      "\n",
      "ImportError: Could not find 'cudart64_.dll'.\n",
      "\n",
      "Segmentation fault using tf.lite.TFLiteConverter with representative_dataset\n",
      "\n",
      "[TF 2.0 API Docs] tf.estimator.SessionRunHook\n",
      "\n",
      "FeatureColumn - implement many-to-one categorical column\n",
      "\n",
      "Inconsistency between names in the conv 1d operations and no support for 'causal' padding\n",
      "\n",
      "Keras Callbacks documentation for on_train_batch_end vs the actual code of ModelCheckpoint\n",
      "\n",
      "Runtime parameters passed to input_fn for tf.estimator \n",
      "\n",
      "[TF 2.0] tf.hessians\n",
      "\n",
      "Improve interface for warmstarting non-TRAINABLE variables in warm_starting_util\n",
      "\n",
      "Undefined symbol: DeleteGpuDelegate(_TfLiteDelegate*)\n",
      "\n",
      "Object Detection API: Unable to train a quantization aware Faster RCNN + Resnet50 object detector\n",
      "\n",
      "Export FeatureColumn and subclasses\n",
      "\n",
      "Tensorflow 2.0 for raspberry pi installation\n",
      "\n",
      "ERROR: Using multiple TPUs in a single session is not yet implemented\n",
      "\n",
      "Tflite results dont match corresponding Tensorflow for Mobilenet.\n",
      "\n",
      "[Find header file after build manually tensor flow lite]\n",
      "\n",
      " [TF 2.0 API Docs] tf.keras.constraints.MinMaxNorm\n",
      "\n",
      "Build tensorflow c library for mips64.\n",
      "\n",
      "[TF 2.0 API Docs] tf.data.experimental.Counter\n",
      "\n",
      "[TF 2.0 API Docs] tf.io.write_file\n",
      "\n",
      "[TF 2.0 API Docs] tf.data.experimental\n",
      "\n",
      "[TF 2.0 API Docs] tf.io.FixedLenSequenceFeature\n",
      "\n",
      "[TF 2.0 API Docs]  tf.io.read_file\n",
      "\n",
      "[TF 2.0 API Docs] tf.io.FixedLenFeature\n",
      "\n",
      "SetShapeFn gets nullptr for custom operator leading to crash\n",
      "\n",
      "[XLA]make xlaop optimization algorithm configurable\n",
      "\n",
      "Allow growth seems to take more memory than needed\n",
      "\n",
      "Tensor flow lite stable release for Linux on ARM 64 bit\n",
      "\n",
      "Switching default model with tf.keras\n",
      "\n",
      "Get global batch size for MirroredStrategy when using bucket_by_sequence_length\n",
      "\n",
      "Post Training Quantization slower in latest tf-nightly vs tf 1.10\n",
      "\n",
      "Inconsistency in Input Pipeline code block for test_dataset\n",
      "\n",
      "Different variable names in keras layers depending on how layer is used\n",
      "\n",
      "Initialize Layer with weights loaded from files\n",
      "\n",
      "Docs should describe how to add a MetaGraphDef to an existing graph\n",
      "\n",
      "Support RaggedTensor in table lookups\n",
      "\n",
      "[TF 2.0 API Docs] tf.io.decode_image\n",
      "\n",
      "TFLite GPU Delegates: Reshape fails to reshape input matrix with batch >1\n",
      "\n",
      "Crash in sess.run on CPU after restoring model from checkpoint.\n",
      "\n",
      "Pix2Pix tutorial BatchNorm issue\n",
      "\n",
      "UnicodeDecodeError: 'utf8' codec can't decode byte 0x80 in position 24: invalid start byte when using TensorBoardDebugWrapperSession\n",
      "\n",
      "[TF 2.0 API Docs] tf.data.experimental.scan\n",
      "\n",
      "[TF 2.0 API Docs] tf.io.decode_base64\n",
      "\n",
      "[TF 2.0 API Docs] tf.io.decode_json_example\n",
      "\n",
      "TF (v1.9+) unable to call subprocess when making a tf.data.dataset during training in an estimator\n",
      "\n",
      "Please provide the list of supported hardware architectures (and/or build options) for the pre-built library\n",
      "\n",
      "[TF 2.0 API Docs] tf.data.Dataset and tf.data.experimental.CsvDataset list_files\n",
      "\n",
      "[TF 2.0 API Docs] tf.identity_n\n",
      "\n",
      "GFile gcs_file_system crashes with bad header\n",
      "\n",
      "[TF 2.0 API Docs] tf.data.experimental.Reducer\n",
      "\n",
      "[TF 2.0 API Docs] tf.nn.convolution\n",
      "\n",
      "tf.data.Options\n",
      "\n",
      "[TF_2.0_API_docs] tf.custom_gradient\n",
      "\n",
      "[TF 2.0 API Docs] tf.VariableSynchronization\n",
      "\n",
      "[TF 2.0 API Docs] tf.hessians\n",
      "\n",
      "[TF 2.0 API Docs] tf.nn.l2_loss\n",
      "\n",
      "[TF 2.0 API Docs] tf.guarantee_const\n",
      "\n",
      "[TF 2.0 API Docs] tf.VariableAggregation\n",
      "\n",
      "[TF 2.0 API Docs] tf.io.extract_jpeg_shape\n",
      "\n",
      "Node.js (JavaScript) TensorFlow Lite Wrapper API\n",
      "\n",
      "Memory leak using C_API\n",
      "\n",
      "[TF 2.0 API Docs] tf.queue.FIFOQueue\n",
      "\n",
      "Deep Learning VM  train failed\n",
      "\n",
      "[2.0] SparseTensor shape becomes none after AutoGraph\n",
      "\n",
      "Unicode op tests fail on s390x with \"Could not create converter for input encoding: SHIFT-JIS\" error\n",
      "\n",
      "TFLite GPU supported ops not working\n",
      "\n",
      "Support RaggedTensors in sequence feature columns \n",
      "\n",
      "Error trying tensorflow litem operations are not supported by GPU delegate\n",
      "\n",
      "Documenting the Ref keyword from operations input/output types\n",
      "\n",
      "What does GradientTape.gradient(target, ...) when target is a list of tensors and not a single tensor ?\n",
      "\n",
      "CMake build fails v1.13.1 fatal error: tensorflow/stream_executor/dnn.pb.h: No such file or directory  #include \"tensorflow/stream_executor/dnn.pb.h\"\n",
      "\n",
      "Support the __cuda_array_interface__ protocol\n",
      "\n",
      "Group convolutions UnimplementedError for nightly build\n",
      "\n",
      "[Translation] Translation for Korean\n",
      "\n",
      "Adding multiclass support to BoostedTreesClassifier\n",
      "\n",
      "Tensorflow lite elementwise operation not working in gpu delegate (ADD & SUB)\n",
      "\n",
      "Eager execution in tf.data map_func\n",
      "\n",
      "C++17 features used even though C++11 standard explicitly given\n",
      "\n",
      "Optimize transpose ops\n",
      "\n",
      "libtensorflowlite.so crash when load model, just crash on SessionOption destructor\n",
      "\n",
      "INTERNAL ERROR reported while trying to apply GpuDelegate to tflite\n",
      "\n",
      "Build Tensorflow Lite from source on Yocto Linux\n",
      "\n",
      "Error loading a TensorRT optimised graph\n",
      "\n",
      "error: default initialization of an object of const type 'const Subgraph::Identity' without a user-provided default constructor\n",
      "\n",
      "Signature defs lost when calling saved_model_cli convert\n",
      "\n",
      "ExtractImagePatches Op Request\n",
      "\n",
      "catch keyboard interrupt in tf.function\n",
      "\n",
      "TF Lite select tf ops example linker error: libtensorflow-lite.a missing tflite subgraph\n",
      "\n",
      "TF Lite can't run example project of select tensorflow operators on iOS\n",
      "\n",
      "TensorFlow 1.14 changes Keras callback order relative to model build\n",
      "\n",
      "[tensorflow/docs]Translate variables.md from English to Chinese.\n",
      "\n",
      "TFLite only slightly faster with GPU on Helio P22\n",
      "\n",
      "Assigning to ressource variables with varying shapes violates the shape information of the tensor of which is assigned from.\n",
      "\n",
      "use GradientTape to compute gradient cost all memory\n",
      "\n",
      "tensorflowjs_converter install fails due to tf-nightly-2.0-preview missing from Python 3.7\n",
      "\n",
      "defun + random + addition = explode\n",
      "\n",
      "How to build Tensorflow C++ API with Visual Studio 2017\n",
      "\n",
      "Unsupported Operation (MEAN) while trying to apply GpuDelegate to tflite \n",
      "\n",
      "meta_optimizer.cc layout failed on frozen_graph\n",
      "\n",
      "Request for public APIs or alternatives: TF-Addons\n",
      "\n",
      "TFLite GPU delegate produces very different results\n",
      "\n",
      "tfdbg and tf.data API:  Inconsistency with handling dataset iterators' OutOfRangeError\n",
      "\n",
      "LSTM vs Conv2D, tf-nightly-gpu (CUDNN_STATUS_INTERNAL_ERROR)\n",
      "\n",
      "cyclic graph is not supported?\n",
      "\n",
      "TF_LoadLibrary in c++ when loading resampler.so\n",
      "\n",
      "TFLite: please don't force copy outputs\n",
      "\n",
      "map_fn() missing GPU implementation\n",
      "\n",
      "Return better error message when using RefVariables with v2 control flow\n",
      "\n",
      "TensorFlow Lite conversion of frozen graph error\n",
      "\n",
      "Why we cannot define two networks in the same graph when they are trained independently\n",
      "\n",
      "[C API] support for CudnnCompatibleLSTMCell in native code\n",
      "\n",
      "How to build static libtensorflow_cc.a and libtensorflow_framework.a for Linux and MacOS\n",
      "\n",
      "[C API] cond op\n",
      "\n",
      "Segmentation fault in  tflite::ops::builtin::BuiltinOpResolver when using libtensorflowlite.so via CMake\n",
      "\n",
      "Random Uniform Not Supported for TFlite Conversion\n",
      "\n",
      "3D CNN with tf.nn.conv3d_transpose on CPU: 50-100 GB RAM free but segfault + \"terminate called after throwing an instance of 'std::bad_alloc' what(): std::bad_alloc\"\n",
      "\n",
      "tf.hessians fails when applied to the output of a RNN\n",
      "\n",
      "Errors in tensorflow/stream_executor/gpu/gpu_timer.cc while compiling on Windows with XLA enabled\n",
      "\n",
      "Graph transform tool  Node Quantization gives error \n",
      "\n",
      "Add `saving_listeners` arg to `TrainSpec`\n",
      "\n",
      "CPU support for dilation rates larger than 1\n",
      "\n",
      "[TF 2.0 API Docs] tf.audio.encode_wav\n",
      "\n",
      "[TF 2.0 API Docs] tf.audio.decode_wav\n",
      "\n",
      "[TF 2.0 API Docs] tf.audio\n",
      "\n",
      "[tf.keras.layers.ReLU] Layer returns 0 if threshold is negative and max_value is set\n",
      "\n",
      "matmul gives different results for tensor based on shape\n",
      "\n",
      "The Nadam optimizer is non-deterministic due to a race condition related to inter_op_parallelism_threads\n",
      "\n",
      "[C API] while loop: unable to access operations defined outside of the loop from within the loop\n",
      "\n",
      "PhasedLSTMCell only accepts 1-dimensional time input\n",
      "\n",
      "TPU + Keras doesn't support multiple input layers\n",
      "\n",
      "[TF2.0] Bug or Feature? Keras model.summary() does not provide useful information for nested models\n",
      "\n",
      "[TF 2.0] Tf.keras.layers.Lambda does not fail if it does not support masking, but mask is passed\n",
      "\n",
      "Error running TFLite Quantized model on NNAPI\n",
      "\n",
      "matmul for RaggedTensors / tf.ragged.ragged_dense_matmul\n",
      "\n",
      "libtensorflowlite.so usage is not documented\n",
      "\n",
      "TensorFlow 2.0: Allow simple tensorboard summary usage\n",
      "\n",
      "[TF 2.0] Inconsistency when timing operations\n",
      "\n",
      "keras ModelCheckpoint support period save by time or step, not only epoch.\n",
      "\n",
      "Failed to get device properties, error code: 30\n",
      "\n",
      "Added an argument to control the padding for flatten_atrous\n",
      "\n",
      "New weight initialisation when using dropout and ReLU\n",
      "\n",
      "first_bn/FusedBatchNorm_mul_0, which is an input to the Add operator producing the output array kws_model/KWS_Model/tower_0/CNN_V1/Relu, is lacking min/max data, which is necessary for quantization. Either target a non-quantized output format, or change the input graph to contain min/max information, or pass --default_ranges_min= and --default_ranges_max= if you do not care about the accuracy of results.\\nAborted (core dumped)\"\n",
      "\n",
      "I understand that java should be faster than Python. But not like this.\n",
      "\n",
      "increase throughput for `ResizeBilinearKernel`\n",
      "\n",
      "Array conv_23/Conv_1/BatchNorm/FusedBatchNorm, which is an input to the Conv operator producing the output array conv_3//0/Conv/Relu6, is lacking min/max data, which is necessary for quantization. \n",
      "\n",
      "Estimator from Keras Model fails to learn when original keras Model learns fine\n",
      "\n",
      "[feature request]check whether a data iterator is initialized\n",
      "\n",
      "Want to optimize Tensorflow CUDA kernels\n",
      "\n",
      "Pylint checks from ci_sanity.sh `--incremental`\n",
      "\n",
      "ResizeBilinear FP16 CUDA Kernel Support\n",
      "\n",
      "Can't easily write custom C++ ops which update resource variables\n",
      "\n",
      "Changing the tf_random_seed (RunConfig) doesn't change results/parameters for LinearClassifier\n",
      "\n",
      "quantize_graph incompatible shapes\n",
      "\n",
      "Add built-in helper functions for _bytes_feature, _float_feature and _int64_feature from \"Using TFRecords and tf.Example\" page\n",
      "\n",
      "Eager: Network with eager execution is not learning and low performance\n",
      "\n",
      "tflite support for Softplus (exp and log are already supported)\n",
      "\n",
      "Fail to use tf.transpose() after tf.nn.embedding_lookup() and tf.layers.conv1d() while build a tflite file\n",
      "\n",
      "KeyError: 'ExperimentalFunctionBufferingResource' in Tf >= 1.13 (mkl)\n",
      "\n",
      "negative pixel values for ssim input?\n",
      "\n",
      "Object detection API config file/protobuf errors\n",
      "\n",
      "Failed to build a debug version of tensorflow from scratch\n",
      "\n",
      "Implementation of class weights with tf.nn.softmax_cross_entropy_with_logits_v2\n",
      "\n",
      "Support large embeddings with `MirroredStrategy` and `MultiWorkerMirroredStrategy`\n",
      "\n",
      "Support synchronous training with parameter servers using Distribution Strategies\n",
      "\n",
      "Support model parallelism in tf.distribute.Strategy\n",
      "\n",
      "When using long time inference TFLite GPU not working in mobile (demo app)\n",
      "\n",
      "axis parameter in tf.random_shuffle\n",
      "\n",
      "nan appearing on False fork of tf.where propgates to e\n",
      "\n",
      "Build from sources fails with undefined symbol _ZN3Aws10FileSystem16GetHomeDirectoryB5cxx11Ev\n",
      "\n",
      "Not found: No registered '_FusedMatMul' OpKernel for CPU devices compatible with node\n",
      "\n",
      "Add name=None argument to roll function\n",
      "\n",
      "saved_model_cli fails when --input_examples option contains string feature\n",
      "\n",
      "Kubernetes Charts for Tensorflow 2.0\n",
      "\n",
      "Training on device with TensorFlow Lite\n",
      "\n",
      "Java: add support for Alpine Linux (Docker)\n",
      "\n",
      "TF Lite conversion of minimal graph with tf.matmul fails on Linux but works on MacOS\n",
      "\n",
      "Huawei (android 6.0，OpenGL3.1 is Mali) GPU slower than CPU\n",
      "\n",
      "'flatten_atrous_conv'  misses to flatten some atrous_convs\n",
      "\n",
      "No improvement in performance of deeplabv3_257_mv_gpu.tflite on TFLite for GPU\n",
      "\n",
      "Poincare hyperbolic embeddings support\n",
      "\n",
      "tf.keras lambda layer with sparse tensor caused AttributeError: 'SparseTensor' object has no attribute 'tocoo'\n",
      "\n",
      "[TF 2.0]  tf.estimator.ProfilerHook... is not compatible with eager execution\n",
      "\n",
      "tf.add_n is inaccurate for float16\n",
      "\n",
      "Unexpected UnicodeDecodeError: invalid continuation byte when reading lines from a file\n",
      "\n",
      "Custom Object Detection crashes when convert_to_grayscale is enabled.\n",
      "\n",
      "TF2.0 / `SequenceFeatures` doesn't have a `_is_feature_layer` property.\n",
      "\n",
      "Bug in tf.parallel_stack\n",
      "\n",
      "segmentation fault using tensorflow debugger\n",
      "\n",
      "A result calculated inside of the tf.while_loop is wrong and not raising any error/alert when it occurred by GPU OOM.\n",
      "\n",
      "[Grappler] RemoveIdentityTranspose also removes conjugate\n",
      "\n",
      "Use of tf.custom_gradient prevents garbage collection of Graph\n",
      "\n",
      "Non-OK-status for CudaLaunchKernel when torch is also imported\n",
      "\n",
      "Workaround for self-attention using tf.matmul\n",
      "\n",
      "[Enhancement] Automatically chose either memory or storage to cache in tf.data API\n",
      "\n",
      "Failing on a in tensorflow_cc.so on Windows 7 on Quadro R5000 16Gb with v1.12 and CUDA 10.0.130 and CUDNN 7.4.2.24 OK under Windows 10 Quadro P5000 and GTX 1060 6Gb\n",
      "\n",
      "LSTM quantization aware training and fully quanitzation in TfLite\n",
      "\n",
      "Reading a tensor from file in python which was saved using C++\n",
      "\n",
      "transpose() can be very slow on CPU\n",
      "\n",
      "Generating fully-quantized models in Pre-trained checkpoints (.chkpt) or GraphDef (.pb)  format\n",
      "\n",
      "[Feature Request] Support Sparse Tensors in tf.linalg operations\n",
      "\n",
      "Dataset c++ extend documentation is outdated for tf 2.0 & DatasetV2\n",
      "\n",
      "[tflite]:operator of type Floor for which the quantized form is not yet implemented\n",
      "\n",
      "import results in Recursion-Error\n",
      "\n",
      "Need tf.signal.rfft op in TFLite\n",
      "\n",
      "AttributeError: 'KerasTPUModel' object has no attribute '_distribution_strategy'\n",
      "\n",
      "Test case reports an exception to stdout, with stack trace, when testing under self.assertRaises()\n",
      "\n",
      "Depthwise separable conv with floating point 16 and batch norm is too slow.\n",
      "\n",
      " Error polling for event status: failed to query event: CUDA_ERROR_UNKNOWN during training process\n",
      "\n",
      "Incorrect flops calculation for operations with complex numbers\n",
      "\n",
      "r1.13 multi-gpu towering fails due to DeviceSpec parsing during model creation\n",
      "\n",
      "Build Issue on windows - cd command does not change drive without /d flag\n",
      "\n",
      "tf.data input pipeline got strange bug with tensorflow-gpu\n",
      "\n",
      "TFLite GPU delegate returns only zeros for part of the outputs\n",
      "\n",
      "Convert Unsupported Operations\n",
      "\n",
      "No way to use string type with Lite C APIs.\n",
      "\n",
      "[TF 2.0 alpha] Keras Callbacks\n",
      "\n",
      "Non-uniform FFT layer\n",
      "\n",
      "Keras support for RaggedTensors\n",
      "\n",
      "std::terminate may be called when environment is misconfigured while using C API\n",
      "\n",
      "Documentation Request: WALSModel with shard\n",
      "\n",
      "TFLite MicroInterpreter Crashes/Seg Faults\n",
      "\n",
      "Built TF 1.13.1 - libcublas.so.10.1: cannot open shared object file with classify_image.py\n",
      "\n",
      "Should `fold_constants` be before or after `flatten_atrous_conv`\n",
      "\n",
      "tf.function-decorated function tried to create variables on non-first call\n",
      "\n",
      "Implement WALS matrix factorization in Tensorflow 2.0\n",
      "\n",
      "Segmentation Fault with TensorRT create interference graph \n",
      "\n",
      "Export user_data from TfLiteContext\n",
      "\n",
      "tensorflow 1.13.1 on linux on python 3.7 (not osx) uses -D_GLIBCXX_USE_CXX11_ABI=1 -- this behavior is undocumented and/or unspecified\n",
      "\n",
      "tf.train.BytesList should accept bytearray as an input type\n",
      "\n",
      "gpu doesn't report error when id is actually out of range in tf.nn.embedding_lookup(embed, ids) \n",
      "\n",
      "Tensorflow-lite segmentation model does not work with NNAPI on Android 9.0\n",
      "\n",
      "Make documentation link to C++ code\n",
      "\n",
      "tf.data.experimental.ignore_errors\n",
      "\n",
      "How horovod integrate itself into tf distribute strategy?\n",
      "\n",
      "2.0 Reference Models: MobileNetV2 (1 GPU, 8 GPU with dist strat and Keras)\n",
      "\n",
      "2.0 Reference Models: MobileNetv2 (TPU with dist strat and Keras)\n",
      "\n",
      "[TF 2.0 API Docs] tf.math modules, new endpoints, gen_math_ops.py\n",
      "\n",
      "Dynamic ksize and strides with AvgPool\n",
      "\n",
      "EagerTensor.numpy() fails with string dtype when called from py_function on GPU\n",
      "\n",
      "How to use tf.embedding_lookup_sparse_with_distributed_aggregation in feature_column&estimator ?\n",
      "\n",
      "Multiple CheckpointSavers when using MonitoredTrainingSession\n",
      "\n",
      "Compilation failed: Compilation failure: Ran out of memory in memory space vmem. Please file a bug against XLA.\n",
      "\n",
      "ERROR: /home/user/.cache/bazel/_bazel_user/b4774fbdb8542988b4e302c9e073f145/external/com_google_absl/absl/types/BUILD.bazel:190:1: C++ compilation of rule '@com_google_absl//absl/types:bad_variant_access' failed (Exit 1) on benchmark_model tool build\n",
      "\n",
      "Incorrect epoch number in TensorBoard callback when using batch-level metrics\n",
      "\n",
      "transform_graph tools fails to produce output for a particular graph definition/also strange behavior of the tool\n",
      "\n",
      "SVD segfaults when given a 0x0 matrix\n",
      "\n",
      "SVD handles small singular values poorly\n",
      "\n",
      "Terribly slow conditional updates\n",
      "\n",
      "Numpy like slicing on Tensors\n",
      "\n",
      "tflite runtime error with depthwise conv2D \n",
      "\n",
      "when using model_to_estimator on a keras model -> accuracy and loss are not stored in events.out.tfevents.xxxxxxxxxx\n",
      "\n",
      "dilated convolution leads to incorrect graph optimization\n",
      "\n",
      "[tflite][java] Interpreter.reset_all_variables() is not supported on Android.\n",
      "\n",
      "[TF 2.0] Respect masking in Keras loss reduction (i.e., support an equivalent of default TF 1.0 loss reduction SUM_OVER_NONZERO_WEIGHTS)\n",
      "\n",
      "Improve testing infrastructure to prevent breaking open source builds\n",
      "\n",
      "Request for Leaky Relu quantization support\n",
      "\n",
      "[Feature] Support RISCV with tfcompile --target_tuple\n",
      "\n",
      "Large Variation in Compute Times\n",
      "\n",
      "Numpy operation on List of Tensor is considerably slow.\n",
      "\n",
      "some tests in //tensorflow/lite/testing/ fail to build\n",
      "\n",
      "TFRecordWriter.flush() do not seem to flush\n",
      "\n",
      "googletest.h used in open source project\n",
      "\n",
      "Multipose outputs\n",
      "\n",
      "Will LSTMBlockFusedCell be supported in tensorflow 2.0?\n",
      "\n",
      "Improve tf_compile to allow emitting HLO module instead of/in addition to executable\n",
      "\n",
      "No improvement in performance of mobilenet_v1_1.0_224 on TFLite for GPU\n",
      "\n",
      "Getting error - Expects arg[0] to be float but uint8 is provided while using bytebuffer for image data in tensorflow inference.\n",
      "\n",
      "tensorflow lite: error while converting frozen model to lite format\n",
      "\n",
      "SparseTensor takes ages to initialize and can not be saved independently\n",
      "\n",
      "Is it possible to set the random seed for tf.feature_column.categorical_column_with_hash_bucket on operation-level？\n",
      "\n",
      "Issue on converting yolo to tflite with bazel-bin toco\n",
      "\n",
      "importing scope with map_fn twice causes internal bug\n",
      "\n",
      "How to use libtensorflow-lite.a and tflite model on Raspi 3?\n",
      "\n",
      "Providing a Prebuilt Binary for TFLite Benchmark\n",
      "\n",
      "Bazel always rebuild all code even I just modify single cpp file when config=cuda is used \n",
      "\n",
      "softmax_cross_entropy_with_logits_v2 doesn't support hypergradients of labels\n",
      "\n",
      "Be able to save a stateful operation\n",
      "\n",
      "Met unsupported operator of type Cast when quantize model get from quantize aware training\n",
      "\n",
      "Keras model save/load is totally wrong with sessions\n",
      "\n",
      "Java API can not create empty tensor\n",
      "\n",
      "Wrong built-in precision metric for estimator\n",
      "\n",
      "Allow py_function to support functions that return RaggedTensor\n",
      "\n",
      "Add SAMD21 or SAMD51 TensorFlow Lite for Microcontrollers\n",
      "\n",
      "Tensorflow.js API documentation example issue line 19, incorect code, build error\n",
      "\n",
      "TFLite LSTM example produces different results for Lite and for standard Tensorflow for variable length input.\n",
      "\n",
      "tf.contrib.training.HParams domain interval support\n",
      "\n",
      "[Tflite] Unable to convert model to tflite with uneven input dimension\n",
      "\n",
      "Missing warning or documentation for upcast in sparse_softmax_cross_entropy_with_logits?\n",
      "\n",
      "GPU placement of tf.nn.conv2d during tf.data.Dataset.map call causes UnimplementedError (NHWC)\n",
      "\n",
      "[TF 2.0] tf.summary should be easier to use with graphs\n",
      "\n",
      "[TF 2.0] tf.summary built-in support in Estimator\n",
      "\n",
      "[TF 2.0] tf.summary step argument should cast to int64\n",
      "\n",
      "[TF 2.0] tf.summary centralized step management\n",
      "\n",
      "[TF 2.0] summary API migration paths\n",
      "\n",
      "Tensor::FromProto should check the type of tensor\n",
      "\n",
      "Converting tensor2tensor transformer to tf-lite graph\n",
      "\n",
      "Kubernetes GPU Docs & k8s-compatible Docker Images\n",
      "\n",
      "TensorFlow Lite Android nightly runs inference x2 faster than latest version 1.13.1\n",
      "\n",
      "Centos7.6+python37+tensorflow1.13.1 : ImportError: /lib64/libstdc++.so.6: version `CXXABI_1.3.8' not found\n",
      "\n",
      "RAM memory issue in Xavier \n",
      "\n",
      "fatal error: tensorflow/cc/ops/array_ops.h\n",
      "\n",
      "[TF 2.0] Provide a tool to convert checkpoints for optimizers, from TF 1.x to TF 2.0.\n",
      "\n",
      "[TF 2.0] Checkpoint breaking change for all optimizers.\n",
      "\n",
      "tf.gradients returns incorrect results if used multiple times on CudnnLSTM\n",
      "\n",
      "Pruning example, ValueError: Could not find a checkpoint at: .\n",
      "\n",
      "LookupError: No gradient defined for operation 'MatrixLogarithm'\n",
      "\n",
      "[FR] A better way instead of  `while True: sess.run(..)`\n",
      "\n",
      "error: call of overloaded 'ResizeBilinearOpModel(<brace-enclosed initializer list>)' is ambiguous\n",
      "\n",
      "[TF 2.0] StridedSlice issue with empty slice.\n",
      "\n",
      "Tensorflow lite gpu delegate inference using opengl and SSBO in android\n",
      "\n",
      "Rename tf.nn.batch_normalization\n",
      "\n",
      "tf.image.crop_and_resize() - weird alignment behavior?\n",
      "\n",
      "`tensorflow.python.client.device_lib.list_local_devices()`  crushes when none of GPU has enough free memory\n",
      "\n",
      "[TF 2.0 API Docs] tf.custom_gradient\n",
      "\n",
      "Even in eager mode, Keras is passing custom models non-eager tensors in 'fit'\n",
      "\n",
      "Any plan to add SoftClippingand SmoothMax\n",
      "\n",
      "InvalidArgumentError Incompatible shapes when multi_gpu_model used on LSTM model in TF 2.0\n",
      "\n",
      "option for returning *state* sequences in tf.keras.layers.rnn\n",
      "\n",
      "The procedure of building the TF r1.13 is not clear. (CUDA10.0 + cuDNN7.5.0+TensorRT5.0.2.6)\n",
      "\n",
      "tf.nn.ctc_beam_search_decoder is very slow on tf >=1.4\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warnings (please do not import '@grpc//third_party/nanopb:pb_common.c' directly ; depends on deprecated target ; ...) during build from source\n",
      "\n",
      "Datasets not reshuffling between epochs in eager mode\n",
      "\n",
      "[Feature Request, INQ] Add functionality for INQ implementation\n",
      "\n",
      "Tensorflow not working with vGPUs (when the machine is not in pass-through mode)\n",
      "\n",
      "Where to place 'tf.contrib.quantize.create_training_graph'  during multi-gpu Quantization-aware training?\n",
      "\n",
      "Build error: Undeclared inclusions of various numpy header files\n",
      "\n",
      "[docs] (non) determinism in TF\n",
      "\n",
      "Estimator.train doesn't respect the RunConfig keep_checkpoint_max limit\n",
      "\n",
      "Java Tensorflow save & restore without using filesystem\n",
      "\n",
      "Parallel quasi-newton optimizers for tensorflow-gpu\n",
      "\n",
      "Kenlm in tf.nn.ctc_beam_search_decoder\n",
      "\n",
      "Cross-compilation from Windows 10 to Ubuntu 16.04\n",
      "\n",
      "tensorflow lite tests on x86\n",
      "\n",
      "TensorForestEstimator throws ValueError due to bad feature column handling\n",
      "\n",
      "Model Memory requirements\n",
      "\n",
      "Many operations don't support uint64\n",
      "\n",
      "Incorrect predictions while exporting keras model to android\n",
      "\n",
      "tf-lite: allocateTensors never returns\n",
      "\n",
      "[TF2.0] Change default types globally\n",
      "\n",
      "SplitPath should split on both forward slashes and backslashes on Windows\n",
      "\n",
      "Failed to convert object of type <class 'tensorflow.python.framework.sparse_tensor.SparseTensor'> to Tensor\n",
      "\n",
      "Able to access nodes/namespace from a graph for downloaded models\n",
      "\n",
      "Requirements of SparseTensorDenseMatMul on GPU\n",
      "\n",
      "Compilation error when building tfcompile on Windows 10 (TF 1.13-rc2)\n",
      "\n",
      "Warning with custom categorical column with custom weights\n",
      "\n",
      "The performance of SparseSoftmaxCrossEntropyWithLogits in inference is very low\n",
      "\n",
      "keras model.compile: allow user-specific label shapes/dtypes for loss/metrics\n",
      "\n",
      "keras `Model.compile` with loss/metrics dict and multiple outputs from same layer\n",
      "\n",
      "BUG: symbolic layer triggers device creation\n",
      "\n",
      "Docker containers with Python 3.7\n",
      "\n",
      "numpy.dtype size changed, may indicate binary incompatibility\n",
      "\n",
      "[performance] CPU is idle even when there are operations ready to be executed\n",
      "\n",
      "Please release the Raspberry Pi Camera Follower Demo\n",
      "\n",
      "Ability to hook Iterator initialisation while using estimators\n",
      "\n",
      "Migration Guide from Tensorflow Android to Tensorflow Lite\n",
      "\n",
      "YOLOv3 TFLite GPU and CPU perform at same speed\n",
      "\n",
      "CUDNN_STATUS_INTERNAL_ERROR while using tensorflow.keras.applications.VGG16\n",
      "\n",
      "report_tensor_allocations_upon_oom and embedding_lookup together lead to memory leak\n",
      "\n",
      "Speed of benchmark code in CPU windows is much slower than ubuntu\n",
      "\n",
      "Image resizing codes of iOS example camera apps might be wrong\n",
      "\n",
      "Wrong semantic of Dense layer for tf.python.keras.Dense when input has rank > 2.\n",
      "\n",
      "tf.nn.dropout is not working properly with keep_rate 1.\n",
      "\n",
      "tf.contrib.quantize.create_eval_graph does not add min/max node to bias\n",
      "\n",
      "Different behavior between Keras and tensorflow.keras in v1.13.0rc1 on placeholder and variable\n",
      "\n",
      "Sudden drop in conv3d_transpose GPU performance with large input sizes\n",
      "\n",
      "Floating point addition is not commutative - Unexpected behavior - leads to NaN issues\n",
      "\n",
      "Cache intermediate results in custom op for backwards pass\n",
      "\n",
      "Missing mathematical definitions of tf.norm\n",
      "\n",
      "Unable to use keras.application within TF Estimator\n",
      "\n",
      "Type Error caused by using AttentionCellWrapper: Tensors cannot be iterated.\n",
      "\n",
      "NNAPI does not accept the reshape operation with the -1 parameter\n",
      "\n",
      "Inconsistent eigh gradients\n",
      "\n",
      "tf.nn.conv2d_transpose different behaviour if output_shape is a list\n",
      "\n",
      "~40% slow down since 1.13.0.dev20190202\n",
      "\n",
      "tf_upgrade_v2 upgrade script should do absl-py conversions\n",
      "\n",
      "tf.contrib.summary.image() fails silently and inconsistently\n",
      "\n",
      "Build Tensorflow version that detects CPU instruction set at runtime and lights-up/down\n",
      "\n",
      "CancelledError: Loop execution was cancelled\n",
      "\n",
      "Bug in CrossShardOptimizer for Windows running with TPU\n",
      "\n",
      "Can LSTM be fully quantized for inference?\n",
      "\n",
      "Raspberry PI - Golang - Tensorflow - std::bad_alloc Issue\n",
      "\n",
      "RTX 2070 8GB tensorflow 1.13 build from source\n",
      "\n",
      "Custom Reader Op results in undefined symbol: _ZTIN10tensorflow8OpKernelE on library load\n",
      "\n",
      "Inference speed slows down after using graph transform tool\n",
      "\n",
      "C- How to draw bounding-box using tensorflow c_api\n",
      "\n",
      "Core dumped bfc allocator \n",
      "\n",
      "Unnecessary logging noise for non-authenticated GCS access\n",
      "\n",
      "2.0 Reference Models: Keras Application Set (TPU)\n",
      "\n",
      "2.0 Reference Models: Transformer (TPU with dist strat and Keras)\n",
      "\n",
      "2.0 Reference Models: NMT Model (TPU with dist strat and Keras)\n",
      "\n",
      "tf.contrib.distribute performance issue with asymmetrical multi GPU setup\n",
      "\n",
      "tf.keras.utils.multi  does not work with WGAN-GP Model\n",
      "\n",
      "There are some error when convert Mask RCNN model to .tflite\n",
      "\n",
      "Tensorflow Lite Multiple Choice Test example app\n",
      "\n",
      "Support SavedModel in WarmStartSettings initialization\n",
      "\n",
      "TF debug crashes when nan in loss\n",
      "\n",
      "tf.linalg.solve segfaults on invalid matrix dimensions\n",
      "\n",
      "Add integer data types to IsTrainable for use with custom gradients\n",
      "\n",
      "how to export tensorrt int8  model with savedmodel format after calibration？\n",
      "\n",
      "Feature: TF Serving compatibility with TF 2.0.\n",
      "\n",
      "Feature: Unify argument names (~70).\n",
      "\n",
      "Feature: TensorBoard compatibility with TF 2.0.\n",
      "\n",
      "Library Conversion: Tensor2Tensor\n",
      "\n",
      "2.0 Reference Models: NCF Model (TPU with Keras)\n",
      "\n",
      "2.0 Reference Models: Keras Application Set (1 GPU)\n",
      "\n",
      "[BUG] gradients of `tf.cond` with float64 breaks with ResourceVariable\n",
      "\n",
      "Data augmentation on 4-D or 5-D data\n",
      "\n",
      "tf.contrib.opt.ScipyOptimizerInterface error\n",
      "\n",
      "bazel build aot armeabi-v7a so failed\n",
      "\n",
      "windows c_api error with tensorflow.dll\n",
      "\n",
      "Build issue of \"cannot find symbol class Fill where T is a type-variable: T extends Object declared in class Zeros\" still persists, when building Bazel. \n",
      "\n",
      "Duplicated Java outer classname\n",
      "\n",
      "Complex step derivatives\n",
      "\n",
      "lite: non-standard variable-length arrays\n",
      "\n",
      "Build from source in RelWithDebInfo build type, get a error : LNK 1248 image size (10068FDA8) exceeds maximum allowable size (FFFFFFFF)\n",
      "\n",
      "Accept h5py.Group in tf.keras save/load functions\n",
      "\n",
      "Contrib package initialization is broken in 1.13.0.rc0\n",
      "\n",
      "Build of 1.13 //tensorflow/python/eager:pywrap_tfe_lib fails with internal compiler error: unexpected expression ‘I’ of kind template_parm_index\n",
      "\n",
      "DNNClassifier estimator train shows unsupported feed type error occasionally \n",
      "\n",
      "TF lite has link error when I try to build with build_rpi_lib.sh\n",
      "\n",
      "[TF Lite] Add support for dilated convolution with valid padding.\n",
      "\n",
      "Xcode: How to use an static library (a.framework) which force_load  libtensorflow-core.a?\n",
      "\n",
      "Two roundings in MultiplyByQuantizedMultiplier(TFLite) leads to inconsistent result with tensorflow\n",
      "\n",
      "No code examples for rewriting existing graph using tf.quantization.fake_quant_with_min_max_vars for quantization-aware training\n",
      "\n",
      "tf.Estimator as checkpointable\n",
      "\n",
      "AdamWOptimizer doesn't work with MirroredStrategy and tf.get_variable\n",
      "\n",
      "Example missing for using the field `tensor_content` with Golang grpc client\n",
      "\n",
      "Error: No gradient defined for operation (op type: CropAndResizeGradImage)\n",
      "\n",
      "[TFLite] Feature request: Add support for DepthToSpace op \n",
      "\n",
      "tf.hessians vs tf.hessian_vector_product won't give the same result\n",
      "\n",
      "TPUEstimatorSpec supports hooks or not?\n",
      "\n",
      "Enable reference code of kernels(internal) in TF-Lite\n",
      "\n",
      "GetTempFilename is not implemented\n",
      "\n",
      "tf.dataset + tf.estimator  slow, starving CPU/GPU\n",
      "\n",
      "native tf and tf.keras optimizer and gradient calculation problem\n",
      "\n",
      "embedding_lookup return a SparseTensor\n",
      "\n",
      "Continuously differentiable eigendecomposition\n",
      "\n",
      "TFLiteConverter unable to convert object detection model from export_tflite_ssd_graph.py\n",
      "\n",
      "Support per channel quantized ops\n",
      "\n",
      "tf.contrib.integrate.odeint can only accept time points in increasing order\n",
      "\n",
      "TFDBG Segmentation Fault\n",
      "\n",
      "Wrong Error Raised: \"The graph couldn't be sorted in topological order\"\n",
      "\n",
      "Recurrent batch normalization : While_loop and control_depenecies bug\n",
      "\n",
      "TensorRT's create_inference_graph() produced output with too big size\n",
      "\n",
      "why is tensorflow.map_fn slow, what is wrong with following code?\n",
      "\n",
      "tf.estimator support multi saver and train_op for training multi model at the same time\n",
      "\n",
      "float16 matmul is way slower than float32 matmul on CPU\n",
      "\n",
      "ARM6/RPI: Executor failed to create kernel _FusedConv2D\n",
      "\n",
      "ExtractImagePatches request\n",
      "\n",
      "Deadlock probably with RandomShuffleQueue and FIFOQueue\n",
      "\n",
      "Incompatible checkpoint being restored on estimator with keras model\n",
      "\n",
      "Incorrect relative include paths in include/tensorflow/core/framework/op_def.pb.h when building external c++ files using Windows\n",
      "\n",
      "tf.log() is missing although it is still used in many code examples\n",
      "\n",
      "tf.contrib.quantize.create_training_graph with tf.keras.activations.relu quantizes before the relu cliping op\n",
      "\n",
      "[Feature Request] Indexing an array of functions\n",
      "\n",
      "Multi-tower support on each GPU in MirroredStrategy and estimator\n",
      "\n",
      "tensorflow parameter server start with no worker_hosts specific in  cluster def\n",
      "\n",
      "TF Identity - SparseTensor - Not Working\n",
      "\n",
      "[Performance] Unnecessary 'memcpy' in Gather Op\n",
      "\n",
      "Request for function argument for tf.device in eager mode\n",
      "\n",
      "Unexpected warning during GeneratorDataset iterator finalization\n",
      "\n",
      "[Perfomance]Dilated/Atrous Conv implementation with cudnn\n",
      "\n",
      "[feature request] make MutableHashTableOfScalars and MutableHashTableOfTensors trainable\n",
      "\n",
      "matrix_triangular_solve is much slower on GPUs than on CPUs\n",
      "\n",
      "tf.count_nonzero not working on TPU\n",
      "\n",
      "tfdbg memory leak\n",
      "\n",
      "Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\n",
      "\n",
      "How to build from source with hdfs support on. No option in configure?\n",
      "\n",
      "Bad accuracy on ML Kit with InceptionV3 tflite model\n",
      "\n",
      "tf.contrib.copy_graph.copy_variable_to_graph does not set the shape of copied variable\n",
      "\n",
      "TypeError: Can't convert Operation 'MutableHashTable' to Tensor\n",
      "\n",
      "Partially-defined shapes and group_norm\n",
      "\n",
      "CheckpointInputPipelineHook not restoring the correct iterator state\n",
      "\n",
      "Support SparseReduceSum on GPU \n",
      "\n",
      "Equivalent c_api for TFE_Py_RecordGradient\n",
      "\n",
      "[Feature Request]Deformable Convolutional Op Support\n",
      "\n",
      "meta_graph_transform does not work with default checkpoint_path\n",
      "\n",
      "ValueError: No variables to save\n",
      "\n",
      "Block Matrix / Block Diagonal Matrix\n",
      "\n",
      " Predict fuel efficiency: regression example should not normalize one-hot values\n",
      "\n",
      "TOCO cannot identify dilated convolution correctly\n",
      "\n",
      "tensorflow.keras printing control characters in progress bars\n",
      "\n",
      "[tf.keras.layers.LSTM] reset_state performance issue\n",
      "\n",
      "Compilation failed when building keras model with CTC on TPU\n",
      "\n",
      "Check failed: is_weights() on creating tensorrt inference graph\n",
      "\n",
      "tf.data equivalent for tf.estimator.inputs.numpy_input_fn\n",
      "\n",
      "Cannot build profiler on windows 10\n",
      "\n",
      "Not able to read objets from private s3 bucket\n",
      "\n",
      "DistributionStrategy and Keras models: support for sample_weight_mode\n",
      "\n",
      "bazel error for saved_model_cli\n",
      "\n",
      "Hard arbitrary limit on Saved model size\n",
      "\n",
      "Spurious syntax highlighting of code blocks in site documentation\n",
      "\n",
      "TextLineDataset supporting start/end file positions\n",
      "\n",
      "Add tf.metrics.std or tf.metrics.var\n",
      "\n",
      "tf.image (CLAHE) contrast limited adaptive histogram equalization.\n",
      "\n",
      "[Feature Request] Batch size scheduler to speed up training\n",
      "\n",
      "InfeedEnqueueTuple placed on TPU device blocks indefinitely\n",
      "\n",
      "Error while running sample cpp program in tensorflow\n",
      "\n",
      "Bug when passing multiple output of Lambda Layer to the Model API\n",
      "\n",
      "Add an ability to terminate tf.data.choose_from_datasets/sample_from_datasets early\n",
      "\n",
      "SparseTensor file format\n",
      "\n",
      "Inconsistent keras API regarding variable_scope\n",
      "\n",
      "CMake build Tensorflow C++ on Windows 10 Error : Test COMPILER_OPT_ARCH_NATIVE_SUPPORTED - failed\n",
      "\n",
      "TPU runs as slow as CPU when using keras_to_tpu_model\n",
      "\n",
      "Major memory leak after calling `tf.estimator.DNNClassifier.train` with certain `tf.train` optimizers.\n",
      "\n",
      "Error compiling iterator_ops.cc when building from source with MSVC 2017\n",
      "\n",
      "Secondary algorithm not provided error when not using cuDNN autotune\n",
      "\n",
      "Pruning: Multi-GPU support\n",
      "\n",
      "TensorForest: the base random seed takes no effects\n",
      "\n",
      "Keras feature DropConnect\n",
      "\n",
      "tf.ConditionalAccumulator does not behave as expected\n",
      "\n",
      "Unable to train a LinearClassifier with categorical columns and CollectiveAllReduce\n",
      "\n",
      "Does GanEstimator support multi GPU ?\n",
      "\n",
      "Update ExternalOptimizerInterface for use with Eager Execution\n",
      "\n",
      "Network Bandwidth Un-expected Behavior \n",
      "\n",
      "C++ compilation of rule '//tensorflow/python:cost_analyzer_lib' failed (Exit 2): msvc_wrapper_for_nvcc.bat failed: error executing command\n",
      "\n",
      "tf.dynamic_partition may cause NaN loss when use it with multi gpus and it performs normally with single gpu\n",
      "\n",
      "Copy value of trainable variable to another trainable variable.\n",
      "\n",
      "No clear_devices in BestExporter\n",
      "\n",
      "Error occurs when importing metagraph that contains cudnnRNN cells\n",
      "\n",
      "StructuralEnsembleRegressor gives warning: Converting sparse IndexedSlices to a dense Tensor of unknown shape\n",
      "\n",
      "Error: ...tensorflow/core/common_runtime/bfc_allocator.cc:373] tried to deallocate nullptr\n",
      "\n",
      "DCGAN implementation differs between GitHub and Colab\n",
      "\n",
      "Keras layers: no update ops added even when used as a \"simplified interface to Tensorflow\"\n",
      "\n",
      "In implementation of ConvLSTM,I  want to know where is the w_ci dot C_t-1 in paper formula (3)\n",
      "\n",
      "Exporting GraphDef from File and using the resulting TFLite model in the TFLite Android App doesn't work.\n",
      "\n",
      "Document about using custom op to build graph in C++\n",
      "\n",
      "Android Bazel Build Error \n",
      "\n",
      "~40% performance decrease since Tensorflow 1.9 when training large models\n",
      "\n",
      "Some confusion about Gather OP implement\n",
      "\n",
      "Relative Device Placement\n",
      "\n",
      "Not compatible with tf.float16\n",
      "\n",
      "Event files created by estimator are not closed after training/evaluation\n",
      "\n",
      "[DeepLabV3+] How to get class probabilities in C++ using frozen graph?\n",
      "\n",
      "TF1.12 building w/ py3.6.4 on HPC\n",
      "\n",
      "Reduced redundancy in Estimator api\n",
      "\n",
      "Docs Needed\n",
      "\n",
      "Infinite loop when printing CSV data in eager execution mode \n",
      "\n",
      "CudaRoot() during compilation should not be used \n",
      "\n",
      "Repeated \"Already exists: Resource\" errors when I increase my model size\n",
      "\n",
      "TypeError: visualize_boxes_and_labels_on_image_array() takes at least 7 arguments (8 given)\n",
      "\n",
      "C++  TensorShape from std::vector or std::array\n",
      "\n",
      "Sparsification of huge matrix\n",
      "\n",
      "Feature Request - Deeplab TFLITE Android Application\n",
      "\n",
      "Riccati solver suggest\n",
      "\n",
      "Sparse Precision Matrix implemetations (for gaussian processes or GMRFs)\n",
      "\n",
      "aarch64 support\n",
      "\n",
      "Audio recognition on 8,000 samples per second audio data, label_wav.py\n",
      "\n",
      "image_ocr.py running error on tf.keras\n",
      "\n",
      "Tensor RT 5 Windows\n",
      "\n",
      "scatter_max doesn't work with MirroredStrategy since v1.11.0\n",
      "\n",
      "Installation of tensorflow r1.10 on windows with visual studio c++ (with gpu) help.\n",
      "\n",
      "__manage.py__ not found\n",
      "\n",
      "run error when i use xla\n",
      "\n",
      "`var_list` will cause untrainable variables to be trained\n",
      "\n",
      "Default argument of tf.placeholder\n",
      "\n",
      "tf.assign does not support gradient?\n",
      "\n",
      "XLA does not know associative law\n",
      "\n",
      "Causal Convolutions in Tensorflow\n",
      "\n",
      "[Cloud TPU] Various issues with uint8 data type\n",
      "\n",
      "Segmentation Fault on `help(tf.RunMetadata)`\n",
      "\n",
      "Tensorboard projector visualisation - PCA keeps loading or not working\n",
      "\n",
      "Move autograph into a separate package and into a separate repo\n",
      "\n",
      "tf.manip.roll executes on CPU instead of GPU\n",
      "\n",
      "tf.nn.embedding_lookup_sparse Converting sparse IndexedSlices Warning\n",
      "\n",
      "//tensorflow/core:util_tensor_slice_set_test fails on 1.12\n",
      "\n",
      "//tensorflow/contrib/lookup:lookup_ops_test fails on 1.12\n",
      "\n",
      "[xrt] Add \"Free all memory\" op\n",
      "\n",
      "Unenforce StreamHandler in tf_logging\n",
      "\n",
      "Memmory leak Using tensorflow::NewSession , C++\n",
      "\n",
      "Relationships between Grappler, GraphOptimizer and GraphOptimizationPass\n",
      "\n",
      "Unacceptable framework overhead for huge networks\n",
      "\n",
      "No C++ symbols exported after built libtensorflow_cc with bazel on windows\n",
      "\n",
      "Build CMAKE C++/Cuda project with tensorflow in debug mode\n",
      "\n",
      "tf.contrib.estimator.InMemoryEvaluatorHook does not work with MirroredStrategy\n",
      "\n",
      "No such file or directory  #include \"npy_1_7_deprecated_api.h\"\n",
      "\n",
      "Fail to configure on Windows(CMake3.11.0-rc2, VS2015, python3.5.3, swigwin-3.0.12)\n",
      "\n",
      "ModelCheckpoint Callback Doesn't Log Validation Accuracy on ML Engine\n",
      "\n",
      "CONV2 occurrence mismatch in timeline\n",
      "\n",
      "request implementation of conv1DLSTM, conv3DLSTM in keras\n",
      "\n",
      "build libtensorflow_cc.so with debug symbols using bazel\n",
      "\n",
      "CPU memory slowly filling, during inference, cannot seem to find the reason\n",
      "\n",
      "Issue in building Tensorflow library on Windows machine\n",
      "\n",
      "i encounter very strange and serious Bug with dataset feeding （Emergency）\n",
      "\n",
      "Wrong gradients in combination with placeholders.\n",
      "\n",
      "Dataset shard index automatic change in Estimator DistributionStrategy\n",
      "\n",
      "intel mkl optimized tensorflow performance degradation\n",
      "\n",
      "TFProf is unable to parse the profile generated by profileContext.\n",
      "\n",
      "Failing isinstance check in tensorflow.python.keras.models._clone_functional_model\n",
      "\n",
      "Add an option to apply SavedModel via command line \n",
      "\n",
      "Prevent Empty Checkpointable Data Structure Restores\n",
      "\n",
      "Eager execution: variables can't be iterated (unlike Tensor)\n",
      "\n",
      "\"Invalid loop structure. Mismatched parent frames\"\n",
      "\n",
      "axis argument for FFT ops (tf.signal.fft, tf.signal.fft2d, etc.)\n",
      "\n",
      "Support for Truncated Backpropagation Through Time with tf.data.TFRecordDataset API\n",
      "\n",
      "Trying to connect from process to Tensorflow Server, cannot use frozen graph\n",
      "\n",
      "Remove output tensors 1 - 4 from FusedBatchNorm\n",
      "\n",
      "CAN'T BUILD TENSORFLOW\n",
      "\n",
      "multiplication of IndexedSlices with Dense Tensors\n",
      "\n",
      "latest cpu version tensorflow report error when run slim model\n",
      "\n",
      "Float16 support for log_uniform_candidate_sampler/uniform_candidate_sampler\n",
      "\n",
      "Problem when try to decode some bmp images with tf.image.decode_bmp\n",
      "\n",
      "Error while calling TFLite interpreter (Similar to #21574)\n",
      "\n",
      "Xla and Ignite support always true from configure.py\n",
      "\n",
      "Losses collection is not thread local so it can't be used inside model_fn call when using MirroredStrategy\n",
      "\n",
      "tanh on CPU exceeds range (and is inconsistent)\n",
      "\n",
      "Batch Normalization with virtual_batch_size not equal to None not implemented correctly for inference time\n",
      "\n",
      "Ghost Batch Normalization performance\n",
      "\n",
      "Bug in function: MutableGraphView::ReplaceInput(..) ?\n",
      "\n",
      "[Feature Request] tf.keras expose align_corners\n",
      "\n",
      "Error using kernel_regularizer with tf.layers.Dense and tf.contrib.distribute.MirroredStrategy TF v1.12.0rc0 regression\n",
      "\n",
      "Keras saved_model.simple_save model bigger and bigger\n",
      "\n",
      "tf.estimator.train's incompatibility with distributed training on Cloud ML Engine is not well-documented\n",
      "\n",
      "tf.make_tensor_proto doesn't work as expected\n",
      "\n",
      "Tensorflow freeze_graph.py: NodeDef mentions attr 'Truncate' not in Op\n",
      "\n",
      "gradients of tf.fake_quant_with_min_max_vars function.\n",
      "\n",
      "Intermittent very long latency in XRT operations\n",
      "\n",
      "LSTMBlockFusedCell not supported by optimize_for_inference?\n",
      "\n",
      "Feature Request: GPUOptions for Go binding\n",
      "\n",
      "Linker error when compiling from head of master on MacOS\n",
      "\n",
      "Can't use CTCBeamSearchDecoder in c++, LINK ERROR occur,BUG in CTCBeamSearchDecoder 's source code\n",
      "\n",
      "train with multi-gpu with MirroredStrategy will hang-up\n",
      "\n",
      "Tensorflow C API: SessionRun batch size (how to properly set)\n",
      "\n",
      "TensorRT INT8 calibration doesn't work with TF r1.12 and TRT 5RC\n",
      "\n",
      "Error with transform_graph tool:  \"Failed to parse --transform argument\"\n",
      "\n",
      "Wav to Spectogram wrong expected height\n",
      "\n",
      "Second order derivative not supported for LRN (tf.nn.lrn)\n",
      "\n",
      "Object detection api training issue\n",
      "\n",
      "PYTHON_LIB_PATH not set during compilation of tensorflow_python_api_gen from source\n",
      "\n",
      "Segfault when loading libtensorflow_cc.so for a second time\n",
      "\n",
      "crash via tf_should_use format_stack\n",
      "\n",
      "nvcc error: string_view.h: constexpr function return is non-constant\n",
      "\n",
      "AdamWOptimizer and learning rate decay\n",
      "\n",
      "[Cloud TPU] Intermittent freezes requiring reset of the TPU\n",
      "\n",
      "r1.11 failed to build on debian\n",
      "\n",
      "C++ low level API documentation\n",
      "\n",
      "Tensorflow 1.10 C++ project errors : logging.h error in line 229\n",
      "\n",
      "\"Not found: Resource does not exist\" exception thrown in runtime\n",
      "\n",
      "Empty tfrecord in gcs would fail the TFRecordDataset.read\n",
      "\n",
      "Feature request: Documentation tf.keras.applications missing for TF 1.11\n",
      "\n",
      "Tensorflow example, Object Detection: Failed to find input Node 'image_tensor\n",
      "\n",
      "Java process crashes during model loading\n",
      "\n",
      "Android tfLite Shared STL support \n",
      "\n",
      "Inconsistent behaviour of tf.range\n",
      "\n",
      "assign_add seems giving wrong and random results in broadcasting\n",
      "\n",
      "Writing a SavedModel on Windows with long file path fails\n",
      "\n",
      "CollectiveAllReduceStrategy \"Out of range: End of sequence\" warnings\n",
      "\n",
      "[Feature Request]: tf.data.Dataset.map parallelism autotune enhancement\n",
      "\n",
      "memory leak cased by function tf.dynamic_partition\n",
      "\n",
      "[bazel] Unrecoverable error while evaluating node '//tensorflow/cc:ops/io_ops_gen_cc\n",
      "\n",
      "the session->close() is ok,but the error is  stack smashing detected \n",
      "\n",
      "GAN train will hand if not use default GANTrainSteps(1, 1)\n",
      "\n",
      "[Feature Request]:Assign the name to SaprseTensor when build_tensor_info of it\n",
      "\n",
      "numpy not found during python_api generation\n",
      "\n",
      "BUG: reciprocal GPU kernel for complex 1/1 division not found\n",
      "\n",
      "tflite can't ResizeInputTensor size\n",
      "\n",
      "when use tensorflow to predict images with different resolution,the forward's cost  unstable\n",
      "\n",
      "Bazel Build all_protos proto_library Support\n",
      "\n",
      "Document how to train with large batch sizes\n",
      "\n",
      "using bazel to build tensorflow.dll\n",
      "\n",
      "Eigenvalue decomposition of asymmetric matrices\n",
      "\n",
      "bazel build tensorflow on windows 10 getting cudnn.h- system cannot find the file specified\n",
      "\n",
      "C++   set_allow_soft_placement does not work\n",
      "\n",
      "Can't initialize keras.Model based network with tf.train.init_from_chekpoint\n",
      "\n",
      "vgg16  transfer learning will be error \"TypeError: provided list of inputs contains objects other than 'EagerTensor'\"\n",
      "\n",
      "Using TF-TRT doubles the size of frozen protobuf file\n",
      "\n",
      "Orthogonal initialization gives inaccurate result when running on GPU\n",
      "\n",
      "C++ API: tensorflow::ops::Mul causes read access violation when second attribute is int constant\n",
      "\n",
      "how to fix  \"tf.nightly-gpu\" caused \"nan\" problem\n",
      "\n",
      "LookupError: No gradient defined for operation type: ResizeNearestNeighborGrad or ResizeBilinearGrad\n",
      "\n",
      "\"master\" in cluster spec for distributed training \n",
      "\n",
      "tensorflow/core/framework/tensor.cc:861] Check failed: buf_ null buf_ with non-zero shape size 1\n",
      "\n",
      "tf.map_fn causes error when importing the same graph twice and connecting them\n",
      "\n",
      "TOCO fails to handle Dilated Convolution properly\n",
      "\n",
      "Depthwise convolution inside Dataset API throws data_format error\n",
      "\n",
      "ps0 will be OOM using MonitoredTrainingSession when workers too many\n",
      "\n",
      "[Feature Request] Exporting TF-Hub for the Best Model\n",
      "\n",
      "Unable to profile tf.keras Model. Am I missing something or is it a known issue?\n",
      "\n",
      "XLA commonly aborts with nvidia error\n",
      "\n",
      "[1.10.1] op_level_cost_estimator.cc:404] Check failed: 0 < gflops (0 vs. 0)\n",
      "\n",
      "tf.nightly-gpu will create \"nan\" in  next batch train value\n",
      "\n",
      "Memory issue when inter_op_parallelism_threads > 1 on Ubuntu 16.04\n",
      "\n",
      "3D convolutions on GPU with large input produces incorrect results on some GPUs\n",
      "\n",
      "AttributeError: 'SsdAnchorGenerator' object has no attribute 'height_stride'\n",
      "\n",
      "TF Record, Example, and SeqeunceExample inconsistency(?) / Documentation request\n",
      "\n",
      "ResourceExhaustedError\n",
      "\n",
      "The performance of FusedBatchNormGradGrad is very low\n",
      "\n",
      "freeze_graph.py with --input_saved_model argument gives `IOError: SavedModel file does not exist`\n",
      "\n",
      "Optimizing slice of variable not possible\n",
      "\n",
      "tf.estimator.Estimator with distribution strategy messes up TensorBoard readability\n",
      "\n",
      "Variables in bijectors cannot be reused.\n",
      "\n",
      "Graph Transorform fold_constants does not work with NoOp as output node\n",
      "\n",
      "tfcompile: minimum_alignment_for_allocation and kAlign = 64\n",
      "\n",
      "Tensorflow build fails with vectorization \n",
      "\n",
      "Newly included absl headers are missing from the include path\n",
      "\n",
      "Map twice on a shuffled dataset got different result\n",
      "\n",
      "TensorRT INT8 assertion failed when performing calibration\n",
      "\n",
      "Strange behavior of loss function in WALSMatrixFactorization TF 1.8\n",
      "\n",
      "Keras models converted to Estimators do not write summaries.\n",
      "\n",
      "[Feature Request] Keeping Keras Callback functionality in Estimators created from `keras.model_to_estimator`\n",
      "\n",
      "[feature request] specify subgraphs to route to TensorRT in tf.contrib.tensorrt\n",
      "\n",
      "import tensorflow.contrib fails on arm32\n",
      "\n",
      "tf.scatter_update gradients silently failing in eager mode\n",
      "\n",
      "À trous 1D convolution slow in certain scenarios \n",
      "\n",
      "TF_GraphImportGraphDef fails with no input mapping on existing nodes\n",
      "\n",
      "Batch normalization implemented incorrectly in canned DNN estimators\n",
      "\n",
      "Search in Embedding Projector using Japanese or Hindi text causes Cannot read property 'toString' of undefined\n",
      "\n",
      "Incompatible shapes between op input and calculated input gradient for nn.conv3d_transpose\n",
      "\n",
      "Error while using create_inference_graph - Can't find a device placement for the op!\n",
      "\n",
      "android yolo sample output format is different from the official documentation\n",
      "\n",
      "tf.test.is_gpu_available(True) allocates all GPU(s) VRAM\n",
      "\n",
      "New feature request: LOBPCG in addition to Lanczos?\n",
      "\n",
      "Canned TensorForest Tracking\n",
      "\n",
      "//tensorflow/contrib/fused_conv:fused_conv2d_bias_activation_op_test  running into `implementation not found` error\n",
      "\n",
      "Dataset.padded_batch doc improvement request\n",
      "\n",
      "Unable to import tensorrt on macOS\n",
      "\n",
      "profiler trace_steps is not match global step in distribution training\n",
      "\n",
      "train_spec.max_steps is ignored by tf.estimator.train_and_evaluate()\n",
      "\n",
      "Distributed tensorflow worker hangs at TF_CloseSession() when using MonitoredTrainingSession\n",
      "\n",
      "Loading contrib ops in Windows via C API\n",
      "\n",
      "Population Based training with tf.Estimator\n",
      "\n",
      "[license] fft2d's license is overly short and provides no enough declaration\n",
      "\n",
      "op type not registered 'DenseToDenseSetOperation' when running tfcompile\n",
      "\n",
      "Feature Request: make grid image summary adapt to different channel number\n",
      "\n",
      "freeze_graph returns with error\n",
      "\n",
      "Endless restarting of session when run distribute traning with tensorflow 1.8\n",
      "\n",
      "ValueError: Attempted to map inputs that were not found in graph_def: [input:0]\n",
      "\n",
      "Not found: FeedInputs: unable to find feed output phase_train\n",
      "\n",
      "Tensorflow Prediction Artifacts\n",
      "\n",
      "NotFoundError (see above for traceback): Key conv1/biases not found in checkpoint \t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:worker/replica:0/task:0/device:CPU:0\"](_recv_save/Const_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n",
      "\n",
      "Fine tuning a model by building a Scaffold in Mirrored Strategy is not supported\n",
      "\n",
      "All metrics evaluate to 0.0 when using  tf.contrib.estimator.InMemoryEvaluatorHook\n",
      "\n",
      "Better  tf.contrib.estimator.early_stopping (Feature request)\n",
      "\n",
      "[Bug] Broken Combination: Non-SGD Optimizer, tf.Variable(), and Estimator Framework\n",
      "\n",
      "Address missing TensorFlow operations to TFLite:\n",
      "\n",
      "Model Parallelism Memory usage issue\n",
      "\n",
      "BUG: optimizer.compute_gradients() produces inconsistent gradient with the same training instance and label \n",
      "\n",
      "Linking of rule '//tensorflow/core/grappler/costs:analytical_cost_estimator_test' failed\n",
      "\n",
      "Consider automatic casting rules for promoting dtypes\n",
      "\n",
      "Tensorflow PIP Package fails to build - AttributeError: module 'pandas' has no attribute 'core'\n",
      "\n",
      "Implement \"quantize_graph\" will print \"tf.estimator package not installed.\"\n",
      "\n",
      "NCCL is not supported on Windows \n",
      "\n",
      "Changing optimizer of restored network messes up training output\n",
      "\n",
      "speech recognition tutorial bug\n",
      "\n",
      "Turning off Teacher Forcing in decoders of Seq2Seq models \n",
      "\n",
      "Reduce_sum error\n",
      "\n",
      "BUG: Initial values are still random, even both the graph-level and the operation seed are set\n",
      "\n",
      "Discrepancy between Python and C++ loading of corrupt SavedModels\n",
      "\n",
      "[Feature Request] Main improvements for the c++-API\n",
      "\n",
      "Crash during folder creation from Estimator exporter.py in Python 3.6\n",
      "\n",
      "contrib.rnn.ConvLSTMCell: zero_state has wrong size if cell has skip connections\n",
      "\n",
      "Build fails: ERROR: infinite symlink expansion detected\n",
      "\n",
      "dnn_dropout in DNNLinearCombinedClassifier doesn't  work\n",
      "\n",
      "Improve tf.data graph representation in TensorBoard\n",
      "\n",
      "[Bug] cpu memory leak while using GPU with variable length ops.\n",
      "\n",
      "Error in impoerting tensorflow 1.9\n",
      "\n",
      "Max pooling cause error on empty batch\n",
      "\n",
      "Problem with softmax on Windows OS\n",
      "\n",
      "Possible bug / Documentation Suggestion - Dataset API + Estimator API does not throw an exception when the input filepath to the dataset is incorrect\n",
      "\n",
      "\"could not initialize a memory descriptor\" error using tensorflow on windows using CMake\n",
      "\n",
      "FusedBatchNorm of TF 1.9 doesn't work fine \n",
      "\n",
      "[TensorFlow Android Camera Demo] add libSVM AAR but \"Native TF methods not found\"\n",
      "\n",
      "building tensorflow shared library in windows debug mode (cmake) /MDd\n",
      "\n",
      "PS/Chief Nodes not terminating & Worker Nodes not accurately terminating with tf.estimator.train_and_evaluate\n",
      "\n",
      "Building TF with custom op support\n",
      "\n",
      "tf.Print support of tf.complex types\n",
      "\n",
      "aggregation of sparse gradient and dense gradient is unexpected\n",
      "\n",
      "report the tf.boolean_mask runtime problem\n",
      "\n",
      "MobileNet v2 slower than v1 when loading from Frozen GraphDef\n",
      "\n",
      "Unimplemented cast int64 to string is not supported\n",
      "\n",
      "tf.shape output is wrong when net input shape is changed during import\n",
      "\n",
      "Allow Keras Callbacks to access predictions on_batch_end, on_epoch_end\n",
      "\n",
      "Feature Idea: Checkpoint API \"split\" assert_consumed\n",
      "\n",
      "Unexpected behavior of tf.hessians on graphs with tf.reduce_prod\n",
      "\n",
      "Estimator model folder format\n",
      "\n",
      "Session.run() on Operation return None. Design question\n",
      "\n",
      "Got different matrix eigenvalues by tensorflow.self_adjoint_eig(A) than by numpy.linalg.eig(A)\n",
      "\n",
      "C++: Add gradient for image operators\n",
      "\n",
      "Waste lots of time to redownload grpc when building with CMake\n",
      "\n",
      "10 minutes to recover from ResourceExhaustedError\n",
      "\n",
      "Estimator does not work with tf.contrib.cudnn_rnn.CudnnGRU\n",
      "\n",
      "Using tf.contrib.training.batch_sequences_with_states with tf.data.Dataset\n",
      "\n",
      "Placeholder will cause incompelet shape bug in tf.profiler.profile\n",
      "\n",
      " the quantized form of Shape operation is not yet implemented\n",
      "\n",
      "tf.contrib.graph_editor.graph_replace is broken for while loops\n",
      "\n",
      "Heavily increased memory consumption for optimizing batch_norm in tf versions > 1.3.0 \n",
      "\n",
      "[Question/Feature request] How to stack variable length tensors in a TensorArray?\n",
      "\n",
      "Will cycle-gan have a Estimator implementation like tf.contrib.gan.estimator.GANEstimator?\n",
      "\n",
      "quantization deeplabv3(mobielentv2) error\n",
      "\n",
      "Why tf.losses.softmax_cross_entropy doesn't have \"dim\" (axis) argument?\n",
      "\n",
      "Issue with tf.gradients() taking too long\n",
      "\n",
      "Java SavedModelBundle.load do not support long path in windows\n",
      "\n",
      "Feature request: preserve cycle order of open iterators in tf.data.Dataset.interleave\n",
      "\n",
      "tf.Print related wired bug\n",
      "\n",
      "Error occurs when I create subprocess after creating tf.train.Server with RDMA.\n",
      "\n",
      "Extend the support of the exponential distribution to include 0\n",
      "\n",
      "[Improvement] Make tf.contrib.model_pruning.masked_conv2d API compatible with tf.layers.conv2d\n",
      "\n",
      "ImportError: cannot import name 'build_info' when using command import tensorflow as tf\n",
      "\n",
      "Trying to open a non-existing file in tf.data.Dataset on Windows crashes Python instance\n",
      "\n",
      "Speeding up TF custom ops on GPU\n",
      "\n",
      "The default values of tf.app.flags are printed event though passed parameters at the first time\n",
      "\n",
      "Compilation with Cuda 9.1 Cudnn 7.0.5, In python3, \"help(tensorflow)\" caused core dump\n",
      "\n",
      "feature request: Robbins-Monro type learning rate decay\n",
      "\n",
      "prefetch_to_device doesn't overlap copy(HtoD) with computation and also may fail\n",
      "\n",
      "Makefile: build_all_ios.sh - No rule to make target 'distclean'.  Stop.\n",
      "\n",
      "Allow restoring subgraph from checkpoint for weight sharing between different models\n",
      "\n",
      "[bug] events.out.tfevents files do not get closed.\n",
      "\n",
      "Tensorflow 1.8 graph destructor hanging indefinitely\n",
      "\n",
      "non-chief workers hang in training distributed seq2seq model when time-major is true\n",
      "\n",
      "[Feature Request] Support S3 KMS client-side decryption when loading data\n",
      "\n",
      "Slow tf.hessians slicing\n",
      "\n",
      "Feature request: verify py_func tensor's shape when evaluating\n",
      "\n",
      "Exception in thread \"main\" java.lang.UnsatisfiedLinkError: Cannot find TensorFlow native library for OS\n",
      "\n",
      "failed to embed python code that import tensorflow in c++ \n",
      "\n",
      "gfile.Glob is Case Sensitive for file extensions in Unix but not in Windows\n",
      "\n",
      "gfile.Glob - Recursive in Windows, not in Unix\n",
      "\n",
      "freeze_graph doesn't work\n",
      "\n",
      "[tf.keras] Stateful Metrics assorted errors.\n",
      "\n",
      "[Feature Request] random_poisson GPU Kernel\n",
      "\n",
      "[Feature Request] Graph Transform Tool support SavedModel as Input\n",
      "\n",
      "[feature request] allclose\n",
      "\n",
      "Add a dictionary to a collection\n",
      "\n",
      "[Feature request] Allow `tf.estimator.train_and_evaluate` to evaluate on multiple datasets\n",
      "\n",
      "[Feature Request] Exponential Integral function Ei\n",
      "\n",
      "[Feature Request] Inverse Wishart Distribution\n",
      "\n",
      "Guidelines to have a working C API (libtensorflow.so) on FreeBSD 11.1\n",
      "\n",
      "fused_batch_norm's gradient implementation is incomplete\n",
      "\n",
      " only supports 'NHWC' format under the GPU mode \n",
      "\n",
      "Feature request: A Layer object wrapping multiple Layer objects\n",
      "\n",
      "TypeError: 'InvalidArgumentError' object is not iterable\n",
      "\n",
      "No registered 'NotEqual' OpKernel for GPU for INT32 type (but other types are fine).\n",
      "\n",
      "Using new tensorflow op for matrix exponential in a c++ library that already uses tensorflow as third party\n",
      "\n",
      "'DNNBoostedTreeCombinedClassifier' default argument setting would lead to error\n",
      "\n",
      "Highlevel API do not well support float64 due to tf.feature_column.input_layer\n",
      "\n",
      "tf.contrib.ffmpeg.decode_video error\n",
      "\n",
      "distributed training with SyncReplicasOptimizer got stuck after a number of iterations\n",
      "\n",
      "Keras Model functional API with custom submodel not working in eager execution?\n",
      "\n",
      "codelabs tutorial poets doesn't work\n",
      "\n",
      "Results are different on MacOS for fixed data\n",
      "\n",
      "Compiling TF from source on Debian9 does not work as documented - partial fix\n",
      "\n",
      "Truncated Distributions in TensorFlow\n",
      "\n",
      "Provide a way to build a Tensorflow wheel without a dependency on Tensorboard\n",
      "\n",
      "Computing gradients in extracted subgraph which contains a 'while_loop'\n",
      "\n",
      "Windows Make: Don't (re)build dependencies, use prebuilt ones\n",
      "\n",
      "Use shallow clones from git repos in CMake build\n",
      "\n",
      "[FEATURE REQUEST] tf.scatter_nd doesn't support half types\n",
      "\n",
      "[FEATURE REQUEST] decode_csv - optionally skip records with empty required fields\n",
      "\n",
      "Improper attribution for GPLv3 in non-existent directory\n",
      "\n",
      " use the stream builder to invoke FFT library fail in custom op \n",
      "\n",
      "Find a bug in tensorflow\n",
      "\n",
      "Can I insert a node in a graph with graph_editor?\n",
      "\n",
      "Feature Request: Document Best Practice For Feeding New Data to a Restored Metagraph\n",
      "\n",
      "Potential overflow in libhdfs wrapper\n",
      "\n",
      "Partial model loaded SavedModelBundle without exception\n",
      "\n",
      "tf.contrib.rnn.LSTMCell wrong documentation and unclear naming  \n",
      "\n",
      "[BUG] tf.train.Saver in non-local filesystem\n",
      "\n",
      "Failed to create CUPTI subcriber when profiling on servers with no GPU's\n",
      "\n",
      "Incorrect name returned in Tensorflow causes “Tensor which does not exist” error while invoking get_tensor_by_name\n",
      "\n",
      "Official profile python API not work on the official mnist example\n",
      "\n",
      "GPU OOM with Keras and Estimator, fine with Keras alone\n",
      "\n",
      "[feature request] Quasi recurrent neural networks (QRNN) in tensorflow \n",
      "\n",
      "tf.dynamic_partition cannot have dynamic num_partitions\n",
      "\n",
      "c++ gradient is not implemented for concat/stack\n",
      "\n",
      "Multiple Prediction With Tensorflow Model On iOS Swift, Objective-C\n",
      "\n",
      "“grp c++/grpc++.h”: No such file or directory ( D:\\tensorflow-master\\tensorflow\\core\\common_runtime\\eager\\context.cc)\n",
      "\n",
      "No dtype for array of arrays into tf.keras.Input\n",
      "\n",
      "set_intersection doesn't work as expectation - tensorflow 1.6.0\n",
      "\n",
      "Feature request: Concurrently serving models with optimizers\n",
      "\n",
      "einsum contraction order (is undocumented)\n",
      "\n",
      "Tensorflow python version incompatibility\n",
      "\n",
      "metagraph loading fails with 'No op named ImageProjectiveTransform' message\n",
      "\n",
      "tensordot/conj interplay\n",
      "\n",
      "DirectSession::Run with Saver restore operation crashes during nsync wait\n",
      "\n",
      "[bug] Regularizers do not allow integer scales.\n",
      "\n",
      "Library not loaded: @rpath/libcublas.8.0.dylib when running TF GPU on MacOS \n",
      "\n",
      "`ScipyOptimizerInterface` fails with `scatter_add` and `scatter_update`\n",
      "\n",
      "Save training using Tensorflow C++ with VS2015\n",
      "\n",
      "Build fails if Nvidia nccl doc files (NCCL-SLA.txt) are relocated\n",
      "\n",
      "Feature: Exiting current variable_scope (parent scope)\n",
      "\n",
      "Tensorflow Serving not using multi GPU/CUDA cores \n",
      "\n",
      "[Bug] AdamOptimizer: No Exception on invalid input\n",
      "\n",
      "[Feature Request] print_tensors_in_checkpoint_file should accept Google Cloud Bucket addresses of the form 'gs://...'\n",
      "\n",
      "Prebuilt binaries do not work with CPUs that do not have AVX instruction sets.\n",
      "\n",
      "sess.run([train_step]) freezes when using batch_normalization and Collection Update\n",
      "\n",
      "[BUG] max_pooling1d can not run in GPU \n",
      "\n",
      "[Feature request] Backpropagation through Dataset API\n",
      "\n",
      "TPUEstimator.evaluate() docstring incorrect for steps param\n",
      "\n",
      "Restore problem when work with multiple tf.contrib.lookup.MutableHashTable\n",
      "\n",
      "Support on TPU for tf.contrib.framework.sort\n",
      "\n",
      "Performance problem with TensorFlow training\n",
      "\n",
      "tensorflow cpu module's speed lower on windows than linux\n",
      "\n",
      "Does eager execution allow dynamic batching like Tensorflow fold?\n",
      "\n",
      "Random initialization of a GPU variable with more than INT32_MAX elements crashes with CUDA_ERROR_ILLEGAL_ADDRESS\n",
      "\n",
      "Slot variables used in an optimizer must have the same shape with the variable to be optimized?\n",
      "\n",
      "CuDNN error while fitting CNN \n",
      "\n",
      "Feature request: Generate java classes from .protos for java library\n",
      "\n",
      "Race codition during updating of checkpoint file on samba file share\n",
      "\n",
      "Feature Request:  128-bit floats\n",
      "\n",
      "Using estimators created by `tf.keras.estimator.model_to_estimator` in `tf.estimator.train_and_evaluate` causes a memory leak of sorts\n",
      "\n",
      "Gridrnn (Grid2LSTM) tied behaviour is inverted\n",
      "\n",
      "Feature Request: Use hwloc to query CPU topologies and support thread/memory binding for improved performance\n",
      "\n",
      "depthwise_conv2d_native too slow\n",
      "\n",
      "The channel dimension of the inputs is `None` when tf.layers.conv2d after tf.slice with tf.shape instead of constant value\n",
      "\n",
      "tf.control_dependencies fails to update the dependent op.\n",
      "\n",
      "[feature request] large scale embedding for sparse features\n",
      "\n",
      "tf.gfile.GFile deadlocks on HDFS access after a fork()\n",
      "\n",
      "[FR] Add warm_start_from in model_to_estimator\n",
      "\n",
      "Feature request: Mask R-CNN support on TensorFlow Lite\n",
      "\n",
      "Gradient Penalty won't execute\n",
      "\n",
      "PosixFileSystem::CreateDir should create directory respecting umask\n",
      "\n",
      "[Eager] Fix for determining input / output shape of the model prior to Model.fit()\n",
      "\n",
      "`foldl` disallows mixing different types of `elems`\n",
      "\n",
      "Difference in output between CPU and GPU \n",
      "\n",
      "Feature request: tf.pad to pad an image with different values correspond to different channels respectively\n",
      "\n",
      "tf.control_dependencies() not work in a custom_gradient function\n",
      "\n",
      "[XLA] input data type support for DT_STRING?\n",
      "\n",
      "precision and recall values kept unchanged for some training steps.\n",
      "\n",
      "build with python3 binary uses wrong path to find Python.h (fix inside)\n",
      "\n",
      "Windows: Adding a new op\n",
      "\n",
      "Cannot build tensorflow on arm with the latest bazel. _pywrap_tensorflow_internal.so: undefined symbol: _ZN10tensorflow9ConcatCPUINS_8bfloat16EEEvPNS_10DeviceBaseERKSt6vectorISt10unique_ptrINS_6TTypesIT_Li2EiE11ConstMatrixESt14default_deleteIS9_EESaISC_EEPNS8_6MatrixE\n",
      "\n",
      "Set the weights in tf.layers with other variables but not as initializers.\n",
      "\n",
      "tensorflow.nn.dynamic_rnn with variable as init_state cannot work with Estimator.train\n",
      "\n",
      "Android tensorflow repository weird permissions\n",
      "\n",
      "Memory Leak when tf.Session run on the sliced tensor\n",
      "\n",
      "Saving Estimators, loading from checkpoints and accessing placeholders.\n",
      "\n",
      "Tensorflow r1.8  ImportError: dlopen: cannot load any more object with static TLS\n",
      "\n",
      "bug of tf.extract_image_patches\n",
      "\n",
      "Patch Request: Move CROSSTOOL_nvcc.tpl to c++14\n",
      "\n",
      "CMake build without GRPC and Python bindings fails\n",
      "\n",
      "Use ghost batch normalization with slim\n",
      "\n",
      "Feature Request: More Granular Dependencies for Official Binaries\n",
      "\n",
      "[Feature Request] Fold batch_norm with depthwise_conv transformation graph\n",
      "\n",
      "CUPTI events are missing from tf.train.Server\n",
      "\n",
      "output 'tensorflow/core/kernels/_objs/gather_functor_gpu/tensorflow/core/kernels/gather_functor_gpu.cu.o' was not created\n",
      "\n",
      "Variables in tf.contrib.autograph\n",
      "\n",
      "Support in the Dataset API for sharding dataset used in stateful LSTMs\n",
      "\n",
      "OpKernel not registered, despite being listed in OpRegistry\n",
      "\n",
      "Default arguments of tensorflow.contrib.signal.inverse_stft do not invert tensorflow.contrib.signal.stft\n",
      "\n",
      "Bad error message: InvalidArgumentError with no further info (besides Invalid argument)\n",
      "\n",
      "Feature Request: Gradients for angles in tf.contrib.image.rotate()\n",
      "\n",
      "build_all_android.sh x86 abi build issue\n",
      "\n",
      "[Docs]Extend graph_viz doc\n",
      "\n",
      "Suggestion for efficient upsample+conv2d and conv2d+pool\n",
      "\n",
      "InvalidArgumentError is raised when restoring large (>2GB) variable on macOS\n",
      "\n",
      "Strange result of float division\n",
      "\n",
      "saved_model.pb, saved_model.pbtxt missing google cloud\n",
      "\n",
      "Slow matrix multiplication using Tensorflow 1.7.0 on a GPU\n",
      "\n",
      "crf_log_likelihood become 2x slower after upgrade TensorFlow from 1.4 to 1.7\n",
      "\n",
      "Remove Python dep on 'enum'\n",
      "\n",
      "A bug related to conv2d_transpose and tf.cond\n",
      "\n",
      "Reopening #9294 error in RNN tutorial\n",
      "\n",
      "Feature Request: Slice replacement operation\n",
      "\n",
      "Include C and C++ APIs with binary distributions\n",
      "\n",
      "SeparableConv2D from tf.keras.layers and tf.layers gives different results\n",
      "\n",
      "The LSTM does not generate reproducible results, but GRU does\n",
      "\n",
      "[Feature request] unsquashing unsorted_segment_x\n",
      "\n",
      "Tensorflow-gpu performance drop\n",
      "\n",
      "[feature] js_func (for javascript) equivalent of py_func\n",
      "\n",
      "tf.while example is not working in eager mode\n",
      "\n",
      "can tensorflow/compiler/aot/libruntime.so be renamed?\n",
      "\n",
      "TensorFlow verbose logging is too verbose even at level 1\n",
      "\n",
      "ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\n",
      "\n",
      "Feature Request:  OutputProjectionWrapper compatible with tf.nn.bidirectional_dynamic_rnn\n",
      "\n",
      "Request a new padding mode\n",
      "\n",
      "pip install tensorflow in anaconda,can't use it in pycharm\n",
      "\n",
      "C++ Const and Assign to initialize variable causes a segfault depending on the Const constructor used\n",
      "\n",
      "Why is tf.images.resize_bicubic different from misc.imresize with bicubic method\n",
      "\n",
      "Feature Request: Support for configuring deterministic options of cudNN Conv routines\n",
      "\n",
      "Distributed TensorFlow got error message with the MPI collective Ops units test\n",
      "\n",
      "No equivalent to theano.tensor.slinalg.Eigvalsh\n",
      "\n",
      "Feature request : warning for feeding unused values\n",
      "\n",
      "Large multinomial sampling on GPU causes OOM\n",
      "\n",
      "tf.sparse_tensor_dense_matmul makes small errors with tf.float32 matrices on GPU\n",
      "\n",
      "tf.igamma (lower regularized incomplete Gamma function) returns the incorrect derivative\n",
      "\n",
      "tf.einsum doesn't perform common subgraph elimination\n",
      "\n",
      "Tensorflow not working properly in Python sub-interperters\n",
      "\n",
      "QuantizedConv2D dimension mismatch\n",
      "\n",
      "tensorflow process hangs with use of cudnn_rnn\n",
      "\n",
      "TensorArray does not work inside `else` clause of `tf.cond`\n",
      "\n",
      "gRPC debug URL scheme support for Windows\n",
      "\n",
      "Memory Leak in SavedModelBundle.load() in the TensorFlow Java API\n",
      "\n",
      "Timeline Logging Duplicates of Operations\n",
      "\n",
      "Failed assert in the TF native code kills JVM\n",
      "\n",
      "`Datasets` sometimes resamples stochastic Tensors during multiple transformations\n",
      "\n",
      "[Feature Request] Multiple GPU Training using Eager Execution\n",
      "\n",
      "Dropout training placeholder fails in tf.while_loop \n",
      "\n",
      "Create/Init a curl handler each time it's expensive. \n",
      "\n",
      "tf.contrib.estimator.add_metrics does not pass label_ids to tf.estimator.DNNClassifier evaluation\n",
      "\n",
      "Segmentation fault in Eigen::internal::InnerMostDimReducer<...>::reduce when passing large tensor to sparse_softmax_cross_entropy_with_logits\n",
      "\n",
      "Bad_alloc when building standalone project in Debug\n",
      "\n",
      "tf.contrib.crf.crf_decode fails when sequence_length is 0\n",
      "\n",
      "Centered padded batch on tf.data.dataset with image-features and bounding boxes\n",
      "\n",
      "tf 1.4 and tf1.5 and tf1.6 \n",
      "\n",
      "Allow tf.estimator.train_and_evaluate evaluation frequency in steps\n",
      "\n",
      "iOS error: Running model failed:Invalid argument: NodeDef mentions attr 'dilations' not in Op<name=Conv2D; signature=input:T, filter:T ->\n",
      "\n",
      "Not found: FeedInputs: unable to find feed output \n",
      "\n",
      "Tensorflow failed when build with MSVC + /permissive-\n",
      "\n",
      "tensorflow macOS build failed\n",
      "\n",
      "Feature request: tf.data.Dataset.unordered_merge()\n",
      "\n",
      "Could the periodic resample operation be accelerated?\n",
      "\n",
      "embed_sequence and embedding_lookup behave differently on CPU vs. GPU\n",
      "\n",
      "[suggestion][tf.estimator] model with multiple labels\n",
      "\n",
      "Error indices[0] = 0 is not in [0, 0) while training an object-detection model with tensorflow\n",
      "\n",
      "ABI for `tensorflow::core::RefCounted` is error-prone\n",
      "\n",
      "Unable to get FLOPs on model with tf.nn.bidirectional_dynamic_rnn\n",
      "\n",
      "CMP0002 error when building TensorFlow cc unit tests\n",
      "\n",
      "'num' must be either a scalar or vector in tf.unstack?\n",
      "\n",
      "Modify distributed TF examples to take kubeflow's TF_CONFIG as well as command line arguments.\n",
      "\n",
      "tf.contrib.layers output_collections inconsistency\n",
      "\n",
      "Problem with Keras sparse_categorical_crossentropy\n",
      "\n",
      "Feature Request: Make beam_search_decoder use multiple CPU threads.\n",
      "\n",
      "Feature request: complex support in initializers\n",
      "\n",
      "Sample for report_tensor_allocations_upon_oom and RunOptions\n",
      "\n",
      "Allocating C++ types instead of Tensors in a new Op - Feature Request\n",
      "\n",
      "tf.reduce_min([inf, nan]).eval() == inf, should be nan\n",
      "\n",
      "seg fault in 1.6rc0 and master on skylake cpu (avx512 related probably)\n",
      "\n",
      "Feature/PR Idea - mean IoU for vector of thresholds\n",
      "\n",
      "LNK2019\tunresolved external symbol __std_reverse_trivially_swappable_8  when compiling proto_text.vcxproj\n",
      "\n",
      "Failed to call cudnnRNNBackwardData: CUDNN_STATUS_INTERNAL_ERROR\n",
      "\n",
      "Tensorflow (0.11.0) returns error while running training and testing scripts. \n",
      "\n",
      "libhdfs3 instead of libhdfs\n",
      "\n",
      "Problems Getting TensorFlow to behave Deterministically\n",
      "\n",
      "tfcompile fails with Executor failed to create kernel. Not found: No registered 'ResizeNearestNeighbor' \n",
      "\n",
      "CPU execution of ops after gradient clipping on windows\n",
      "\n",
      "Loading model from local in RNN prediction is slower than from HDFS due to page fault \n",
      "\n",
      "concurrent.futures for Python 2 installed with tensorflow-gpu with pip3 \n",
      "\n",
      "Feature request: Enable validate_args for all distributions based on a global parameter\n",
      "\n",
      "Optimized einsum\n",
      "\n",
      "Cross-Compiling the TensorFlow wheel for NVIDIA Jetson with CUDA support\n",
      "\n",
      "XLA with frozen protobuf: Tuples do not have a rank error\n",
      "\n",
      "core dump is occured using reduce_sum\n",
      "\n",
      "CUDA Fail in Tensorflow Inference on Jetson TX2\n",
      "\n",
      "Feature request: Support SparseTensor in Dataset.from_generator \n",
      "\n",
      "ImportError after compiling\n",
      "\n",
      "How to get RunMetadata for tf.data.Dataset ops?\n",
      "\n",
      "contrib.tfgan: batch_norm is_training=True for both training and inferencing, non-slim version\n",
      "\n",
      "Feature deprecated in h5py is used in TF1.5\n",
      "\n",
      "Feature request: tf.estimator hyperparameter tuning\n",
      "\n",
      "Feature request: tf.Print should either print (not log), or accept a log level\n",
      "\n",
      "./configure [--help|-h] does not work\n",
      "\n",
      "[Feature request] Adding a PR curves to canned estimators for (binary) classifiers\n",
      "\n",
      "bug with frame_step in tf.contrib.signal overlap_and_add inverse_stft\n",
      "\n",
      "Distributed TensorFlow without shared directory\n",
      "\n",
      "Include grpc_tensorflow_std_server in Docker image\n",
      "\n",
      "Warning: Table trying to initialize from file ... is already initialized\n",
      "\n",
      "Segmentation fault in _pywrap_tensorflow_internal.so\n",
      "\n",
      "Linking against system-installed cuda and cudnn\n",
      "\n",
      "Include netstat in the tensorflow docker container\n",
      "\n",
      "Graph Transform Tool unable to build in TF source r1.5?\n",
      "\n",
      "Faster R-CNN: too many resources requested for launch\n",
      "\n",
      "Segmentation fault when running optimization step with 3d convolution\n",
      "\n",
      "Feature request: Allow the build to use the system-installed protobuf lib\n",
      "\n",
      "  tf.estimator.train_and_evaluate list  of eval_spec\n",
      "\n",
      "Lack of Complex64 support for Java API\n",
      "\n",
      "Optimzer: Better handling of gradients for min/max ops.\n",
      "\n",
      "Allow tensorflow/tensorflow/workspace.bzl to customize dependencies\n",
      "\n",
      "[Tracking bug] Building Tensorflow with Clang on Windows\n",
      "\n",
      "cmake CUDA include-path whitespaces not supported\n",
      "\n",
      "Crash when using CUDA API while using Tensorflow. \"current context was not created by the StreamExecutor cuda_driver API\"\n",
      "\n",
      "Feature Request: Sparse Cholesky decomposition\n",
      "\n",
      "Allow full deallocation of GPU memory\n",
      "\n",
      "tensorflow/go for Windows?\n",
      "\n",
      "Reinitializing an iterator throws an OutOfRangeError when using a MonitoredSession with NanTensorHook\n",
      "\n",
      "Numerous ::`anonymous namespace':: Linking errors in Tensorflow 1.3.1 Windows build with GPU\n",
      "\n",
      "Unable to convert LSTM model to .tflite model\n",
      "\n",
      "Feature Request: tf.train.MonitoredTrainingSession implementation for slim.learning.train\n",
      "\n",
      "CUDA crashes during Cholesky_grad procedure\n",
      "\n",
      "Flexible input shape for map method in class RandomFourierFeatureMapper\n",
      "\n",
      "Out of range: End of sequence\n",
      "\n",
      "Image Adjustments API doesn't clearly specify input range\n",
      "\n",
      "Latency of simple tf.data.Dataset transformations is higher than raw Python\n",
      "\n",
      "mkl_cpu_allocator.h is not compiled under windows anymoe\n",
      "\n",
      "[Feature request] define axis in 'tf.unique()' and 'tf.unique_with_counts'\n",
      "\n",
      "Problem with tf.data.Dataset managing shapes of sparse tensors\n",
      "\n",
      "Tensorflow doesn't show the Cuda import messages \n",
      "\n",
      "tensorflow::gtl::string_as_array crashes on Windows\n",
      "\n",
      "Cannot run run_cc_test_windows.bat: command line too long\n",
      "\n",
      "Bug?: reading from Google Cloud Storage appears to be accessing cached version\n",
      "\n",
      "No gradient defined for operation 'MatrixExponential' (op type: MatrixExponential)\n",
      "\n",
      "[cmake] CPU only build error in tf_stream_executor.cmake\n",
      "\n",
      "tensorflow-aarch64 with keras\n",
      "\n",
      "Bug: reshape shape inference for parital defined shape\n",
      "\n",
      "CMake OBJECT library Xcode problems\n",
      "\n",
      "Allow tf.Estimator.evaluate() to return summary protos / add tooling to produce useful eval image summaries\n",
      "\n",
      "Add full cmake support for Android builds\n",
      "\n",
      "Encrypted Data\n",
      "\n",
      "Feature request: provide a means to configure, build, and install that includes cc\n",
      "\n",
      "tf.while_loop and tf.foldl do not support second order gradients\n",
      "\n",
      "tf.profiler overrides shape_invariants in while_loop\n",
      "\n",
      "XLA/AOT Windows support\n",
      "\n",
      "contrib STFT magnitudes different to librosa's\n",
      "\n",
      "CUDA_ERROR_LAUNCH_FAILED with TensorFlow example\n",
      "\n",
      "NadamOptimizer does not work with sparse gradients\n",
      "\n",
      "ctc_loss with best align path\n",
      "\n",
      "Extend reshape with begin_axis and end_axis like in cntk\n",
      "\n",
      "Bug: tf.estimator.Estimator.export_savedmodel does not work with pathlib.Path in py36\n",
      "\n",
      "Documentation of tf.nn.raw_rnn is confusing\n",
      "\n",
      "Multi-arch docker images\n",
      "\n",
      "What is supported by broadcasting? Is the dimensions still limited?\n",
      "\n",
      "avg_pool ignores channel stride dimension, but max_pool does not\n",
      "\n",
      "Extend estimators.md to cover Keras\n",
      "\n",
      "Got NAN when calling embedding_lookup_sparse with weights and \"mean\" combiner\n",
      "\n",
      "Does TF provide C++ interface corresponding to the Python ops \"tensorflow.nn.ctc_greedy_decoder\"?\n",
      "\n",
      "tf.set_random_seed doesn't work after any operations have been constructed\n",
      "\n",
      "Feature Request : Hierarchical Softmax implementation using Tensorflow\n",
      "\n",
      "Small change to graph changes initial values of variables\n",
      "\n",
      "DropoutWrapper and dynamic_rnn with parallel iterations not reproducible\n",
      "\n",
      "Feature request: tf.nn.ctc_loss lacks the API to handle sequences with all blanks\n",
      "\n",
      "Contradicting behaviour in variations of tf.cond usage with tf.nn.static_state_saving_rnn\n",
      "\n",
      "Feature request for making dynamic gradient clipping \n",
      "\n",
      "DataSet user provided shuffled order\n",
      "\n",
      "XLA reports error with 1000 steps of static_bidirectional_rnn\n",
      "\n",
      "sess.run hangs forever despite operation_timeout_in_ms being set\n",
      "\n",
      "[Fetaure Request] Bicubic interpolation using map coordinates\n",
      "\n",
      "GPU support for sparse_dense_binary_op_shared.cc\n",
      "\n",
      "error when I try mpi_collectives features\n",
      "\n",
      "Support Windows builds through clang\n",
      "\n",
      "configure.py should remember previous session as defaults\n",
      "\n",
      "graph_editor doesn't update graph_version\n",
      "\n",
      "bayeslfow.hmc - provide option for skipping the MH step\n",
      "\n",
      "TensorFlow r1.4 does not be compiled from 32bit environment.\n",
      "\n",
      "Make CMake on TensorFlow Incrementally Compile on Windows\n",
      "\n",
      "TF 1.4 build_all_android.sh fails with nsync.a\n",
      "\n",
      "Equivalent of caffe iter_size in TF\n",
      "\n",
      "Eager: Random seeds\n",
      "\n",
      "Feature Request: C++ gradient for Cast\n",
      "\n",
      "Momentum SGD is very slow with large embedding layer\n",
      "\n",
      "[feature] \"Periodic\" mode for tf.pad\n",
      "\n",
      "compile tensorflow-1.4.0-rc1 failed with error: SWIGing tensorflow/python/tensorflow.i failed (Segmentation fault): swig failed: error executing command\n",
      "\n",
      "tfdbg ps -b command does not work on Windows\n",
      "\n",
      "Fused batch norm can be folded with atrous conv2d \n",
      "\n",
      "Tensorflow import fails with Segmentation fault error\n",
      "\n",
      "Build from source issue (CUDA 7.5)\n",
      "\n",
      "conv2d_transpose crashing, \"NotFoundError: No algorithm worked!\", only with batch size >=2^16\n",
      "\n",
      "Fail in configuration due to different CUDA libraries path\n",
      "\n",
      "Feature Request: use S3 for checkpoint loading/saving\n",
      "\n",
      "AV in nvcuda on Win10 amd64\n",
      "\n",
      "Add support for Mel Generalized Cepstrum Analysis to tf.signal.\n",
      "\n",
      "Extend SVD gradient to support backpropagating through complex and (strongly) rectangular U and V\n",
      "\n",
      "HDFS user impersonation\n",
      "\n",
      "Building custom op instructions out of date\n",
      "\n",
      "SVD on GPU is slower than SVD on CPU\n",
      "\n",
      "Feature request: segment_argmax\n",
      "\n",
      "Sin family identities for y=x yield bad gradients\n",
      "\n",
      "//tensorflow/contrib/android:libtensorflow_inference.so build fails when compiling @protobuf//:protobuf\n",
      "\n",
      "Feature request: tf.reduce_median()\n",
      "\n",
      "Auto-Parallel excludes update operators of sparse tensors\n",
      "\n",
      "Attempting to use the CPU Work Sharder segfaults on g++ 5.4.0\n",
      "\n",
      "Pre-built binaries with symbol information?\n",
      "\n",
      "TensorFlow 1.0 and 1.2 behave differently on MultiRNNCell.\n",
      "\n",
      "Linking of rule '//tensorflow/contrib/factorization:gen_gen_clustering_ops_py_wrappers_cc' failed (missing -lcuda?)\n",
      "\n",
      "SVD in TensorFlow is slower than in numpy\n",
      "\n",
      "installing tensorflow to a local folder results in import error with realtive paths\n",
      "\n",
      "tensorflow.python.debug.cli.offline_analyzer failed to read debug data from HDFS filesys\n",
      "\n",
      "No OpKernel was registered to support Op 'QuantizeV2'\n",
      "\n",
      "Variance update in tf.contrib.layer.batch_norm\n",
      "\n",
      "Proposal: Making the cmake build distribution friendly\n",
      "\n",
      "Reading tfrecord reaches deadock or crushes in one computer and works just fine on another.\n",
      "\n",
      "tf.Session() freezes on GPU nodes of a SGE cluster\n",
      "\n",
      "tf.extract_image_patches gradient transpose extremely slow\n",
      "\n",
      "Distributed variable initialization never reaches some nodes (affects MonitoredTrainingSession too)\n",
      "\n",
      "Feature Request / Workaround for Variable size multi-label candidate sampling in TensorFlow.\n",
      "\n",
      "Feature request: Google Authentication support for OAuth2 AccessTokenCredentials\n",
      "\n",
      "get_session_handle has no effect if not directly fetched\n",
      "\n",
      "[feature] ONNX Support\n",
      "\n",
      "Sub-gradient for self_adjoin_eig when eigen values are equal \n",
      "\n",
      "Dataset Unzip Operation\n",
      "\n",
      "Error message of scatter_update is misleading \n",
      "\n",
      "User-defined functions loaders\n",
      "\n",
      "Allow to build tensorflow as a bazel external dependency.\n",
      "\n",
      "Feature Request: callback argument for tf.contrib.data.Dataset.ignore_errors() to enable error logging\n",
      "\n",
      "tf.maximum does not return nan when inputs contain nan\n",
      "\n",
      "Feature request: QueueRunner for C++ API that initializes from queues and ops and start with ClientSession\n",
      "\n",
      "[building tensorflow with bazel ] Error:C++ compilation of rule '@boringssl//:crypto' failed.\n",
      "\n",
      "No gradient for `cdf`, `sample` and other functions for several distributions in `tf.distributions`\n",
      "\n",
      "FPGA Implementation on TensorFlow\n",
      "\n",
      "Feature request: stop requiring the same dtype for inputs in tf.shape_n\n",
      "\n",
      "Feature request: sparse_tensor_dense_matmul optimization on GPU\n",
      "\n",
      "[bug] gradients of scatter_nd_add return None \n",
      "\n",
      "PEP 484 Type Annotations (feature request)\n",
      "\n",
      "memory leak bug\n",
      "\n",
      "Simple EditDistance constructor is missing in C++\n",
      "\n",
      "Why not develop  32-bit？Thank you very much!\n",
      "\n",
      "build_all_xxx.sh support for Tizen target\n",
      "\n",
      "Feature request: document which inputs have gradients\n",
      "\n",
      "slim.separable_conv2d is too slow\n",
      "\n",
      "Occur error when compile tf_core_gpu_kernels/generated_adjust_hue_op_gpu.cu.cc file in VS2015\n",
      "\n",
      "Numerical instability of gradient calculation of tf.norm (nan at 0, inf for small values) \n",
      "\n",
      "tf.scatter_update to variable pinned on GPU fails\n",
      "\n",
      "foldr is too restrictive:  dimension 0 in both shapes must be equal\n",
      "\n",
      "Expose Tensorflow Go library as cgo_library rule in Bazel\n",
      "\n",
      "Feature Request: Add separable_conv2d_transpose operation\n",
      "\n",
      "Dilated convolution does not preserve tensor shape\n",
      "\n",
      "tensorflow-1.2.1 import tensorflow Segmentation fault\n",
      "\n",
      "Fractional 3d Max Pooling\n",
      "\n",
      "Snappy related tests are failing\n",
      "\n",
      "Feature request: equivalent of tf.nn.maxpooling_with_argmax for 3D maxpooling\n",
      "\n",
      "Infinity mask breaks gradient\n",
      "\n",
      "Distributed Tensorflow Authorization\n",
      "\n",
      "Unable to compile a quantized graph using XLA AOT?\n",
      "\n",
      "tfcompile won't work with graph that has no inputs\n",
      "\n",
      "Feature Request: sparse matrix triangular solver\n",
      "\n",
      "Feature Request: Change REGISTER_OP macro to facilitate customization on static initialization sequence\n",
      "\n",
      "Catch keyboard interrupt in session run\n",
      "\n",
      " [Feature] Node mirroring for GPU-memory reduction\n",
      "\n",
      "Self-contained source code package of tensorflow\n",
      "\n",
      "MultiRNNCell cannot stack PhasedLSTMCell\n",
      "\n",
      "Tensorflow - Metal Support for Mac OS\n",
      "\n",
      "terminate called after throwing an instance of 'std::out_of_range' error when call made to tf.contrib.tensor_forest.random_forest.TensorForestEstimator.predict()\n",
      "\n",
      "There should be tf.fill_like\n",
      "\n",
      "Upgrade to jemalloc 5.0.0\n",
      "\n",
      "Constant operator is not appropriate for variable initialize\n",
      "\n",
      "Not possible to use tf.contrib.training.stratified_sample with a SparseTensor\n",
      "\n",
      "Feature Request-Randomized Hashing\n",
      "\n",
      "[Feature Request] Exclude ties in function in_top_k\n",
      "\n",
      "recovery_wait_secs feature for tf.train.MonitoredTrainingSession() similar to the one present in tf.train.SessionManager()\n",
      "\n",
      "tf.estimator generator_input_fn multi thread bug \n",
      "\n",
      "Possible bug: LSTMCell with use_peephole=True breaks when using initializer=tf.orthogonal_initializer\n",
      "\n",
      "Feature request: Update OpDef proto to ease 1-based indexing\n",
      "\n",
      "Performance degradation with large lookup tables - optimizer._apply_sparse_duplicate_indices  (TF V1.0.1)\n",
      "\n",
      "Error on importing metagraph that uses unbound input multiple times\n",
      "\n",
      "Use freeze_graph only with an input checkpoint\n",
      "\n",
      "compilation errors due to missing op classes when using selective registration (cmake windows build)\n",
      "\n",
      "Cannot build jemalloc support using CMake on Linux (fails trying to include <windows.h>)\n",
      "\n",
      "In reader.py, line 17 throws error on python 3.5. The decode phrase should be removed, and then it works\n",
      "\n",
      "Convolution_transpose layer now gives an error (Tensorflow 1.0.0). \n",
      "\n",
      "graph_editor.copy_with_input_replacements crashes for some orderings of inputs\n",
      "\n",
      "ValueError: Refusing to perform an overparameterized separable convolution\n",
      "\n",
      "Would you please accomodate for building tensorflow with a custom clang (4.0.0) and libc++ instead of stdlibc++? \n",
      "\n",
      "graph_editor copy_with_input_replacements doesn't update colocation constraints\n",
      "\n",
      "preserving specific checkpoints\n",
      "\n",
      "Feature request: tf.nn.depthwise_conv2d_transpose\n",
      "\n",
      "tf.self_adjoint_eig doesn't behave the same way with float32 and float64\n",
      "\n",
      "tf.gradients runtime scales suboptimally with size of the graph\n",
      "\n",
      "Chrome timeline for Keras?\n",
      "\n",
      "Optimizers in the C++ API\n",
      "\n",
      "atrous_conv2d does not support NCHW format\n",
      "\n",
      "Run half-precision models on Android\n",
      "\n",
      "[feature] Support Cross Compiling with tfcompile\n",
      "\n",
      "Gather/Slice/StridedSlice Gradients Support in the C++ API\n",
      "\n",
      "Ops for Reading from Cloud Spanner\n",
      "\n",
      "Using replace to evaluate multiple gradients during training in Keras\n",
      "\n",
      "[Java] Distributed mode support\n",
      "\n",
      "PreventGradients in SoftmaxCrossEntropyWithLogit ops\n",
      "\n",
      "[feature] Mobile Integration with NNPACK\n",
      "\n",
      "xorshift128+ version of (stateless) random ops\n",
      "\n",
      "Sampling from a categorical distribution without replacement\n",
      "\n",
      "Create support for a score threshold in NonMaxSuppression to skip over boxes with low score\n",
      "\n",
      "Feature: Sparse matrix multiplications for Tensors with rank > 2\n",
      "\n",
      "Tensorflow Still Trying to use CUDA even when Session Created with device_count={'GPU': 0}\n",
      "\n",
      "Make `py_func` accept `SparseTensor`\n",
      "\n",
      "C API Tensors\n",
      "\n",
      "Make bounding box operators consistent\n",
      "\n",
      "Image Distortions should be able to be applied to batches. [Feature Request?]\n",
      "\n",
      "8-bit quantized atrous conv2d op not supported\n",
      "\n",
      "Feature Request: Accelerate TensorFlow core on FPGA - How?\n",
      "\n",
      "Can I use tensorflow on Windows10 with c++ and gpu support\n",
      "\n",
      "graph_editor.graph_replace produces WARNING\n",
      "\n",
      "TF for Xeon Phi\n",
      "\n",
      "Better control of logging verbosity\n",
      "\n",
      "non_max_suppression should support batches\n",
      "\n",
      "XLA segfaults with large graphs\n",
      "\n",
      "Taking gradients after using SparseTensor in while_loop leads to TypeError\n",
      "\n",
      "TensorFlow equivalent to numpy.repeat\n",
      "\n",
      "[feature] Smarter Handling of Image Data Format\n",
      "\n",
      "Support consistent data_format between tf.layers and everything else\n",
      "\n",
      "Support MPSCNN (MetalPerformanceShaders) on iOS\n",
      "\n",
      "Method log_prob_with_logits() for Dirichlet\n",
      "\n",
      "[Feature Request] Predict posterior probability of data per each component in GMM \n",
      "\n",
      "Returning argmax with tf.nn.pool\n",
      "\n",
      "TensorFlow builds LLVM even if ./configured without XLA\n",
      "\n",
      "dynamic_rnn_decoder returns shape [?, batch_size, cell.output_size]\n",
      "\n",
      "Add a dynamic_partial_sum operator to tensorflow?\n",
      "\n",
      "'LookupError: No gradient defined for operation '...' (op type: ResizeBicubic)' is raised\n",
      "\n",
      "[feature requests] DecodeCSVOP to parse only the first len(record_defaults) columns of a csv\n",
      "\n",
      "non_max_suppression is very slow and doesn't appear to have a cuda or multi-threaded implementation\n",
      "\n",
      "preprocessor definition clash with glog\n",
      "\n",
      "Feature: Add reduce_average (weighted reduce_mean)\n",
      "\n",
      "`tf.dynamic_stitch` gradient is incorrect\n",
      "\n",
      "Packet16q16i does not name a type\n",
      "\n",
      "Build with --define tensorflow_xsmm=1 fails on MacOS Sierra\n",
      "\n",
      "support for depth pooling in maxpool3d?\n",
      "\n",
      "tf.signal CPU FFT implementation is slower than NumPy, PyTorch, etc.\n",
      "\n",
      "Feature Request: Gradient for QR op\n",
      "\n",
      "Saver can't handle filename only \n",
      "\n",
      "Poor colors for embedding projector\n",
      "\n",
      "Allow using raw values for Projector?\n",
      "\n",
      "Feature request: GPU ops for tf.unique; tf.where; and tf.dynamic_partition \n",
      "\n",
      "einsum with ellipses \"...\" (indefinite number of axes)\n",
      "\n",
      "CuSPARSE support\n",
      "\n",
      "Keeping gradient of sqrt(x) stable for x = 0\n",
      "\n",
      "[Feature Request] Make streaming metrics resettable\n",
      "\n",
      "einsum not fully implemented\n",
      "\n",
      "Implement NumPy style boolean indexing\n",
      "\n",
      "Implement advanced indexing (and mixed basic/advanced)\n",
      "\n",
      "Multi-CPU kernel for sparse_tensor_dense_matmul\n",
      "\n",
      "tf.gather produces zeros for invalid indices on GPU\n",
      "\n",
      "Use Basic neural network subroutines (BNNS) on iOS\n",
      "\n",
      "Numerically stable summation methods\n",
      "\n",
      "Default for tf.nn.conv2d_transpose output_shape\n",
      "\n",
      "[clang+CUDA] No ZeroesLike[DT_BOOL] kernel\n",
      "\n",
      "Feature request: other types of padding besides zero-padding.\n",
      "\n",
      "Warn when calling session.run() on a very large scatter\n",
      "\n",
      "Make TensorFlow compatible with PyPy\n",
      "\n",
      "OpenCL support\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(data)):\n",
    "    print(data[i][0])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4029\n"
     ]
    }
   ],
   "source": [
    "issue_type=[]\n",
    "for i in range(len(labels_list)):\n",
    "    lb=labels_list[i]\n",
    "    l=len(lb)\n",
    "    if(l>0):\n",
    "        if(lb[l-1]=='type:bug'):\n",
    "            issue_type=issue_type+[['bug']]\n",
    "        elif(lb[l-1]=='type:docs-bug'):\n",
    "            issue_type=issue_type+[['docs-bug']]\n",
    "        elif(lb[l-1]=='type:feature'):\n",
    "            issue_type=issue_type+[['feature']]\n",
    "        elif(lb[l-1]=='type:support'):\n",
    "            issue_type=issue_type+[['support']]\n",
    "        elif(lb[l-1]=='type:performance'):\n",
    "            issue_type=issue_type+[['performance']]\n",
    "        elif(lb[l-1]=='type:build/install'):\n",
    "            issue_type=issue_type+[['build/install']]\n",
    "        else:\n",
    "            issue_type=issue_type+[['unrecognized']]\n",
    "    else:\n",
    "        issue_type=issue_type+[['nolabel']]\n",
    "print(len(issue_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nolabel']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['support']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['support']\n",
      "['bug']\n",
      "['support']\n",
      "['build/install']\n",
      "['bug']\n",
      "['support']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['performance']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['support']\n",
      "['feature']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['support']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['build/install']\n",
      "['bug']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['feature']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['performance']\n",
      "['support']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['build/install']\n",
      "['support']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['build/install']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['feature']\n",
      "['support']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['support']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['performance']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['support']\n",
      "['build/install']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['support']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['performance']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['build/install']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['performance']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['build/install']\n",
      "['support']\n",
      "['feature']\n",
      "['build/install']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['build/install']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['support']\n",
      "['support']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['support']\n",
      "['feature']\n",
      "['support']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['performance']\n",
      "['feature']\n",
      "['feature']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['performance']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['support']\n",
      "['support']\n",
      "['support']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['performance']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['build/install']\n",
      "['support']\n",
      "['support']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['bug']\n",
      "['build/install']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['performance']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['docs-bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['support']\n",
      "['performance']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['performance']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['support']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['performance']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['support']\n",
      "['performance']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['performance']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['performance']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['performance']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['support']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['build/install']\n",
      "['performance']\n",
      "['support']\n",
      "['performance']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['build/install']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['support']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['build/install']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['support']\n",
      "['build/install']\n",
      "['feature']\n",
      "['feature']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['performance']\n",
      "['support']\n",
      "['support']\n",
      "['build/install']\n",
      "['support']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['performance']\n",
      "['performance']\n",
      "['feature']\n",
      "['support']\n",
      "['support']\n",
      "['performance']\n",
      "['support']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['bug']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['support']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['feature']\n",
      "['feature']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['support']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['performance']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['support']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['performance']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['performance']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['build/install']\n",
      "['support']\n",
      "['support']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['support']\n",
      "['bug']\n",
      "['build/install']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['performance']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['support']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['docs-bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['build/install']\n",
      "['support']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['build/install']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['build/install']\n",
      "['performance']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['performance']\n",
      "['feature']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['support']\n",
      "['docs-bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['build/install']\n",
      "['support']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['support']\n",
      "['performance']\n",
      "['feature']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['performance']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['performance']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['performance']\n",
      "['support']\n",
      "['support']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['performance']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['build/install']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['support']\n",
      "['build/install']\n",
      "['feature']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['support']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['performance']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['performance']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['support']\n",
      "['support']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['support']\n",
      "['feature']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['support']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['support']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['bug']\n",
      "['support']\n",
      "['support']\n",
      "['build/install']\n",
      "['support']\n",
      "['build/install']\n",
      "['support']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['docs-bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['performance']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['performance']\n",
      "['bug']\n",
      "['build/install']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['performance']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['build/install']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['support']\n",
      "['performance']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['performance']\n",
      "['feature']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['performance']\n",
      "['feature']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['bug']\n",
      "['build/install']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['performance']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['performance']\n",
      "['performance']\n",
      "['feature']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['performance']\n",
      "['feature']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['support']\n",
      "['performance']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['support']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['performance']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['performance']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['support']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['performance']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['feature']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['performance']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['performance']\n",
      "['docs-bug']\n",
      "['support']\n",
      "['performance']\n",
      "['performance']\n",
      "['support']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['performance']\n",
      "['build/install']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['feature']\n",
      "['performance']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['support']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['support']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['performance']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['feature']\n",
      "['build/install']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['performance']\n",
      "['build/install']\n",
      "['bug']\n",
      "['build/install']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['build/install']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['performance']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['nolabel']\n",
      "['performance']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['support']\n",
      "['nolabel']\n",
      "['performance']\n",
      "['bug']\n",
      "['build/install']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['feature']\n",
      "['support']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['performance']\n",
      "['performance']\n",
      "['bug']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['support']\n",
      "['build/install']\n",
      "['performance']\n",
      "['feature']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['build/install']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['support']\n",
      "['feature']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['docs-bug']\n",
      "['build/install']\n",
      "['nolabel']\n",
      "['docs-bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['build/install']\n",
      "['support']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['build/install']\n",
      "['bug']\n",
      "['support']\n",
      "['support']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['support']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['performance']\n",
      "['support']\n",
      "['build/install']\n",
      "['support']\n",
      "['support']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['performance']\n",
      "['support']\n",
      "['feature']\n",
      "['support']\n",
      "['support']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['support']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['support']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['support']\n",
      "['performance']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['performance']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['support']\n",
      "['performance']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['support']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['nolabel']\n",
      "['performance']\n",
      "['bug']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['build/install']\n",
      "['support']\n",
      "['docs-bug']\n",
      "['build/install']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['support']\n",
      "['bug']\n",
      "['build/install']\n",
      "['support']\n",
      "['feature']\n",
      "['support']\n",
      "['docs-bug']\n",
      "['build/install']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['build/install']\n",
      "['support']\n",
      "['feature']\n",
      "['support']\n",
      "['feature']\n",
      "['build/install']\n",
      "['docs-bug']\n",
      "['nolabel']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['support']\n",
      "['feature']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['support']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['support']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['build/install']\n",
      "['nolabel']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['support']\n",
      "['support']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['support']\n",
      "['support']\n",
      "['docs-bug']\n",
      "['build/install']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['docs-bug']\n",
      "['support']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['support']\n",
      "['support']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['support']\n",
      "['support']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['feature']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['support']\n",
      "['support']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['support']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['build/install']\n",
      "['docs-bug']\n",
      "['nolabel']\n",
      "['build/install']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['build/install']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['support']\n",
      "['support']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['support']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['build/install']\n",
      "['feature']\n",
      "['feature']\n",
      "['support']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['support']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bug']\n",
      "['docs-bug']\n",
      "['support']\n",
      "['build/install']\n",
      "['support']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['support']\n",
      "['build/install']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['build/install']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['docs-bug']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['support']\n",
      "['support']\n",
      "['docs-bug']\n",
      "['support']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['support']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['docs-bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['bug']\n",
      "['build/install']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['build/install']\n",
      "['docs-bug']\n",
      "['nolabel']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['build/install']\n",
      "['nolabel']\n",
      "['build/install']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['build/install']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['docs-bug']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['docs-bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['build/install']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['build/install']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['support']\n",
      "['support']\n",
      "['support']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['support']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['support']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['support']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['support']\n",
      "['support']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['docs-bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['docs-bug']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['support']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['build/install']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['feature']\n",
      "['support']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['build/install']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['build/install']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['build/install']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['build/install']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['build/install']\n",
      "['feature']\n",
      "['feature']\n",
      "['support']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['feature']\n",
      "['support']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(issue_type)):\n",
    "    print(issue_type[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4029\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "issue_type=[]\n",
    "for i in range(len(labels_list)):\n",
    "    lb=labels_list[i]\n",
    "    l=len(lb)\n",
    "    if(l>0):\n",
    "        if(re.search(\"type:bug\", lb[l-1])):\n",
    "            issue_type=issue_type+[['bug']]\n",
    "        elif(re.search(\"type:docs-bug\", lb[l-1])):\n",
    "            issue_type=issue_type+[['docs-bug']]\n",
    "        elif(re.search(\"type:feature\", lb[l-1])):\n",
    "            issue_type=issue_type+[['feature']]\n",
    "        elif(re.search(\"type:support\", lb[l-1])):\n",
    "            issue_type=issue_type+[['support']]\n",
    "        elif(re.search(\"type:performance\", lb[l-1])):\n",
    "            issue_type=issue_type+[['performance']]\n",
    "        elif(re.search(\"type:build/install\", lb[l-1])):\n",
    "            issue_type=issue_type+[['build/install']]\n",
    "        else:\n",
    "            issue_type=issue_type+[['unrecognized']]\n",
    "    else:\n",
    "        issue_type=issue_type+[['nolabel']]\n",
    "print(len(issue_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nolabel']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['support']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['support']\n",
      "['bug']\n",
      "['support']\n",
      "['build/install']\n",
      "['bug']\n",
      "['support']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['performance']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['support']\n",
      "['feature']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['support']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['build/install']\n",
      "['bug']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['feature']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['performance']\n",
      "['support']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['build/install']\n",
      "['support']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['build/install']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['feature']\n",
      "['support']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['support']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['performance']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['support']\n",
      "['build/install']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['support']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['performance']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['build/install']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['performance']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['build/install']\n",
      "['support']\n",
      "['feature']\n",
      "['build/install']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['build/install']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['support']\n",
      "['support']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['support']\n",
      "['feature']\n",
      "['support']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['performance']\n",
      "['feature']\n",
      "['feature']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['performance']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['support']\n",
      "['support']\n",
      "['support']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['performance']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['build/install']\n",
      "['support']\n",
      "['support']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['bug']\n",
      "['build/install']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['performance']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['docs-bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['support']\n",
      "['performance']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['performance']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['support']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['performance']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['support']\n",
      "['performance']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['performance']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['performance']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['performance']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['support']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['build/install']\n",
      "['performance']\n",
      "['support']\n",
      "['performance']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['build/install']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['support']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['build/install']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['support']\n",
      "['build/install']\n",
      "['feature']\n",
      "['feature']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['performance']\n",
      "['support']\n",
      "['support']\n",
      "['build/install']\n",
      "['support']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['performance']\n",
      "['performance']\n",
      "['feature']\n",
      "['support']\n",
      "['support']\n",
      "['performance']\n",
      "['support']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['bug']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['support']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['feature']\n",
      "['feature']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['support']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['performance']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['support']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['performance']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['performance']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['build/install']\n",
      "['support']\n",
      "['support']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['support']\n",
      "['bug']\n",
      "['build/install']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['performance']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['support']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['docs-bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['build/install']\n",
      "['support']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['build/install']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['build/install']\n",
      "['performance']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['performance']\n",
      "['feature']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['support']\n",
      "['docs-bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['build/install']\n",
      "['support']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['support']\n",
      "['performance']\n",
      "['feature']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['performance']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['performance']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['performance']\n",
      "['support']\n",
      "['support']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['performance']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['build/install']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['support']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['support']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['performance']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['performance']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['support']\n",
      "['support']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['support']\n",
      "['feature']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['support']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['support']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['bug']\n",
      "['support']\n",
      "['support']\n",
      "['build/install']\n",
      "['support']\n",
      "['build/install']\n",
      "['support']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['docs-bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['performance']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['performance']\n",
      "['bug']\n",
      "['build/install']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['performance']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['build/install']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['support']\n",
      "['performance']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['performance']\n",
      "['feature']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['performance']\n",
      "['feature']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['bug']\n",
      "['build/install']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['performance']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['performance']\n",
      "['performance']\n",
      "['feature']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['performance']\n",
      "['feature']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['support']\n",
      "['performance']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['support']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['performance']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['performance']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['support']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['performance']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['feature']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['performance']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['performance']\n",
      "['docs-bug']\n",
      "['support']\n",
      "['performance']\n",
      "['performance']\n",
      "['support']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['performance']\n",
      "['build/install']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['feature']\n",
      "['performance']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['support']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['support']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['performance']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['feature']\n",
      "['build/install']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['performance']\n",
      "['build/install']\n",
      "['bug']\n",
      "['build/install']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['build/install']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['performance']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['nolabel']\n",
      "['performance']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['support']\n",
      "['nolabel']\n",
      "['performance']\n",
      "['bug']\n",
      "['build/install']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['feature']\n",
      "['support']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['performance']\n",
      "['performance']\n",
      "['bug']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['support']\n",
      "['build/install']\n",
      "['performance']\n",
      "['feature']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['build/install']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['support']\n",
      "['feature']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['docs-bug']\n",
      "['build/install']\n",
      "['nolabel']\n",
      "['docs-bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['build/install']\n",
      "['support']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['build/install']\n",
      "['bug']\n",
      "['support']\n",
      "['support']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['support']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['performance']\n",
      "['support']\n",
      "['build/install']\n",
      "['support']\n",
      "['support']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['performance']\n",
      "['support']\n",
      "['feature']\n",
      "['support']\n",
      "['support']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['support']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['support']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['support']\n",
      "['performance']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['performance']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['support']\n",
      "['performance']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['support']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['nolabel']\n",
      "['performance']\n",
      "['bug']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['build/install']\n",
      "['support']\n",
      "['docs-bug']\n",
      "['build/install']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['support']\n",
      "['bug']\n",
      "['build/install']\n",
      "['support']\n",
      "['feature']\n",
      "['support']\n",
      "['docs-bug']\n",
      "['build/install']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['build/install']\n",
      "['support']\n",
      "['feature']\n",
      "['support']\n",
      "['feature']\n",
      "['build/install']\n",
      "['docs-bug']\n",
      "['nolabel']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['support']\n",
      "['feature']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['support']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['support']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['build/install']\n",
      "['nolabel']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['support']\n",
      "['support']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['support']\n",
      "['support']\n",
      "['docs-bug']\n",
      "['build/install']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['docs-bug']\n",
      "['support']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['support']\n",
      "['support']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['support']\n",
      "['support']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['feature']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['support']\n",
      "['support']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['support']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['build/install']\n",
      "['docs-bug']\n",
      "['nolabel']\n",
      "['build/install']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['build/install']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['support']\n",
      "['support']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['support']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['build/install']\n",
      "['feature']\n",
      "['feature']\n",
      "['support']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['support']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['support']\n",
      "['build/install']\n",
      "['support']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['support']\n",
      "['build/install']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['build/install']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['docs-bug']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['support']\n",
      "['support']\n",
      "['docs-bug']\n",
      "['support']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['support']\n",
      "['bug']\n",
      "['support']\n",
      "['support']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['docs-bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['bug']\n",
      "['build/install']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['build/install']\n",
      "['docs-bug']\n",
      "['nolabel']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['build/install']\n",
      "['nolabel']\n",
      "['build/install']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['build/install']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['docs-bug']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['docs-bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['build/install']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['build/install']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['support']\n",
      "['support']\n",
      "['support']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['support']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['support']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['support']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['support']\n",
      "['support']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['docs-bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['docs-bug']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['support']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['build/install']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['feature']\n",
      "['support']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['build/install']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['build/install']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['build/install']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['build/install']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['build/install']\n",
      "['feature']\n",
      "['feature']\n",
      "['support']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['feature']\n",
      "['support']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(issue_type)):\n",
    "    print(issue_type[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4029\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "issue_type=[]\n",
    "for i in range(len(labels_list)):\n",
    "    lb=labels_list[i]\n",
    "    l=len(lb)\n",
    "    if(l>0):\n",
    "        if(re.search(\"docs-bug\", lb[l-1])):\n",
    "            issue_type=issue_type+[['docs-bug']]\n",
    "        elif(re.search(\"bug\", lb[l-1])):\n",
    "            issue_type=issue_type+[['bug']]\n",
    "        elif(re.search(\"feature\", lb[l-1])):\n",
    "            issue_type=issue_type+[['feature']]\n",
    "        elif(re.search(\"support\", lb[l-1])):\n",
    "            issue_type=issue_type+[['support']]\n",
    "        elif(re.search(\"performance\", lb[l-1])):\n",
    "            issue_type=issue_type+[['performance']]\n",
    "        elif(re.search(\"build/install\", lb[l-1])):\n",
    "            issue_type=issue_type+[['build/install']]\n",
    "        else:\n",
    "            issue_type=issue_type+[['unrecognized']]\n",
    "    else:\n",
    "        issue_type=issue_type+[['nolabel']]\n",
    "print(len(issue_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nolabel']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['support']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['support']\n",
      "['bug']\n",
      "['support']\n",
      "['build/install']\n",
      "['bug']\n",
      "['support']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['performance']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['support']\n",
      "['feature']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['support']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['build/install']\n",
      "['bug']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['feature']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['performance']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['performance']\n",
      "['support']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['build/install']\n",
      "['support']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['build/install']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['feature']\n",
      "['support']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['support']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['performance']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['support']\n",
      "['build/install']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['support']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['performance']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['build/install']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['performance']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['build/install']\n",
      "['support']\n",
      "['feature']\n",
      "['build/install']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['build/install']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['support']\n",
      "['support']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['support']\n",
      "['feature']\n",
      "['support']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['performance']\n",
      "['feature']\n",
      "['feature']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['performance']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['support']\n",
      "['support']\n",
      "['support']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['performance']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['build/install']\n",
      "['support']\n",
      "['support']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['bug']\n",
      "['build/install']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['performance']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['support']\n",
      "['performance']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['docs-bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['support']\n",
      "['performance']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['performance']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['support']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['performance']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['support']\n",
      "['performance']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['performance']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['performance']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['feature']\n",
      "['support']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['performance']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['support']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['build/install']\n",
      "['performance']\n",
      "['support']\n",
      "['performance']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['build/install']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['support']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['build/install']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['support']\n",
      "['build/install']\n",
      "['feature']\n",
      "['feature']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['performance']\n",
      "['support']\n",
      "['support']\n",
      "['build/install']\n",
      "['support']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['performance']\n",
      "['performance']\n",
      "['feature']\n",
      "['support']\n",
      "['support']\n",
      "['performance']\n",
      "['support']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['bug']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['support']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['feature']\n",
      "['feature']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['performance']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['support']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['performance']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['performance']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['build/install']\n",
      "['support']\n",
      "['support']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['support']\n",
      "['bug']\n",
      "['build/install']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['performance']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['support']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['docs-bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['build/install']\n",
      "['support']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['build/install']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['build/install']\n",
      "['performance']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['performance']\n",
      "['feature']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['support']\n",
      "['docs-bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['build/install']\n",
      "['support']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['support']\n",
      "['performance']\n",
      "['feature']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['performance']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['performance']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['performance']\n",
      "['support']\n",
      "['support']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['performance']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['docs-bug']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['build/install']\n",
      "['bug']\n",
      "['build/install']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['support']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['support']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['performance']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['performance']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['support']\n",
      "['support']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['support']\n",
      "['feature']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['support']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['support']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['bug']\n",
      "['support']\n",
      "['support']\n",
      "['build/install']\n",
      "['support']\n",
      "['build/install']\n",
      "['support']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['docs-bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['performance']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['performance']\n",
      "['performance']\n",
      "['bug']\n",
      "['build/install']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['performance']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['build/install']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['support']\n",
      "['performance']\n",
      "['feature']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['performance']\n",
      "['feature']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['performance']\n",
      "['feature']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['bug']\n",
      "['build/install']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['performance']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['performance']\n",
      "['performance']\n",
      "['feature']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['performance']\n",
      "['feature']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['support']\n",
      "['performance']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['support']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['performance']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['performance']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['support']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['performance']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['feature']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['performance']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['performance']\n",
      "['docs-bug']\n",
      "['support']\n",
      "['performance']\n",
      "['performance']\n",
      "['support']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['performance']\n",
      "['build/install']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['feature']\n",
      "['performance']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['support']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['support']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['performance']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['feature']\n",
      "['build/install']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['performance']\n",
      "['build/install']\n",
      "['bug']\n",
      "['build/install']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['build/install']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['performance']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['nolabel']\n",
      "['performance']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['support']\n",
      "['nolabel']\n",
      "['performance']\n",
      "['bug']\n",
      "['build/install']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['feature']\n",
      "['support']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['performance']\n",
      "['performance']\n",
      "['bug']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['support']\n",
      "['build/install']\n",
      "['performance']\n",
      "['feature']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['build/install']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['support']\n",
      "['feature']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['performance']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['docs-bug']\n",
      "['build/install']\n",
      "['nolabel']\n",
      "['docs-bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['build/install']\n",
      "['support']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['performance']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['build/install']\n",
      "['bug']\n",
      "['support']\n",
      "['support']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['support']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['performance']\n",
      "['support']\n",
      "['build/install']\n",
      "['support']\n",
      "['support']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['performance']\n",
      "['support']\n",
      "['feature']\n",
      "['support']\n",
      "['support']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['support']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['support']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['support']\n",
      "['performance']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['performance']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['support']\n",
      "['performance']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['support']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['nolabel']\n",
      "['performance']\n",
      "['bug']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['build/install']\n",
      "['support']\n",
      "['docs-bug']\n",
      "['build/install']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['support']\n",
      "['feature']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['support']\n",
      "['bug']\n",
      "['build/install']\n",
      "['support']\n",
      "['feature']\n",
      "['support']\n",
      "['docs-bug']\n",
      "['build/install']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['build/install']\n",
      "['support']\n",
      "['feature']\n",
      "['support']\n",
      "['feature']\n",
      "['build/install']\n",
      "['docs-bug']\n",
      "['nolabel']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['support']\n",
      "['feature']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['support']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['support']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['build/install']\n",
      "['nolabel']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['support']\n",
      "['support']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['support']\n",
      "['support']\n",
      "['docs-bug']\n",
      "['build/install']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['docs-bug']\n",
      "['support']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['support']\n",
      "['support']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['support']\n",
      "['support']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['feature']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['support']\n",
      "['support']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['support']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['build/install']\n",
      "['docs-bug']\n",
      "['nolabel']\n",
      "['build/install']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['build/install']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['support']\n",
      "['support']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['support']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['build/install']\n",
      "['feature']\n",
      "['feature']\n",
      "['support']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['support']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['support']\n",
      "['build/install']\n",
      "['support']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['support']\n",
      "['build/install']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['build/install']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['docs-bug']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['support']\n",
      "['support']\n",
      "['docs-bug']\n",
      "['support']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['support']\n",
      "['bug']\n",
      "['support']\n",
      "['support']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['docs-bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['support']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['build/install']\n",
      "['nolabel']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['bug']\n",
      "['build/install']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['docs-bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['build/install']\n",
      "['docs-bug']\n",
      "['nolabel']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['build/install']\n",
      "['nolabel']\n",
      "['build/install']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['build/install']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['docs-bug']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['docs-bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['support']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['build/install']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['build/install']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['support']\n",
      "['support']\n",
      "['support']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['support']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['support']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['support']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['support']\n",
      "['support']\n",
      "['support']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['docs-bug']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['docs-bug']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['support']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['support']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['support']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['build/install']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['support']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['feature']\n",
      "['support']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['build/install']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['build/install']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['support']\n",
      "['nolabel']\n",
      "['support']\n",
      "['build/install']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['build/install']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['build/install']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['support']\n",
      "['support']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['support']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['support']\n",
      "['bug']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['build/install']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['nolabel']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['support']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['feature']\n",
      "['feature']\n",
      "['support']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['nolabel']\n",
      "['support']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['support']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['support']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['build/install']\n",
      "['feature']\n",
      "['feature']\n",
      "['support']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['nolabel']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['build/install']\n",
      "['bug']\n",
      "['bug']\n",
      "['bug']\n",
      "['docs-bug']\n",
      "['build/install']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['bug']\n",
      "['feature']\n",
      "['bug']\n",
      "['build/install']\n",
      "['unrecognized']\n",
      "['unrecognized']\n",
      "['performance']\n",
      "['feature']\n",
      "['support']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['docs-bug']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n",
      "['feature']\n",
      "['feature']\n",
      "['feature']\n",
      "['unrecognized']\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(issue_type)):\n",
    "    print(issue_type[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
